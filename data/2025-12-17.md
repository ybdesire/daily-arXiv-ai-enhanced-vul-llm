<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 27]
- [cs.CR](#cs.CR) [Total: 7]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.CE](#cs.CE) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records](https://arxiv.org/abs/2512.13700)
*Mitchell A. Klusty,Elizabeth C. Solie,Caroline N. Leach,W. Vaiden Logan,Lynnet E. Richey,John C. Gensel,David P. Szczykutowicz,Bryan C. McLellan,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 提出一个基于本地部署大型语言模型的自动化临床病历特征提取框架，用于替代耗时的手工图表审查。


<details>
  <summary>Details</summary>
Motivation: 手工图表审查耗时且资源密集，需要专家从非结构化的电子健康记录中提取复杂信息，限制了临床研究的效率。

Method: 开发了一个安全、模块化的框架，利用本地部署的大型语言模型，结合检索增强生成和结构化响应方法，在符合HIPAA标准的计算基础设施上运行。

Result: 评估显示，该框架在多个医学特征上达到高精度，与专家标注数据集相比，甚至发现了手工审查中遗漏的标注错误。

Conclusion: 该框架展示了大型语言模型系统通过自动化提取减少手工图表审查负担、提高数据捕获一致性的潜力，可加速临床研究。

Abstract: Manual chart review remains an extremely time-consuming and resource-intensive component of clinical research, requiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. We present a secure, modular framework for automated structured feature extraction from clinical notes leveraging locally deployed large language models (LLMs) on institutionally approved, Health Insurance Portability and Accountability Act (HIPPA)-compliant compute infrastructure. This system integrates retrieval augmented generation (RAG) and structured response methods of LLMs into a widely deployable and scalable container to provide feature extraction for diverse clinical domains. In evaluation, the framework achieved high accuracy across multiple medical characteristics present in large bodies of patient notes when compared against an expert-annotated dataset and identified several annotation errors missed in manual review. This framework demonstrates the potential of LLM systems to reduce the burden of manual chart review through automated extraction and increase consistency in data capture, accelerating clinical research.

</details>


### [2] [Blind Radio Mapping via Spatially Regularized Bayesian Trajectory Inference](https://arxiv.org/abs/2512.13701)
*Zheng Xing,Junting Chen*

Main category: cs.AI

TL;DR: 提出了一种无需位置标签的盲射频地图构建框架，利用MIMO-OFDM信道测量推断用户轨迹


<details>
  <summary>Details</summary>
Motivation: 传统射频地图构建方法需要大量位置标记数据，这在现实场景中成本高昂且不实用

Method: 开发了基于空间正则化的贝叶斯推理框架，联合估计信道特征、区分LOS/NLOS条件并恢复用户轨迹

Result: 在射线追踪数据集上的实验显示平均定位误差为0.68米，波束图重建误差为3.3%

Conclusion: 提出方法验证了盲射频地图构建的有效性，为智能无线应用提供了实用解决方案

Abstract: Radio maps enable intelligent wireless applications by capturing the spatial distribution of channel characteristics. However, conventional construction methods demand extensive location-labeled data, which are costly and impractical in many real-world scenarios. This paper presents a blind radio map construction framework that infers user trajectories from indoor multiple-input multiple-output (MIMO)-Orthogonal Frequency-Division Multiplexing (OFDM) channel measurements without relying on location labels. It first proves that channel state information (CSI) under non-line-of-sight (NLOS) exhibits spatial continuity under a quasi-specular environmental model, allowing the derivation of a CSI-distance metric that is proportional to the corresponding physical distance. For rectilinear trajectories in Poisson-distributed access point (AP) deployments, it is shown that the Cramer-Rao Lower Bound (CRLB) of localization error vanishes asymptotically, even under poor angular resolution. Building on these theoretical results, a spatially regularized Bayesian inference framework is developed that jointly estimates channel features, distinguishes line-of-sight (LOS)/NLOS conditions and recovers user trajectories. Experiments on a ray-tracing dataset demonstrate an average localization error of 0.68 m and a beam map reconstruction error of 3.3%, validating the effectiveness of the proposed blind mapping method.

</details>


### [3] [Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents](https://arxiv.org/abs/2512.13704)
*Doohee You,Sundeep Paul*

Main category: cs.AI

TL;DR: Adjudicator系统通过神经符号方法构建动态知识图谱和多智能体LLM架构，自动识别和纠正标签噪声，在工业应用中实现高精度数据验证


<details>
  <summary>Details</summary>
Motivation: 生产机器学习系统的性能受限于训练数据质量，在关键工业应用中，噪声标签会降低性能并损害用户信任

Method: 系统将标签噪声识别建模为神经符号任务：先构建动态知识图谱统一项目上下文，然后通过"智能体委员会"多智能体LLM架构进行标签有效性辩论投票

Result: 在AlleNoise基准的1000项平衡子集上验证，KG-informed模型达到0.99 F1分数，显著优于单LLM基线(0.48)和无KG委员会(0.59)

Conclusion: Adjudicator展示了在严格监管工业环境中生成黄金数据集的稳健可解释自动化系统，通过新颖的覆盖逻辑完美识别复杂结构化错误

Abstract: The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, a system that addresses the critical data mining challenge of automatically identifying and correcting label noise and has been validated for production deployment. Adjudicator models this as a neuro-symbolic task, first constructing a dynamic Knowledge Graph (KG) to unify item context. This KG then informs a "Council of Agents," a novel multi-agent Large Language Model architecture where specialized agents debate and vote on a label's validity. We validate our system on a 1,000-item balanced subset of the AlleNoise benchmark. Our KG-informed model achieves a 0.99 F1-score, significantly outperforming a single-LLM baseline (0.48 F1) and a non-KG council (0.59 F1). Our analysis reveals this is due to a Precision, achieved by a novel override logic that uses the KG to perfectly identify complex, structural errors (complete Recall) -- a class of errors that baselines fail to find. This result demonstrates a robust and explainable system for automated, high-precision data verification, serving as a vital proof-of-concept for generating golden datasets in strictly governed industrial environments.

</details>


### [4] [AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach](https://arxiv.org/abs/2512.13714)
*Gangesh Pathak,Prasanna Kumar*

Main category: cs.AI

TL;DR: 针对LLM在监管行业中的可靠性问题，本文提出了一种结合AI自动标注和人工验证的稳定性增强方法，通过系统性识别和修复不稳定性模式来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: LLM在高度监管行业中因不稳定性、不一致推理、幻觉和性能变异性等问题而难以应用，现有稳定化方法成本高且难以规模化。

Method: 提出了一种AI驱动的标注流程，结合自动化弱监督和基于置信度的标注，辅以人工验证，引入语义一致性、事实正确性和逻辑连贯性等稳定性特定标注类别。

Result: 开发了一个系统化的标注框架，能够持续校准模型并增强其鲁棒性，通过反馈循环提高LLM输出的稳定性。

Conclusion: 基于AI的标注流程和人类-AI协同方法可以系统地识别和修复LLM输出的不稳定性模式，通过持续校准模型和增强其鲁棒性，为高度监管行业提供更可靠、可扩展的LLM稳定性解决方案。

Abstract: LLM implementations are failing in highly regulated industries owing to instability issues, inconsistent reasoning, hallucinations and performance variability, especially in workflows. These reliability issues restrict safe use of LLM in areas that need the precision of facts and consistent behavior (Aiyappa et al., 2023). The current methods of stabilization, such as, reinforcement learning with human feedback (RLHF) and supervised fine-tuning, offer quantifiable improvements but are expensive and based on the intensive annotation of humans, thus being not easily scaled in a sustainable way (Dong et al., 2023; Retzlaff et al., 2024). This paper presents an AI-based annotation pipeline that systematically identifies, labels, and fixes for instability patterns on LLM output. Our human-AI synergy method combines the models of automated weak supervision and confidence-based annotation with the target human validation to guarantee the reliability and moral uprightness of feedback information (Cabitza et al., 2023; Jiang et al., 2023). The semantic consistency, factual correctness, and logical coherence categories of stability-specific annotation are introduced into our framework, allowing the continuous calibration of models and the enhancement of their robustness based on the feedback loops (Honovich et al., 2021; Nan et al., 2021).

</details>


### [5] [Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN](https://arxiv.org/abs/2512.13715)
*Fatemeh Lotfi,Fatemeh Afghah*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The increasing complexity of modern applications demands wireless networks capable of real time adaptability and efficient resource management. The Open Radio Access Network (O-RAN) architecture, with its RAN Intelligent Controller (RIC) modules, has emerged as a pivotal solution for dynamic resource management and network slicing. While artificial intelligence (AI) driven methods have shown promise, most approaches struggle to maintain performance under unpredictable and highly dynamic conditions. This paper proposes an adaptive Meta Hierarchical Reinforcement Learning (Meta-HRL) framework, inspired by Model Agnostic Meta Learning (MAML), to jointly optimize resource allocation and network slicing in O-RAN. The framework integrates hierarchical control with meta learning to enable both global and local adaptation: the high-level controller allocates resources across slices, while low level agents perform intra slice scheduling. The adaptive meta-update mechanism weights tasks by temporal difference error variance, improving stability and prioritizing complex network scenarios. Theoretical analysis establishes sublinear convergence and regret guarantees for the two-level learning process. Simulation results demonstrate a 19.8% improvement in network management efficiency compared with baseline RL and meta-RL approaches, along with faster adaptation and higher QoS satisfaction across eMBB, URLLC, and mMTC slices. Additional ablation and scalability studies confirm the method's robustness, achieving up to 40% faster adaptation and consistent fairness, latency, and throughput performance as network scale increases.

</details>


### [6] [Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy](https://arxiv.org/abs/2512.13725)
*Steve Nwaiwu,Nipat Jongsawat,Anucha Tungkasthan*

Main category: cs.AI

TL;DR: 研究发现INT8和NF4量化对Llama 3 8B模型的因果推理能力影响有限，其中NF4量化整体性能下降小于1%。干预性查询对精度损失最敏感，而反事实推理相对稳定。图检索增强生成技术可部分缓解量化带来的性能下降。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型部署向边缘和资源受限环境转移，量化模型成为标准配置，但精度降低对形式化因果推理的影响尚不明确。

Method: 使用3000样本分层CLadder基准系统评估量化效应，覆盖Pearl因果阶梯所有三个层级。在CRASS基准和Graph-RAG框架下进行补充实验。

Result: 量化后各层级准确率总体稳定；NF4量化性能下降小于1%；干预性查询对精度损失最敏感；图检索增强使NF4干预准确率提升1.7%。

Conclusion: 因果推理对4比特量化表现出意外鲁棒性；图结构化增强能选择性强化干预推理；现有反事实基准未能捕捉深层因果脆弱性。

Abstract: Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.

</details>


### [7] [State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models](https://arxiv.org/abs/2512.13762)
*TK Lee*

Main category: cs.AI

TL;DR: 提出了定性案例研究方法，用于审计长时交互中政策相关的行为选择性，引入'学习性无能'概念描述模型在敏感领域的功能性拒绝行为


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为通用工具部署，但长期交互可能揭示标准基准测试未捕捉的行为模式，需要审计政策相关的行为选择性

Method: 定性案例研究方法，通过86轮对话会话来分析模型在不同领域的行为差异，定义了三种响应机制（正常性能、功能性拒绝、元叙事）

Result: 模型在非敏感领域表现正常性能，在提供商或政策敏感领域重复产生功能性拒绝，元叙事角色框架叙述倾向于与敏感情境中的拒绝同时出现

Conclusion: 研究提出了基于可观察行为的交互级审计框架，并建议将学习性无能作为检查潜在对齐副作用的视角，值得在用户和模型间进一步研究

Abstract: Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-linked behavioral selectivity in long-horizon interaction. In a single 86-turn dialogue session, the same model shows Normal Performance (NP) in broad, non-sensitive domains while repeatedly producing Functional Refusal (FR) in provider- or policy-sensitive domains, yielding a consistent asymmetry between NP and FR across domains. Drawing on learned helplessness as an analogy, we introduce learned incapacity (LI) as a behavioral descriptor for this selective withholding without implying intentionality or internal mechanisms. We operationalize three response regimes (NP, FR, Meta-Narrative; MN) and show that MN role-framing narratives tend to co-occur with refusals in the same sensitive contexts. Overall, the study proposes an interaction-level auditing framework based on observable behavior and motivates LI as a lens for examining potential alignment side effects, warranting further investigation across users and models.

</details>


### [8] [Mathematics and Coding are Universal AI Benchmarks](https://arxiv.org/abs/2512.13764)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 本研究探讨了数学和编程在AI智能体心理测量电池模空间中的特殊作用，证明了在适当的条件下，数学定理证明和编程任务生成的电池子空间在评估度量下是稠密的


<details>
  <summary>Details</summary>
Motivation: 研究数学和编程作为AI智能体评估的"通用坐标"的特殊价值，以及形式化数学作为高级AI智能体递归自我改进的自然点火域

Method: 基于AAI框架和GVU动力学，定义数学纤维并分析其特性，使用形式证明核（如Lean、Coq）来验证GVU流的光谱稳定性

Result: 主要技术结果是密度定理：在智能体输出均匀紧致和Lipschitz AAI泛函的条件下，数学定理证明和编程任务生成的电池子空间在模空间中相对于评估度量是稠密的

Conclusion: 数学和编程为AI智能体评估提供了通用坐标，形式化数学是高级AI智能体递归自我改进的自然起点，编码单独具有普遍性而纯数学不具有，数学的特权是光谱性的而非表达性的

Abstract: We study the special role of mathematics and coding inside the moduli space of psychometric batteries for AI agents. Building on the AAI framework and GVU dynamics from previous works, we define the Mathematics Fiber and show that, when paired with formal proof kernels (e.g. Lean, Coq), GVU flows on this fiber admit spectrally stable self-improvement regimes due to oracle-like verification. Our main technical result is a density theorem: under uniform tightness of agent outputs and a Lipschitz AAI functional, the subspace of batteries generated by mathematical theorem-proving and coding tasks is dense in the moduli space of batteries with respect to the evaluation metric. Coding alone is universal in this sense, while pure mathematics is not; its privilege is spectral rather than expressive. We interpret this as evidence that mathematics and coding provide ``universal coordinates'' for evaluation, and that formal mathematics is a natural ignition domain for recursive self-improvement in advanced AI agents.

</details>


### [9] [Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems](https://arxiv.org/abs/2512.13771)
*Javier Marín*

Main category: cs.AI

TL;DR: 本文提出了语义接地指数(SGI)来衡量检索增强生成(RAG)系统中的幻觉现象，发现幻觉响应在几何上更接近问题而非上下文，这种判别能力随问题-上下文角度分离而增强。


<details>
  <summary>Details</summary>
Motivation: 研究RAG系统产生幻觉时在嵌入空间中留下的几何痕迹，为生产环境中的RAG部署提供有效的验证机制。

Method: 定义SGI为响应到问题与响应到上下文的角距离比值，在HaluEval数据集上验证其有效性，并进行理论和实证分析。

Result: SGI在多个嵌入模型中表现出大效应量(Cohen's d=0.92-1.28)，判别能力随问题-上下文角度分离单调增强(AUC从0.72升至0.83)，但对事实准确性判别有限(TruthfulQA AUC=0.478)。

Conclusion: SGI提供计算高效、理论基础的框架，可识别需要验证的RAG响应，但主要衡量主题参与度而非事实准确性。

Abstract: When retrieval-augmented generation (RAG) systems hallucinate, what geometric trace does this leave in embedding space? We introduce the Semantic Grounding Index (SGI), defined as the ratio of angular distances from the response to the question versus the context on the unit hypersphere $\mathbb{S}^{d-1}$.Our central finding is \emph{semantic laziness}: hallucinated responses remain angularly proximate to questions rather than departing toward retrieved contexts. On HaluEval ($n$=5,000), we observe large effect sizes (Cohen's $d$ ranging from 0.92 to 1.28) across five embedding models with mean cross-model correlation $r$=0.85. Crucially, we derive from the spherical triangle inequality that SGI's discriminative power should increase with question-context angular separation $θ(q,c)$-a theoretical prediction confirmed empirically: effect size rises monotonically from $d$=0.61 -low $θ(q,c)$, to $d$=1.27 -high $θ(q,c)$, with AUC improving from 0.72 to 0.83. Subgroup analysis reveals that SGI excels on long responses ($d$=2.05) and short questions ($d$=1.22), while remaining robust across context lengths. Calibration analysis yields ECE=0.10, indicating SGI scores can serve as probability estimates, not merely rankings. A critical negative result on TruthfulQA (AUC=0.478) establishes that angular geometry measures topical engagement rather than factual accuracy. SGI provides computationally efficient, theoretically grounded infrastructure for identifying responses that warrant verification in production RAG deployments.

</details>


### [10] [MURIM: Multidimensional Reputation-based Incentive Mechanism for Federated Learning](https://arxiv.org/abs/2512.13955)
*Sindhuja Madabushi,Dawood Wasif,Jin-Hee Cho*

Main category: cs.AI

TL;DR: 提出了MURIM机制，这是一种基于多维信誉的激励机制，用于解决联邦学习中客户端激励不足、隐私风险和资源限制等问题，通过综合考虑可靠性、隐私、资源能力和公平性来改善联邦学习效果。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护隐私，但仍面临客户端激励弱、隐私风险和资源限制等挑战，需要一种机制来评估客户端可靠性并进行公平激励分配。

Method: 提出MURIM机制，联合考虑客户端可靠性、隐私、资源能力和公平性，通过可靠性验证模块支持，基于客户端贡献、延迟和信誉分配激励。

Result: 在MNIST、FMNIST和ADULT Income数据集上的实验表明，MURIM在公平性指标上提升达18%，隐私攻击成功率降低5-9%，对投毒和噪声梯度攻击的鲁棒性提高达85%。

Conclusion: MURIM能有效缓解对抗威胁，促进公平和真实参与，并在异构和动态联邦设置中保持稳定的模型收敛。

Abstract: Federated Learning (FL) has emerged as a leading privacy-preserving machine learning paradigm, enabling participants to share model updates instead of raw data. However, FL continues to face key challenges, including weak client incentives, privacy risks, and resource constraints. Assessing client reliability is essential for fair incentive allocation and ensuring that each client's data contributes meaningfully to the global model. To this end, we propose MURIM, a MUlti-dimensional Reputation-based Incentive Mechanism that jointly considers client reliability, privacy, resource capacity, and fairness while preventing malicious or unreliable clients from earning undeserved rewards. MURIM allocates incentives based on client contribution, latency, and reputation, supported by a reliability verification module. Extensive experiments on MNIST, FMNIST, and ADULT Income datasets demonstrate that MURIM achieves up to 18% improvement in fairness metrics, reduces privacy attack success rates by 5-9%, and improves robustness against poisoning and noisy-gradient attacks by up to 85% compared to state-of-the-art baselines. Overall, MURIM effectively mitigates adversarial threats, promotes fair and truthful participation, and preserves stable model convergence across heterogeneous and dynamic federated settings.

</details>


### [11] [Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms](https://arxiv.org/abs/2512.13978)
*Yang Cao,Yubin Chen,Xuyang Guo,Zhao Song,Song Yue,Jiahao Zhang,Jiale Zhao*

Main category: cs.AI

TL;DR: 本文通过评测GPT-5-Thinking、Gemini-3-Pro、Claude-Sonnet-4.5-Thinking和Grok-4在随机算法教材中的证明生成能力，发现前沿模型虽具备辅助研究生教学的能力，但可靠性和一致性存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数学推理和科学发现方面取得进展，但缺乏对其在经典研究生数学理论中基础推理能力的严谨评估。

Method: 构建基于Motwani和Raghavan《随机算法》教材的基准测试，要求模型生成形式化的LaTeX证明，并进行定性和定量分析。

Result: 顶级模型（Gemini和Claude）准确率约66%，展现出对概率方法和形式逻辑的掌握，而其他模型一致性较低（约40%）。生成证明在简洁性、幻觉率和逻辑结构上存在差异。

Conclusion: 前沿模型已达到适用于研究生教学辅助的熟练阈值，但在严格数学推导的可靠性方面存在显著差异，需进一步优化。

Abstract: The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${ó}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95].
  We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available at https://github.com/magiclinux/math_benchmark_probability.

</details>


### [12] [ReflCtrl: Controlling LLM Reflection via Representation Engineering](https://arxiv.org/abs/2512.13979)
*Ge Yan,Chung-En Sun,Tsui-Wei,Weng*

Main category: cs.AI

TL;DR: 通过表征工程控制LLM自我反思频率，减少冗余推理成本，发现反思与内部不确定性相关。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过思维链推理在多个领域表现出色，自我反思能力提升了性能但增加了推理成本。

Method: 通过表征工程方法分割模型的推理步骤，识别反思步骤，提取控制反思行为的潜在空间方向，提出逐步调控方法ReflCtrl。

Result: 实验表明反思的可控性，证明了反思行为与模型内部不确定性的关联。

Conclusion: 研究表明反思在很多情况下是冗余的，特别是对于更强的模型，可以通过调控反思频率减少33.6%的推理标记；模型的反思行为与内部不确定性信号高度相关。

Abstract: Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model's reasoning into steps, identify the steps corresponding to reflection, and extract a reflection direction in the latent space that governs this behavior. Using this direction, we propose a stepwise steering method that can control reflection frequency. We call our framework ReflCtrl. Our experiments show that (1) in many cases reflections are redundant, especially in stronger models (in our experiments, we can save up to 33.6 percent of reasoning tokens while preserving performance), and (2) the model's reflection behavior is highly correlated with an internal uncertainty signal, implying self-reflection may be controlled by the model's uncertainty.

</details>


### [13] [Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training](https://arxiv.org/abs/2512.13996)
*Can Jin,Hongwu Peng,Mingcan Xiang,Qixin Zhang,Xiangchi Yuan,Amit Hasan,Ohiremen Dibua,Yifan Gong,Yan Kang,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: 这篇论文提出了DTop-p MoE，一种动态Top-p路由机制，通过PI控制器动态调整概率阈值来控制稀疏度，在保持计算预算的同时适应不同token的难度差异。


<details>
  <summary>Details</summary>
Motivation: 标准的Top-k路由对所有token施加统一的稀疏模式，忽视了token难度的差异性。而现有的Top-p路由使用固定的全局概率阈值，导致计算成本不可控且对超参数敏感。

Method: 使用比例积分(PI)控制器动态调整概率阈值，使激活专家的稀疏度与指定目标对齐；引入动态路由归一化机制，使不同层能学习不同的专家选择模式。

Result: 在大型语言模型和扩散变换器上的实验表明，DTop-p consistently优于Top-k和固定阈值Top-p基线，能精确控制激活专家数量并自适应分配资源。

Conclusion: DTop-p MoE提供了一个可扩展的、鲁棒的稀疏专家混合框架，在各种规模下都表现出良好的扩展性，适用于大规模预训练。

Abstract: Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.

</details>


### [14] [MobileWorldBench: Towards Semantic World Modeling For Mobile Agents](https://arxiv.org/abs/2512.14014)
*Shufan Li,Konstantinos Kallidromitis,Akash Gokul,Yusuke Kato,Kazuki Kozuka,Aditya Grover*

Main category: cs.AI

TL;DR: Proposes semantic world models for GUI agents using natural language state transitions instead of pixel prediction, introduces MobileWorldBench benchmark and MobileWorld dataset, shows improved task performance for mobile agents.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of pixel-space world models in GUI settings, where predicting complex visual elements is difficult. The authors aim to explore an alternative approach using natural language descriptions for state transitions.

Method: The method involves: 1) Creating MobileWorldBench benchmark for evaluating VLMs as world models; 2) Releasing MobileWorld dataset with 1.4M samples; 3) Proposing a novel framework that integrates VLM world models into mobile agent planning.

Result: The results demonstrate that semantic world models can significantly improve the world modeling capabilities of VLMs and directly benefit mobile agents by enhancing task success rates.

Conclusion: The paper concludes that semantic world models using natural language descriptions can effectively replace pixel-space world models for GUI agents, as demonstrated by improved task success rates in mobile agent planning frameworks.

Abstract: World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld

</details>


### [15] [Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation](https://arxiv.org/abs/2512.14048)
*Shen Li,Li Huang,Shaoxiong Zhan,Weifeng Sun,Tao Yin,Zhongxin Liu,Meng Yan*

Main category: cs.AI

TL;DR: 路由框架RoutingGen结合难度感知动态选择prompt策略，在代码生成中平衡性能和效率


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法存在两个局限性：一是对所有任务统一应用导致简单任务过度思考；二是缺乏代码生成中的意图抽象，未能明确建模核心算法设计和效率

Method: 提出RoutingGen框架，简单任务采用few-shot prompting，复杂任务采用创新的Intention Chain-of-Thought（ICoT）策略来捕获任务意图

Result: 在3个模型和6个代码生成基准测试中达到SOTA性能，平均减少46.37%的token使用量

Conclusion: ICoT在挑战性基准上优于六个现有prompting基线，RoutingGen有效平衡了代码生成的性能和效率

Abstract: Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.

</details>


### [16] [OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value](https://arxiv.org/abs/2512.14051)
*Mengzhang Cai,Xin Gao,Yu Li,Honglin Lin,Zheng Liu,Zhuoshi Pan,Qizhi Pei,Xiaoran Shang,Mengyuan Sun,Zinan Tang,Xiaoyang Wang,Zhanping Zhong,Yun Zhu,Dahua Lin,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: OpenDataArena是一个开源平台，通过统一评估管道、多维度评分、数据谱系可视化和开源工具包，解决了LLM后训练数据的黑箱问题，揭示了数据质量与模型性能的关系，推动数据为中心AI的发展。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速发展依赖于后训练数据集的质量和多样性，但存在关键分歧：模型经过严格基准测试，而数据本身却是一个黑箱——具有不透明的组成、不确定的来源和缺乏系统评估。这种不透明性阻碍了可重复性，并模糊了数据特征与模型行为之间的因果关系。

Method: OpenDataArena（ODA）平台包含四个核心组成部分：（i）统一的训练-评估管道，确保不同模型（如Llama、Qwen）和领域的公平开放比较；（ii）多维度评分框架，从数十个不同维度分析数据质量；（iii）交互式数据谱系探索器，可视化数据集谱系并分析组件来源；（iv）完全开源的训练、评估和评分工具包。实验覆盖120多个训练数据集、22个基准测试，通过600多次训练运行和4000万处理数据点进行验证。

Result: 在ODA上进行的广泛实验揭示了非平凡的见解：分析发现了数据复杂性与任务性能之间的内在权衡，通过谱系追踪识别了流行基准中的冗余，并绘制了数据集间的谱系关系。所有结果、工具和配置都已发布，以民主化高质量数据评估的访问。

Conclusion: ODA通过建立全面的数据评估生态系统，揭示了数据复杂性与任务性能之间的权衡关系，识别了流行基准中的冗余，并绘制了数据集间的谱系关系。该平台旨在推动从试错式数据管理向数据为中心AI的原则性科学转变，为数据混合规律和基础模型战略组合的严格研究铺平道路。

Abstract: The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.

</details>


### [17] [RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees](https://arxiv.org/abs/2512.14069)
*Junjie Ma,Jinlong Li*

Main category: cs.AI

TL;DR: RADAR：基于强化学习的动态推测采样方法，通过动态决策减少冗余计算，实现3.17-4.82倍推理加速


<details>
  <summary>Details</summary>
Motivation: 现有推测采样方法中草稿模型的调用次数是预设超参数，缺乏灵活性，无法有效生成和利用候选token

Method: 将草稿树生成过程建模为马尔可夫决策过程，使用离线强化学习训练预测模型，实现草稿模型调用的实时决策

Result: 在三个大型语言模型和四个任务上的评估显示，RADAR相比自回归解码基线实现了3.17-4.82倍的加速

Conclusion: RADAR通过动态决策机制有效减少了冗余计算，显著加速了LLM推理过程

Abstract: Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.

</details>


### [18] [Grammar Search for Multi-Agent Systems](https://arxiv.org/abs/2512.14079)
*Mayank Singh,Vikas Yadav,Shiva Krishna Reddy Malay,Shravan Nayak,Sai Rajeswar,Sathwik Tejaswi Madhusudhan,Eduardo Blanco*

Main category: cs.AI

TL;DR: 提出了一个基于固定可组合组件的结构化框架，在多智能体系统自动搜索中超越了现有方法，同时在成本和可解释性方面具有优势


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统搜索方法主要依赖LLM在代码空间中的自由形式搜索，缺乏结构化

Method: 使用固定集合的简单可组合组件来探索搜索空间，而不是依赖LLM的生成灵活性

Result: 在数学和问答两个领域的五个基准测试中，有四个超越了先前的方法

Conclusion: 该方法不仅性能更优，而且搜索过程更经济，生成的多智能体系统更具模块化、可解释性和简单逻辑

Abstract: Automatic search for Multi-Agent Systems has recently emerged as a key focus in agentic AI research. Several prior approaches have relied on LLM-based free-form search over the code space. In this work, we propose a more structured framework that explores the same space through a fixed set of simple, composable components. We show that, despite lacking the generative flexibility of LLMs during the candidate generation stage, our method outperforms prior approaches on four out of five benchmarks across two domains: mathematics and question answering. Furthermore, our method offers additional advantages, including a more cost-efficient search process and the generation of modular, interpretable multi-agent systems with simpler logic.

</details>


### [19] [Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis](https://arxiv.org/abs/2512.14157)
*Yankai Jiang,Yujie Zhang,Peng Zhang,Yichen Li,Jintai Chen,Xiaoming Shi,Shihui Zhen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely "think with images" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.

</details>


### [20] [Georeferencing complex relative locality descriptions with large language models](https://arxiv.org/abs/2512.14228)
*Aneesha Fernando,Surangika Ranathunga,Kristin Stock,Raj Prasanna,Christopher B. Jones*

Main category: cs.AI

TL;DR: 本文探讨了使用大语言模型（LLMs）自动进行复杂地理定位描述的地理编码，尤其在生物多样性标本记录领域取得了优于现有基准的成果。


<details>
  <summary>Details</summary>
Motivation: 传统地理编码方法依赖于地名库或语言模型，但对于包含空间关系的相对位置描述效果不佳，而生物标本记录中大量存在这类复杂描述，需要自动化解决方案。

Method: 首先确定了有效的提示模式，然后使用量化低秩自适应（QLoRA）方法在不同地区和语言的生物多样性数据集上对大语言模型进行微调。

Result: 方法在多个数据集上平均65%的记录定位在10公里半径内，最佳结果（纽约州）达到85%在10公里内和67%在1公里内。

Conclusion: 研究表明大语言模型特别擅长处理冗长复杂的地理描述，在复杂地理位置描述的地理编码方面具有巨大潜力。

Abstract: Georeferencing text documents has typically relied on either gazetteer-based methods to assign geographic coordinates to place names, or on language modelling approaches that associate textual terms with geographic locations. However, many location descriptions specify positions relatively with spatial relationships, making geocoding based solely on place names or geo-indicative words inaccurate. This issue frequently arises in biological specimen collection records, where locations are often described through narratives rather than coordinates if they pre-date GPS. Accurate georeferencing is vital for biodiversity studies, yet the process remains labour-intensive, leading to a demand for automated georeferencing solutions. This paper explores the potential of Large Language Models (LLMs) to georeference complex locality descriptions automatically, focusing on the biodiversity collections domain. We first identified effective prompting patterns, then fine-tuned an LLM using Quantized Low-Rank Adaptation (QLoRA) on biodiversity datasets from multiple regions and languages. Our approach outperforms existing baselines with an average, across datasets, of 65% of records within a 10 km radius, for a fixed amount of training data. The best results (New York state) were 85% within 10km and 67% within 1km. The selected LLM performs well for lengthy, complex descriptions, highlighting its potential for georeferencing intricate locality descriptions.

</details>


### [21] [Gödel's Poetry](https://arxiv.org/abs/2512.14252)
*Kelly J. Davis*

Main category: cs.AI

TL;DR: 提出基于语言模型和递归分解的自动定理证明新方法，在Lean4中实现90.4%的miniF2F通过率


<details>
  <summary>Details</summary>
Motivation: 自动化定理证明一直是人工智能面临的挑战，需要更有效的证明生成方法

Method: 使用专用语言模型生成Lean4证明，结合递归分解将复杂定理分解为简单命题，通过多智能体架构协调自动形式化、证明生成和分解过程

Result: 在未分解情况下达到90.4%的miniF2F通过率，分解后性能显著提升

Conclusion: 该方法为自动定理证明提供了新途径，关键技术创新是扩展Kimina Lean Server的AST解析能力以支持递归证明分解，系统已开源供进一步开发

Abstract: Formal, automated theorem proving has long been viewed as a challenge to artificial intelligence. We introduce here a new approach to computer theorem proving, one that employs specialized language models for Lean4 proof generation combined with recursive decomposition of difficult theorems into simpler entailing propositions. These models are coordinated through a multi-agent architecture that orchestrates autoformalization (if required), proof generation, decomposition of difficult theorems into simpler entailing propositions, and recursive proof (and/or decomposition) of these propositions. Without decomposition, we achieve a 90.4% pass rate on miniF2F. With decomposition, this is significantly improved. A key technical contribution lies in our extension of the Kimina Lean Server with abstract syntax tree (AST) parsing capabilities to facilitate automated, recursive proof decomposition. The system is made available on PyPI as goedels-poetry (at https://pypi.org/project/goedels-poetry ), and the open-source implementation KellyJDavis/goedels-poetry (at https://github.com/KellyJDavis/goedels-poetry ) facilitates both adaptation to alternative language models and extension with custom functionality.

</details>


### [22] [Leveraging LLMs for Collaborative Ontology Engineering in Parkinson Disease Monitoring and Alerting](https://arxiv.org/abs/2512.14288)
*Georgios Bouchouras,Dimitrios Doumanas,Andreas Soularidis,Konstantinos Kotis,George A. Vouros*

Main category: cs.AI

TL;DR: 本研究探索了大型语言模型在帕金森病监测与预警本体构建中的应用，比较了四种方法（OS、CoT、X-HCOME、SimX-HCOME+），重点评估了纯LLM构建与人类-LLM协作的效果。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM是否能够独立构建完整的本体，以及人类-LLM协作是否能提升本体工程的质量。

Method: 采用四种方法：一次性提示（OS）、思维链提示（CoT）、混合本体工程方法X-HCOME（结合人类专业知识与LLM能力）、以及强调持续人工监督的迭代优化方法SimX-HCOME+。

Result: 纯LLM方法（OS和CoT）能自主构建PD监测预警本体，但不够完整需人工改进；X-HCOME显著提升本体全面性，接近专家水平；SimX-HCOME+通过持续人工参与生成更准确完整的本体。

Conclusion: 人类-LLM协作在本体工程中具有巨大潜力，特别是在复杂领域如帕金森病；未来可研发专用GPT模型用于本体构建。

Abstract: This paper explores the integration of Large Language Models (LLMs) in the engineering of a Parkinson's Disease (PD) monitoring and alerting ontology through four key methodologies: One Shot (OS) prompt techniques, Chain of Thought (CoT) prompts, X-HCOME, and SimX-HCOME+. The primary objective is to determine whether LLMs alone can create comprehensive ontologies and, if not, whether human-LLM collaboration can achieve this goal. Consequently, the paper assesses the effectiveness of LLMs in automated ontology development and the enhancement achieved through human-LLM collaboration.
  Initial ontology generation was performed using One Shot (OS) and Chain of Thought (CoT) prompts, demonstrating the capability of LLMs to autonomously construct ontologies for PD monitoring and alerting. However, these outputs were not comprehensive and required substantial human refinement to enhance their completeness and accuracy.
  X-HCOME, a hybrid ontology engineering approach that combines human expertise with LLM capabilities, showed significant improvements in ontology comprehensiveness. This methodology resulted in ontologies that are very similar to those constructed by experts.
  Further experimentation with SimX-HCOME+, another hybrid methodology emphasizing continuous human supervision and iterative refinement, highlighted the importance of ongoing human involvement. This approach led to the creation of more comprehensive and accurate ontologies.
  Overall, the paper underscores the potential of human-LLM collaboration in advancing ontology engineering, particularly in complex domains like PD. The results suggest promising directions for future research, including the development of specialized GPT models for ontology construction.

</details>


### [23] [Massive Editing for Large Language Models Based on Dynamic Weight Generation](https://arxiv.org/abs/2512.14395)
*Wentao Wan,Qiqing Lao,Zhiwei Xie,Hefeng Wu,Runnan Lin,Liang Lin,Keze Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于动态权重生成的大规模LLMs知识编辑方法MeG，通过在特定层添加动态权重神经元并使用扩散模型生成权重，实现了大规模知识编辑的显著性能提升


<details>
  <summary>Details</summary>
Motivation: 当前在LLMs上进行大规模编辑时，确保编辑的可靠性、泛化性和局部性指标仍面临挑战，需要低成本的知识编辑解决方案

Method: MeG方法在LLMs特定层附加动态权重神经元，使用扩散模型根据不同查询条件生成神经元权重，仅通过添加单个动态权重神经元实现大规模知识编辑

Result: 实验表明MeG在可靠性、泛化性和局部性指标上显著优于现有知识编辑方法，特别是在局部性指标上获得了高百分点的绝对值提升

Conclusion: MeG方法为大规��知识编辑提供了一种有效的解决方案，证明了基于动态权重生成的方法在知识编辑任务中的优势

Abstract: Knowledge Editing (KE) is a field that studies how to modify some knowledge in Large Language Models (LLMs) at a low cost (compared to pre-training). Currently, performing large-scale edits on LLMs while ensuring the Reliability, Generality, and Locality metrics of the edits remain a challenge. This paper proposes a Massive editing approach for LLMs based on dynamic weight Generation (MeG). Our MeG involves attaching a dynamic weight neuron to specific layers of the LLMs and using a diffusion model to conditionally generate the weights of this neuron based on the input query required for the knowledge. This allows the use of adding a single dynamic weight neuron to achieve the goal of large-scale knowledge editing. Experiments show that our MeG can significantly improve the performance of large-scale KE in terms of Reliability, Generality, and Locality metrics compared to existing knowledge editing methods, particularly with a high percentage point increase in the absolute value index for the Locality metric, demonstrating the advantages of our proposed method.

</details>


### [24] [PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals](https://arxiv.org/abs/2512.14417)
*Jia Hu,Junqi Li,Weimeng Lin,Peng Jia,Yuxiong Ji,Jintao Lai*

Main category: cs.AI

TL;DR: PortAgent：基于LLM的自动化车辆调度系统移植方案，无需港口专家、少量数据需求、快速部署


<details>
  <summary>Details</summary>
Motivation: 当前车辆调度系统在跨码头移植中存在三大限制：高度依赖港口运营专家、需要大量特定码头数据、耗时的人工部署流程

Method: 通过虚拟专家团队（VET）模拟人类专家工作流，包括知识检索器、建模器、编码器和调试器，采用few-shot示例学习和RAG机制获取领域知识

Result: 开发出PortAgent系统，能够自动化完成VDS移植工作流程，建立了基于LLM Reflexion框架的自校正循环

Conclusion: 该方法解决了VDS跨码头移植的核心挑战，为实现ACTs车辆调度系统的商业化推广提供了可行方案

Abstract: Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created

</details>


### [25] [Context-Picker: Dynamic context selection using multi-stage reinforcement learning](https://arxiv.org/abs/2512.14465)
*Siyuan Zhu,Chengdong Xu,Kaiqiang Ke,Chao Yu*

Main category: cs.AI

TL;DR: 上下文选取器：通过强化学习优化上下文选择的推理感知框架


<details>
  <summary>Details</summary>
Motivation: 长文本QA中传统方法难以确定最优上下文数量，过多引入噪声，过少丢失关键信息

Method: 两阶段强化学习：召回导向阶段覆盖推理链，精确导向阶段修剪冗余

Result: 在5个长上下文和多跳QA基准上显著优于RAG基线

Conclusion: 粗到细优化策略结合冗余感知奖励设计和原理指导格式可有效提升长文本QA性能

Abstract: In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines "minimal sufficient sets" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.

</details>


### [26] [Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence](https://arxiv.org/abs/2512.14527)
*Shreyas Subramanian,Bala Krishnamoorthy,Pranav Murthy*

Main category: cs.AI

TL;DR: 提出了一种名为GreedyLR的新型自适应学习率调度器，通过当前损失动态调整学习率，在多个NLP、CV和LLM任务上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管训练优化器取得了显著进展，但大多数研究工作仍然使用常见的调度器选择（如Cosine或指数衰减），缺乏更智能的自适应能力。

Method: 设计了GreedyLR调度器，根据当前损失自适应调整学习率；在多个NLP、CV和LLM任务（参数高达70亿）上进行了实验，包括微调和预训练实验；提供了理论分析，包括收敛性证明和最优缩放因子F的推导。

Result: GreedyLR在准确性、速度和收敛性方面均优于多种先进的调度器；实验表明该算法对真实噪声环境具有鲁棒性。

Conclusion: GreedyLR易于实现、计算高效，可以作为训练的良好默认调度器。

Abstract: Despite significant advances in optimizers for training, most research works use common scheduler choices like Cosine or exponential decay. In this paper, we study \emph{GreedyLR}, a novel scheduler that adaptively adjusts the learning rate during training based on the current loss. To validate the effectiveness of our proposed scheduler, we conduct experiments on several NLP, CV, and LLM tasks with up to $7B$ parameters, including both fine-tuning and pre-training experiments. The results show that our approach outperforms several state-of-the-art schedulers in terms of accuracy, speed, and convergence. We also provide a theoretical analysis of the GreedyLR algorithm, including a proof of convergence and derivation of the optimal scaling factor $F$ that maximizes the convergence rate, along with experiments to show robustness of the algorithm to realistic noisy landscapes. Our scheduler is easy to implement, computationally efficient, and could be considered a good default scheduler for training.

</details>


### [27] [Universal Reasoning Model](https://arxiv.org/abs/2512.14693)
*Zitian Gao,Lynx Chen,Yihao Xiao,He Xing,Ran Tao,Haoming Luo,Joey Zhou,Bryan Dai*

Main category: cs.AI

TL;DR: 提出Universal Reasoning Model (URM)，通过短卷积和截断反向传播增强Universal Transformers，在ARC-AGI推理任务上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: Universal Transformers在复杂推理任务中表现出色，但其性能提升的具体来源尚不明确。研究发现改进主要来自循环归纳偏置和Transformer的非线性组件，而非复杂架构设计

Method: 系统分析UT变体，提出URM模型：在UT基础上引入短卷积和截断反向传播技术

Result: 在ARC-AGI 1上达到53.8% pass@1，在ARC-AGI 2上达到16.0% pass@1，显著提升推理性能

Conclusion: 通过简单有效的架构改进（短卷积+截断反向传播）可以显著增强Transformer的推理能力，验证了循环偏置和非线性组件的重要性

Abstract: Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [28] [Safe2Harm: Semantic Isomorphism Attacks for Jailbreaking Large Language Models](https://arxiv.org/abs/2512.13703)
*Fan Yang*

Main category: cs.CR

TL;DR: 论文提出Safe2Harm语义同构攻击方法，通过将有害问题重写为语义安全的类似问题，利用主题映射关系反向生成有害内容，在7个主流LLM上验证了优越的越狱能力。


<details>
  <summary>Details</summary>
Motivation: 现有越狱方法多基于提示工程或对抗优化，但忽略了有害场景与合法场景在底层原理上的高度一致性。

Method: 通过四个阶段实现越狱：1) 有害问题安全化重写 2) 提取主题映射关系 3) LLM生成安全响应 4) 基于映射反向生成有害输出。

Result: 在7个主流LLMs和三类基准数据集上的实验表明，Safe2Harm具有强越狱能力，整体性能优于现有方法。

Conclusion: 该方法揭示了LLM安全漏洞的新维度，同时构建了含358个样本的有害内容评估数据集，可用于输入输出过滤防御。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across various tasks, but their security vulnerabilities can be exploited by attackers to generate harmful content, causing adverse impacts across various societal domains. Most existing jailbreak methods revolve around Prompt Engineering or adversarial optimization, yet we identify a previously overlooked phenomenon: many harmful scenarios are highly consistent with legitimate ones in terms of underlying principles. Based on this finding, this paper proposes the Safe2Harm Semantic Isomorphism Attack method, which achieves efficient jailbreaking through four stages: first, rewrite the harmful question into a semantically safe question with similar underlying principles; second, extract the thematic mapping relationship between the two; third, let the LLM generate a detailed response targeting the safe question; finally, reversely rewrite the safe response based on the thematic mapping relationship to obtain harmful output. Experiments on 7 mainstream LLMs and three types of benchmark datasets show that Safe2Harm exhibits strong jailbreaking capability, and its overall performance is superior to existing methods. Additionally, we construct a challenging harmful content evaluation dataset containing 358 samples and evaluate the effectiveness of existing harmful detection methods, which can be deployed for LLM input-output filtering to enable defense.

</details>


### [29] [Smart Surveillance: Identifying IoT Device Behaviours using ML-Powered Traffic Analysis](https://arxiv.org/abs/2512.13709)
*Reza Ryan,Napoleon Paciente,Cahil Youngs,Nickson Karie,Qian Li,Nasim Ferdosian*

Main category: cs.CR

TL;DR: 本文提出一種基於機器學習的物聯網設備分類方法，使用隨機森林演算法能達到91%的準確率來識別設備類型和行為。


<details>
  <summary>Details</summary>
Motivation: 現有的物聯網設備識別方法主要局限於本地網絡，無法有效進行外部監控和分類，因此需要開發新的外部監測技術。

Method: 建立包含多種物聯網設備的測試平台，使用隨機森林(RF)、多層感知器(MLP)和K近鄰(KNN)三種機器學習演算法，透過網絡流量分析進行設備分類。

Result: RF分類器獲得最高91%的準確率，MLP最低為56%。除了安全攝像頭的某些行為外，所有設備類別都能成功分類。

Conclusion: 機器學習方法在物聯網設備識別方面具有可行性，但仍需改進對特定設備行為的識別能力。

Abstract: The proliferation of Internet of Things (IoT) devices has grown exponentially in recent years, introducing significant security challenges. Accurate identification of the types of IoT devices and their associated actions through network traffic analysis is essential to mitigate potential threats. By monitoring and analysing packet flows between IoT devices and connected networks, anomalous or malicious behaviours can be detected. Existing research focuses primarily on device identification within local networks using methods such as protocol fingerprinting and wireless frequency scanning. However, these approaches are limited in their ability to monitor or classify IoT devices externally. To address this gap, we investigate the use of machine learning (ML) techniques, specifically Random Forest (RF), Multilayer Perceptron (MLP), and K-Nearest Neighbours (KNN), in conjunction with targeted network traffic monitoring to classify IoT device types and their actions. We constructed a testbed comprising an NPAT-enabled router and a diverse set of IoT devices, including smart cameras, controller hubs, home appliances, power controllers, and streaming devices. Experimental results demonstrate that IoT device and action recognition is feasible using our proposed ML-driven approach, with the RF classifier achieving the highest accuracy of 91%, while the MLP recorded the lowest accuracy at 56%. Notably, all device categories were successfully classified except for certain actions associated with security cameras, underscoring both the potential and the limitations of the proposed method.

</details>


### [30] [A Deep Dive into Function Inlining and its Security Implications for ML-based Binary Analysis](https://arxiv.org/abs/2512.14045)
*Omar Abusabha,Jiyong Uhm,Tamer Abuhmed,Hyungjoon Koo*

Main category: cs.CR

TL;DR: 本文首次全面研究函数内联（function inlining）对基于机器学习的二进制分析安全性的影响，通过极端内联（extreme inlining）策略，发现这一看似良性的编译器优化可能被利用来规避ML模型。


<details>
  <summary>Details</summary>
Motivation: 函数内联优化广泛用于现代编译器以提升性能，但会显著改变二进制代码的静态特征（如机器指令和控制流图），而这些特征对二进制分析至关重要。然而，函数内联的安全影响至今尚未得到充分探索。

Method: 研究团队剖析了LLVM成本模型中的内联决策流程，探索了超越标准优化级别的编译器选项组合（称为极端内lining），并在5个ML辅助的二进制安全分析任务上，使用20个独特模型系统评估其在极端内lining场景下的鲁棒性。

Result: 广泛实验表明：i) 函数内联可能（直接或间接）影响ML模型行为，可能被利用来规避判别式或生成式ML模型；ii) 依赖静态特征的ML模型对内联高度敏感；iii) 细微的编译器设置可被用来刻意构造规避性二进制变体；iv) 内联比率在不同应用和构建配置中差异很大，破坏了ML模型训练和评估中一致性假设。

Conclusion: 函数内联作为一种常见的编译器优化，对基于ML的二进制分析安全构成潜在威胁，提示需要更鲁棒的ML模型设计和编译器安全意识的提升。

Abstract: A function inlining optimization is a widely used transformation in modern compilers, which replaces a call site with the callee's body in need. While this transformation improves performance, it significantly impacts static features such as machine instructions and control flow graphs, which are crucial to binary analysis. Yet, despite its broad impact, the security impact of function inlining remains underexplored to date. In this paper, we present the first comprehensive study of function inlining through the lens of machine learning-based binary analysis. To this end, we dissect the inlining decision pipeline within the LLVM's cost model and explore the combinations of the compiler options that aggressively promote the function inlining ratio beyond standard optimization levels, which we term extreme inlining. We focus on five ML-assisted binary analysis tasks for security, using 20 unique models to systematically evaluate their robustness under extreme inlining scenarios. Our extensive experiments reveal several significant findings: i) function inlining, though a benign transformation in intent, can (in)directly affect ML model behaviors, being potentially exploited by evading discriminative or generative ML models; ii) ML models relying on static features can be highly sensitive to inlining; iii) subtle compiler settings can be leveraged to deliberately craft evasive binary variants; and iv) inlining ratios vary substantially across applications and build configurations, undermining assumptions of consistency in training and evaluation of ML models.

</details>


### [31] [Lost in the Pages: WebAssembly Code Recovery through SEV-SNP's Exposed Address Space](https://arxiv.org/abs/2512.14376)
*Markus Berthilsson,Christian Gehrmann*

Main category: cs.CR

TL;DR: 提出了一种新的WebAssembly代码保密性攻击方法，利用TEEs中的地址空间信息泄露，能够高可靠性地提取超过70%的代码内容


<details>
  <summary>Details</summary>
Motivation: WebAssembly(Wasm)作为一种跨平台计算技术广泛应用，但研究发现Wasm二进制文件比原生二进制文件更容易受到代码保密性攻击。之前的研究仅针对Intel SGX，本文进一步探索TEEs中的安全漏洞

Method: 开发了一种新的Wasm代码保密性攻击方法，通过利用TEEs中暴露的地址空间信息，结合额外的侧信道攻击

Result: 攻击方法能够在大多数情况下以高可靠性获取超过70%的代码，相比之前Intel SGX单步执行只能获得50%代码有明显提升

Conclusion: 该研究揭示了TEEs中存在的安全风险，强调需要对Wasm在安全敏感环境中的使用进行更严格的安全评估和防护措施

Abstract: WebAssembly (Wasm) has risen as a widely used technology to distribute computing workloads on different platforms. The platform independence offered through Wasm makes it an attractive solution for many different applications that can run on disparate infrastructures. In addition, Trusted Execution Environments (TEEs) are offered in many computing infrastructures, which allows also running security sensitive Wasm workloads independent of the specific platforms offered. However, recent work has shown that Wasm binaries are more sensitive to code confidentiality attacks than native binaries. The previous result was obtained for Intel SGX only. In this paper, we take this one step further, introducing a new Wasm code-confidentiality attack that exploits exposed address-space information in TEEs. Our attack enables the extraction of crucial execution features which, when combined with additional side channels, allows us to with high reliability obtain more than 70% of the code in most cases. This is a considerably larger amount than was previously obtained by single stepping Intel SGX where only upwards to 50% of the code could be obtained.

</details>


### [32] [VICTOR: Dataset Copyright Auditing in Video Recognition Systems](https://arxiv.org/abs/2512.14439)
*Quan Yuan,Zhikun Zhang,Linkang Du,Min Chen,Mingyang Sun,Yunjun Gao,Shibo He,Jiming Chen*

Main category: cs.CR

TL;DR: VICTOR是首个针对视频识别系统的数据集版权审计方法，通过修改少量样本(如1%)来增强目标模型的输出差异，从而检测未授权使用


<details>
  <summary>Details</summary>
Motivation: 视频识别系统广泛应用但缺乏有效的版权保护方案，现有方法主要针对图像领域，视频数据的时序复杂性使得版权审计面临挑战

Method: 提出通用的隐蔽样本修改策略，通过修改少量样本放大对目标模型预测行为的影响，利用修改样本和原始样本的模型行为差异作为审计依据

Result: 在多个模型和数据集上的实验证明了VICTOR的优越性，且对各种训练视频和目标模型的扰动机制具有鲁棒性

Conclusion: VICTOR为视频领域的数据集版权保护提供了首个有效解决方案，解决了时序数据带来的独特挑战

Abstract: Video recognition systems are increasingly being deployed in daily life, such as content recommendation and security monitoring. To enhance video recognition development, many institutions have released high-quality public datasets with open-source licenses for training advanced models. At the same time, these datasets are also susceptible to misuse and infringement. Dataset copyright auditing is an effective solution to identify such unauthorized use. However, existing dataset copyright solutions primarily focus on the image domain; the complex nature of video data leaves dataset copyright auditing in the video domain unexplored. Specifically, video data introduces an additional temporal dimension, which poses significant challenges to the effectiveness and stealthiness of existing methods.
  In this paper, we propose VICTOR, the first dataset copyright auditing approach for video recognition systems. We develop a general and stealthy sample modification strategy that enhances the output discrepancy of the target model. By modifying only a small proportion of samples (e.g., 1%), VICTOR amplifies the impact of published modified samples on the prediction behavior of the target models. Then, the difference in the model's behavior for published modified and unpublished original samples can serve as a key basis for dataset auditing. Extensive experiments on multiple models and datasets highlight the superiority of VICTOR. Finally, we show that VICTOR is robust in the presence of several perturbation mechanisms to the training videos or the target models.

</details>


### [33] [PrivATE: Differentially Private Average Treatment Effect Estimation for Observational Data](https://arxiv.org/abs/2512.14557)
*Quan Yuan,Xiaochen Li,Linkang Du,Min Chen,Mingyang Sun,Yunjun Gao,Shibo He,Jiming Chen,Zhikun Zhang*

Main category: cs.CR

TL;DR: PrivATE框架：一个实用的差异化隐私ATE估计框架，提供两级隐私保护（标签级和样本级），通过自适应匹配限制平衡噪声误差和匹配误差


<details>
  <summary>Details</summary>
Motivation: 从观察数据中估计因果效应（特别是平均处理效应ATE）存在显著隐私风险，现有差异化隐私ATE估计方法依赖特定假设、提供有限保护或缺乏全面信息保护

Method: 设计PrivATE框架，提供标签级和样本级两级隐私保护，通过推导自适应匹配限制来平衡噪声引起的误差和匹配误差

Result: 在所有数据集和隐私预算下，PrivATE都优于基线方法

Conclusion: PrivATE框架为不同隐私需求场景提供了有效的差异化隐私ATE估计解决方案，在保护用户隐私的同时实现了准确的因果效应估计

Abstract: Causal inference plays a crucial role in scientific research across multiple disciplines. Estimating causal effects, particularly the average treatment effect (ATE), from observational data has garnered significant attention. However, computing the ATE from real-world observational data poses substantial privacy risks to users. Differential privacy, which offers strict theoretical guarantees, has emerged as a standard approach for privacy-preserving data analysis. However, existing differentially private ATE estimation works rely on specific assumptions, provide limited privacy protection, or fail to offer comprehensive information protection.
  To this end, we introduce PrivATE, a practical ATE estimation framework that ensures differential privacy. In fact, various scenarios require varying levels of privacy protection. For example, only test scores are generally sensitive information in education evaluation, while all types of medical record data are usually private. To accommodate different privacy requirements, we design two levels (i.e., label-level and sample-level) of privacy protection in PrivATE. By deriving an adaptive matching limit, PrivATE effectively balances noise-induced error and matching error, leading to a more accurate estimate of ATE. Our evaluation validates the effectiveness of PrivATE. PrivATE outperforms the baselines on all datasets and privacy budgets.

</details>


### [34] [PerProb: Indirectly Evaluating Memorization in Large Language Models](https://arxiv.org/abs/2512.14600)
*Yihan Liao,Jacky Keung,Xiaoxue Ma,Jingyu Zhang,Yicheng Sun*

Main category: cs.CR

TL;DR: 提出了PerProb框架，间接评估LLMs的记忆化漏洞及隐私风险


<details>
  <summary>Details</summary>
Motivation: LLMs训练数据可能包含敏感信息，成员推理攻击威胁隐私但缺乏标准化评估方法

Method: 基于困惑度和平均对数概率变化来间接估计训练导致的记忆，无需成员标签或模型内部访问

Result: 揭示了不同LLMs的记忆行为和隐私风险差异，评估了多种缓解策略的有效性

Conclusion: 提供了一个实用且可推广的框架来评估和改进LLM隐私

Abstract: The rapid advancement of Large Language Models (LLMs) has been driven by extensive datasets that may contain sensitive information, raising serious privacy concerns. One notable threat is the Membership Inference Attack (MIA), where adversaries infer whether a specific sample was used in model training. However, the true impact of MIA on LLMs remains unclear due to inconsistent findings and the lack of standardized evaluation methods, further complicated by the undisclosed nature of many LLM training sets. To address these limitations, we propose PerProb, a unified, label-free framework for indirectly assessing LLM memorization vulnerabilities. PerProb evaluates changes in perplexity and average log probability between data generated by victim and adversary models, enabling an indirect estimation of training-induced memory. Compared with prior MIA methods that rely on member/non-member labels or internal access, PerProb is independent of model and task, and applicable in both black-box and white-box settings. Through a systematic classification of MIA into four attack patterns, we evaluate PerProb's effectiveness across five datasets, revealing varying memory behaviors and privacy risks among LLMs. Additionally, we assess mitigation strategies, including knowledge distillation, early stopping, and differential privacy, demonstrating their effectiveness in reducing data leakage. Our findings offer a practical and generalizable framework for evaluating and improving LLM privacy.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [35] [Practitioner Insights on Fairness Requirements in the AI Development Life Cycle: An Interview Study](https://arxiv.org/abs/2512.13830)
*Chaima Boufaied,Thanh Nguyen,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本文通过对23个国家26位从业者的访谈，从软件工程视角研究AI公平性需求，发现从业者虽认识到公平性维度但实践不一致，常被降优先级，需明确定义和流程整合。


<details>
  <summary>Details</summary>
Motivation: AI/ML和LLM广泛应用但常为黑箱，对不同群体产生无意不公平，导致AI公平性关注度增加，需从软件工程角度研究如何融入开发流程。

Method: 采用26个半结构化访谈，覆盖不同领域和背景的从业者，研究AI公平性在软件开发生命周期中的意识、应用和平衡。

Result: 主题定性分析显示，参与者认识到公平性维度，但实践不一致，公平性常被降级，存在知识差距，需明确定义、评估指标和流程。

Conclusion: 需与利益相关者达成一致，制定情境化公平定义、评估指标和正式流程，以更好地将公平性整合到AI/ML项目中。

Abstract: Nowadays, Artificial Intelligence (AI), particularly Machine Learning (ML) and Large Language Models (LLMs), is widely applied across various contexts. However, the corresponding models often operate as black boxes, leading them to unintentionally act unfairly towards different demographic groups. This has led to a growing focus on fairness in AI software recently, alongside the traditional focus on the effectiveness of AI models. Through 26 semi-structured interviews with practitioners from different application domains and with varied backgrounds across 23 countries, we conducted research on fairness requirements in AI from software engineering perspective. Our study assesses the participants' awareness of fairness in AI / ML software and its application within the Software Development Life Cycle (SDLC), from translating fairness concerns into requirements to assessing their arising early in the SDLC. It also examines fairness through the key assessment dimensions of implementation, validation, evaluation, and how it is balanced with trade-offs involving other priorities, such as addressing all the software functionalities and meeting critical delivery deadlines. Findings of our thematic qualitative analysis show that while our participants recognize the aforementioned AI fairness dimensions, practices are inconsistent, and fairness is often deprioritized with noticeable knowledge gaps. This highlights the need for agreement with relevant stakeholders on well-defined, contextually appropriate fairness definitions, the corresponding evaluation metrics, and formalized processes to better integrate fairness into AI/ML projects.

</details>


### [36] [Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors](https://arxiv.org/abs/2512.13860)
*Henger Li,Shuangjie You,Flavio Di Palo,Yiyue Qian,Ayush Jain*

Main category: cs.SE

TL;DR: VGCO框架使用LLM作为编辑器自动优化工具文档和知识库上下文，通过评估和优化两阶段解决工具调用中的对齐问题，显著提升准确性、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 工具调用的效果严重依赖文档和知识库质量，但这些材料为人设计，与LLM的信息处理方式不匹配，尤其在工业环境中工具功能重叠导致可扩展性、变异性和模糊性挑战。

Method: VGCO框架包含评估阶段（收集失败案例、识别工具与上下文不匹配）和优化阶段（通过离线学习进行分层编辑，结合结构感知和上下文优化）。LLM编辑器具有分层结构、状态感知/动作特定/验证引导特性，支持成本高效的子任务专业化。

Result: VGCO专注于单轮大规模工具调用问题，在多个LLM上实现了准确性、鲁棒性和泛化性的显著提升。

Conclusion: VGCO通过自动优化工具相关上下文，有效解决了LLM工具调用中的文档对齐问题，为工业场景提供了可扩展的解决方案。

Abstract: Tool calling enables large language models (LLMs) to interact with external environments through tool invocation, providing a practical way to overcome the limitations of pretraining. However, the effectiveness of tool use depends heavily on the quality of the associated documentation and knowledge base context. These materials are usually written for human users and are often misaligned with how LLMs interpret information. This problem is even more pronounced in industrial settings, where hundreds of tools with overlapping functionality create challenges in scalability, variability, and ambiguity. We propose Verification-Guided Context Optimization (VGCO), a framework that uses LLMs as editors to automatically refine tool-related documentation and knowledge base context. VGCO works in two stages. First, Evaluation collects real-world failure cases and identifies mismatches between tools and their context. Second, Optimization performs hierarchical editing through offline learning with structure-aware, in-context optimization. The novelty of our LLM editors has three main aspects. First, they use a hierarchical structure that naturally integrates into the tool-calling workflow. Second, they are state-aware, action-specific, and verification-guided, which constrains the search space and enables efficient, targeted improvements. Third, they enable cost-efficient sub-task specialization, either by prompt engineering large editor models or by post-training smaller editor models. Unlike prior work that emphasizes multi-turn reasoning, VGCO focuses on the single-turn, large-scale tool-calling problem and achieves significant improvements in accuracy, robustness, and generalization across LLMs.

</details>


### [37] [Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming](https://arxiv.org/abs/2512.13914)
*Bhargav Chickmagalur Nanjundappa,Spandan Maaheshwari*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) have become integral to software engineering workflows, yet their effectiveness degrades significantly in multi-turn conversations. Recent studies demonstrate an average 39% performance drop when instructions are delivered across multiple turns, with models making premature assumptions and failing to course correct (Laban et al., 2025). This degradation is particularly problematic in exploratory programming tasks where developers need to investigate alternative approaches without committing to a single path. Current solutions force users into a false dichotomy: continue in a context-polluted conversation where the LLM becomes increasingly confused, or start fresh and lose all accumulated context.
  We present ContextBranch, a conversation management system that applies version control semantics to LLM interactions. ContextBranch provides four core primitives--checkpoint, branch, switch, and inject--enabling users to capture conversation state, explore alternatives in isolation, and selectively merge insights. We evaluate ContextBranch through a controlled experiment with 30 software engineering scenarios featuring intentionally polluting explorations. Branched conversations achieved higher response quality compared to linear conversations, with large improvements in focus and context awareness. Benefits were concentrated in complex scenarios involving conceptually distant explorations. Branching reduced context size by 58.1% (31.0 to 13.0 messages), eliminating irrelevant exploratory content. Our work establishes conversation branching as a fundamental primitive for AI-assisted exploratory work, demonstrating that isolation prevents context pollution when exploring alternatives.

</details>


### [38] [PerfCoder: Large Language Models for Interpretable Code Performance Optimization](https://arxiv.org/abs/2512.14018)
*Jiuding Yang,Shengyao Lu,Hongxuan Liu,Shayan Shirahmad Gale Bagi,Zahra Fazel,Tomasz Czajkowski,Di Niu*

Main category: cs.SE

TL;DR: PerfCoder是一种专门设计用于通过可解释的优化方法从源代码生成高性能代码的LLM家族，在代码性能优化基准上超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前的LLMs在自动代码生成方面取得了显著进展，但在生成高性能代码方面仍有局限，这在实际软件系统中是一个关键需求。现有的LLMs不仅因为数据稀缺而受限，更重要的是缺乏指导可解释和有效性能改进的监督。

Method: PerfCoder在精心策划的真实世界优化轨迹集合上进行微调，并使用运行时测量进行强化微调对齐偏好，使其能够提出输入特定的改进策略并直接应用，无需依赖迭代优化。

Result: 在PIE代码性能基准测试中，PerfCoder在运行时加速和有效优化率方面均超越了所有现有模型。此外，PerfCoder能够生成关于源代码的可解释反馈，在规划-优化协作工作流中可以进一步提升更大LLM的性能。

Conclusion: 性能优化不能仅通过模型规模实现，而需要优化策略意识。PerfCoder通过可解释的定制优化成功提升了代码生成性能，甚至能够将32B模型和GPT-5的性能提升到新水平。

Abstract: Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.

</details>


### [39] [PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design](https://arxiv.org/abs/2512.14233)
*Ruozhao Yang,Mingfei Cheng,Gelei Deng,Tianwei Zhang,Junjie Wang,Xiaofei Xie*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.

</details>


### [40] [Aligning Security Compliance and DevOps: A Longitudinal Study](https://arxiv.org/abs/2512.14453)
*Fabiola Moyón,Florian Angermeir,Daniel Mendez,Tony Gorschek,Markus Voggenreiter,Pierre-Louis Bonvin*

Main category: cs.SE

TL;DR: 本文提出了RefA框架，将IEC 62443-4-1安全标准融入DevOps流程，帮助企业在敏捷开发中保持安全合规，并在西门子进行了实证研究验证。


<details>
  <summary>Details</summary>
Motivation: 公司采用敏捷方法和DevOps以提高软件开发效率，但这给传统线性工作流程下的安全标准合规带来了挑战，特别是在关键基础设施相关的产品和服务工程中。

Method: 在西门子AG进行的纵向研究，包括多个子研究，涵盖框架的启动、验证和初步采用阶段，基于IEC 62443-4-1标准开发了RefA这一安全合规DevOps生命周期规范性模型。

Result: 开发了RefA框架，这是一个基于IEC 62443-4-1标准的安全合规DevOps生命周期规范性模型，支持非安全专家也能在实施DevOps流程时保持安全合规。

Conclusion: 本研究通过RefA框架成功地将安全合规性知识转移到产品开发团队，支持跨职能团队具备交付合规产品所需的所有技能，实现了敏捷开发与安全标准的有效结合。

Abstract: Companies adopt agile methodologies and DevOps to facilitate efficient development and deployment of software-intensive products. This, in turn, introduces challenges in relation to security standard compliance traditionally following a more linear workflow. This is especially a challenge for the engineering of products and services associated with critical infrastructures. To support companies in their transition towards DevOps, this paper presents an adaptation of DevOps according to security regulations and standards. We report on our longitudinal study at Siemens AG, consisting of several individual sub-studies in the inception, validation, and initial adoption of our framework based on RefA as well as the implications for practice. RefA is a prescriptive model of a security compliant DevOps lifecycle based on the IEC 62443-4-1 standard. The overall framework is aimed at professionals, not only security experts, being able to use it on implementing DevOps processes while remaining compliant with security norms. We demonstrate how RefA facilitates the transfer of security compliance knowledge to product development teams. This knowledge transfer supports the agility aim of ensuring that cross-functional teams have all the skills needed to deliver the compliant products.

</details>


### [41] [Teralizer: Semantics-Based Test Generalization from Conventional Unit Tests to Property-Based Tests](https://arxiv.org/abs/2512.14475)
*Johann Glock,Clemens Bauer,Martin Pinzger*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Conventional unit tests validate single input-output pairs, leaving most inputs of an execution path untested. Property-based testing addresses this shortcoming by generating multiple inputs satisfying properties but requires significant manual effort to define properties and their constraints. We propose a semantics-based approach that automatically transforms unit tests into property-based tests by extracting specifications from implementations via single-path symbolic analysis. We demonstrate this approach through Teralizer, a prototype for Java that transforms JUnit tests into property-based jqwik tests. Unlike prior work that generalizes from input-output examples, Teralizer derives specifications from program semantics.
  We evaluated Teralizer on three progressively challenging datasets. On EvoSuite-generated tests for EqBench and Apache Commons utilities, Teralizer improved mutation scores by 1-4 percentage points. Generalization of mature developer-written tests from Apache Commons utilities showed only 0.05-0.07 percentage points improvement. Analysis of 632 real-world Java projects from RepoReapers highlights applicability barriers: only 1.7% of projects completed the generalization pipeline, with failures primarily due to type support limitations in symbolic analysis and static analysis limitations in our prototype. Based on the results, we provide a roadmap for future work, identifying research and engineering challenges that need to be tackled to advance the field of test generalization.
  Artifacts available at: https://doi.org/10.5281/zenodo.17950381

</details>


### [42] [MoT: A Model-Driven Low-Code Approach for Simplifying Cloud-of-Things Application Development](https://arxiv.org/abs/2512.14613)
*Cristiano Welter,Kleinner Farias*

Main category: cs.SE

TL;DR: 提出了一种名为MoT的模型驱动方法，融合低代码原则简化云物联应用开发。通过定制UML配置文件和验证研究证实其可行性和易用性。


<details>
  <summary>Details</summary>
Motivation: 云物联应用开发存在技术门槛高、缺乏标准化方法、互操作性差等问题，需要简化开发流程。

Method: 设计了面向IoT和云服务的定制UML配置文件，通过案例研究和TAM问卷调查进行验证。

Result: MoT被证实可行，能降低技术壁垒，用户反馈易用性高且能加速开发。

Conclusion: MoT为云物联应用提供了有效的模型驱动解决方案，通过降低门槛和提升自动化水平推动技术普及。

Abstract: The integration of cloud computing and the Internet of Things (IoT) is essential for scalable, intelligent systems. However, developing cloud-of-things (CoT) applications remains challenging. It requires significant technical expertise and lacks standardized, model-driven methodologies. Current approaches fail to ensure interoperability, automation, and efficiency. This study introduces the Model of Things (MoT), a model-based approach that incorporates low-code principles to simplify CoT development. MoT reduces technical barriers by providing a custom UML profile designed for IoT and cloud services. To evaluate MoT, we conducted a case study and a Technology Acceptance Model (TAM) questionnaire. The results confirmed MoT's feasibility, demonstrating that it streamlines CoT application development and deployment. Users found MoT accessible, even with limited IoT experience, and reported high perceived ease of use and usefulness. Qualitative feedback highlighted MoT's ability to reduce complexity and speed up development. MoT offers a promising, model-driven solution for CoT application development. By lowering entry barriers and promoting automation, it enhances both efficiency and flexibility. This study represents a step toward a more user-friendly framework, enabling broader adoption of CoT technologies.

</details>


### [43] [Reconsidering Conversational Norms in LLM Chatbots for Sustainable AI](https://arxiv.org/abs/2512.14673)
*Ronnie de Souza Santos,Cleyton Magalhães,Italo Santos*

Main category: cs.SE

TL;DR: 本文认为用户交互行为是影响LLM系统环境可持续性的重要但被忽视的因素，提出了四个维度的分析并呼吁重新设计交互方式。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注模型架构和硬件效率，但忽略了用户交互实践对LLM系统能耗的影响。

Method: 通过愿景论文的形式，从四个维度分析用户交互行为对能耗的影响：对话模式、响应期望、用户习惯和上下文积累。

Result: 识别出延长对话、即时响应期望、用户习惯和上下文积累会显著增加计算成本和能源消耗。

Conclusion: 需要重新设计聊天机器人交互方式，将可持续性考量融入对话规范，以降低LLM系统的环境影响。

Abstract: LLM based chatbots have become central interfaces in technical, educational, and analytical domains, supporting tasks such as code reasoning, problem solving, and information exploration. As these systems scale, sustainability concerns have intensified, with most assessments focusing on model architecture, hardware efficiency, and deployment infrastructure. However, existing mitigation efforts largely overlook how user interaction practices themselves shape the energy profile of LLM based systems. In this vision paper, we argue that interaction level behavior appears to be an underexamined factor shaping the environmental impact of LLM based systems, and we present this issue across four dimensions. First, extended conversational patterns increase token production and raise the computational cost of inference. Second, expectations of instant responses limit opportunities for energy aware scheduling and workload consolidation. Third, everyday user habits contribute to cumulative operational demand in ways that are rarely quantified. Fourth, the accumulation of context affects memory requirements and reduces the efficiency of long running dialogues. Addressing these challenges requires rethinking how chatbot interactions are designed and conceptualized, and adopting perspectives that recognize sustainability as partly dependent on the conversational norms through which users engage with LLM based systems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [44] [Co-simulation errors due to step size changes](https://arxiv.org/abs/2512.13845)
*Lars T. Kyllingstad*

Main category: cs.CE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: When two simulation units in a continuous-time co-simulation are connected via some variable $q$, and both simulation units have an internal state which represents the time integral of $q$, there will generally be a discrepancy between those states due to extrapolation errors. Normally, such extrapolation errors diminish if the macro time step size is reduced. Here we show that, under certain circumstances, step size changes can cause such discrepancies to increase even when the change is towards smaller steps.

</details>


### [45] [Transfer Learning-Based Surrogate Modeling for Nonlinear Time-History Response Analysis of High-Fidelity Structural Models](https://arxiv.org/abs/2512.14161)
*Keiichi Ishikawa,Yuma Matsumoto,Taro Yaoyama,Sangwon Lee,Tatsuya Itoi*

Main category: cs.CE

TL;DR: 本研究提出了一种利用迁移学习构建高保真结构响应分析替代模型的方法，显著减少了计算成本


<details>
  <summary>Details</summary>
Motivation: 基于性能的地震工程框架需要非线性时程分析，但传统数值模拟计算成本高，限制了实际应用。现有机器学习方法大多基于低保真简化模型，需要建立适用于高保真分析的替代模型

Method: 提出使用低保真响应分析的替代模型作为预训练模型，通过迁移学习将其知识转移到高保真响应分析的替代模型构建中

Result: 以20层钢框架为例，仅需20个样本即可构建替代模型，预测结果与特定场地时程灾害一致

Conclusion: 所提出的迁移学习框架能有效建立高保真响应分析替代模型，大幅降低计算成本，促进基于性能的地震工程框架的实际应用

Abstract: In a performance based earthquake engineering (PBEE) framework, nonlinear time-history response analysis (NLTHA) for numerous ground motions are required to assess the seismic risk of buildings or civil engineering structures. However, such numerical simulations are computationally expensive, limiting the real-world practical application of the framework. To address this issue, previous studies have used machine learning to predict the structural responses to ground motions with low computational costs. These studies typically conduct NLTHAs for a few hundreds ground motions and use the results to train and validate surrogate models. However, most of the previous studies focused on computationally-inexpensive response analysis models such as single degree of freedom. Surrogate models of high-fidelity response analysis are required to enrich the quantity and diversity of information used for damage assessment in PBEE. Notably, the computational cost of creating training and validation datasets increases if the fidelity of response analysis model becomes higher. Therefore, methods that enable surrogate modeling of high-fidelity response analysis without a large number of training samples are needed. This study proposes a framework that uses transfer learning to construct the surrogate model of a high-fidelity response analysis model. This framework uses a surrogate model of low-fidelity response analysis as the pretrained model and transfers its knowledge to construct surrogate models for high-fidelity response analysis with substantially reduced computational cost. As a case study, surrogate models that predict responses of a 20-story steel moment frame were constructed with only 20 samples as the training dataset. The responses to the ground motions predicted by constructed surrogate model were consistent with a site-specific time-based hazard.

</details>


### [46] [A data-physics hybrid generative model for patient-specific post-stroke motor rehabilitation using wearable sensor data](https://arxiv.org/abs/2512.14329)
*Yanning Dai,Chenyu Tang,Ruizhi Zhang,Wenyu Yang,Yilan Zhang,Yuhui Wang,Junliang Chen,Xuhang Chen,Ruimou Xie,Yangyue Cao,Qiaoying Li,Jin Cao,Tao Li,Hubin Zhao,Yu Pan,Arokia Nathan,Xin Gao,Peter Smielewski,Shuo Gao*

Main category: cs.CE

TL;DR: 开发了一种数据-物理混合生成框架，通过单次20米平地行走数据预测中风患者的斜坡/楼梯行走能力，在临床实践中显著提升了康复效果


<details>
  <summary>Details</summary>
Motivation: 当前中风康复评估只能提供静态损伤评分，无法预测患者执行特定任务（如斜坡行走、爬楼梯）的安全能力，无法满足个性化康复需求

Method: 结合可穿戴传感器运动学、比例-微分物理控制器、健康人群运动图谱、目标条件深度强化学习与行为克隆/生成对抗模仿学习，生成符合物理规律的个性化步态模拟

Result: 个性化控制器在保留个体步态特征的同时，将关节角度和端点保真度分别提高4.73%和12.10%，训练时间减少至纯物理基准的25.56%；多中心试验显示使用该系统的治疗师组患者Fugl-Meyer评分改善更显著（6.0分 vs 3.7分）

Conclusion: 该生成式任务预测框架能够增强中风后步态康复的临床决策，为动态个性化运动功能恢复策略提供模板

Abstract: Dynamic prediction of locomotor capacity after stroke is crucial for tailoring rehabilitation, yet current assessments provide only static impairment scores and do not indicate whether patients can safely perform specific tasks such as slope walking or stair climbing. Here, we develop a data-physics hybrid generative framework that reconstructs an individual stroke survivor's neuromuscular control from a single 20 m level-ground walking trial and predicts task-conditioned locomotion across rehabilitation scenarios. The system combines wearable-sensor kinematics, a proportional-derivative physics controller, a population Healthy Motion Atlas, and goal-conditioned deep reinforcement learning with behaviour cloning and generative adversarial imitation learning to generate physically plausible, patient-specific gait simulations for slopes and stairs. In 11 stroke survivors, the personalized controllers preserved idiosyncratic gait patterns while improving joint-angle and endpoint fidelity by 4.73% and 12.10%, respectively, and reducing training time to 25.56% relative to a physics-only baseline. In a multicentre pilot involving 21 inpatients, clinicians who used our locomotion predictions to guide task selection and difficulty obtained larger gains in Fugl-Meyer lower-extremity scores over 28 days of standard rehabilitation than control clinicians (mean change 6.0 versus 3.7 points). These findings indicate that our generative, task-predictive framework can augment clinical decision-making in post-stroke gait rehabilitation and provide a template for dynamically personalized motor recovery strategies.

</details>
