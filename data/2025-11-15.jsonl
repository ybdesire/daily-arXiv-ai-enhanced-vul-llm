{"id": "2511.09794", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.09794", "abs": "https://arxiv.org/abs/2511.09794", "authors": ["Wasique Islam Shafin", "Md Nakhla Rafi", "Zhenhao Li", "Tse-Hsun Chen"], "title": "Evaluating Software Process Models for Multi-Agent Class-Level Code Generation", "comment": null, "summary": "Modern software systems require code that is not only functional but also maintainable and well-structured. Although Large Language Models (LLMs) are increasingly used to automate software development, most studies focus on isolated, single-agent function-level generation. This work examines how process structure and role specialization shape multi-agent LLM workflows for class-level code generation. We simulate a Waterfall-style development cycle covering Requirement, Design, Implementation, and Testing using three LLMs (GPT-4o-mini, DeepSeek-Chat, and Claude-3.5-Haiku) on 100 Python tasks from the ClassEval benchmark. Our findings show that multi-agent workflows reorganize, rather than consistently enhance, model performance. Waterfall-style collaboration produces cleaner and more maintainable code but often reduces functional correctness (-37.8\\% for GPT-4o-mini and -39.8\\% for DeepSeek-Chat), with Claude-3.5-Haiku as a notable exception (+9.5\\%). Importantly, process constraints shift failure characteristics: structural issues such as missing code decrease, while semantic and validation errors become more frequent. Among all stages, Testing exerts the strongest influence by improving verification coverage but also introducing new reasoning failures, whereas Requirement and Design have comparatively modest effects. Overall, this study provides empirical evidence that software process structure fundamentally alters how LLMs reason, collaborate, and fail, revealing inherent trade-offs between rigid workflow discipline and flexible problem-solving in multi-agent code generation."}
{"id": "2511.09964", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.09964", "abs": "https://arxiv.org/abs/2511.09964", "authors": ["Noah van der Vleuten", "Anthony Flores", "Shray Mathur", "Max Rakitin", "Thomas Hopkins", "Kevin G. Yager", "Esther H. R. Tsai"], "title": "EnvTrace: Simulation-Based Semantic Evaluation of LLM Code via Execution Trace Alignment -- Demonstrated at Synchrotron Beamlines", "comment": null, "summary": "Evaluating large language models (LLMs) for instrument control requires methods that go beyond standard, stateless algorithmic benchmarks, since the behavior of physical systems cannot be fully captured by unit tests alone. Here we introduce EnvTrace, a simulation-based method that evaluates execution traces to assess semantic code equivalence. EnvTrace is demonstrated with a beamline control-logic digital twin to facilitate the evaluation of instrument control code, with the digital twin itself also enabling the pre-execution validation of live experiments. Over 30 LLMs were evaluated using trace alignment to generate a multi-faceted score for functional correctness across key behavioral dimensions, showing that many top-tier models can approach human-level performance in rapid control-code generation. This is a first step toward a broader vision where LLMs and digital twins work symbiotically: LLMs providing intuitive control and agentic orchestration, and digital twins offering safe and high-fidelity environments, paving the way towards autonomous embodied AI."}
{"id": "2511.10049", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10049", "abs": "https://arxiv.org/abs/2511.10049", "authors": ["Divyanshu Saxena", "Rishikesh Maurya", "Xiaoxuan Ou", "Gagan Somashekar", "Shachee Mishra Gupta", "Arun Iyer", "Yu Kang", "Chetan Bansal", "Aditya Akella", "Saravan Rajmohan"], "title": "Continuous Benchmark Generation for Evaluating Enterprise-scale LLM Agents", "comment": "5 pages", "summary": "The rapid adoption of AI agents across domains has made systematic evaluation crucial for ensuring their usefulness and successful production deployment. Evaluation of AI agents typically involves using a fixed set of benchmarks and computing multiple evaluation metrics for the agent. While sufficient for simple coding tasks, these benchmarks fall short for enterprise-scale agents, where services and requirements evolve continuously and ground-truth examples are sparse. We propose a process of benchmark generation that helps evolve the benchmarks as the requirements change and perform robust evaluation of evolving AI agents. We instantiate this approach for a case study of service migration from one deployment platform to another at a large public enterprise. Our approach relies on semi-structured documents where developers express the high-level intent, and uses state-of-the-art LLMs to generate benchmarks from just a small number of such documents. Overall, this process results in a maintainable evaluation framework, enabling rapid feedback on agent performance and facilitating targeted improvements."}
{"id": "2511.10271", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10271", "abs": "https://arxiv.org/abs/2511.10271", "authors": ["Xin Sun", "Daniel Ståhl", "Kristian Sandahl", "Christoph Kessler"], "title": "Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics", "comment": null, "summary": "In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality."}
{"id": "2511.10064", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2511.10064", "abs": "https://arxiv.org/abs/2511.10064", "authors": ["Lennart Justin Schulze", "Vito Zago", "Giuseppe Bilotta", "Robert Anthony Dalrymple"], "title": "Localized kernel gradient correction for SPH simulations of water wave propagation", "comment": "Parts of this manuscript have been previously presented in the internal proceedings of the SPHERIC 2022 International Workshop, June 7-9 2022, Catania, Italy", "summary": "Basic Smoothed Particle Hydrodynamics (SPH) models exhibit excessive, numerical dissipation in the simulation of water wave propagation. This can be remedied using higher-order approaches such as kernel gradient correction, which introduce additional computational effort. The present work demonstrates, that the higher-order scheme is only required in a limited part of the water wave in order to obtain satisfying results. The criterion for distinguishing particles in need of special treatment from those that do not is motivated by water wave mechanics. Especially for deep water waves, the approach potentially spares large amounts of computational effort. The present paper also proposes a remedy for issues of the kernel gradient correction occurring at the free surface. Satisfying results for the proposed approach are shown for a standing wave in a basin and a progressive wave train in a long wave tank."}
{"id": "2511.09582", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.09582", "abs": "https://arxiv.org/abs/2511.09582", "authors": ["Banhirup Sengupta", "Peenal Gupta", "Souvik Sengupta"], "title": "Revisit to the Bai-Galbraith signature scheme", "comment": null, "summary": "Dilithium is one of the NIST approved lattice-based signature schemes. In this short note we describe the Bai-Galbraith signature scheme proposed in BG14, which differs to Dilithium, due to the fact that there is no public key compression. This lattice-based signature scheme is based on Learning with Errors (LWE)."}
{"id": "2511.10323", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10323", "abs": "https://arxiv.org/abs/2511.10323", "authors": ["Dávid Kószó", "Tamás Aladics", "Rudolf Ferenc", "Péter Hegedűs"], "title": "A Large-Scale Collection Of (Non-)Actionable Static Code Analysis Reports", "comment": "Under publication to Nature Scientific Data journal", "summary": "Static Code Analysis (SCA) tools, while invaluable for identifying potential coding problems, functional bugs, or vulnerabilities, often generate an overwhelming number of warnings, many of which are non-actionable. This overload of alerts leads to ``alert fatigue'', a phenomenon where developers become desensitized to warnings, potentially overlooking critical issues and ultimately hindering productivity and code quality. Analyzing these warnings and training machine learning models to identify and filter them requires substantial datasets, which are currently scarce, particularly for Java. This scarcity impedes efforts to improve the accuracy and usability of SCA tools and mitigate the effects of alert fatigue. In this paper, we address this gap by introducing a novel methodology for collecting and categorizing SCA warnings, effectively distinguishing actionable from non-actionable ones. We further leverage this methodology to generate a large-scale dataset of over 1 million entries of Java source code warnings, named NASCAR: (Non-)Actionable Static Code Analysis Reports. To facilitate follow-up research in this domain, we make both the dataset and the tools used to generate it publicly available."}
{"id": "2511.10355", "categories": ["cs.CE", "cond-mat.mtrl-sci", "physics.app-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2511.10355", "abs": "https://arxiv.org/abs/2511.10355", "authors": ["Y. Tu", "B. Wu", "E. Martínez-Pañeda"], "title": "Phase field modelling of cracking and capacity fade in core-shell cathode particles for lithium-ion batteries", "comment": null, "summary": "Core-shell electrode particles are a promising morphology control strategy for high-performance lithium-ion batteries. However, experimental observations reveal that these structures remain prone to mechanical failure, with shell fractures and core-shell debonding occurring after a single charge. In this work, we present a novel, comprehensive computational framework to predict and gain insight into the failure of core-shell morphologies and the associated degradation in battery performance. The fully coupled chemo-mechano-damage model presented captures the interplay between mechanical damage and electrochemical behaviours, enabling the quantification of particle cracking and capacity fade. Both bulk material fracture and interface debonding are captured by utilising the phase field method. We quantify the severity of particle cracking and capacity loss through case studies on a representative core-shell system (NMC811@NMC532). The results bring valuable insights into cracking patterns, underlying mechanisms, and their impact on capacity loss. Surface cracks are found to initiate when a significantly higher lithium concentration accumulates in the core compared to the shell. Interfacial debonding is shown to arise from localised hoop stresses near the core-shell interface, due to greater shell expansion. This debonding develops rapidly, impedes lithium-ion transport, and can lead to more than 10\\% capacity loss after a single discharge. Furthermore, larger particles may experience crack branching driven by extensive tensile zones, potentially fragmenting the entire particle. The framework developed can not only bring new insight into the degradation mechanisms of core-shell particles but also be used to design electrode materials with improved performance and extended lifetime."}
{"id": "2511.09603", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09603", "abs": "https://arxiv.org/abs/2511.09603", "authors": ["Noor Hazlina Abdul Mutalib", "Aznul Qalid Md Sabri", "Ainuddin Wahid Abdul Wahab", "Erma Rahayu Mohd Faizal Abdullah", "Nouar AlDahoul"], "title": "An explainable Recursive Feature Elimination to detect Advanced Persistent Threats using Random Forest classifier", "comment": null, "summary": "Intrusion Detection Systems (IDS) play a vital role in modern cybersecurity frameworks by providing a primary defense mechanism against sophisticated threat actors. In this paper, we propose an explainable intrusion detection framework that integrates Recursive Feature Elimination (RFE) with Random Forest (RF) to enhance detection of Advanced Persistent Threats (APTs). By using CICIDS2017 dataset, the approach begins with comprehensive data preprocessing and narrows down the most significant features via RFE. A Random Forest (RF) model was trained on the refined feature set, with SHapley Additive exPlanations (SHAP) used to interpret the contribution of each selected feature. Our experiment demonstrates that the explainable RF-RFE achieved a detection accuracy of 99.9%, reducing false positive and computational cost in comparison to traditional classifiers. The findings underscore the effectiveness of integrating explainable AI and feature selection to develop a robust, transparent, and deployable IDS solution."}
{"id": "2511.10326", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10326", "abs": "https://arxiv.org/abs/2511.10326", "authors": ["Shuangyu Lyu", "Chuan Luo", "Ruizhi Shi", "Wei Wu", "Chanjuan Liu", "Chunming Hu"], "title": "Towards Comprehensive Sampling of SMT Solutions", "comment": null, "summary": "This work focuses on effectively generating diverse solutions for satisfiability modulo theories (SMT) formulas, targeting the theories of bit-vectors, arrays, and uninterpreted functions, which is a critical task in software and hardware testing. Generating diverse SMT solutions helps uncover faults and detect safety violations during the verification and testing process, resulting in the SMT sampling problem, i.e., constructing a small number of solutions while achieving comprehensive coverage of the constraint space. While high coverage is crucial for exploring system behaviors, reducing the number of solutions is of great importance, as excessive solutions increase testing time and resource usage, undermining efficiency. In this work, we introduce PanSampler, a novel SMT sampler that achieves high coverage with a small number of solutions. It incorporates three novel techniques, i.e., diversity-aware SMT algorithm, abstract syntax tree (AST)-guided scoring function and post-sampling optimization technology, enhancing its practical performance. It iteratively samples solutions, evaluates candidates, and employs local search to refine solutions, ensuring high coverage with a small number of samples. Extensive experiments on practical benchmarks demonstrate that PanSampler exhibits a significantly stronger capability to reach high target coverage, while requiring fewer solutions than current samplers to achieve the same coverage level. Furthermore, our empirical evaluation on practical subjects, which are collected from real-world software systems, shows that PanSampler achieves higher fault detection capability and reduces the number of required test cases from 32.6\\% to 76.4\\% to reach the same fault detection effectiveness, leading to a substantial improvement in testing efficiency. PanSampler advances SMT sampling, reducing the cost of software testing and hardware verification."}
{"id": "2511.09606", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.09606", "abs": "https://arxiv.org/abs/2511.09606", "authors": ["Fujiao Ji", "Doowon Kim"], "title": "How Can We Effectively Use LLMs for Phishing Detection?: Evaluating the Effectiveness of Large Language Model-based Phishing Detection Models", "comment": null, "summary": "Large language models (LLMs) have emerged as a promising phishing detection mechanism, addressing the limitations of traditional deep learning-based detectors, including poor generalization to previously unseen websites and a lack of interpretability. However, LLMs' effectiveness for phishing detection remains unexplored. This study investigates how to effectively leverage LLMs for phishing detection (including target brand identification) by examining the impact of input modalities (screenshots, logos, HTML, and URLs), temperature settings, and prompt engineering strategies. Using a dataset of 19,131 real-world phishing websites and 243 benign sites, we evaluate seven LLMs -- two commercial models (GPT 4.1 and Gemini 2.0 flash) and five open-source models (Qwen, Llama, Janus, DeepSeek-VL2, and R1) -- alongside two deep learning (DL)-based baselines (PhishIntention and Phishpedia).\n  Our findings reveal that commercial LLMs generally outperform open-source models in phishing detection, while DL models demonstrate better performance on benign samples. For brand identification, screenshot inputs achieve optimal results, with commercial LLMs reaching 93-95% accuracy and open-source models, particularly Qwen, achieving up to 92%. However, incorporating multiple input modalities simultaneously or applying one-shot prompts does not consistently enhance performance and may degrade results. Furthermore, higher temperature values reduce performance. Based on these results, we recommend using screenshot inputs with zero temperature to maximize accuracy for LLM-based detectors with HTML serving as auxiliary context when screenshot information is insufficient."}
{"id": "2511.09610", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.09610", "abs": "https://arxiv.org/abs/2511.09610", "authors": ["Daniyal Ganiuly", "Nurzhau Bolatbek"], "title": "Slice-Aware Spoofing Detection in 5G Networks Using Lightweight Machine Learning", "comment": null, "summary": "The increasing virtualization of fifth generation (5G) networks expands the attack surface of the user plane, making spoofing a persistent threat to slice integrity and service reliability. This study presents a slice-aware lightweight machine-learning framework for detecting spoofing attacks within 5G network slices. The framework was implemented on a reproducible Open5GS and srsRAN testbed emulating three service classes such as enhanced Mobile Broadband (eMBB), Ultra-Reliable Low-Latency Communication (URLLC), and massive Machine-Type Communication (mMTC) under controlled benign and adversarial traffic. Two efficient classifiers, Logistic Regression and Random Forest, were trained independently for each slice using statistical flow features derived from mirrored user-plane traffic. Slice-aware training improved detection accuracy by up to 5% and achieved F1-scores between 0.93 and 0.96 while maintaining real-time operation on commodity edge hardware. The results demonstrate that aligning security intelligence with slice boundaries enhances detection reliability and preserves operational isolation, enabling practical deployment in 5G network-security environments. Conceptually, the work bridges network-security architecture and adaptive machine learning by showing that isolation-aware intelligence can achieve scalable, privacy-preserving spoofing defense without high computational cost."}
{"id": "2511.09696", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.09696", "abs": "https://arxiv.org/abs/2511.09696", "authors": ["Bikash Chandra Singh", "Md Jakir Hossain", "Rafael Diaz", "Sandip Roy", "Ravi Mukkamala", "Sachin Shetty"], "title": "Cooperative Local Differential Privacy: Securing Time Series Data in Distributed Environments", "comment": null, "summary": "The rapid growth of smart devices such as phones, wearables, IoT sensors, and connected vehicles has led to an explosion of continuous time series data that offers valuable insights in healthcare, transportation, and more. However, this surge raises significant privacy concerns, as sensitive patterns can reveal personal details. While traditional differential privacy (DP) relies on trusted servers, local differential privacy (LDP) enables users to perturb their own data. However, traditional LDP methods perturb time series data by adding user-specific noise but exhibit vulnerabilities. For instance, noise applied within fixed time windows can be canceled during aggregation (e.g., averaging), enabling adversaries to infer individual statistics over time, thereby eroding privacy guarantees.\n  To address these issues, we introduce a Cooperative Local Differential Privacy (CLDP) mechanism that enhances privacy by distributing noise vectors across multiple users. In our approach, noise is collaboratively generated and assigned so that when all users' perturbed data is aggregated, the noise cancels out preserving overall statistical properties while protecting individual privacy. This cooperative strategy not only counters vulnerabilities inherent in time-window-based methods but also scales effectively for large, real-time datasets, striking a better balance between data utility and privacy in multiuser environments."}
{"id": "2511.09775", "categories": ["cs.CR", "cs.AI", "cs.IT", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.09775", "abs": "https://arxiv.org/abs/2511.09775", "authors": ["Dilli Prasad Sharma", "Xiaowei Sun", "Liang Xue", "Xiaodong Lin", "Pulei Xiong"], "title": "Privacy-Preserving Explainable AIoT Application via SHAP Entropy Regularization", "comment": null, "summary": "The widespread integration of Artificial Intelligence of Things (AIoT) in smart home environments has amplified the demand for transparent and interpretable machine learning models. To foster user trust and comply with emerging regulatory frameworks, the Explainable AI (XAI) methods, particularly post-hoc techniques such as SHapley Additive exPlanations (SHAP), and Local Interpretable Model-Agnostic Explanations (LIME), are widely employed to elucidate model behavior. However, recent studies have shown that these explanation methods can inadvertently expose sensitive user attributes and behavioral patterns, thereby introducing new privacy risks. To address these concerns, we propose a novel privacy-preserving approach based on SHAP entropy regularization to mitigate privacy leakage in explainable AIoT applications. Our method incorporates an entropy-based regularization objective that penalizes low-entropy SHAP attribution distributions during training, promoting a more uniform spread of feature contributions. To evaluate the effectiveness of our approach, we developed a suite of SHAP-based privacy attacks that strategically leverage model explanation outputs to infer sensitive information. We validate our method through comparative evaluations using these attacks alongside utility metrics on benchmark smart home energy consumption datasets. Experimental results demonstrate that SHAP entropy regularization substantially reduces privacy leakage compared to baseline models, while maintaining high predictive accuracy and faithful explanation fidelity. This work contributes to the development of privacy-preserving explainable AI techniques for secure and trustworthy AIoT applications."}
{"id": "2511.09876", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.09876", "abs": "https://arxiv.org/abs/2511.09876", "authors": ["Shuo Shi", "Jinghuai Zhang", "Shijie Jiang", "Chunyi Zhou", "Yuyuan Li", "Mengying Zhu", "Yangyang Wu", "Tianyu Du"], "title": "DP-GENG : Differentially Private Dataset Distillation Guided by DP-Generated Data", "comment": "14 pages, 9 figures, published in AAAI 2026", "summary": "Dataset distillation (DD) compresses large datasets into smaller ones while preserving the performance of models trained on them. Although DD is often assumed to enhance data privacy by aggregating over individual examples, recent studies reveal that standard DD can still leak sensitive information from the original dataset due to the lack of formal privacy guarantees. Existing differentially private (DP)-DD methods attempt to mitigate this risk by injecting noise into the distillation process. However, they often fail to fully leverage the original dataset, resulting in degraded realism and utility. This paper introduces \\libn, a novel framework that addresses the key limitations of current DP-DD by leveraging DP-generated data. Specifically, \\lib initializes the distilled dataset with DP-generated data to enhance realism. Then, generated data refines the DP-feature matching technique to distill the original dataset under a small privacy budget, and trains an expert model to align the distilled examples with their class distribution. Furthermore, we design a privacy budget allocation strategy to determine budget consumption across DP components and provide a theoretical analysis of the overall privacy guarantees. Extensive experiments show that \\lib significantly outperforms state-of-the-art DP-DD methods in terms of both dataset utility and robustness against membership inference attacks, establishing a new paradigm for privacy-preserving dataset distillation."}
{"id": "2511.09879", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09879", "abs": "https://arxiv.org/abs/2511.09879", "authors": ["Catherine Xia", "Manar H. Alalfi"], "title": "Taught by the Flawed: How Dataset Insecurity Breeds Vulnerable AI Code", "comment": null, "summary": "AI programming assistants have demonstrated a tendency to generate code containing basic security vulnerabilities. While developers are ultimately responsible for validating and reviewing such outputs, improving the inherent quality of these generated code snippets remains essential. A key contributing factor to insecure outputs is the presence of vulnerabilities in the training datasets used to build large language models (LLMs). To address this issue, we propose curating training data to include only code that is free from detectable vulnerabilities. In this study, we constructed a secure dataset by filtering an existing Python corpus using a static analysis tool to retain only vulnerability-free functions. We then trained two transformer-based models: one on the curated dataset and one on the original, unfiltered dataset. The models were evaluated on both the correctness and security of the code they generated in response to natural language function descriptions. Our results show that the model trained on the curated dataset produced outputs with fewer security issues, while maintaining comparable functional correctness. These findings highlight the importance of secure training data in improving the reliability of AI-based programming assistants, though further enhancements to model architecture and evaluation are needed to reinforce these outcomes."}
{"id": "2511.09957", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.09957", "abs": "https://arxiv.org/abs/2511.09957", "authors": ["Duc-Ly Vu", "Thanh-Cong Nguyen", "Minh-Khanh Vu", "Ngoc-Thanh Nguyen", "Kim-Anh Do Thi"], "title": "Pack-A-Mal: A Malware Analysis Framework for Open-Source Packages", "comment": "4 pages, 5 figures, 2 tables", "summary": "The increasingly sophisticated environment in which attackers operate makes software security an even greater challenge in open-source projects, where malicious packages are prevalent. Static analysis tools, such as Malcontent, are highly useful but are often incapable of dealing with obfuscated malware. Such situations lead to an unreasonably high rate of false positives. This paper highlights that dynamic analysis, rather than static analysis, provides greater insight but is also more resource-intensive for understanding software behaviour during execution. In this study, we enhance a dynamic analysis tool, package-analysis, to capture key runtime behaviours, including commands executed, files accessed, and network communications. This modification enables the use of container sandboxing technologies, such as gVisor, to analyse potentially malicious packages without significantly compromising the host system."}
{"id": "2511.10050", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10050", "abs": "https://arxiv.org/abs/2511.10050", "authors": ["Go Tsuruoka", "Takami Sato", "Qi Alfred Chen", "Kazuki Nomoto", "Ryunosuke Kobayashi", "Yuna Tanaka", "Tatsuya Mori"], "title": "Trapped by Their Own Light: Deployable and Stealth Retroreflective Patch Attacks on Traffic Sign Recognition Systems", "comment": null, "summary": "Traffic sign recognition plays a critical role in ensuring safe and efficient transportation of autonomous vehicles but remain vulnerable to adversarial attacks using stickers or laser projections. While existing attack vectors demonstrate security concerns, they suffer from visual detectability or implementation constraints, suggesting unexplored vulnerability surfaces in TSR systems. We introduce the Adversarial Retroreflective Patch (ARP), a novel attack vector that combines the high deployability of patch attacks with the stealthiness of laser projections by utilizing retroreflective materials activated only under victim headlight illumination. We develop a retroreflection simulation method and employ black-box optimization to maximize attack effectiveness. ARP achieves $\\geq$93.4\\% success rate in dynamic scenarios at 35 meters and $\\geq$60\\% success rate against commercial TSR systems in real-world conditions. Our user study demonstrates that ARP attacks maintain near-identical stealthiness to benign signs while achieving $\\geq$1.9\\% higher stealthiness scores than previous patch attacks. We propose the DPR Shield defense, employing strategically placed polarized filters, which achieves $\\geq$75\\% defense success rates for stop signs and speed limit signs against micro-prism patches."}
{"id": "2511.10111", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.10111", "abs": "https://arxiv.org/abs/2511.10111", "authors": ["Alexander Krause", "Jacques Suray", "Lea Schmüser", "Marten Oltrogge", "Oliver Wiese", "Maximilian Golla", "Sascha Fahl"], "title": "An In-Depth Systematic Analysis of the Security, Usability, and Automation Capabilities of Password Update Processes on Top-Ranked Websites", "comment": null, "summary": "Password updates are a critical account security measure and an essential part of the password lifecycle. Service providers and common security recommendations advise users to update their passwords in response to incidents or as a critical cyber hygiene measure. However, password update processes are often cumbersome and require manual password creation. Inconsistent and complex workflows and a lack of automation capabilities for password managers further negatively impact overall password security.\n  In this work, we perform the first in-depth systematic analysis of 111 password update processes deployed on top-ranked websites. We provide novel insights into their overall security, usability, and automation capabilities and contribute to authentication security research through a better understanding of password update processes. Websites deploy highly diverse, often complex, confusing password update processes and lack the support of password managers. Processes are often hard to use, and end-users can barely transfer experiences and knowledge across websites. Notably, protective measures designed to enhance security frequently obstruct password manager automation. We conclude our work by discussing our findings and giving recommendations for web developers, the web standardization community, and security researchers."}
{"id": "2511.10248", "categories": ["cs.CR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.10248", "abs": "https://arxiv.org/abs/2511.10248", "authors": ["Rinieri Lorenzo", "Gori Giacomo", "Melis Andrea", "Girau Roberto", "Prandini Marco", "Callegati Franco"], "title": "Pk-IOTA: Blockchain empowered Programmable Data Plane to secure OPC UA communications in Industry 4.0", "comment": null, "summary": "The OPC UA protocol is becoming the de facto standard for Industry 4.0 machine-to-machine communication. It stands out as one of the few industrial protocols that provide robust security features designed to prevent attackers from manipulating and damaging critical infrastructures. However, prior works showed that significant challenges still exists to set up secure OPC UA deployments in practice, mainly caused by the complexity of certificate management in industrial scenarios and the inconsistent implementation of security features across industrial OPC UA devices. In this paper, we present Pk-IOTA, an automated solution designed to secure OPC UA communications by integrating programmable data plane switches for in-network certificate validation and leveraging the IOTA Tangle for decen- tralized certificate distribution. Our evaluation is performed on a physical testbed representing a real-world industrial scenario and shows that Pk-IOTA introduces a minimal overhead while providing a scalable and tamper-proof mechanism for OPC UA certificate management."}
{"id": "2511.10265", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.10265", "abs": "https://arxiv.org/abs/2511.10265", "authors": ["Tomasz Truderung"], "title": "Enhanced Anonymous Credentials for E-Voting Systems", "comment": null, "summary": "A simple and practical method for achieving everlasting privacy in e-voting systems, without relying on advanced cryptographic techniques, is to use anonymous voter credentials. The simplicity of this approach may, however, create some challenges, when combined with other security features, such as cast-as-intended verifiability with second device and second-factor authentication.\n  This paper considers a simple augmentation to the anonymous credential mechanism, using perfectly hiding commitments to link such credentials to the voter identities. This solution strengthens the binding between voters and their credentials while preserving everlasting privacy. It ensures that published ballots remain unlinkable to voter identities, yet enables necessary consistency checks during ballot casting and ballot auditing"}
{"id": "2511.10423", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.10423", "abs": "https://arxiv.org/abs/2511.10423", "authors": ["Jiayang Meng", "Tao Huang", "Hong Chen", "Chen Hou", "Guolong Zheng"], "title": "Enhanced Privacy Leakage from Noise-Perturbed Gradients via Gradient-Guided Conditional Diffusion Models", "comment": null, "summary": "Federated learning synchronizes models through gradient transmission and aggregation. However, these gradients pose significant privacy risks, as sensitive training data is embedded within them. Existing gradient inversion attacks suffer from significantly degraded reconstruction performance when gradients are perturbed by noise-a common defense mechanism. In this paper, we introduce Gradient-Guided Conditional Diffusion Models (GG-CDMs) for reconstructing private images from leaked gradients without prior knowledge of the target data distribution. Our approach leverages the inherent denoising capability of diffusion models to circumvent the partial protection offered by noise perturbation, thereby improving attack performance under such defenses. We further provide a theoretical analysis of the reconstruction error bounds and the convergence properties of attack loss, characterizing the impact of key factors-such as noise magnitude and attacked model architecture-on reconstruction quality. Extensive experiments demonstrate our attack's superior reconstruction performance with Gaussian noise-perturbed gradients, and confirm our theoretical findings."}
{"id": "2511.10502", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10502", "abs": "https://arxiv.org/abs/2511.10502", "authors": ["Vincenzo Carletti", "Pasquale Foggia", "Carlo Mazzocca", "Giuseppe Parrella", "Mario Vento"], "title": "On the Detectability of Active Gradient Inversion Attacks in Federated Learning", "comment": null, "summary": "One of the key advantages of Federated Learning (FL) is its ability to collaboratively train a Machine Learning (ML) model while keeping clients' data on-site. However, this can create a false sense of security. Despite not sharing private data increases the overall privacy, prior studies have shown that gradients exchanged during the FL training remain vulnerable to Gradient Inversion Attacks (GIAs). These attacks allow reconstructing the clients' local data, breaking the privacy promise of FL. GIAs can be launched by either a passive or an active server. In the latter case, a malicious server manipulates the global model to facilitate data reconstruction. While effective, earlier attacks falling under this category have been demonstrated to be detectable by clients, limiting their real-world applicability. Recently, novel active GIAs have emerged, claiming to be far stealthier than previous approaches. This work provides the first comprehensive analysis of these claims, investigating four state-of-the-art GIAs. We propose novel lightweight client-side detection techniques, based on statistically improbable weight structures and anomalous loss and gradient dynamics. Extensive evaluation across several configurations demonstrates that our methods enable clients to effectively detect active GIAs without any modifications to the FL training protocol."}
{"id": "2511.10516", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.10516", "abs": "https://arxiv.org/abs/2511.10516", "authors": ["Josep Domingo-Ferrer"], "title": "How Worrying Are Privacy Attacks Against Machine Learning?", "comment": null, "summary": "In several jurisdictions, the regulatory framework on the release and sharing of personal data is being extended to machine learning (ML). The implicit assumption is that disclosing a trained ML model entails a privacy risk for any personal data used in training comparable to directly releasing those data. However, given a trained model, it is necessary to mount a privacy attack to make inferences on the training data. In this concept paper, we examine the main families of privacy attacks against predictive and generative ML, including membership inference attacks (MIAs), property inference attacks, and reconstruction attacks. Our discussion shows that most of these attacks seem less effective in the real world than what a prima face interpretation of the related literature could suggest."}
{"id": "2511.10554", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.10554", "abs": "https://arxiv.org/abs/2511.10554", "authors": ["Lingzhi Wang", "Vinod Yegneswaran", "Xinyi Shi", "Ziyu Li", "Ashish Gehani", "Yan Chen"], "title": "GraphFaaS: Serverless GNN Inference for Burst-Resilient, Real-Time Intrusion Detection", "comment": "Accepted by ML For Systems workshop at Neural Information Processing Systems (NeurIPS 2025)", "summary": "Provenance-based intrusion detection is an increasingly popular application of graphical machine learning in cybersecurity, where system activities are modeled as provenance graphs to capture causality and correlations among potentially malicious actions. Graph Neural Networks (GNNs) have demonstrated strong performance in this setting. However, traditional statically-provisioned GNN inference architectures fall short in meeting two crucial demands of intrusion detection: (1) maintaining consistently low detection latency, and (2) handling highly irregular and bursty workloads. To holistically address these challenges, we present GraphFaaS, a serverless architecture tailored for GNN-based intrusion detection. GraphFaaS leverages the elasticity and agility of serverless computing to dynamically scale the GNN inference pipeline. We parallelize and adapt GNN workflows to a serverless environment, ensuring that the system can respond in real time to fluctuating workloads. By decoupling compute resources from static provisioning, GraphFaaS delivers stable inference latency, which is critical for dependable intrusion detection and timely incident response in cybersecurity operations. Preliminary evaluation shows GraphFaaS reduces average detection latency by 85% and coefficient of variation (CV) by 64% compared to the baseline."}
{"id": "2511.10627", "categories": ["cs.AI", "cs.CV", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10627", "abs": "https://arxiv.org/abs/2511.10627", "authors": ["Edward Kim", "Devan Shanker", "Varun Bharadwaj", "Hongbeen Park", "Jinkyu Kim", "Hazem Torfah", "Daniel J Fremont", "Sanjit A Seshia"], "title": "Querying Labeled Time Series Data with Scenario Programs", "comment": null, "summary": "Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data."}
{"id": "2511.09603", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09603", "abs": "https://arxiv.org/abs/2511.09603", "authors": ["Noor Hazlina Abdul Mutalib", "Aznul Qalid Md Sabri", "Ainuddin Wahid Abdul Wahab", "Erma Rahayu Mohd Faizal Abdullah", "Nouar AlDahoul"], "title": "An explainable Recursive Feature Elimination to detect Advanced Persistent Threats using Random Forest classifier", "comment": null, "summary": "Intrusion Detection Systems (IDS) play a vital role in modern cybersecurity frameworks by providing a primary defense mechanism against sophisticated threat actors. In this paper, we propose an explainable intrusion detection framework that integrates Recursive Feature Elimination (RFE) with Random Forest (RF) to enhance detection of Advanced Persistent Threats (APTs). By using CICIDS2017 dataset, the approach begins with comprehensive data preprocessing and narrows down the most significant features via RFE. A Random Forest (RF) model was trained on the refined feature set, with SHapley Additive exPlanations (SHAP) used to interpret the contribution of each selected feature. Our experiment demonstrates that the explainable RF-RFE achieved a detection accuracy of 99.9%, reducing false positive and computational cost in comparison to traditional classifiers. The findings underscore the effectiveness of integrating explainable AI and feature selection to develop a robust, transparent, and deployable IDS solution."}
{"id": "2511.09775", "categories": ["cs.CR", "cs.AI", "cs.IT", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.09775", "abs": "https://arxiv.org/abs/2511.09775", "authors": ["Dilli Prasad Sharma", "Xiaowei Sun", "Liang Xue", "Xiaodong Lin", "Pulei Xiong"], "title": "Privacy-Preserving Explainable AIoT Application via SHAP Entropy Regularization", "comment": null, "summary": "The widespread integration of Artificial Intelligence of Things (AIoT) in smart home environments has amplified the demand for transparent and interpretable machine learning models. To foster user trust and comply with emerging regulatory frameworks, the Explainable AI (XAI) methods, particularly post-hoc techniques such as SHapley Additive exPlanations (SHAP), and Local Interpretable Model-Agnostic Explanations (LIME), are widely employed to elucidate model behavior. However, recent studies have shown that these explanation methods can inadvertently expose sensitive user attributes and behavioral patterns, thereby introducing new privacy risks. To address these concerns, we propose a novel privacy-preserving approach based on SHAP entropy regularization to mitigate privacy leakage in explainable AIoT applications. Our method incorporates an entropy-based regularization objective that penalizes low-entropy SHAP attribution distributions during training, promoting a more uniform spread of feature contributions. To evaluate the effectiveness of our approach, we developed a suite of SHAP-based privacy attacks that strategically leverage model explanation outputs to infer sensitive information. We validate our method through comparative evaluations using these attacks alongside utility metrics on benchmark smart home energy consumption datasets. Experimental results demonstrate that SHAP entropy regularization substantially reduces privacy leakage compared to baseline models, while maintaining high predictive accuracy and faithful explanation fidelity. This work contributes to the development of privacy-preserving explainable AI techniques for secure and trustworthy AIoT applications."}
{"id": "2511.09879", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09879", "abs": "https://arxiv.org/abs/2511.09879", "authors": ["Catherine Xia", "Manar H. Alalfi"], "title": "Taught by the Flawed: How Dataset Insecurity Breeds Vulnerable AI Code", "comment": null, "summary": "AI programming assistants have demonstrated a tendency to generate code containing basic security vulnerabilities. While developers are ultimately responsible for validating and reviewing such outputs, improving the inherent quality of these generated code snippets remains essential. A key contributing factor to insecure outputs is the presence of vulnerabilities in the training datasets used to build large language models (LLMs). To address this issue, we propose curating training data to include only code that is free from detectable vulnerabilities. In this study, we constructed a secure dataset by filtering an existing Python corpus using a static analysis tool to retain only vulnerability-free functions. We then trained two transformer-based models: one on the curated dataset and one on the original, unfiltered dataset. The models were evaluated on both the correctness and security of the code they generated in response to natural language function descriptions. Our results show that the model trained on the curated dataset produced outputs with fewer security issues, while maintaining comparable functional correctness. These findings highlight the importance of secure training data in improving the reliability of AI-based programming assistants, though further enhancements to model architecture and evaluation are needed to reinforce these outcomes."}
{"id": "2511.09964", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.09964", "abs": "https://arxiv.org/abs/2511.09964", "authors": ["Noah van der Vleuten", "Anthony Flores", "Shray Mathur", "Max Rakitin", "Thomas Hopkins", "Kevin G. Yager", "Esther H. R. Tsai"], "title": "EnvTrace: Simulation-Based Semantic Evaluation of LLM Code via Execution Trace Alignment -- Demonstrated at Synchrotron Beamlines", "comment": null, "summary": "Evaluating large language models (LLMs) for instrument control requires methods that go beyond standard, stateless algorithmic benchmarks, since the behavior of physical systems cannot be fully captured by unit tests alone. Here we introduce EnvTrace, a simulation-based method that evaluates execution traces to assess semantic code equivalence. EnvTrace is demonstrated with a beamline control-logic digital twin to facilitate the evaluation of instrument control code, with the digital twin itself also enabling the pre-execution validation of live experiments. Over 30 LLMs were evaluated using trace alignment to generate a multi-faceted score for functional correctness across key behavioral dimensions, showing that many top-tier models can approach human-level performance in rapid control-code generation. This is a first step toward a broader vision where LLMs and digital twins work symbiotically: LLMs providing intuitive control and agentic orchestration, and digital twins offering safe and high-fidelity environments, paving the way towards autonomous embodied AI."}
{"id": "2511.10271", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10271", "abs": "https://arxiv.org/abs/2511.10271", "authors": ["Xin Sun", "Daniel Ståhl", "Kristian Sandahl", "Christoph Kessler"], "title": "Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics", "comment": null, "summary": "In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality."}
{"id": "2511.10502", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10502", "abs": "https://arxiv.org/abs/2511.10502", "authors": ["Vincenzo Carletti", "Pasquale Foggia", "Carlo Mazzocca", "Giuseppe Parrella", "Mario Vento"], "title": "On the Detectability of Active Gradient Inversion Attacks in Federated Learning", "comment": null, "summary": "One of the key advantages of Federated Learning (FL) is its ability to collaboratively train a Machine Learning (ML) model while keeping clients' data on-site. However, this can create a false sense of security. Despite not sharing private data increases the overall privacy, prior studies have shown that gradients exchanged during the FL training remain vulnerable to Gradient Inversion Attacks (GIAs). These attacks allow reconstructing the clients' local data, breaking the privacy promise of FL. GIAs can be launched by either a passive or an active server. In the latter case, a malicious server manipulates the global model to facilitate data reconstruction. While effective, earlier attacks falling under this category have been demonstrated to be detectable by clients, limiting their real-world applicability. Recently, novel active GIAs have emerged, claiming to be far stealthier than previous approaches. This work provides the first comprehensive analysis of these claims, investigating four state-of-the-art GIAs. We propose novel lightweight client-side detection techniques, based on statistically improbable weight structures and anomalous loss and gradient dynamics. Extensive evaluation across several configurations demonstrates that our methods enable clients to effectively detect active GIAs without any modifications to the FL training protocol."}
