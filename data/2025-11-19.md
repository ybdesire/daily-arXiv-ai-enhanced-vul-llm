<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 16]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.CE](#cs.CE) [Total: 4]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [ExplainableGuard: Interpretable Adversarial Defense for Large Language Models Using Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.13771)
*Shaowei Guan,Yu Zhai,Zhengyu Zhang,Yanze Wang,Hin Chi Kwok*

Main category: cs.CR

TL;DR: ExplainableGuard is an interpretable adversarial defense framework using DeepSeek-Reasoner's chain-of-thought reasoning to detect and neutralize text perturbations while providing step-by-step explanations.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly vulnerable to adversarial attacks, and existing defense mechanisms lack transparency in decision-making.

Method: Leverages tailored CoT prompts to guide LLM through multi-faceted analysis (character, word, structural, semantic) for adversarial defense.

Result: Promising defense efficacy on GLUE Benchmark and IMDB Movie Reviews dataset; human evaluation shows 72.5% deployability-trust rating with superior explanation clarity.

Conclusion: ExplainableGuard demonstrates potential for more trustworthy LLM deployments through interpretable adversarial defense with actionable explanations.

Abstract: Large Language Models (LLMs) are increasingly vulnerable to adversarial attacks that can subtly manipulate their outputs. While various defense mechanisms have been proposed, many operate as black boxes, lacking transparency in their decision-making. This paper introduces ExplainableGuard, an interpretable adversarial defense framework leveraging the chain-of-thought (CoT) reasoning capabilities of DeepSeek-Reasoner. Our approach not only detects and neutralizes adversarial perturbations in text but also provides step-by-step explanations for each defense action. We demonstrate how tailored CoT prompts guide the LLM to perform a multi-faceted analysis (character, word, structural, and semantic) and generate a purified output along with a human-readable justification. Preliminary results on the GLUE Benchmark and IMDB Movie Reviews dataset show promising defense efficacy. Additionally, a human evaluation study reveals that ExplainableGuard's explanations outperform ablated variants in clarity, specificity, and actionability, with a 72.5% deployability-trust rating, underscoring its potential for more trustworthy LLM deployments.

</details>


### [2] [Hashpower allocation in Pay-per-Share blockchain mining pools](https://arxiv.org/abs/2511.13777)
*Pierre-Olivier Goffard,Hansjoerg Albrecher,Jean-Pierre Fouque*

Main category: cs.CR

TL;DR: Analysis of how miners should allocate computing resources between mining pools in a PPS system, considering risk transfer vs. management fees.


<details>
  <summary>Details</summary>
Motivation: Miners face financial risk due to high operational costs and infrequent rewards in Proof-of-Work mining. Mining pools help mitigate risk but involve fees.

Method: Simplified wealth model analyzing PPS reward system with adjustable share difficulty and management fees.

Result: Provides framework for optimal resource allocation decisions between pools based on risk tolerance and fee structures.

Conclusion: Miners must balance risk transfer benefits against management fees when choosing pool allocation strategies.

Abstract: Mining blocks in a blockchain using the \textit{Proof-of-Work} consensus protocol involves significant risk, as network participants face continuous operational costs while earning infrequent capital gains upon successfully mining a block. A common risk mitigation strategy is to join a mining pool, which combines the computing resources of multiple miners to provide a more stable income. This article examines a Pay-per-Share (PPS) reward system, where the pool manager can adjust both the share difficulty and the management fee. Using a simplified wealth model for miners, we explore how miners should allocate their computing resources among different mining pools, considering the trade-off between risk transfer to the manager and management fees.

</details>


### [3] [Human-Centered Threat Modeling in Practice: Lessons, Challenges, and Paths Forward](https://arxiv.org/abs/2511.13781)
*Warda Usman,Yixin Zou,Daniel Zappala*

Main category: cs.CR

TL;DR: Interview study on human-centered threat modeling practices, revealing it as evolving practices shaped by relationships and values, with challenges in emotional strain and impact translation.


<details>
  <summary>Details</summary>
Motivation: To understand how researchers practically engage with human-centered threat modeling (HCTM) in various contexts, as little is known about their actual practices.

Method: Conducted 23 semi-structured interviews with researchers to examine HCTM study design, threat elicitation, and navigation of values and constraints.

Result: HCTM is not prescriptive but evolves through relationships, disciplinary backgrounds, and institutional structures. Researchers use groundwork and participant-centered inquiry guided by care, justice, and autonomy values, while facing emotional strain and structural barriers.

Conclusion: Opportunities exist to advance HCTM through shared infrastructure, recognition of diverse contributions, and better mechanisms for translating findings into policy and societal change.

Abstract: Human-centered threat modeling (HCTM) is an emerging area within security and privacy research that focuses on how people define and navigate threats in various social, cultural, and technological contexts. While researchers increasingly approach threat modeling from a human-centered perspective, little is known about how they prepare for and engage with HCTM in practice. In this work, we conduct 23 semi-structured interviews with researchers to examine the state of HCTM, including how researchers design studies, elicit threats, and navigate values, constraints, and long-term goals. We find that HCTM is not a prescriptive process but a set of evolving practices shaped by relationships with participants, disciplinary backgrounds, and institutional structures. Researchers approach threat modeling through sustained groundwork and participant-centered inquiry, guided by values such as care, justice, and autonomy. They also face challenges including emotional strain, ethical dilemmas, and structural barriers that complicate efforts to translate findings into real-world impact. We conclude by identifying opportunities to advance HCTM through shared infrastructure, broader recognition of diverse contributions, and stronger mechanisms for translating findings into policy, design, and societal change.

</details>


### [4] [Uncovering and Aligning Anomalous Attention Heads to Defend Against NLP Backdoor Attacks](https://arxiv.org/abs/2511.13789)
*Haotian Jin,Yang Li,Haihui Fan,Lin Shen,Xiangfang Li,Bo Li*

Main category: cs.CR

TL;DR: Proposes a backdoor detection method using attention similarity and a mitigation approach via attention safety alignment and head-wise fine-tuning, effective against dynamic/implicit triggers without needing trigger knowledge or clean model.


<details>
  <summary>Details</summary>
Motivation: Backdoor attacks with dynamic/implicit triggers are hard to detect as defenders lack trigger knowledge; existing defenses are trigger-specific or require clean models.

Method: Detects backdoors via high attention head similarity when triggers are present; mitigates via attention safety alignment and head-wise fine-tuning of contaminated heads.

Result: Method significantly reduces backdoor attack success rates while maintaining downstream task performance, validated extensively.

Conclusion: The attention-based approach effectively detects and mitigates backdoor attacks without prior trigger knowledge, offering a versatile defense.

Abstract: Backdoor attacks pose a serious threat to the security of large language models (LLMs), causing them to exhibit anomalous behavior under specific trigger conditions. The design of backdoor triggers has evolved from fixed triggers to dynamic or implicit triggers. This increased flexibility in trigger design makes it challenging for defenders to identify their specific forms accurately. Most existing backdoor defense methods are limited to specific types of triggers or rely on an additional clean model for support. To address this issue, we propose a backdoor detection method based on attention similarity, enabling backdoor detection without prior knowledge of the trigger. Our study reveals that models subjected to backdoor attacks exhibit unusually high similarity among attention heads when exposed to triggers. Based on this observation, we propose an attention safety alignment approach combined with head-wise fine-tuning to rectify potentially contaminated attention heads, thereby effectively mitigating the impact of backdoor attacks. Extensive experimental results demonstrate that our method significantly reduces the success rate of backdoor attacks while preserving the model's performance on downstream tasks.

</details>


### [5] [Zipf-Gramming: Scaling Byte N-Grams Up to Production Sized Malware Corpora](https://arxiv.org/abs/2511.13808)
*Edward Raff,Ryan R. Curtin,Derek Everett,Robert J. Joyce,James Holt*

Main category: cs.CR

TL;DR: 提出了一种名为Zipf-Gramming的新型top-k n-gram提取算法，比现有方法快35倍，能够在恶意软件检测中实现更频繁的模型更新和30%的AUC提升。


<details>
  <summary>Details</summary>
Motivation: 现有的字节n-gram分类器在恶意软件检测中效果良好，但更新模型耗时太长，无法从太字节级数据中快速提取最频繁的n-gram。

Method: 利用n-gram的Zipf分布特性开发新算法，通过理论分析和工程优化实现高效提取。

Result: 新算法比之前最佳方法快35倍，生产训练集扩展到更大规模，恶意软件检测AUC提升30%。

Conclusion: Zipf-Gramming算法成功解决了大规模n-gram提取的效率瓶颈，为恶意软件检测系统的实时更新提供了可行方案。

Abstract: A classifier using byte n-grams as features is the only approach we have found fast enough to meet requirements in size (sub 2 MB), speed (multiple GB/s), and latency (sub 10 ms) for deployment in numerous malware detection scenarios. However, we've consistently found that 6-8 grams achieve the best accuracy on our production deployments but have been unable to deploy regularly updated models due to the high cost of finding the top-k most frequent n-grams over terabytes of executable programs. Because the Zipfian distribution well models the distribution of n-grams, we exploit its properties to develop a new top-k n-gram extractor that is up to $35\times$ faster than the previous best alternative. Using our new Zipf-Gramming algorithm, we are able to scale up our production training set and obtain up to 30\% improvement in AUC at detecting new malware. We show theoretically and empirically that our approach will select the top-k items with little error and the interplay between theory and engineering required to achieve these results.

</details>


### [6] [The Battle of Metasurfaces: Understanding Security in Smart Radio Environments](https://arxiv.org/abs/2511.13939)
*Paul Staat,Christof Paar,Swarun Kumar*

Main category: cs.CR

TL;DR: 本文研究对称场景下双方都拥有可重构智能表面(RIS)能力的无线安全问题，通过理论和实验分析发现对立的RIS可能互相抵消效果，这对现有安全方案提出了挑战


<details>
  <summary>Details</summary>
Motivation: 传统研究主要关注单方面的RIS应用(攻击者或防御者)，本文首次研究双方都具有RIS能力的对称场景及相互作用

Method: 采用理论建模和现实实验相结合的方法，分析竞争性RIS在不同目标(信号功率、感知性能)下的相互作用

Result: 研究结果显示RIS对抗的结果取决于时机、位置、算法策略和硬件规模的相互作用，在Wi-Fi环境中的案例表明对立的RIS可能大幅或完全抵消彼此效果

Conclusion: 这些发现破坏了先前提出的安全和隐私方案，为设计智能无线电环境中具有弹性和高保障的物理层系统开辟了新机会

Abstract: Metasurfaces, or Reconfigurable Intelligent Surfaces (RISs), have emerged as a transformative technology for next-generation wireless systems, enabling digitally controlled manipulation of electromagnetic wave propagation. By turning the traditionally passive radio environment into a smart, programmable medium, metasurfaces promise advances in communication and sensing. However, metasurfaces also present a new security frontier: both attackers and defenders can exploit them to alter wireless propagation for their own advantage. While prior security research has primarily explored unilateral metasurface applications - empowering either attackers or defenders - this work investigates symmetric scenarios, where both sides possess comparable metasurface capabilities. Using both theoretical modeling and real-world experiments, we analyze how competing metasurfaces interact for diverse objectives, including signal power and sensing perception. Thereby, we present the first systematic study of context-agnostic metasurface-to-metasurface interactions and their implications for wireless security. Our results reveal that the outcome of metasurface "battles" depends on an interplay of timing, placement, algorithmic strategy, and hardware scale. Across multiple case studies in Wi-Fi environments, including wireless jamming, channel obfuscation for sensing and communication, and sensing spoofing, we demonstrate that opposing metasurfaces can substantially or fully negate each other's effects. By undermining previously proposed security and privacy schemes, our findings open new opportunities for designing resilient and high-assurance physical-layer systems in smart radio environments.

</details>


### [7] [Privis: Towards Content-Aware Secure Volumetric Video Delivery](https://arxiv.org/abs/2511.14005)
*Kaiyuan Hu,Hong Kang,Yili Jin,Junhua Liu,Chengming Hu,Haolun Wu,Xue Liu*

Main category: cs.CR

TL;DR: Privis is a content-aware security framework for volumetric video that uses saliency-guided encryption and selective traffic shaping to balance privacy protection with low latency requirements in XR applications.


<details>
  <summary>Details</summary>
Motivation: Volumetric video streaming lacks tailored security solutions. Current encryption schemes from 2D video don't address the heterogeneous privacy sensitivity of 3D geometry content or meet XR's strict latency constraints.

Method: Partitions volumetric assets into independent units, applies lightweight authenticated encryption with adaptive key rotation, and uses selective traffic shaping based on content saliency.

Result: A generalized transport-layer security architecture for volumetric media with core abstractions and adaptive protection mechanisms. Prototype implementation shows feasibility through initial latency measurements.

Conclusion: Privis provides an initial step toward real-time, saliency-conditioned secure delivery of volumetric video, offering design tradeoffs for future work in XR security.

Abstract: Volumetric video has emerged as a key paradigm in eXtended Reality (XR) and immersive multimedia because it enables highly interactive, spatially consistent 3D experiences. However, the transport-layer security for such 3D content remains largely unaddressed. Existing volumetric streaming pipelines inherit uniform encryption schemes from 2D video, overlooking the heterogeneous privacy sensitivity of different geometry and the strict motion-to-photon latency constraints of real-time XR.
  We take an initial step toward content-aware secure volumetric video delivery by introducing Privis, a saliency-guided transport framework that (i) partitions volumetric assets into independent units, (ii) applies lightweight authenticated encryption with adaptive key rotation, and (iii) employs selective traffic shaping to balance confidentiality and low latency. Privis specifies a generalized transport-layer security architecture for volumetric media, defining core abstractions and adaptive protection mechanisms. We further explore a prototype implementation and present initial latency measurements to illustrate feasibility and design tradeoffs, providing early empirical guidance toward future work on real-time, saliency-conditioned secure delivery.

</details>


### [8] [Location-Dependent Cryptosystem](https://arxiv.org/abs/2511.14032)
*Kunal Mukherjee*

Main category: cs.CR

TL;DR: This paper presents a location-dependent cryptosystem that encodes decryption keys in UWB packet timings rather than transmitting them directly, ensuring only receivers within a specific geographic area can decrypt content.


<details>
  <summary>Details</summary>
Motivation: Conventional encryption schemes are vulnerable to key leakage as decryption keys can be misused from any location once intercepted. There is a need for a system that binds decryption capability to physical location.

Method: The system uses ultra-wideband (UWB) data transmission with precise timing hardware and a custom JMTK protocol to map a SHA-256 hashed AES key onto packet transmission timestamps, creating location-dependent key reconstruction.

Result: The prototype successfully encrypts and transmits audio data, with decryption only possible when the receiver is within the authorized spatial region. Eavesdroppers outside this region observe incorrect keys.

Conclusion: The location-dependent cryptosystem eliminates the need for electronic key sharing, prevents key recovery by eavesdroppers, and provides spatial tolerance for legitimate users while maintaining security.

Abstract: Digital content distribution and proprietary research-driven industries face persistent risks from intellectual property theft and unauthorized redistribution. Conventional encryption schemes such as AES, TDES, ECC, and ElGamal provide strong cryptographic guarantees, but they remain fundamentally agnostic to where decryption takes place.In practice, this means that once a decryption key is leaked or intercepted, any adversary can misuse the key to decrypt the protected content from any location. We present a location-dependent cryptosystem in which the decryption key is not transmitted as human- or machine-readable data, but implicitly encoded in precise time-of-flight differences of ultra-wideband (UWB) data transmission packets. The system leverages precise timing hardware and a custom JMTK protocol to map a SHA-256 hashed AES key onto scheduled transmission timestamps. Only receivers located within a predefined spatial region can observe the packet timings that align with the intended "time slot" pattern, enabling them to reconstruct the key and decrypt the secret. Receivers outside the authorized region observe incorrect keys. We implement a complete prototype that encrypts and transmits audio data using our cryptosystem, and only when the receiver is within the authorized data, they are able to decrypt the data. Our evaluation demonstrates that the system (i) removes the need to share decryption passwords electronically or physically, (ii) ensures the decryption key cannot be recovered by the eavesdropper, and (iii) provides a non-trivial spatial tolerance for legitimate users.

</details>


### [9] [GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards](https://arxiv.org/abs/2511.14045)
*Yule Liu,Heyi Zhang,Jinyi Zheng,Zhen Sun,Zifan Peng,Tianshuo Cong,Yilong Yang,Xinlei He,Zhuo Ma*

Main category: cs.CR

TL;DR: 提出了首个针对RLVR训练框架的成员推理攻击方法DIBA，通过分析模型行为变化而非记忆效应来检测训练数据，在隐私审计上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: RLVR训练的在线性质带来了独特的隐私泄露模式：由于训练依赖自生成响应而无固定真值输出，成员推理需要判断提示是否用于微调，而非基于答案记忆。

Method: DIBA框架将焦点从记忆转向行为变化，利用模型在两个轴上的可测量偏移：优势侧改进（如正确性提升）和对数侧分歧（如策略漂移）。

Result: DIBA显著优于现有基线，达到约0.8的AUC和数量级更高的TPR@0.1%FPR，在多场景下保持鲁棒性，包括跨数据集、跨算法和黑盒设置。

Conclusion: 这是首个系统分析RLVR隐私漏洞的工作，表明即使没有显式监督，也可以通过行为轨迹可靠推断训练数据暴露，揭示了RLVR范式下的新型隐私风险。

Abstract: Membership inference attacks (MIAs) on large language models (LLMs) pose significant privacy risks across various stages of model training. Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have brought a profound paradigm shift in LLM training, particularly for complex reasoning tasks. However, the on-policy nature of RLVR introduces a unique privacy leakage pattern: since training relies on self-generated responses without fixed ground-truth outputs, membership inference must now determine whether a given prompt (independent of any specific response) is used during fine-tuning. This creates a threat where leakage arises not from answer memorization.
  To audit this novel privacy risk, we propose Divergence-in-Behavior Attack (DIBA), the first membership inference framework specifically designed for RLVR. DIBA shifts the focus from memorization to behavioral change, leveraging measurable shifts in model behavior across two axes: advantage-side improvement (e.g., correctness gain) and logit-side divergence (e.g., policy drift). Through comprehensive evaluations, we demonstrate that DIBA significantly outperforms existing baselines, achieving around 0.8 AUC and an order-of-magnitude higher TPR@0.1%FPR. We validate DIBA's superiority across multiple settings--including in-distribution, cross-dataset, cross-algorithm, black-box scenarios, and extensions to vision-language models. Furthermore, our attack remains robust under moderate defensive measures.
  To the best of our knowledge, this is the first work to systematically analyze privacy vulnerabilities in RLVR, revealing that even in the absence of explicit supervision, training data exposure can be reliably inferred through behavioral traces.

</details>


### [10] [Dynamic Black-box Backdoor Attacks on IoT Sensory Data](https://arxiv.org/abs/2511.14074)
*Ajesh Koyatan Chathoth,Stephen Lee*

Main category: cs.CR

TL;DR: Proposes a dynamic trigger-generation technique for black-box adversarial attacks on sensor-based IoT systems, showing effectiveness across datasets with minimal perturbations, comparing with other poisoning methods, and discussing defense mechanisms.


<details>
  <summary>Details</summary>
Motivation: Sensor data-based systems like gait authentication and HAR using deep learning are vulnerable to security risks, necessitating exploration of adversarial attack techniques.

Method: Development of a novel dynamic trigger-generation method for black-box adversarial attacks on IMU sensor data, tested on various datasets and classifier models.

Result: The attack successfully perturbs input data minimally while compromising system integrity across different models, with comparative analysis showing its stealthiness and performance advantages over other poisoning techniques.

Conclusion: The technique poses significant security threats to sensor-based IoT applications, highlighting the need for robust adversarial defense mechanisms to mitigate such attacks.

Abstract: Sensor data-based recognition systems are widely used in various applications, such as gait-based authentication and human activity recognition (HAR). Modern wearable and smart devices feature various built-in Inertial Measurement Unit (IMU) sensors, and such sensor-based measurements can be fed to a machine learning-based model to train and classify human activities. While deep learning-based models have proven successful in classifying human activity and gestures, they pose various security risks. In our paper, we discuss a novel dynamic trigger-generation technique for performing black-box adversarial attacks on sensor data-based IoT systems. Our empirical analysis shows that the attack is successful on various datasets and classifier models with minimal perturbation on the input data. We also provide a detailed comparative analysis of performance and stealthiness to various other poisoning techniques found in backdoor attacks. We also discuss some adversarial defense mechanisms and their impact on the effectiveness of our trigger-generation technique.

</details>


### [11] [Resolving Availability and Run-time Integrity Conflicts in Real-Time Embedded Systems](https://arxiv.org/abs/2511.14088)
*Adam Caulfield,Muhammad Wasif Kamran,N. Asokan*

Main category: cs.CR

TL;DR: PAIR提出了一种实时系统运行时完整性监控方法，通过硬件方式在检测到违规时仅中止违规任务，保证其他任务继续执行，实现安全性和可用性的平衡。


<details>
  <summary>Details</summary>
Motivation: 实时系统中运行时完整性检查与可用性存在冲突，现有方法要么牺牲安全性保证可用性，要么中止所有执行确保安全，需要中间解决方案。

Method: PAIR监控实时任务的运行时完整性违规，维护安全任务可用区域(AR)，检测到违规时通过不可屏蔽中断杀死违规任务，继续执行AR内非违规任务。

Result: PAIR采用硬件方法，对执行任务不产生运行时开销，与实时操作系统集成，内存和硬件使用开销仅增加2.3%，适用于低端微控制器。

Conclusion: PAIR在实时系统中实现了安全性和可用性的有效平衡，通过选择性中止违规任务，既保证了系统安全又不影响其他任务的执行时效性。

Abstract: Run-time integrity enforcement in real-time systems presents a fundamental conflict with availability. Existing approaches in real- time systems primarily focus on minimizing the execution-time overhead of monitoring. After a violation is detected, prior works face a trade-off: (1) prioritize availability and allow a compromised system to continue to ensure applications meet their deadlines, or (2) prioritize security by generating a fault to abort all execution. In this work, we propose PAIR, an approach that offers a middle ground between the stark extremes of this trade-off. PAIR monitors real-time tasks for run-time integrity violations and maintains an Availability Region (AR) of all tasks that are safe to continue. When a task causes a violation, PAIR triggers a non-maskable interrupt to kill the task and continue executing a non-violating task within AR. Thus, PAIR ensures only violating tasks are prevented from execution, while granting availability to remaining tasks. With its hardware approach, PAIR does not cause any run-time overhead to the executing tasks, integrates with real-time operating systems (RTOSs), and is affordable to low-end microcontroller units (MCUs) by incurring +2.3% overhead in memory and hardware usage.

</details>


### [12] [A Fuzzy Logic-Based Cryptographic Framework For Real-Time Dynamic Key Generation For Enhanced Data Encryption](https://arxiv.org/abs/2511.14132)
*Kavya Bhand,Payal Khubchandani,Jyoti Khubchandani*

Main category: cs.CR

TL;DR: Novel fuzzy logic-based cryptographic framework for dynamic real-time key generation using system entropy and hardware trust for adaptive encryption in high-security environments.


<details>
  <summary>Details</summary>
Motivation: Static key encryption is vulnerable to brute-force attacks and key compromise; need for adaptive, non-deterministic encryption solutions.

Method: Uses Fuzzy Inference System to evaluate CPU utilization, process count, and timestamp, combined with hardware randomness sealed by TPM, integrated with AES-GCM.

Result: Framework enables dynamic key generation from system parameters, enhancing security against traditional attacks.

Conclusion: Provides scalable, adaptive encryption solution suitable for zero-trust and cloud environments, improving resilience.

Abstract: With the ever-growing demand for cybersecurity, static key encryption mechanisms are increasingly vulnerable to adversarial attacks due to their deterministic and non-adaptive nature. Brute-force attacks, key compromise, and unauthorized access have become highly common cyber threats. This research presents a novel fuzzy logic-based cryptographic framework that dynamically generates encryption keys in real-time by accessing system-level entropy and hardware-bound trust. The proposed system leverages a Fuzzy Inference System (FIS) to evaluate system parameters that include CPU utilization, process count, and timestamp variation. It assigns entropy level based on linguistically defined fuzzy rules which are fused with hardware-generated randomness and then securely sealed using a Trusted Platform Module (TPM). The sealed key is incorporated in an AES-GCM encryption scheme to ensure both confidentiality and integrity of the data. This system introduces a scalable solution for adaptive encryption in high-assurance computing, zero-trust environments, and cloud-based infrastructure.

</details>


### [13] [Beyond Fixed and Dynamic Prompts: Embedded Jailbreak Templates for Advancing LLM Security](https://arxiv.org/abs/2511.14140)
*Hajun Kim,Hyunsik Na,Daeseon Choi*

Main category: cs.CR

TL;DR: Introduces Embedded Jailbreak Template for safer LLM testing by embedding harmful queries in existing templates, with protocols for generation/evaluation.


<details>
  <summary>Details</summary>
Motivation: Current jailbreak attack templates are limited (fixed templates or LLM-generated ones lacking clarity), needing better methods for red-teaming and defense.

Method: Proposes Embedded Jailbreak Template that retains template structure while embedding harmful queries, using progressive prompt-engineering for quality control.

Result: Provides a benchmark reflecting real-world scenarios for improved red-teaming and policy testing.

Conclusion: The approach enhances safety evaluation accuracy and supports robust defensive measures against jailbreak attacks.

Abstract: As the use of large language models (LLMs) continues to expand, ensuring their safety and robustness has become a critical challenge. In particular, jailbreak attacks that bypass built-in safety mechanisms are increasingly recognized as a tangible threat across industries, driving the need for diverse templates to support red-teaming efforts and strengthen defensive techniques. However, current approaches predominantly rely on two limited strategies: (i) substituting harmful queries into fixed templates, and (ii) having the LLM generate entire templates, which often compromises intent clarity and reproductibility. To address this gap, this paper introduces the Embedded Jailbreak Template, which preserves the structure of existing templates while naturally embedding harmful queries within their context. We further propose a progressive prompt-engineering methodology to ensure template quality and consistency, alongside standardized protocols for generation and evaluation. Together, these contributions provide a benchmark that more accurately reflects real-world usage scenarios and harmful intent, facilitating its application in red-teaming and policy regression testing.

</details>


### [14] [Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion](https://arxiv.org/abs/2511.14301)
*Eric Xue,Ruiyi Zhang,Zijun Zhang,Pengtao Xie*

Main category: cs.CR

TL;DR: SteganoBackdoor is a stealthy backdoor attack method that uses semantic triggers optimized via steganography techniques to embed hidden behaviors in Transformer models, achieving high attack success with low poisoning rates while evading detection.


<details>
  <summary>Details</summary>
Motivation: Current backdoor research focuses on artificial triggers, neglecting the realistic threat of semantic triggers (e.g., specific names/entities) that could manipulate real-world systems. SteganoBackdoor addresses this gap by aligning stealth techniques with practical threats.

Method: Leverages natural-language steganography and gradient-guided data optimization to transform semantic triggers into steganographic carriers that embed backdoor payloads while maintaining fluency and avoiding detectable patterns.

Result: Achieves >99% attack success rate with significantly lower data-poisoning rates than prior methods, while effectively evading various data-level defenses in diverse experimental settings.

Conclusion: SteganoBackdoor exposes a critical vulnerability in current defenses, highlighting the need for improved adversarial data defenses and more realistic threat modeling focused on semantic triggers.

Abstract: Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.

</details>


### [15] [Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection](https://arxiv.org/abs/2511.14422)
*Zhengchunmin Dai,Jiaxiong Tang,Peng Sun,Honglong Chen,Liantao Wu*

Main category: cs.CR

TL;DR: Sigil is a mandatory watermarking framework for capability-limited servers in decentralized ML (like SFL/U-shaped SFL) that embeds watermarks via gradient injection as statistical constraints on activation space, without requiring data knowledge or client cooperation.


<details>
  <summary>Details</summary>
Motivation: Capability-limited servers in decentralized ML are vulnerable to model theft by malicious clients, but existing watermarking methods are either unreliable (client-dependent) or infeasible (requires server capabilities).

Method: Defines watermark as statistical constraint on server-visible activation space, embeds via gradient injection with adaptive gradient clipping for stealth and mandatory enforcement.

Result: Experimental results on multiple datasets/models show Sigil maintains model fidelity, provides robustness against attacks (including adaptive subspace removal), and remains stealthy against detection methods.

Conclusion: Sigil effectively protects server IP in capability-limited decentralized ML settings through mandatory, stealthy watermarking that requires no client cooperation or server data access.

Abstract: In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels.
  To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.

</details>


### [16] [SecureSign: Bridging Security and UX in Mobile Web3 through Emulated EIP-6963 Sandboxing](https://arxiv.org/abs/2511.14611)
*Charles Cheng Ji,Brandon Kong*

Main category: cs.CR

TL;DR: SecureSign is a PWA-based mobile Web3 architecture that eliminates the tradeoff between security and usability by using EIP-6963 provider sandboxing to isolate dApps in iframes, achieving click-jacking immunity while maintaining native mobile features.


<details>
  <summary>Details</summary>
Motivation: Mobile Web3 faces catastrophic user retention (<5%) due to security-usability tradeoffs: embedded wallets are vulnerable to click-jacking, while app wallets suffer from 2-3% retention loss due to download friction.

Method: Adapts desktop browser extension security to mobile via EIP-6963 provider sandboxing, isolating dApp execution in iframes within a trusted parent PWA application with a drop-in SDK requiring no code changes.

Result: Achieves immunity to click-jacking, overlay, and skimming attacks while maintaining wallet interoperability and native mobile capabilities like push notifications and zero context-switching.

Conclusion: SecureSign provides a viable solution to mobile Web3's retention problem by delivering both security and usability without compromising on native mobile experience or requiring dApp modifications.

Abstract: Mobile Web3 faces catastrophic retention (< 5%) yielding effective acquisition costs of \$500 - \$1,000 per retained user. Existing solutions force an impossible tradeoff: embedded wallets achieve moderate usability but suffer inherent click-jacking vulnerabilities; app wallets maintain security at the cost of 2 - 3% retention due to download friction and context-switching penalties. We present SecureSign, a PWA-based architecture that adapts desktop browser extension security to mobile via EIP-6963 provider sandboxing. SecureSign isolates dApp execution in iframes within a trusted parent application, achieving click-jacking immunity and transaction integrity while enabling native mobile capabilities (push notifications, home screen installation, zero context-switching). Our drop-in SDK requires no codebase changes for existing Web3 applications. Threat model analysis demonstrates immunity to click-jacking, overlay, and skimming attacks while maintaining wallet interoperability across dApps.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [Imagine in Space: Exploring the Frontier of Spatial Intelligence and Reasoning Efficiency in Vision Language Models](https://arxiv.org/abs/2511.13782)
*Xiaoxing Lian,Aidong Yang,Jun Zhu,Peng Wang,Yue Zhang*

Main category: cs.AI

TL;DR: SpatiaLite is a synthetic benchmark that reveals VLMs rely more on linguistic representations than visual-spatial reasoning, show inefficiency in complex spatial tasks, and proposes an Imagery Driven Framework to improve spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: Current VLMs excel in many reasoning tasks but struggle with spatial reasoning like mental rotation and navigation, which requires internal simulation of spatial states.

Method: Introduced SpatiaLite benchmark to measure spatial reasoning accuracy and efficiency, revealing VLMs' reliance on linguistic representations and proposing Imagery Driven Framework for improvement.

Result: VLMs show significant deficiencies in visual-centric spatial tasks, inefficiency with complex transformations, but IDF framework shows promise for developing spatial reasoning capabilities.

Conclusion: SpatiaLite identifies key limitations in VLM spatial reasoning and provides a pathway for developing more effective spatial reasoning mechanisms through world model construction.

Abstract: Large language models (LLMs) and vision language models (VLMs), such as DeepSeek R1,OpenAI o3, and Gemini 2.5 Pro, have demonstrated remarkable reasoning capabilities across logical inference, problem solving, and decision making. However, spatial reasoning:a fundamental component of human cognition that includes mental rotation, navigation, and spatial relationship comprehension remains a significant challenge for current advanced VLMs. We hypothesize that imagination, the internal simulation of spatial states, is the dominant reasoning mechanism within a spatial world model. To test this hypothesis and systematically probe current VLM spatial reasoning mechanisms, we introduce SpatiaLite, a fully synthetic benchmark that jointly measures spatial reasoning accuracy and reasoning efficiency. Comprehensive experiments reveal three key findings. First, advanced VLMs predominantly rely on linguistic representations for reasoning and imagination, resulting in significant deficiencies on visual centric tasks that demand perceptual spatial relations and 3D geometry transformations such as mental rotation or projection prediction. Second, advanced VLMs exhibit severe inefficiency in their current spatial reasoning mechanisms, with token usage growing rapidly as transformation complexity increases. Third, we propose an Imagery Driven Framework (IDF) for data synthesis and training, which can implicitly construct an internal world model that is critical for spatial reasoning in VLMs. Building on SpatiaLite, this work delineates the spatial reasoning limits and patterns of advanced VLMs, identifies key shortcomings, and informs future advances

</details>


### [18] [Causal computations in Semi Markovian Structural Causal Models using divide and conquer](https://arxiv.org/abs/2511.13852)
*Anna Rodum Bjøru,Rafael Cabañas,Helge Langseth,Antonio Salmerón*

Main category: cs.AI

TL;DR: This paper extends Bjøru et al.'s divide-and-conquer algorithm for bounding counterfactual probabilities from Markovian to semi-Markovian SCMs, identifies challenges via a minimal example, and proposes and evaluates alternative solution strategies.


<details>
  <summary>Details</summary>
Motivation: To extend a recent algorithm for bounding counterfactual probabilities in Markovian Structural Causal Models (SCMs) to semi-Markovian SCMs, which can represent confounding relationships not possible in Markovian models.

Method: The paper investigates the extension by illustrating the challenges with a minimal example. Based on these challenges, it motivates and develops a set of alternative solution strategies.

Result: The proposed strategies are evaluated both theoretically and through a computational study.

Conclusion: The research presents and assesses methods to adapt the bounding algorithm for the more complex semi-Markovian setting.

Abstract: Recently, Bjøru et al. proposed a novel divide-and-conquer algorithm for bounding counterfactual probabilities in structural causal models (SCMs). They assumed that the SCMs were learned from purely observational data, leading to an imprecise characterization of the marginal distributions of exogenous variables. Their method leveraged the canonical representation of structural equations to decompose a general SCM with high-cardinality exogenous variables into a set of sub-models with low-cardinality exogenous variables. These sub-models had precise marginals over the exogenous variables and therefore admitted efficient exact inference. The aggregated results were used to bound counterfactual probabilities in the original model. The approach was developed for Markovian models, where each exogenous variable affects only a single endogenous variable. In this paper, we investigate extending the methodology to \textit{semi-Markovian} SCMs, where exogenous variables may influence multiple endogenous variables. Such models are capable of representing confounding relationships that Markovian models cannot. We illustrate the challenges of this extension using a minimal example, which motivates a set of alternative solution strategies. These strategies are evaluated both theoretically and through a computational study.

</details>


### [19] [Jailbreaking Large Vision Language Models in Intelligent Transportation Systems](https://arxiv.org/abs/2511.13892)
*Badhan Chandra Das,Md Tasnim Jawad,Md Jueal Mia,M. Hadi Amini,Yanzhao Wu*

Main category: cs.AI

TL;DR: This paper analyzes vulnerabilities of Large Vision Language Models (LVLMs) in Intelligent Transportation Systems (ITS) to jailbreaking attacks, proposing a new attack method using image typography manipulation and multi-turn prompting, and a multi-layered defense technique.


<details>
  <summary>Details</summary>
Motivation: LVLMs are increasingly used in ITS applications but are vulnerable to jailbreaking attacks that could lead to inappropriate responses, highlighting the need for systematic security analysis and robust defenses.

Method: The paper presents three main contributions: 1) Construction of a harmful query dataset for transportation, 2) A novel jailbreaking attack combining image typography manipulation and multi-turn prompting, 3) A multi-layered response filtering defense technique.

Result: Extensive experiments on state-of-the-art LVLMs show the effectiveness of the proposed attack method, which outperforms existing techniques, and the defense method's ability to mitigate risks. Evaluation uses GPT-4 judgment and manual verification.

Conclusion: The study reveals severe security risks in LVLMs integrated in ITS and demonstrates that the proposed defense can effectively protect against sophisticated jailbreaking attacks.

Abstract: Large Vision Language Models (LVLMs) demonstrate strong capabilities in multimodal reasoning and many real-world applications, such as visual question answering. However, LVLMs are highly vulnerable to jailbreaking attacks. This paper systematically analyzes the vulnerabilities of LVLMs integrated in Intelligent Transportation Systems (ITS) under carefully crafted jailbreaking attacks. First, we carefully construct a dataset with harmful queries relevant to transportation, following OpenAI's prohibited categories to which the LVLMs should not respond. Second, we introduce a novel jailbreaking attack that exploits the vulnerabilities of LVLMs through image typography manipulation and multi-turn prompting. Third, we propose a multi-layered response filtering defense technique to prevent the model from generating inappropriate responses. We perform extensive experiments with the proposed attack and defense on the state-of-the-art LVLMs (both open-source and closed-source). To evaluate the attack method and defense technique, we use GPT-4's judgment to determine the toxicity score of the generated responses, as well as manual verification. Further, we compare our proposed jailbreaking method with existing jailbreaking techniques and highlight severe security risks involved with jailbreaking attacks with image typography manipulation and multi-turn prompting in the LVLMs integrated in ITS.

</details>


### [20] [Making Evidence Actionable in Adaptive Learning](https://arxiv.org/abs/2511.14052)
*Amirreza Mehrabi,Jason W. Morphew,Breejha Quezada,N. Sanjay Rebello*

Main category: cs.AI

TL;DR: 本研究提出了一种教师主导的自适应学习算法，通过三个保障机制（充分性、注意力、多样性）将概念评估转化为精准的微干预，在物理课程中验证了其有效性


<details>
  <summary>Details</summary>
Motivation: 解决自适应学习中诊断准确但干预薄弱的问题，传统方法常常提供时机不当或内容不匹配的帮助

Method: 将干预分配建模为带约束的二元整数规划问题，包含覆盖率、时间、难度窗口等约束，提出贪婪选择、梯度松弛和混合方法三种求解策略

Result: 在1204名学生的物理课程部署中，两种方法都能在有限时间内实现几乎所有学习者的全技能覆盖，梯度方法比贪婪方法减少冗余覆盖约12%

Conclusion: 该算法实现了可追溯、可审计的控制系统，闭合了诊断-教学循环，在课堂规模下实现了公平且负载感知的个性化学习

Abstract: Adaptive learning often diagnoses precisely yet intervenes weakly, yielding help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted micro-interventions. The adaptive learning algorithm contains three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted constraint for time and redundancy, and diversity as protection against overfitting to a single resource. We formalize intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows informed by ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy enforced through diversity. Greedy selection serves low-richness and tight-latency regimes, gradient-based relaxation serves rich repositories, and a hybrid method transitions along a richness-latency frontier. In simulation and in an introductory physics deployment with one thousand two hundred four students, both solvers achieved full skill coverage for essentially all learners within bounded watch time. The gradient-based method reduced redundant coverage by approximately twelve percentage points relative to greedy and harmonized difficulty across slates, while greedy delivered comparable adequacy with lower computational cost in scarce settings. Slack variables localized missing content and supported targeted curation, sustaining sufficiency across subgroups. The result is a tractable and auditable controller that closes the diagnostic-pedagogical loop and delivers equitable, load-aware personalization at classroom scale.

</details>


### [21] [CORGI: Efficient Pattern Matching With Quadratic Guarantees](https://arxiv.org/abs/2511.13942)
*Daniel Weitekamp*

Main category: cs.AI

TL;DR: 引入CORGI算法解决基于规则的系统中模式匹配的时间空间问题，相比RETE算法有更好的性能保证


<details>
  <summary>Details</summary>
Motivation: 实时AI系统和数据库查询需要高效的模式匹配，但自动生成的规则容易导致指数级复杂度

Method: CORGI采用两步法：前向构建关系图，后向迭代生成匹配，无需传统β内存

Result: 在组合匹配任务上显著优于SOAR和OPS5的RETE实现

Conclusion: CORGI为自动生成的规则提供了实用的匹配方案，避免内存溢出

Abstract: Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $β$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.

</details>


### [22] [Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios](https://arxiv.org/abs/2511.13970)
*Sanjay Acharjee,Abir Khan Ratul,Diego Patino,Md Nazmus Sakib*

Main category: cs.AI

TL;DR: A framework using scene graphs and AI to generate realistic workplace hazard images from OSHA reports, with a novel VQA-based evaluation metric.


<details>
  <summary>Details</summary>
Motivation: Difficulty in obtaining real images of workplace accidents necessitates synthetic data generation for training safety detection models.

Method: GPT-4o analyzes OSHA reports to create scene graphs, which guide a diffusion model to generate hazard images. A VQA framework evaluates realism.

Result: The proposed VQA Graph Score outperformed CLIP and BLIP metrics in evaluating generated image quality across four generative models.

Conclusion: The framework successfully generates realistic hazard images and introduces a more sensitive evaluation metric for synthetic safety data.

Abstract: Training vision models to detect workplace hazards accurately requires realistic images of unsafe conditions that could lead to accidents. However, acquiring such datasets is difficult because capturing accident-triggering scenarios as they occur is nearly impossible. To overcome this limitation, this study presents a novel scene graph-guided generative AI framework that synthesizes photorealistic images of hazardous scenarios grounded in historical Occupational Safety and Health Administration (OSHA) accident reports. OSHA narratives are analyzed using GPT-4o to extract structured hazard reasoning, which is converted into object-level scene graphs capturing spatial and contextual relationships essential for understanding risk. These graphs guide a text-to-image diffusion model to generate compositionally accurate hazard scenes. To evaluate the realism and semantic fidelity of the generated data, a visual question answering (VQA) framework is introduced. Across four state-of-the-art generative models, the proposed VQA Graph Score outperforms CLIP and BLIP metrics based on entropy-based validation, confirming its higher discriminative sensitivity.

</details>


### [23] [Artificial Intelligence Agents in Music Analysis: An Integrative Perspective Based on Two Use Cases](https://arxiv.org/abs/2511.13987)
*Antonio Manuel Martínez-Heredia,Dolores Godrid Rodríguez,Andrés Ortiz García*

Main category: cs.AI

TL;DR: Review and experimental validation of AI agents for music analysis and education, covering evolution from rule-based to deep learning models and their pedagogical applications.


<details>
  <summary>Details</summary>
Motivation: To synthesize AI's historical progression in music and evaluate its educational implications through case studies.

Method: Integrative review and dual-case methodology: generative AI in secondary education and multi-agent system for symbolic music analysis.

Result: AI agents enhance musical pattern recognition, composition, and education feedback, outperforming traditional methods in interpretability and adaptability.

Conclusion: Research provides a framework bridging technical, pedagogical, and ethical aspects for AI in musicology and education, highlighting challenges like transparency and bias.

Abstract: This paper presents an integrative review and experimental validation of artificial intelligence (AI) agents applied to music analysis and education. We synthesize the historical evolution from rule-based models to contemporary approaches involving deep learning, multi-agent architectures, and retrieval-augmented generation (RAG) frameworks. The pedagogical implications are evaluated through a dual-case methodology: (1) the use of generative AI platforms in secondary education to foster analytical and creative skills; (2) the design of a multiagent system for symbolic music analysis, enabling modular, scalable, and explainable workflows.
  Experimental results demonstrate that AI agents effectively enhance musical pattern recognition, compositional parameterization, and educational feedback, outperforming traditional automated methods in terms of interpretability and adaptability. The findings highlight key challenges concerning transparency, cultural bias, and the definition of hybrid evaluation metrics, emphasizing the need for responsible deployment of AI in educational environments.
  This research contributes to a unified framework that bridges technical, pedagogical, and ethical considerations, offering evidence-based guidance for the design and application of intelligent agents in computational musicology and music education.

</details>


### [24] [ALEX:A Light Editing-knowledge Extractor](https://arxiv.org/abs/2511.14018)
*Minghu Wang,Shuliang Zhao,Yuanyuan Zhao,Hongxia Xu*

Main category: cs.AI

TL;DR: This paper introduces ALEX, a lightweight knowledge editing framework that improves LLM adaptability through hierarchical memory architecture and two-stage retrieval, achieving better accuracy and efficiency on multi-hop questions.


<details>
  <summary>Details</summary>
Motivation: Static knowledge in LLMs makes them struggle with evolving information. Current knowledge editing methods face scalability and retrieval efficiency issues, especially for complex multi-hop questions requiring multi-step reasoning.

Method: Hierarchical memory architecture organizing knowledge edits into semantic clusters; Inferential Query Synthesis module to bridge semantic gaps; Dynamic Evidence Adjudication engine for two-stage retrieval.

Result: Significantly improves multi-hop answer accuracy (MultiHop-ACC) and reasoning reliability (HopWise-ACC) on MQUAKE benchmark while reducing search space by over 80%.

Conclusion: ALEX presents a promising path toward building scalable, efficient, and accurate knowledge editing systems by fundamentally reducing retrieval complexity from O(N) to O(K+N/C).

Abstract: The static nature of knowledge within Large Language Models (LLMs) makes it difficult for them to adapt to evolving information, rendering knowledge editing a critical task. However, existing methods struggle with challenges of scalability and retrieval efficiency, particularly when handling complex, multi-hop questions that require multi-step reasoning. To address these challenges, this paper introduces ALEX (A Light Editing-knowledge Extractor), a lightweight knowledge editing framework. The core innovation of ALEX is its hierarchical memory architecture, which organizes knowledge updates (edits) into semantic clusters. This design fundamentally reduces retrieval complexity from a linear O(N) to a highly scalable O(K+N/C). Furthermore, the framework integrates an Inferential Query Synthesis (IQS) module to bridge the semantic gap between queries and facts , and a Dynamic Evidence Adjudication (DEA) engine that executes an efficient two-stage retrieval process. Experiments on the MQUAKE benchmark demonstrate that ALEX significantly improves both the accuracy of multi-hop answers (MultiHop-ACC) and the reliability of reasoning paths (HopWise-ACC). It also reduces the required search space by over 80% , presenting a promising path toward building scalable, efficient, and accurate knowledge editing systems.

</details>


### [25] [Syn-STARTS: Synthesized START Triage Scenario Generation Framework for Scalable LLM Evaluation](https://arxiv.org/abs/2511.14023)
*Chiharu Hagiwara,Naoki Nonaka,Yuhta Hashimoto,Ryu Uchimido,Jun Seita*

Main category: cs.AI

TL;DR: Syn-STARTS is an LLM-based framework for generating synthetic triage cases that are indistinguishable from real datasets, enabling AI model development for mass casualty incidents where real data is scarce.


<details>
  <summary>Details</summary>
Motivation: Mass casualty incidents (MCIs) are rare but require efficient triage systems. AI can optimize triage decisions but lacks sufficient real-world training data due to the infrequency of MCIs.

Method: Developed Syn-STARTS framework using Large Language Models (LLMs) to generate synthetic triage cases. Compared quality against manually curated TRIAGE dataset and evaluated LLM accuracy across different triage categories (green, yellow, red, black) using START method.

Result: Synthetic cases were qualitatively indistinguishable from real datasets. LLM accuracy remained highly stable across all triage categories, demonstrating reliable performance.

Conclusion: Syn-STARTS enables creation of high-quality synthetic data for developing AI triage systems, addressing data scarcity in critical medical scenarios.

Abstract: Triage is a critically important decision-making process in mass casualty incidents (MCIs) to maximize victim survival rates. While the role of AI in such situations is gaining attention for making optimal decisions within limited resources and time, its development and performance evaluation require benchmark datasets of sufficient quantity and quality. However, MCIs occur infrequently, and sufficient records are difficult to accumulate at the scene, making it challenging to collect large-scale realworld data for research use. Therefore, we developed Syn-STARTS, a framework that uses LLMs to generate triage cases, and verified its effectiveness. The results showed that the triage cases generated by Syn-STARTS were qualitatively indistinguishable from the TRIAGE open dataset generated by manual curation from training materials. Furthermore, when evaluating the LLM accuracy using hundreds of cases each from the green, yellow, red, and black categories defined by the standard triage method START, the results were found to be highly stable. This strongly indicates the possibility of synthetic data in developing high-performance AI models for severe and critical medical situations.

</details>


### [26] [Collaborative QA using Interacting LLMs. Impact of Network Structure, Node Capability and Distributed Data](https://arxiv.org/abs/2511.14098)
*Adit Jain,Vikram Krishnamurthy,Yiming Zhang*

Main category: cs.AI

TL;DR: This paper models how networks of LLMs collaboratively answer questions, analyzing how hallucinations spread through the network using mean-field dynamics and randomized utility models.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) often hallucinate when evidence is lacking, and these hallucinations become more pronounced and spread in networks of interacting LLMs, affecting previously accurate models.

Method: Combines mean-field dynamics from network science and randomized utility models from economics to create a generative model. Each LLM has a latent state (truthful/hallucinating), and the model analyzes information diffusion in directed networks.

Result: For networks where each LLM has two latent states, sufficient conditions for fixed point existence/uniqueness are established. Behavior is analyzed concerning individual LLM incentives (e.g., test-time compute). Experimental analysis on 100 open-source LLMs examines data heterogeneity, node capability, network structure, and framing sensitivity.

Conclusion: The framework provides a tractable way to model and analyze hallucination propagation in LLM networks, with implications for understanding and controlling collaborative AI system behavior.

Abstract: In this paper, we model and analyze how a network of interacting LLMs performs collaborative question-answering (CQA) in order to estimate a ground truth given a distributed set of documents. This problem is interesting because LLMs often hallucinate when direct evidence to answer a question is lacking, and these effects become more pronounced in a network of interacting LLMs. The hallucination spreads, causing previously accurate LLMs to hallucinate. We study interacting LLMs and their hallucination by combining novel ideas of mean-field dynamics (MFD) from network science and the randomized utility model from economics to construct a useful generative model. We model the LLM with a latent state that indicates if it is truthful or not with respect to the ground truth, and extend a tractable analytical model considering an MFD to model the diffusion of information in a directed network of LLMs. To specify the probabilities that govern the dynamics of the MFD, we propose a randomized utility model. For a network of LLMs, where each LLM has two possible latent states, we posit sufficient conditions for the existence and uniqueness of a fixed point and analyze the behavior of the fixed point in terms of the incentive (e.g., test-time compute) given to individual LLMs. We experimentally study and analyze the behavior of a network of $100$ open-source LLMs with respect to data heterogeneity, node capability, network structure, and sensitivity to framing on multiple semi-synthetic datasets.

</details>


### [27] [APD-Agents: A Large Language Model-Driven Multi-Agents Collaborative Framework for Automated Page Design](https://arxiv.org/abs/2511.14101)
*Xinpeng Chen,Xiaofeng Han,Kaihao Zhang,Guochao Ren,Yujie Wang,Wenhao Cao,Yang Zhou,Jianfeng Lu,Zhenbo Song*

Main category: cs.AI

TL;DR: APD-agents是一个基于大语言模型的移动应用页面自动化设计多智能体框架，能够根据用户描述自动生成移动应用页面布局设计


<details>
  <summary>Details</summary>
Motivation: 移动应用页面布局设计需要设计师花费大量时间考虑控件布局、调整尺寸位置和样式，现有设计软件需要专业培训，跨页面协作设计需要额外时间统一标准

Method: 提出APD-agents框架，包含五个智能体：OrchestratorAgent动态协调任务，SemanticParserAgent解析用户描述为结构化数据，PrimaryLayoutAgent生成粗粒度布局，TemplateRetrievalAgent获取相关示例，RecursiveComponentAgent递归生成细粒度子元素

Result: 在RICO数据集上的实验结果表明，APD-agents达到了最先进的性能表现

Conclusion: 该工作充分利用了大模型驱动的多智能体系统的自动协作能力，能够有效自动化移动应用页面设计流程

Abstract: Layout design is a crucial step in developing mobile app pages. However, crafting satisfactory designs is time-intensive for designers: they need to consider which controls and content to present on the page, and then repeatedly adjust their size, position, and style for better aesthetics and structure. Although many design software can now help to perform these repetitive tasks, extensive training is needed to use them effectively. Moreover, collaborative design across app pages demands extra time to align standards and ensure consistent styling. In this work, we propose APD-agents, a large language model (LLM) driven multi-agent framework for automated page design in mobile applications. Our framework contains OrchestratorAgent, SemanticParserAgent, PrimaryLayoutAgent, TemplateRetrievalAgent, and RecursiveComponentAgent. Upon receiving the user's description of the page, the OrchestratorAgent can dynamically can direct other agents to accomplish users' design task. To be specific, the SemanticParserAgent is responsible for converting users' descriptions of page content into structured data. The PrimaryLayoutAgent can generate an initial coarse-grained layout of this page. The TemplateRetrievalAgent can fetch semantically relevant few-shot examples and enhance the quality of layout generation. Besides, a RecursiveComponentAgent can be used to decide how to recursively generate all the fine-grained sub-elements it contains for each element in the layout. Our work fully leverages the automatic collaboration capabilities of large-model-driven multi-agent systems. Experimental results on the RICO dataset show that our APD-agents achieve state-of-the-art performance.

</details>


### [28] [Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation](https://arxiv.org/abs/2511.14131)
*Yu Zhong,Zihao Zhang,Rui Zhang,Lingdong Huang,Haihan Gao,Shuo Wang,Da Li,Ruijian Han,Jiaming Guo,Shaohui Peng,Di Huang,Yunji Chen*

Main category: cs.AI

TL;DR: R3是一个用于视觉与语言导航(VLN)的双过程思维框架，结合了轻量级专家模型和大型语言模型的优势，通过Runner、Ruminator和Regulator三个模块协同工作，在REVERIE基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的VLN方法存在两个主要问题：1）LLM难以精确理解真实世界的空间关系，与领域专家存在性能差距；2）LLM带来巨大的计算成本和推理延迟。

Method: 提出了R3框架，包含三个核心模块：Runner（轻量级Transformer专家模型，负责常规导航）、Ruminator（基于多模态LLM，采用思维链提示进行结构化推理）、Regulator（根据三个标准监控导航进度并控制思维模式）。

Result: 在REVERIE基准测试中，R3显著优于其他最先进方法，SPL和RGSPL分别提高了3.28%和3.30%。

Conclusion: R3框架通过整合LLM的泛化能力和VLN特定专业知识，在零样本设置下有效解决了VLN任务中的挑战，证明了双过程思维框架的有效性。

Abstract: Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.

</details>


### [29] [HFL-FlowLLM: Large Language Models for Network Traffic Flow Classification in Heterogeneous Federated Learning](https://arxiv.org/abs/2511.14199)
*Jiazhuo Tian,Yachao Yuan*

Main category: cs.AI

TL;DR: HFL-FlowLLM is a novel framework that applies large language models to network traffic flow classification in heterogeneous federated learning, improving F1 scores by 13% over SOTA methods while reducing training costs by 87%.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional centralized ML (privacy issues) and existing federated learning (high costs, poor generalization) for network traffic classification in 5G/IoT environments.

Method: Proposes HFL-FlowLLM, the first framework to apply large language models to network traffic flow classification within a heterogeneous federated learning setting.

Result: Achieves approximately 13% improvement in average F1 score compared to state-of-the-art heterogeneous FL methods, and up to 5% improvement with 87% reduction in training costs compared to existing LLM FL frameworks.

Conclusion: HFL-FlowLLM demonstrates strong potential and practical value for security applications in modern communication networks, showing compelling performance and robustness.

Abstract: In modern communication networks driven by 5G and the Internet of Things (IoT), effective network traffic flow classification is crucial for Quality of Service (QoS) management and security. Traditional centralized machine learning struggles with the distributed data and privacy concerns in these heterogeneous environments, while existing federated learning approaches suffer from high costs and poor generalization. To address these challenges, we propose HFL-FlowLLM, which to our knowledge is the first framework to apply large language models to network traffic flow classification in heterogeneous federated learning. Compared to state-of-the-art heterogeneous federated learning methods for network traffic flow classification, the proposed approach improves the average F1 score by approximately 13%, demonstrating compelling performance and strong robustness. When compared to existing large language models federated learning frameworks, as the number of clients participating in each training round increases, the proposed method achieves up to a 5% improvement in average F1 score while reducing the training costs by about 87%. These findings prove the potential and practical value of HFL-FlowLLM in modern communication networks security.

</details>


### [30] [Do Large Language Models (LLMs) Understand Chronology?](https://arxiv.org/abs/2511.14214)
*Pattaraphon Kenny Wongchamcharoen,Paul Glasserman*

Main category: cs.AI

TL;DR: This study evaluates LLMs' ability to understand chronology through ordering tasks, finding performance declines with complexity but improves with explicit reasoning budgets, especially with GPT-5 and Claude-3.7 Sonnet with Extended Thinking.


<details>
  <summary>Details</summary>
Motivation: To test if LLMs fundamentally understand chronology, which is crucial for finance applications where look-ahead bias is a concern.

Method: Evaluated GPT-4.1, Claude-3.7 Sonnet (with/without Extended Thinking), and GPT-5 on chronological ordering, conditional sorting, and anachronism detection tasks with varying complexity.

Result: LLMs struggle with globally consistent timelines as sequence length increases; conditional sorting failures mainly from filtering; anachronism detection is easiest but declines with complexity. GPT-5 with medium/high reasoning effort achieved perfect performance.

Conclusion: Explicit reasoning allocation improves chronological understanding, revealing current LLM limitations and providing insights for financial applications.

Abstract: Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.

</details>


### [31] [Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation](https://arxiv.org/abs/2511.14219)
*Kumud Tripathi,Aditya Srinivas Menon,Aman Gaurav,Raj Prakash Gohil,Pankaj Wasnik*

Main category: cs.AI

TL;DR: 提出了一个两阶段架构来减少Whisper ASR系统中的幻觉错误，包括自适应层注意力和多目标知识蒸馏框架。


<details>
  <summary>Details</summary>
Motivation: Whisper模型在噪声条件下经常出现幻觉错误，之前的研究主要关注预处理或后处理，而对模型本身的修改研究不足。

Method: 第一阶段使用自适应层注意力增强编码器鲁棒性；第二阶段使用多目标知识蒸馏框架抑制幻觉。

Result: 在噪声语音基准测试中显示幻觉错误和词错误率显著降低，同时保持干净语音的性能。

Conclusion: 提出的方法为改善Whisper在真实噪声条件下的可靠性提供了一个有原则的策略。

Abstract: The Whisper model, an open-source automatic speech recognition system, is widely adopted for its strong performance across multilingual and zero-shot settings. However, it frequently suffers from hallucination errors, especially under noisy acoustic conditions. Previous works to reduce hallucinations in Whisper-style ASR systems have primarily focused on audio preprocessing or post-processing of transcriptions to filter out erroneous content. However, modifications to the Whisper model itself remain largely unexplored to mitigate hallucinations directly. To address this challenge, we present a two-stage architecture that first enhances encoder robustness through Adaptive Layer Attention (ALA) and further suppresses hallucinations using a multi-objective knowledge distillation (KD) framework. In the first stage, ALA groups encoder layers into semantically coherent blocks via inter-layer correlation analysis. A learnable multi-head attention module then fuses these block representations, enabling the model to jointly exploit low- and high-level features for more robust encoding. In the second stage, our KD framework trains the student model on noisy audio to align its semantic and attention distributions with a teacher model processing clean inputs. Our experiments on noisy speech benchmarks show notable reductions in hallucinations and word error rates, while preserving performance on clean speech. Together, ALA and KD offer a principled strategy to improve Whisper's reliability under real-world noisy conditions.

</details>


### [32] [DevPiolt: Operation Recommendation for IoT Devices at Xiaomi Home](https://arxiv.org/abs/2511.14227)
*Yuxiang Wang,Siwen Wang,Haowei Han,Ao Wang,Boya Liu,Yong Zhao,Chengbo Wu,Bin Zhu,Bin Qin,Xiaokai Zhou,Xiao Yan,Jiawei Jiang,Bo Du*

Main category: cs.AI

TL;DR: DevPiolt是一种基于LLM的IoT设备操作推荐模型，通过领域知识预训练、用户偏好对齐和置信度控制机制，显著提升了推荐效果，已在小米家庭应用中部署验证。


<details>
  <summary>Details</summary>
Motivation: 现有推荐模型在处理IoT设备操作时面临操作逻辑复杂、用户偏好多样以及对次优建议敏感等挑战，限制了其在IoT场景的适用性。

Method: 首先通过持续预训练和多任务微调为LLM提供IoT操作领域知识，然后使用直接偏好优化对齐用户偏好，最后设计基于置信度的曝光控制机制避免低质量推荐。

Result: 实验显示DevPiolt在所有数据集上显著优于基线方法，各项指标平均提升69.5%。在线部署结果显示唯一访客设备覆盖率增加21.6%，页面查看接受率增加29.1%。

Conclusion: DevPiolt通过结合领域知识预训练、偏好对齐和智能曝光控制，有效解决了IoT设备操作推荐的特殊挑战，证明了LLM在该领域的实用价值。

Abstract: Operation recommendation for IoT devices refers to generating personalized device operations for users based on their context, such as historical operations, environment information, and device status. This task is crucial for enhancing user satisfaction and corporate profits. Existing recommendation models struggle with complex operation logic, diverse user preferences, and sensitive to suboptimal suggestions, limiting their applicability to IoT device operations. To address these issues, we propose DevPiolt, a LLM-based recommendation model for IoT device operations. Specifically, we first equip the LLM with fundamental domain knowledge of IoT operations via continual pre-training and multi-task fine-tuning. Then, we employ direct preference optimization to align the fine-tuned LLM with specific user preferences. Finally, we design a confidence-based exposure control mechanism to avoid negative user experiences from low-quality recommendations. Extensive experiments show that DevPiolt significantly outperforms baselines on all datasets, with an average improvement of 69.5% across all metrics. DevPiolt has been practically deployed in Xiaomi Home app for one quarter, providing daily operation recommendations to 255,000 users. Online experiment results indicate a 21.6% increase in unique visitor device coverage and a 29.1% increase in page view acceptance rates.

</details>


### [33] [Enhancing Regional Airbnb Trend Forecasting Using LLM-Based Embeddings of Accessibility and Human Mobility](https://arxiv.org/abs/2511.14248)
*Hongju Lee,Youngjun Park,Jisun An,Dongman Lee*

Main category: cs.AI

TL;DR: Novel LLM-based time-series framework for forecasting regional Airbnb metrics (Revenue, Reservation Days, Number of Reservations) 1-3 months ahead, integrating listing features with urban context, achieving 48% lower error than baselines.


<details>
  <summary>Details</summary>
Motivation: Airbnb expansion disrupts housing markets, causing affordability issues, requiring accurate regional forecasts for policy interventions.

Method: Sliding-window approach converts structured tabular data into LLM prompts for regional embeddings, fed into RNN/LSTM/Transformer models to capture spatio-temporal dynamics.

Result: Experiments on Seoul data show 48% reduction in RMSE and MAE compared to traditional statistical and ML baselines.

Conclusion: Framework improves forecasting accuracy and provides practical insights for detecting oversupply and supporting data-driven urban policy decisions.

Abstract: The expansion of short-term rental platforms, such as Airbnb, has significantly disrupted local housing markets, often leading to increased rental prices and housing affordability issues. Accurately forecasting regional Airbnb market trends can thus offer critical insights for policymakers and urban planners aiming to mitigate these impacts. This study proposes a novel time-series forecasting framework to predict three key Airbnb indicators -- Revenue, Reservation Days, and Number of Reservations -- at the regional level. Using a sliding-window approach, the model forecasts trends 1 to 3 months ahead. Unlike prior studies that focus on individual listings at fixed time points, our approach constructs regional representations by integrating listing features with external contextual factors such as urban accessibility and human mobility. We convert structured tabular data into prompt-based inputs for a Large Language Model (LLM), producing comprehensive regional embeddings. These embeddings are then fed into advanced time-series models (RNN, LSTM, Transformer) to better capture complex spatio-temporal dynamics. Experiments on Seoul's Airbnb dataset show that our method reduces both average RMSE and MAE by approximately 48% compared to conventional baselines, including traditional statistical and machine learning models. Our framework not only improves forecasting accuracy but also offers practical insights for detecting oversupplied regions and supporting data-driven urban policy decisions.

</details>


### [34] [PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2511.14256)
*Yu Liu,Xixun Lin,Yanmin Shang,Yangxi Li,Shi Wang,Yanan Cao*

Main category: cs.AI

TL;DR: PathMind是一个新的知识图谱推理框架，通过'检索-优先级-推理'范式，使用语义感知路径优先级机制筛选重要推理路径，减少噪声并降低LLM调用需求，在复杂推理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的知识图谱推理方法存在两个关键局限：1）无差别提取推理路径可能引入无关噪声误导LLM；2）动态探索推理路径需要高检索需求和频繁LLM调用。

Method: PathMind采用'检索-优先级-推理'三阶段：首先检索查询子图，然后通过语义感知路径优先级函数识别重要推理路径（综合考虑累积成本和预估未来成本），最后通过双阶段训练策略生成准确响应。

Result: 在基准数据集上的实验表明，PathMind consistently outperforms competitive baselines，特别是在复杂推理任务中使用更少的输入token。

Conclusion: PathMind通过选择性引导LLM使用重要推理路径，实现了更忠实和可解释的推理，解决了现有方法的局限性。

Abstract: Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a "Retrieve-Prioritize-Reason" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.

</details>


### [35] [DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning](https://arxiv.org/abs/2511.14299)
*Xiaochuan Liu,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.AI

TL;DR: 提出DataSage多智能体框架，解决当前数据洞察Agent在领域知识利用、分析深度和代码生成方面的不足


<details>
  <summary>Details</summary>
Motivation: 现有数据洞察智能体存在领域知识利用不足、分析深度浅、代码生成易出错的问题

Method: 采用多智能体框架，包含外部知识检索、多角色辩论机制和多路径推理三大创新特性

Result: 在InsightBench上的广泛实验显示DataSage在所有难度级别上均优于现有数据洞察智能体

Conclusion: DataSage为自动化数据洞察发现提供了有效解决方案，弥补了现有方法的局限性

Abstract: In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.

</details>


### [36] [When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling](https://arxiv.org/abs/2511.14334)
*Alessio Pellegrino,Jacopo Mauro*

Main category: cs.AI

TL;DR: LLMs struggle with contextual variations in constraint programming problems, showing performance declines when problem descriptions are rephrased or perturbed despite preserving structure.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs' success in generating constraint programming models stems from genuine reasoning or data contamination from training on standard benchmarks.

Method: Systematically rephrased and perturbed well-known CSPLib problems while preserving structure but changing context and adding misleading elements, then compared model outputs from three representative LLMs.

Result: LLMs produce syntactically valid models but performance drops significantly under contextual/linguistic variation, revealing shallow understanding and wording sensitivity.

Conclusion: LLMs' apparent success in constraint programming model generation may derive more from data contamination than genuine reasoning capabilities.

Abstract: One of the long-standing goals in optimisation and constraint programming is to describe a problem in natural language and automatically obtain an executable, efficient model. Large language models appear to bring this vision closer, showing impressive results in automatically generating models for classical benchmarks. However, much of this apparent success may derive from data contamination rather than genuine reasoning: many standard CP problems are likely included in the training data of these models. To examine this hypothesis, we systematically rephrased and perturbed a set of well-known CSPLib problems to preserve their structure while modifying their context and introducing misleading elements. We then compared the models produced by three representative LLMs across original and modified descriptions. Our qualitative analysis shows that while LLMs can produce syntactically valid and semantically plausible models, their performance drops sharply under contextual and linguistic variation, revealing shallow understanding and sensitivity to wording.

</details>


### [37] [Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior](https://arxiv.org/abs/2511.14476)
*Dalia Ali,Dora Zhao,Allison Koenecke,Orestis Papakyriakopoulos*

Main category: cs.AI

TL;DR: 本研究探讨了在LLM对齐过程中纳入多元化人类价值观的影响，揭示了人口统计差异和技术设计选择对模型行为的显著影响。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐决策往往忽视人类社会的多样性，需要研究如何平衡安全性与公平代表性。

Method: 从美德参与者收集27,375条评级数据，在不同社会群体偏好基础上微调多个LLM/LRM模型，并比较不同评分尺度、分歧处理方法和优化技术。

Result: 发现系统性人口统计效应（如男性评分毒性低18%），技术选择影响显著（5点量表比二分格式效果提升22%，DPO优于GRPO）。

Conclusion: 这是解决对齐过程中如何平衡专家驱动与用户驱动信号的重要初步探索。

Abstract: Although large language models (LLMs) are increasingly trained using human feedback for safety and alignment with human values, alignment decisions often overlook human social diversity. This study examines how incorporating pluralistic values affects LLM behavior by systematically evaluating demographic variation and design parameters in the alignment pipeline. We collected alignment data from US and German participants (N = 1,095, 27,375 ratings) who rated LLM responses across five dimensions: Toxicity, Emotional Awareness (EA), Sensitivity, Stereotypical Bias, and Helpfulness. We fine-tuned multiple Large Language Models and Large Reasoning Models using preferences from different social groups while varying rating scales, disagreement handling methods, and optimization techniques. The results revealed systematic demographic effects: male participants rated responses 18% less toxic than female participants; conservative and Black participants rated responses 27.9% and 44% more emotionally aware than liberal and White participants, respectively. Models fine-tuned on group-specific preferences exhibited distinct behaviors. Technical design choices showed strong effects: the preservation of rater disagreement achieved roughly 53% greater toxicity reduction than majority voting, and 5-point scales yielded about 22% more reduction than binary formats; and Direct Preference Optimization (DPO) consistently outperformed Group Relative Policy Optimization (GRPO) in multi-value optimization. These findings represent a preliminary step in answering a critical question: How should alignment balance expert-driven and user-driven signals to ensure both safety and fair representation?

</details>


### [38] [A Neuro-Symbolic Framework for Reasoning under Perceptual Uncertainty: Bridging Continuous Perception and Discrete Symbolic Planning](https://arxiv.org/abs/2511.14533)
*Jiahao Wu,Shengwen Yu*

Main category: cs.AI

TL;DR: 这篇论文提出了一种神经符号框架，通过概率符号状态建模将感知不确定性传播到规划中，在桌面机器人操作任务中表现出色，平均成功率90.7%，优于传统POMDP方法。


<details>
  <summary>Details</summary>
Motivation: 在AI系统中，连接连续性感知信号和离散符号推理是一个基本挑战，特别是在不确定性环境下。需要一种能够原则性地在感知和规划两个抽象层次之间传播不确定性的方法。

Method: 结合transformer感知前端和GNN关系推理来从视觉观察中提取概率符号状态，使用不确定性感知的符号规划器在置信度低时主动收集信息。

Result: 在10,047个PyBullet生成场景(3-10个物体)上测试，概率谓词提取F1=0.68。在Simple Stack、Deep Stack和Clear+Stack任务中分别达到94%/90%/88%成功率，规划时间仅15ms，比最强POMDP基线高10-14个百分点。

Conclusion: 该框架建立了校准不确定性与规划收敛之间的定量联系，提供了理论保证。该通用框架可应用于任何需要从感知输入进行不确定性感知符号规划的领域。

Abstract: Bridging continuous perceptual signals and discrete symbolic reasoning is a fundamental challenge in AI systems that must operate under uncertainty. We present a neuro-symbolic framework that explicitly models and propagates uncertainty from perception to planning, providing a principled connection between these two abstraction levels. Our approach couples a transformer-based perceptual front-end with graph neural network (GNN) relational reasoning to extract probabilistic symbolic states from visual observations, and an uncertainty-aware symbolic planner that actively gathers information when confidence is low. We demonstrate the framework's effectiveness on tabletop robotic manipulation as a concrete application: the translator processes 10,047 PyBullet-generated scenes (3--10 objects) and outputs probabilistic predicates with calibrated confidences (overall F1=0.68). When embedded in the planner, the system achieves 94\%/90\%/88\% success on Simple Stack, Deep Stack, and Clear+Stack benchmarks (90.7\% average), exceeding the strongest POMDP baseline by 10--14 points while planning within 15\,ms. A probabilistic graphical-model analysis establishes a quantitative link between calibrated uncertainty and planning convergence, providing theoretical guarantees that are validated empirically. The framework is general-purpose and can be applied to any domain requiring uncertainty-aware reasoning from perceptual input to symbolic planning.

</details>


### [39] [Rate-Distortion Guided Knowledge Graph Construction from Lecture Notes Using Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2511.14595)
*Yuan An,Ruhma Hashmi,Michelle Rogers,Jane Greenberg,Brian K. Smith*

Main category: cs.AI

TL;DR: 论文提出基于率失真理论和最优传输几何的知识图谱构建与优化框架，用于从教学材料自动生成高质量选择题


<details>
  <summary>Details</summary>
Motivation: 将非结构化教学材料（如讲义和幻灯片）转化为能够捕捉关键教学内容的知识图谱仍然具有挑战性

Method: 将讲座内容建模为度量-测度空间，使用Fused Gromov-Wasserstein耦合量化语义失真，通过精炼操作（添加、合并、拆分、删除、重连）最小化率失真拉格朗日量

Result: 在数据科学讲座上的应用显示，从优化后知识图谱生成的选择题在15个质量标准上 consistently 优于原始笔记生成的选择题

Conclusion: 该研究为个性化AI辅助教育中的信息论知识图谱优化建立了理论基础

Abstract: Task-oriented knowledge graphs (KGs) enable AI-powered learning assistant systems to automatically generate high-quality multiple-choice questions (MCQs). Yet converting unstructured educational materials, such as lecture notes and slides, into KGs that capture key pedagogical content remains difficult. We propose a framework for knowledge graph construction and refinement grounded in rate-distortion (RD) theory and optimal transport geometry. In the framework, lecture content is modeled as a metric-measure space, capturing semantic and relational structure, while candidate KGs are aligned using Fused Gromov-Wasserstein (FGW) couplings to quantify semantic distortion. The rate term, expressed via the size of KG, reflects complexity and compactness. Refinement operators (add, merge, split, remove, rewire) minimize the rate-distortion Lagrangian, yielding compact, information-preserving KGs. Our prototype applied to data science lectures yields interpretable RD curves and shows that MCQs generated from refined KGs consistently surpass those from raw notes on fifteen quality criteria. This study establishes a principled foundation for information-theoretic KG optimization in personalized and AI-assisted education.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [40] [Show and Tell: Prompt Strategies for Style Control in Multi-Turn LLM Code Generation](https://arxiv.org/abs/2511.13972)
*Jeremiah Bohr*

Main category: cs.SE

TL;DR: 研究比较了指令提示、示例提示和组合提示在代码生成中的风格控制效果，发现组合提示在初始压缩和扩展控制方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 语言模型生成的代码通常过于冗长，与人类基准不符，需要研究提示机制如何在保持功能准确性的同时控制代码风格。

Method: 采用四组系统提示条件，在双轮协议中让模型首先生成Python任务解决方案，然后在通用改进指令下修订代码（N=160对程序）。

Result: 组合提示产生最强的初始压缩和最大的扩展控制；指令提示有较大的初始效果和中等扩展控制；示例提示初始效果一般且无扩展控制。

Conclusion: 初始提示效果和扩展控制是提示设计的两个独立方面，组合方法在双轮工作流程中提供最稳定的风格控制。

Abstract: Language models generate functionally correct code that tends toward excessive verbosity, with elaborate documentation and defensive patterns that diverge from human baselines. Two prompting mechanisms have emerged for stylistic control: instruction based prompts that articulate abstract directives, and example based prompts that provide concrete code demonstrations. The core problem is whether stylistic constraints persist when models enhance initial implementations with additional features while maintaining high functional accuracy. Here we show that instruction-based, example-based, and combined prompts produce distinct patterns of initial control and expansion discipline over one enhancement turn. We manipulated system prompts across four conditions in a paired two-turn protocol where models first generated solutions to an intermediate Python task, then revised their code under general improvement directives, holding the user task fixed (N = 160 paired programs). Combined prompts produced the strongest initial compression and greatest expansion discipline. Instructions showed large initial effects and moderate expansion discipline. Examples showed modest initial effects with no expansion discipline. These results show that initial prompt effectiveness and expansion discipline are separate aspects of prompt design, and that combined approaches provide the most stable stylistic control in this two-turn workflow.

</details>


### [41] [Exploring the Use of ChatGPT by Computer Science Students in Software Development: Applications, Ethical Considerations, and Insights for Engineering Education](https://arxiv.org/abs/2511.13996)
*Daihan Xu,Diana Martin*

Main category: cs.SE

TL;DR: 该研究通过定性访谈探讨英国计算机科学学生如何在软件开发中策略性和伦理性地使用ChatGPT，揭示了学习模式从传统编程转向AI辅助协作的变化，学生普遍限制AI贡献率并重视伦理准则，但缺乏深入代码审查，呼吁制定明确使用指南。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖调查数据，侧重于趋势而非学生具体策略和伦理意识的深度分析。本研究通过定性调查填补这一空白，旨在深入理解计算机科学学生在软件开发项目中如何策略性和伦理性地使用ChatGPT。

Method: 采用半结构化访谈进行定性研究，聚焦于英国一所高校的计算机科学学生，探讨两个核心问题：学生在软件开发中如何策略性和伦理性地使用ChatGPT，以及他们对相关伦理问题的认知和看法。

Result: 发现学生学习模式从“独立思考-手动编码-迭代调试”转变为“AI辅助构思-交互编程-协作优化”。多数学生对话式使用ChatGPT以加深理解，但自觉保留创意和高级决策任务，限制AI贡献率约30%并评估输出以避免过度依赖。然而，仅少数深入分析AI生成代码，存在批判性参与不足的担忧。学生普遍反对未注明使用的行为，强调隐私泄露和技能退化风险，并呼吁教师制定明确使用指南。

Conclusion: 本研究揭示了学习者与AI动态关系的新见解，强调需要明确指导以支持负责任和教学合理的工具使用。学生在适应AI辅助时表现出策略性和伦理意识，但需加强代码审查等批判性实践，教育者应提供结构化框架以平衡创新与学术诚信。

Abstract: ChatGPT has been increasingly used in computer science, offering efficient support across software development tasks. While it helps students navigate programming challenges, its use also raises concerns about academic integrity and overreliance. Despite growing interest in this topic, prior research has largely relied on surveys, emphasizing trends over in-depth analysis of students' strategies and ethical awareness. This study complements existing work through a qualitative investigation of how computer science students in one UK institution strategically and ethically engage with ChatGPT in software development projects. Drawing on semi-structured interviews, it explores two key questions: How do computer science students ethically and strategically report using ChatGPT in software development projects? How do students understand and perceive the ethical issues associated with using ChatGPT in academic and professional contexts? Findings reveal a shift in students' learning models, moving from traditional "independent thinking-manual coding-iterative debugging" to "AI-assisted ideation-interactive programming-collaborative optimization." Importantly, many use ChatGPT conversationally to deepen understanding, while consciously reserving creative and high-level decision-making tasks for themselves. Students tend to cap ChatGPT's contribution to roughly 30%, and evaluate its output to mitigate overreliance. However, only a minority thoroughly analyze AI-generated code, raising concerns about reduced critical engagement. Meanwhile, students reject uncredited use, highlight risks such as privacy breaches and skill degradation, and call for clear usage guidelines set by their teachers. This research offers novel insights into the evolving learner-AI dynamic and highlights the need for explicit guidance to support responsible and pedagogically sound use of such tools.

</details>


### [42] [LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering](https://arxiv.org/abs/2511.13998)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Roshan Ram,Akshara Prabhakar,Tulika Awalgaonkar,Zixiang Chen,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench-Agent is a new evaluation framework for LLM agents that extends the original LoCoBench to assess multi-turn interactions, tool usage, and adaptive reasoning in realistic software engineering workflows with context lengths up to 1M tokens.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like LoCoBench focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents.

Method: Extends LoCoBench's 8,000 scenarios into interactive agent environments with 8 specialized tools (file operations, search, code analysis) and evaluates across context lengths from 10K to 1M tokens using 9 metrics across comprehension and efficiency dimensions.

Result: Evaluation revealed: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation; (3) conversation efficiency varies dramatically across models with strategic tool usage patterns differentiating high-performing agents.

Conclusion: LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale as the first long-context LLM agent benchmark for software engineering.

Abstract: As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.

</details>


### [43] [FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale](https://arxiv.org/abs/2511.14002)
*Chengpeng Li,Farnaz Behrang,August Shi,Peng Liu*

Main category: cs.SE

TL;DR: FlakyGuard is a new approach that fixes flaky tests using graph-based code analysis to provide optimal context to LLMs, achieving 47.6% repair rate with high developer acceptance.


<details>
  <summary>Details</summary>
Motivation: Flaky tests waste developer time but existing LLM repair tools fail in industrial settings due to providing either too little or too much context.

Method: Treats code as a graph structure and uses selective graph exploration to find only the most relevant context for LLM-based repair.

Result: Repairs 47.6% of reproducible flaky tests with 51.8% developer acceptance, outperforming state-of-the-art by at least 22%.

Conclusion: FlakyGuard effectively solves the context problem in flaky test repair and developers find its root cause explanations highly useful.

Abstract: Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.

</details>


### [44] [Keeping Code-Aware LLMs Fresh: Full Refresh, In-Context Deltas, and Incremental Fine-Tuning](https://arxiv.org/abs/2511.14022)
*Pradeep Kumar Sharma,Ishaan Puri,Mantinder Jit Singh,Swapnil Shivaprasad,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: The paper compares three strategies (Full Refresh, In-Context Learning, and Incremental Fine-Tuning) for maintaining code search model freshness as codebases evolve, finding that Incremental Fine-Tuning with old-aware mixing provides the best overall balance, while ICL with English delta summaries offers the fastest new-code improvement.


<details>
  <summary>Details</summary>
Motivation: Codebases continuously evolve, causing models trained to map natural-language questions to relevant code files to degrade over time due to domain drift between codebase snapshots.

Method: The study compares three update strategies: Full Refresh (retraining entire model), In-Context Learning (injecting recent deltas at inference), and Incremental Fine-Tuning (fine-tuning on delta-derived training sets with controlled mixing to prevent catastrophic forgetting).

Result: Inc-FT with old-aware mixing delivered the best overall balance across Flask, SQLAlchemy, Pandas, and Poetry repositories. ICL with English delta summaries provided fastest new-code improvement when training isn't feasible. Full Refresh remained the accuracy ceiling when maximum new-code accuracy is critical.

Conclusion: Inc-FT with careful NEW:OLD mixing is optimal for maintaining model freshness while preserving knowledge of older code, with ICL as a practical alternative when retraining is not possible.

Abstract: Modern codebases evolve continuously: files are renamed or deleted; public APIs drift; behavior shifts within otherwise familiar modules. A model trained yesterday to map a developer's natural-language question to the exact set of repository file paths that matter will degrade tomorrow, even if the questions themselves look unchanged. In this paper we study, at system scale and across several widely used repositories, how to keep such a model fresh without surrendering retention on earlier code. We frame freshness as a form of domain drift between a base snapshot and the current HEAD, and we compare three families of update strategies: (A) Full Refresh, retraining the entire model at the new snapshot; (B) In-Context Learning (ICL) that injects recent deltas (raw git diffs or concise English summaries) at inference; and (C) Incremental Fine-Tuning (Inc-FT) on delta-derived training sets, with carefully controlled NEW:OLD mixing to mitigate catastrophic forgetting. We contribute an alias-aware evaluation protocol that credits rename while never rewarding deleted paths, and a practical Forgetting Probe that quantifies residual emissions of obsolete paths. Across Flask, SQLAlchemy, Pandas, and Poetry, Inc-FT with old-aware mixes delivers the best overall balance on mixed sets, ICL with English delta summaries delivers the fastest new-code lift when training is not feasible, and Full Refresh remains the ceiling when maximum NEW accuracy matters. We also compare Git-diff Inc-FT to full-file Inc-FT, showing that diffs excel in rename/delete-heavy windows while full-file context wins in behavior-change-heavy windows.

</details>


### [45] [LogPurge: Log Data Purification for Anomaly Detection via Rule-Enhanced Filtering](https://arxiv.org/abs/2511.14062)
*Shenglin Zhang,Ziang Chen,Zijing Que,Yilun Liu,Yongqian Sun,Sicheng Wei,Dan Pei,Hailin Li*

Main category: cs.SE

TL;DR: 论文提出LogPurge框架，通过两阶段过滤算法自动从受污染的日志序列中筛选正常日志样本，用于训练异常检测模型，在多个数据集上取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有的日志异常检测方法需要干净的无异常日志数据进行训练，但获取这类数据需要昂贵的人工标注，现有自动清洗方法未能充分整合日志的特定特征和实际语义。

Method: 提出成本感知、规则增强的净化框架LogPurge，采用两阶段过滤算法：第一阶段使用LLM去除聚集异常模式并增强系统规则；第二阶段采用分治策略将剩余污染区域分解为子问题。

Result: 在三个数据集上的实验表明，该方法平均移除98.74%的异常同时保留82.39%的正常样本，F-1分数相比最新无监督方法提升35.7%-149.72%。

Conclusion: LogPurge框架能有效自动净化日志数据，显著提升异常检测模型性能，解决了获取干净训练数据的技术难题。

Abstract: Log anomaly detection, which is critical for identifying system failures and preempting security breaches, detects irregular patterns within large volumes of log data, and impacts domains such as service reliability, performance optimization, and database log analysis. Modern log anomaly detection methods rely on training deep learning models on clean, anomaly-free log sequences. However, obtaining such clean log data requires costly and tedious human labeling, and existing automatic cleaning methods fail to fully integrate the specific characteristics and actual semantics of logs in their purification process. In this paper, we propose a cost-aware, rule-enhanced purification framework, LogPurge, that automatically selects a sufficient subset of normal log sequences from contamination log sequences to train a anomaly detection model. Our approach involves a two-stage filtering algorithm: In the first stage, we use a large language model (LLM) to remove clustered anomalous patterns and enhance system rules to improve LLM's understanding of system logs; in the second stage, we utilize a divide-and-conquer strategy that decomposes the remaining contaminated regions into smaller subproblems, allowing each to be effectively purified through the first stage procedure. Our experiments, conducted on two public datasets and one industrial dataset, show that our method significantly removes an average of 98.74% of anomalies while retaining 82.39% of normal samples. Compared to the latest unsupervised log sample selection algorithms, our method achieves F-1 score improvements of 35.7% and 84.11% on the public datasets, and an impressive 149.72% F-1 improvement on the private dataset, demonstrating the effectiveness of our approach.

</details>


### [46] [A Practical Implementation of Customized Scrum-Based Agile Framework in Aerospace Software Development Under DO-178C Constraints](https://arxiv.org/abs/2511.14215)
*Malik Muhammad Umer*

Main category: cs.SE

TL;DR: Study presents a Scrum-based Agile framework for DO-178C compliant aerospace software development, showing significant improvements in efficiency and defect metrics compared to Waterfall while maintaining full compliance.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of aerospace systems requires development processes that balance agility with stringent safety and certification demands like DO-178C.

Method: Customized Scrum framework with multi-disciplinary product ownership, dual acceptance criteria, independent testing teams, and certification liaisons. Evaluated through comparative analysis of Agile vs Waterfall projects.

Result: 76% reduction in Total Effort per Requirement, 75% faster Defect Detection, 78% faster Defect Resolution, over 50% lower Defect Density, while maintaining DO-178C Level A compliance.

Conclusion: Agile practices and regulatory compliance can coexist effectively with disciplined tailoring. Future opportunities include workflow automation and CI/CD practices for further gains.

Abstract: The increasing complexity of aerospace systems requires development processes that balance agility with stringent safety and certification demands. This study presents an empirically validated Scrum-based Agile framework tailored for DO-178C compliant, safety-critical aerospace software. The framework adapts core Scrum roles, artifacts, and events to meet certification, verification, and independence objectives. Key enhancements include a multi-disciplinary product ownership model, dual compliance-and-functionality acceptance criteria, independent testing and documentation teams, and dedicated certification liaisons. The approach was evaluated through two comparable aerospace projects-one using the customized Agile process and the other a traditional Waterfall model. Results showed significant improvements: a 76% reduction in Total Effort per Requirement, 75% faster Defect Detection, 78% faster Defect Resolution, and over 50% lower Defect Density, while maintaining full compliance with DO-178C Design Assurance Level A. These findings demonstrate that Agile practices and regulatory compliance can coexist effectively when supported by disciplined tailoring and proactive engagement with certification authorities. The study also notes challenges, including increased V&V effort due to recurring Sprint activities and refactoring inherent to iterative development. Nonetheless, it identifies substantial opportunities for further gains through workflow automation, CI/CD practices, and automated documentation, verification, and configuration management. Future research should expand validation of this framework across the aerospace domain and other safety-critical industries with similar certification requirements.

</details>


### [47] [KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation](https://arxiv.org/abs/2511.14224)
*Anji Li,Mingwei Liu,Zhenxi Chen,Zheng Pei,Zike Li,Dekun Dai,Yanlin Wang,Zibin Zheng*

Main category: cs.SE

TL;DR: KTester integrates project-specific and testing domain knowledge with LLMs to generate more correct and maintainable unit tests, outperforming existing methods in multiple metrics.


<details>
  <summary>Details</summary>
Motivation: LLM-based test generation struggles with producing tests that are both correct and maintainable in real-world projects. Current approaches lack integration of project-specific context and systematic testing knowledge.

Method: Extracts project structure and usage knowledge through static analysis, employs testing-domain-knowledge-guided separation of test case design and method generation, and uses multi-perspective prompting with testing heuristics.

Result: Significantly outperforms SOTA baselines: 5.69% higher execution pass rate, 8.83% higher line coverage, generates fewer tests in less time. Human evaluators rate tests higher for correctness, readability, and maintainability.

Conclusion: KTester's knowledge-driven approach effectively enhances LLM-based test generation, demonstrating practical advantages for real-world software testing through its structured, context-aware methodology.

Abstract: Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.

</details>


### [48] [How Does Cognitive Capability and Personality Influence Problem-Solving in Coding Interview Puzzles?](https://arxiv.org/abs/2511.14367)
*Dulaji Hidellaarachchi,Sebastian Baltes,John Grundy*

Main category: cs.SE

TL;DR: Study examines cognitive capability and personality traits in software problem solving, finding practitioners slightly outperform students, with conscientiousness and reasoning accuracy being key predictors of performance.


<details>
  <summary>Details</summary>
Motivation: To understand how individual differences in cognitive capability and personality traits influence software problem solving performance.

Method: Surveyed 80 participants (40 practitioners, 40 students) using Baddeley's reasoning test, IPIP NEO personality test, and 9 problem-solving questions (6 coding, 3 logical).

Result: Practitioners had higher reasoning accuracy and task performance; reasoning accuracy positively correlated with performance; conscientiousness and openness positively related to performance.

Conclusion: Conscientiousness and openness complement reasoning ability in software problem solving, while neuroticism may hinder precision under pressure. Implications for education and recruitment.

Abstract: Software engineering is a deeply cognitive activity shaped by individual differences that extend beyond technical skill. This study investigates how cognitive capability and personality traits jointly relate to software problem solving among 80 participants (40 software practitioners, 40 software engineering students). Cognitive capability was measured using Baddeleys three minute grammatical reasoning test, while personality was assessed using the IPIP NEO 50 test. Participants further completed nine interview style problem solving questions. Six questions were related to coding and three were related to logical reasoning. Descriptive and correlational analyses show that practitioners achieved slightly higher grammatical reasoning accuracy and overall task performance than students. Grammatical-reasoning accuracy correlated positively with problem solving performance, indicating that stronger cognitive capability is associated with better performance in coding and logical tasks. Personality performance links were systematic. We identified that the conscientiousness trait correlated most strongly with problem solving and with reasoning accuracy, while the openness to experience trait was positively related to both outcomes. Neuroticism showed small, negative associations with accuracy and performance. Taken together, our results suggest that conscientiousness and openness to experience characteristics complement reasoning accuracy to support software problem solving, whereas elevated negative affect may hinder precision under time pressure. Our findings suggest practical implications for education and industry such as integrating structured reasoning tasks in curricula, and considering personality cognition in recruitment and role allocation. We highlight directions for future research such as longitudinal and task diverse replications with larger samples.

</details>


### [49] [Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems](https://arxiv.org/abs/2511.14435)
*Angelo Ferrando*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Assuring the safety and trustworthiness of autonomous systems is particularly difficult when learning-enabled components and open environments are involved. Formal methods provide strong guarantees but depend on complete models and static assumptions. Runtime verification (RV) complements them by monitoring executions at run time and, in its predictive variants, by anticipating potential violations. Large language models (LLMs), meanwhile, excel at translating natural language into formal artefacts and recognising patterns in data, yet they remain error-prone and lack formal guarantees. This vision paper argues for a symbiotic integration of RV and LLMs. RV can serve as a guardrail for LLM-driven autonomy, while LLMs can extend RV by assisting specification capture, supporting anticipatory reasoning, and helping to handle uncertainty. We outline how this mutual reinforcement differs from existing surveys and roadmaps, discuss challenges and certification implications, and identify future research directions towards dependable autonomy.

</details>


### [50] [LLM-Assisted Thematic Analysis: Opportunities, Limitations, and Recommendations](https://arxiv.org/abs/2511.14528)
*Tatiane Ornelas,Allysson Allex Araújo,Júlia Araújo,Marina Araújo,Bianca Trinkenreich,Marcos Kalinowski*

Main category: cs.SE

TL;DR: This study explores how experienced Software Engineering researchers view the use of Large Language Models (LLMs) in thematic analysis, highlighting both potential benefits and significant risks.


<details>
  <summary>Details</summary>
Motivation: To investigate the methodological implications, including opportunities, risks, and the impact on rigor and transparency, when integrating LLMs into qualitative research processes like thematic analysis in Software Engineering.

Method: A reflective workshop with 25 ISERN researchers, using structured discussions and color-coded canvases to document insights on LLM-assisted open coding, theme generation, and theme reviewing.

Result: Participants acknowledged potential efficiency gains but raised concerns about bias, loss of context, reproducibility issues, and the need for human oversight and prompting literacy due to LLMs' rapid evolution.

Conclusion: LLMs are seen as supportive tools that cannot replace human interpretive analysis. The study contributes to discussions on responsibly enhancing qualitative SE research with LLMs.

Abstract: [Context] Large Language Models (LLMs) are increasingly used to assist qualitative research in Software Engineering (SE), yet the methodological implications of this usage remain underexplored. Their integration into interpretive processes such as thematic analysis raises fundamental questions about rigor, transparency, and researcher agency. [Objective] This study investigates how experienced SE researchers conceptualize the opportunities, risks, and methodological implications of integrating LLMs into thematic analysis. [Method] A reflective workshop with 25 ISERN researchers guided participants through structured discussions of LLM-assisted open coding, theme generation, and theme reviewing, using color-coded canvases to document perceived opportunities, limitations, and recommendations. [Results] Participants recognized potential efficiency and scalability gains, but highlighted risks related to bias, contextual loss, reproducibility, and the rapid evolution of LLMs. They also emphasized the need for prompting literacy and continuous human oversight. [Conclusion] Findings portray LLMs as tools that can support, but not substitute, interpretive analysis. The study contributes to ongoing community reflections on how LLMs can responsibly enhance qualitative research in SE.

</details>


### [51] [FHIRconnect: Towards a seamless integration of openEHR and FHIR](https://arxiv.org/abs/2511.14618)
*Severin Kohler,Jordi Piera Jiménez,Michael Anywar,Lars Fuhrmann,Heather Leslie,Maximilian Meixner,Julian Saß,Florian Kärcher,Diego Boscá,Birger Haarbrandt,Michael Marschollek,Roland Eils*

Main category: cs.SE

TL;DR: FHIRconnect is a novel DSL and open-source engine enabling standardized bidrectional data exchange between openEHR and HL7 FHIR, addressing interoperability challenges through a triple-layered architecture.


<details>
  <summary>Details</summary>
Motivation: Healthcare interoperability between openEHR and HL7 FHIR remains challenging due to fundamental differences in data modeling approaches and lack of standardized transformation mechanisms.

Method: A triple-layered architecture using a domain-specific language and open-source transformation engine that leverages international archetype-based foundations while supporting local customizations.

Result: Achieved 65% mapping reuse across projects and successfully mapped 24 international archetypes to 15 FHIR profiles across seven clinical domains.

Conclusion: FHIRconnect establishes the technical basis for community-driven mapping standardization, reducing reliance on custom ETL solutions and advancing syntactic and semantic interoperability in healthcare IT systems.

Abstract: Healthcare interoperability between openEHR and HL7 FHIR remains challenging due to fundamental differences in their data modeling approaches and the absence of standardized transformation mechanisms. This paper presents FHIRconnect, a novel domain-specific language and open-source transformation engine that enables standardized, bidirectional data exchange between openEHR and FHIR. Our approach addresses critical interoperability gaps through a triple-layered architecture that achieves 65% mapping reuse across projects by leveraging international archetype-based foundations while supporting local customizations. Using this framework, FHIRconnect successfully mapped 24 international archetypes to 15 FHIR profiles across seven clinical domains. Key contributions include the first comprehensive DSL for openEHR-FHIR transformation with a formal specification, an open-source execution engine (openFHIR), and an accessible mapping library covering high-impact clinical archetypes. Together, these components establish the technical basis for community-driven mapping standardization, reducing reliance on custom ETL solutions and advancing syntactic and semantic interoperability in healthcare IT systems built on open standards.

</details>


### [52] [From Legacy Fortran to Portable Kokkos: An Autonomous Agentic AI Workflow](https://arxiv.org/abs/2509.12443)
*Sparsh Gupta,Kamalavasan Kamalakkannan,Maxim Moraru,Galen Shipman,Patrick Diehl*

Main category: cs.SE

TL;DR: This paper presents an AI-driven workflow using specialized LLM agents to automatically translate legacy Fortran code into performance-portable Kokkos C++ programs for GPU-accelerated HPC systems, achieving successful modernization with commercial AI models.


<details>
  <summary>Details</summary>
Motivation: Legacy Fortran codes face challenges in GPU-accelerated HPC due to lack of native Fortran bindings, requiring manual porting expertise. There's a need for automated approaches to modernize scientific applications for heterogeneous architectures.

Method: An agentic AI workflow employing specialized LLM agents that collaborate to translate, validate, compile, run, test, debug, and optimize Fortran kernels into Kokkos C++ programs across multiple hardware platforms.

Result: The pipeline successfully modernized benchmark kernels, producing portable Kokkos codes. Commercial models (GPT-5, o4-mini-high) generated optimized codes surpassing Fortran baselines cost-effectively, while open-source models often failed to produce functional codes.

Conclusion: Agentic AI is feasible for Fortran-to-Kokkos transformation, offering an autonomous pathway to modernize legacy scientific applications for diverse supercomputers, demonstrating LLMs' potential for structured reasoning in scientific applications.

Abstract: Scientific applications continue to rely on legacy Fortran codebases originally developed for homogeneous, CPU-based systems. As High-Performance Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many accelerators lack native Fortran bindings, creating an urgent need to modernize legacy codes for portability. Frameworks like Kokkos provide performance portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos porting demands significant expertise and time. Large language models (LLMs) have shown promise in source-to-source code generation, yet their use in fully autonomous workflows for translating and optimizing parallel code remains largely unexplored, especially for performance portability across diverse hardware. This paper presents an agentic AI workflow where specialized LLM "agents" collaborate to translate, validate, compile, run, test, debug, and optimize Fortran kernels into portable Kokkos C++ programs. Results show the pipeline modernizes a range of benchmark kernels, producing performance-portable Kokkos codes across hardware partitions. Paid OpenAI models such as GPT-5 and o4-mini-high executed the workflow for only a few U.S. dollars, generating optimized codes that surpassed Fortran baselines, whereas open-source models like Llama4-Maverick often failed to yield functional codes. This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos transformation and offers a pathway for autonomously modernizing legacy scientific applications to run portably and efficiently on diverse supercomputers. It further highlights the potential of LLM-driven agentic systems to perform structured, domain-specific reasoning tasks in scientific and systems-oriented applications.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [53] [Variational multiscale enrichment method for dynamic response of hyperelastic materials at finite deformation](https://arxiv.org/abs/2511.13723)
*Abhishek Arora,Caglar Oskay*

Main category: cs.CE

TL;DR: 本文扩展了变分多尺度富集方法用于模拟超弹性材料在大变形下的动态响应，能够模拟尺度不可分离条件下的波传播，包括短波范围内的非线性效应。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以准确模拟在尺度不可分离条件下包含材料和几何非线性的波传播问题，特别是在短波范围内需要考虑微观惯性效应和波形的陡化或平坦化现象。

Method: 采用位移场的加性分解，推导粗尺度和细尺度问题的多尺度控制方程，使用算子分裂程序迭代求解半离散方程。粗尺度问题显式积分，细尺度问题可采用显式或隐式时间积分方案。

Result: 数值算例表明多尺度耗散方案能有效抑制虚假振荡。研究发现材料和几何非线性以及异质微结构中的弹性刚度对比对波的色散、衰减和陡化等关键特性有显著影响。

Conclusion: 该多尺度计算框架为研究构筑材料的动态响应提供了基础，能够准确捕捉短波范围内的波传播行为，同时考虑复杂的非线性效应。

Abstract: In this manuscript, we extend the variational multiscale enrichment (VME) method to model the dynamic response of hyperelastic materials undergoing large deformations. This approach enables the simulation of wave propagation under scale-inseparable conditions, including short-wavelength regimes, while accounting for material and geometric nonlinearities that lead to wave steepening or flattening. By employing an additive decomposition of the displacement field, we derive multiscale governing equations for the coarse- and fine-scale problems, which naturally incorporate micro-inertial effects. The framework allows the discretization of each unit cell with a patch of coarse-scale elements, which is essential to accurately capture wave propagation in short-wavelength regimes. An operator-split procedure is used to iteratively solve the semi-discrete equations at both scales until convergence is achieved. The coarse-scale problem is integrated explicitly, while the fine-scale problem is solved using either explicit or implicit time integration schemes, including both dissipative and non-dissipative methods. Numerical examples demonstrate that multiscale dissipative schemes effectively suppress spurious oscillations. The multiscale framework was applied to investigate how material and geometric nonlinearities, along with elastic stiffness contrast in heterogeneous microstructures, influence key wave characteristics such as dispersion, attenuation, and steepening. This multiscale computational framework provides a foundation for studying the dynamic response of architected materials.

</details>


### [54] [PGD-TO: A Scalable Alternative to MMA Using Projected Gradient Descent for Multi-Constraint Topology Optimization](https://arxiv.org/abs/2511.13905)
*Amin Heyrani Nobari,Faez Ahmed*

Main category: cs.CE

TL;DR: PGD-TO是一种新的拓扑优化框架，通过将投影步重新表述为正则化凸二次问题，解决了传统PGD方法在多约束和非线性问题中的局限性，实现了比MMA和OC方法快10-312倍的计算速度。


<details>
  <summary>Details</summary>
Motivation: 传统的投影梯度下降（PGD）方法在拓扑优化中面临多约束和非线性问题的挑战，特别是在有效集检测方面存在困难，限制了其在实际大规模设计问题中的应用。

Method: 将投影步重新表述为正则化凸二次问题，消除有效集搜索需求；针对多约束情况使用半光滑牛顿求解器，针对单约束或独立约束使用二分搜索投影；集成谱步长自适应和非线性共轭梯度方向以提高稳定性和效率。

Result: 在四种拓扑优化基准问题上测试（包括线性和非线性、单约束和多约束情况），PGD-TO的收敛性和最终柔度与MMA和OC方法相当，但每轮迭代计算时间减少10-43倍（一般问题）和115-312倍（约束独立时）。

Conclusion: PGD-TO建立了一个快速、鲁棒且可扩展的MMA替代方案，推动了拓扑优化向实际大规模、多约束和非线性设计问题的应用。

Abstract: Projected Gradient Descent (PGD) methods offer a simple and scalable approach to topology optimization (TO), yet they often struggle with nonlinear and multi-constraint problems due to the complexity of active-set detection. This paper introduces PGD-TO, a framework that reformulates the projection step into a regularized convex quadratic problem, eliminating the need for active-set search and ensuring well-posedness even when constraints are infeasible. The framework employs a semismooth Newton solver for general multi-constraint cases and a binary search projection for single or independent constraints, achieving fast and reliable convergence. It further integrates spectral step-size adaptation and nonlinear conjugate-gradient directions for improved stability and efficiency. We evaluate PGD-TO on four benchmark families representing the breadth of TO problems: (i) minimum compliance with a linear volume constraint, (ii) minimum volume under a nonlinear compliance constraint, (iii) multi-material minimum compliance with four independent volume constraints, and (iv) minimum compliance with coupled volume and center-of-mass constraints. Across these single- and multi-constraint, linear and nonlinear cases, PGD-TO achieves convergence and final compliance comparable to the Method of Moving Asymptotes (MMA) and Optimality Criteria (OC), while reducing per-iteration computation time by 10-43x on general problems and 115-312x when constraints are independent. Overall, PGD-TO establishes a fast, robust, and scalable alternative to MMA, advancing topology optimization toward practical large-scale, multi-constraint, and nonlinear design problems. Public code available at: https://github.com/ahnobari/pyFANTOM

</details>


### [55] [MoMoE: A Mixture of Expert Agent Model for Financial Sentiment Analysis](https://arxiv.org/abs/2511.13983)
*Peng Shu,Junhao Chen,Zhengliang Liu,Hanqi Jiang,Yi Pan,Khanh Nhu Nguyen,Zihao Wu,Huaqin Zhao,Yiwei Li,Enze Shi,ShaoChen Xu*

Main category: cs.CE

TL;DR: 提出MoMoE方法，结合MoE架构与多智能体框架，在LLaMA 3.1 8B中为每个智能体加入MoE层，形成协同专家系统


<details>
  <summary>Details</summary>
Motivation: 结合MoE架构的高效任务分解能力与多智能体框架的协同优势，提升语言任务的性能

Method: 修改LLaMA 3.1 8B架构，在分层协作结构的每个智能体最终注意力块中加入MoE层，创建专业化专家智能体 ensemble

Result: 在多项语言理解与生成基准测试中取得显著提升

Conclusion: 神经层面和智能体层面的专家路由协同结合能产生显著增效作用

Abstract: We present a novel approach called Mixture of Mixture of Expert (MoMoE) that combines the strengths of Mixture-of-Experts (MoE) architectures with collaborative multi-agent frameworks. By modifying the LLaMA 3.1 8B architecture to incorporate MoE layers in each agent of a layered collaborative structure, we create an ensemble of specialized expert agents that iteratively refine their outputs. Each agent leverages an MoE layer in its final attention block, enabling efficient task decomposition while maintaining computational feasibility. This hybrid approach creates specialized pathways through both the model architecture and the agent collaboration layers. Experimental results demonstrate significant improvements across multiple language understanding and generation benchmarks, highlighting the synergistic benefits of combining expert routing at both the neural and agent levels.

</details>


### [56] [Enhancing Cyber-Resilience in Cyber-Physical Systems of Systems:A Methodical Approach](https://arxiv.org/abs/2511.14548)
*Elisabeth Vogel,Peter Langendörfer*

Main category: cs.CE

TL;DR: Proposes a modified Cyber-Resilience Life-Cycle framework for Cyber-physical Systems of Systems (CPSoS) to enhance adaptability and resilience.


<details>
  <summary>Details</summary>
Motivation: Address challenges and resilience requirements of increasingly prevalent CPSoS in sectors like Industry 4.0 and smart homes.

Method: Enhanced Cyber-Resilience Life-Cycle framework designed for sustainable risk mitigation.

Result: Framework improves CPSoS adaptability and resilience against system complexities and disruptions.

Conclusion: Outlines application scenarios and emphasizes the framework's relevance for fostering cyber-resilience in operational systems.

Abstract: Cyber-physical Systems of Systems (CPSoS) are becoming increasingly prevalent across sectors such as Industry 4.0 and smart homes, where they play a critical role in enabling intelligent, interconnected functionality. Addressing the challenges and resilience requirements of these complex environments, we propose a modified Cyber-Resilience Life-Cycle as a practical framework for sustainable risk mitigation. Our approach enhances the adaptability of CPSoS and supports resilience against evolving system complexities and potential disruptions. We conclude by outlining application scenarios for the modified life-cycle and highlighting its relevance in fostering cyber-resilience in operational systems.

</details>
