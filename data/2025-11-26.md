<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Z-Space: A Multi-Agent Tool Orchestration Framework for Enterprise-Grade LLM Automation](https://arxiv.org/abs/2511.19483)
*Qingsong He,Jing Nan,Jiayu Jiao,Liangjie Tang,Xiaodong Xu,Mengmeng Sun,Qingyao Wang,Minghui Yan*

Main category: cs.SE

TL;DR: 提出了Z-Space框架，通过多智能体协作解决大规模MCP服务中工具匹配的挑战，显著降低了推理开销并提高了工具调用准确率


<details>
  <summary>Details</summary>
Motivation: 企业级MCP服务快速发展导致异构工具数量激增，现有基于全提示注入或静态语义检索的方法存在语义脱节、上下文膨胀和高延迟等问题

Method: 建立多智能体协作架构：1) 意图解析模型实现结构化语义理解；2) 基于融合子空间加权算法的工具过滤模块实现细粒度语义对齐；3) 推理执行智能体支持多步骤任务的动态规划和容错执行

Result: 在饿了么技术部部署后，工具推理平均token消耗降低96.26%，工具调用准确率达到92%

Conclusion: Z-Space框架有效解决了大规模工具匹配的挑战，显著提升了智能测试数据生成系统的效率和可靠性

Abstract: Large Language Models can break through knowledge and timeliness limitations by invoking external tools within the Model Context Protocol framework to achieve automated execution of complex tasks. However, with the rapid growth of enterprise-scale MCP services, efficiently and accurately matching target functionalities among thousands of heterogeneous tools has become a core challenge restricting system practicality. Existing approaches generally rely on full-prompt injection or static semantic retrieval, facing issues including semantic disconnection between user queries and tool descriptions, context inflation in LLM input, and high inference latency. To address these challenges, this paper proposes Z-Space, a data-generation-oriented multi-agent collaborative tool invocation framework Z-Space. The Z-Space framework establishes a multi-agent collaborative architecture and tool filtering algorithm: (1) A structured semantic understanding of user queries is achieved through an intent parsing model; (2) A tool filtering module (FSWW) based on fused subspace weighted algorithm realizes fine-grained semantic alignment between intents and tools without parameter tuning; (3) An inference execution agent is constructed to support dynamic planning and fault-tolerant execution for multi-step tasks. This framework has been deployed in the Eleme platform's technical division, serving large-scale test data generation scenarios across multiple business units including Taotian, Gaode, and Hema. Production data demonstrates that the system reduces average token consumption in tool inference by 96.26\% while achieving a 92\% tool invocation accuracy rate, significantly enhancing the efficiency and reliability of intelligent test data generation systems.

</details>


### [2] [stable-pretraining-v1: Foundation Model Research Made Simple](https://arxiv.org/abs/2511.19484)
*Randall Balestriero,Hugues Van Assel,Sami BuGhanem,Lucas Maes*

Main category: cs.SE

TL;DR: 研究人员开发了stable-pretraining库，解决基础模型研究的复杂性，提供模块化、可扩展的工具，加快实验迭代并支持大规模实验。


<details>
  <summary>Details</summary>
Motivation: 当前SSL和基础模型研究受限于复杂代码库、重复实现和繁重的工程负担，阻碍了研究进展。

Method: 基于PyTorch等构建模块化库，集成SSL实用工具（探针、崩溃检测、数据增强等），采用全面日志记录设计。

Result: 验证显示该库能以低开销生成新研究见解，如深度表示探测和分析CLIP在合成数据下的性能退化。

Conclusion: stable-pretraining通过降低入门门槛和保持可扩展性，旨在加速基础模型研究的发现和创新。

Abstract: Foundation models and self-supervised learning (SSL) have become central to modern AI, yet research in this area remains hindered by complex codebases, redundant re-implementations, and the heavy engineering burden of scaling experiments. We present stable-pretraining, a modular, extensible, and performance-optimized library built on top of PyTorch, Lightning, Hugging Face, and TorchMetrics. Unlike prior toolkits focused narrowly on reproducing state-of-the-art results, stable-pretraining is designed for flexibility and iteration speed: it unifies essential SSL utilities--including probes, collapse detection metrics, augmentation pipelines, and extensible evaluation routines--within a coherent and reliable framework. A central design principle is logging everything, enabling fine-grained visibility into training dynamics that makes debugging, monitoring, and reproducibility seamless. We validate the library by demonstrating its ability to generate new research insights with minimal overhead, including depthwise representation probing and the analysis of CLIP degradation under synthetic data finetuning. By lowering barriers to entry while remaining scalable to large experiments, stable-pretraining aims to accelerate discovery and expand the possibilities of foundation model research.

</details>


### [3] [Evolution without an Oracle: Driving Effective Evolution with LLM Judges](https://arxiv.org/abs/2511.19489)
*Zhe Zhao,Yuheng Yang,Haibin Wen,Xiaojie Qiu,Zaixi Zhang,Qingfu Zhang*

Main category: cs.SE

TL;DR: MADE框架通过问题分解将主观评估转化为稳定选择压力，在不依赖客观函数的情况下实现进化优化


<details>
  <summary>Details</summary>
Motivation: 突破传统进化计算依赖可计算适应度函数的限制，探索在纯主观LLM评估下的进化可能性

Method: 引入问题规范方法，将模糊指令分解为可验证子要求，降低LLM评估噪声

Result: 在DevAI和InfoBench基准上超越基线50%以上，软件需求满足率达61.9%，复杂指令遵循完美通过率95%

Conclusion: 验证了从优化可计算指标转向描述性品质的范式转变，为无真实标签的开放领域开辟进化优化新路径

Abstract: The integration of Large Language Models (LLMs) with Evolutionary Computation (EC) has unlocked new frontiers in scientific discovery but remains shackled by a fundamental constraint: the reliance on an Oracle--an objective, machine-computable fitness function. This paper breaks this barrier by asking: Can evolution thrive in a purely subjective landscape governed solely by LLM judges? We introduce MADE (Multi-Agent Decomposed Evolution), a framework that tames the inherent noise of subjective evaluation through "Problem Specification." By decomposing vague instructions into specific, verifiable sub-requirements, MADE transforms high-variance LLM feedback into stable, precise selection pressure. The results are transformative: across complex benchmarks like DevAI and InfoBench, MADE outperforms strong baselines by over 50% in software requirement satisfaction (39.9% to 61.9%) and achieves a 95% perfect pass rate on complex instruction following. This work validates a fundamental paradigm shift: moving from optimizing "computable metrics" to "describable qualities," thereby unlocking evolutionary optimization for the vast open-ended domains where no ground truth exists.

</details>


### [4] [CodeR3: A GenAI-Powered Workflow Repair and Revival Ecosystem](https://arxiv.org/abs/2511.19510)
*Asif Zaman,Kallol Naha,Khalid Belhajjame,Hasan M. Jamil*

Main category: cs.SE

TL;DR: 提出CodeR³系统，使用生成式AI将过时的Taverna工作流迁移到Snakemake和VisFlow等现代技术，并通过案例研究验证可行性


<details>
  <summary>Details</summary>
Motivation: 科学工作流包含宝贵的领域专业知识，但大量已发布工作流随时间衰败，特别是像Taverna这样的传统系统因服务终止和依赖过时而失效

Method: 开发CodeR³系统，集成生成式AI分析衰败工作流特征，包含逐步工作流分析可视化、自动化服务替代和人在回路验证

Result: 案例研究表明自动化显著减少工作流解析和服务识别的手动工作量，但服务替代和数据验证仍需领域专家参与

Conclusion: 提出平衡自动化效率与必要人工判断的工作流复兴框架，将开发众包平台促进社区协作验证复兴工作流

Abstract: Scientific workflows encode valuable domain expertise and computational methodologies. Yet studies consistently show that a significant proportion of published workflows suffer from decay over time. This problem is particularly acute for legacy workflow systems like Taverna, where discontinued services, obsolete dependencies, and system retirement render previously functional workflows unusable. We present a novel legacy workflow migration system, called CodeR$^3$ (stands for Code Repair, Revival and Reuse), that leverages generative AI to analyze the characteristics of decayed workflows, reproduce them into modern workflow technologies like Snakemake and VisFlow. Our system additionally integrates stepwise workflow analysis visualization, automated service substitution, and human-in-the-loop validation. Through several case studies of Taverna workflow revival, we demonstrate the feasibility of this approach while identifying key challenges that require human oversight. Our findings reveal that automation significantly reduces manual effort in workflow parsing and service identification. However, critical tasks such as service substitution and data validation still require domain expertise. Our result will be a crowdsourcing platform that enables the community to collaboratively revive decayed workflows and validate the functionality and correctness of revived workflows. This work contributes a framework for workflow revival that balances automation efficiency with necessary human judgment.

</details>


### [5] [CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection](https://arxiv.org/abs/2511.19875)
*Qingyu Zhang,Puzhuo Liu,Peng Di,Chenxiong Qian*

Main category: cs.SE

TL;DR: CODEFUSE-COMMITEVAL是首个用于评估大语言模型检测提交信息与代码差异不一致性的基准，结果表明模型能有效检测不一致提交，但不同增强策略对性能有不同影响。


<details>
  <summary>Details</summary>
Motivation: 版本控制中提交信息与代码差异的不一致会误导审核、阻碍维护并污染研究数据集，但缺乏专门的基准来评估MCI检测模型。

Method: 基于ApacheCM数据集，通过规则驱动的突变生成七种不一致信息类型，并使用双重验证。评估了六个开源LLMs在普通设置和三种增强策略下的性能。

Result: 模型检测不一致提交比一致提交更可靠（平均召回率85.95%，精确率80.28%，特异性63.8%）。增强策略效果各异，类型分析显示组件、文件路径和操作不一致更易检测。

Conclusion: CODEFUSE-COMMITEVAL为MCI检测提供了严格基准，揭示了需要更丰富上下文和平衡数据来捕捉高级语义差距。

Abstract: Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level "purpose" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.

</details>


### [6] [LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework](https://arxiv.org/abs/2511.20403)
*Andrea Lops,Fedelucio Narducci,Azzurra Ragone,Michelantonio Trizio,Claudio Barto*

Main category: cs.SE

TL;DR: AgoneTest是一个用于评估LLM生成的Java单元测试的自动化框架，提供标准化评估流程和真实环境测试


<details>
  <summary>Details</summary>
Motivation: 单元测试是软件开发中重要但资源密集的环节，需要标准化工具来比较不同LLM和提示策略的效果

Method: 开发了AgoneTest框架和Classes2Test数据集，集成变异得分和测试异味等高级评估指标进行综合评估

Result: 实验结果表明确实能够编译的测试子集中，LLM生成的测试在覆盖率和缺陷检测方面能够达到或超过人工编写的测试

Conclusion: AgoneTest阐明了LLM在软件测试中的潜力，为模型设计、提示工程和测试实践的改进提供了见解

Abstract: Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [7] [SPQR: A Standardized Benchmark for Modern Safety Alignment Methods in Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.19558)
*Mohammed Talha Alam,Nada Saadi,Fahad Shamshad,Nils Lukas,Karthik Nandakumar,Fahkri Karray,Samuele Poppi*

Main category: cs.CR

TL;DR: 论文提出了SPQR基准来评估文生图扩散模型在良性微调后的安全对齐稳定性，发现现有方法经常失效。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐评估很少测试部署后良性微调（如LoRA个性化、风格适配）下的安全性持久性。

Method: 引入SPQR基准（安全-提示遵循-质量-鲁棒性），通过多语言、领域特定和分布外分析评估安全对齐技术。

Result: 当前安全方法在良性微调下频繁失效，SPQR能识别安全对齐失败的具体场景。

Conclusion: SPQR为文生图安全对齐技术提供了简洁而全面的标准化评估框架。

Abstract: Text-to-image diffusion models can emit copyrighted, unsafe, or private content. Safety alignment aims to suppress specific concepts, yet evaluations seldom test whether safety persists under benign downstream fine-tuning routinely applied after deployment (e.g., LoRA personalization, style/domain adapters). We study the stability of current safety methods under benign fine-tuning and observe frequent breakdowns. As true safety alignment must withstand even benign post-deployment adaptations, we introduce the SPQR benchmark (Safety-Prompt adherence-Quality-Robustness). SPQR is a single-scored metric that provides a standardized and reproducible framework to evaluate how well safety-aligned diffusion models preserve safety, utility, and robustness under benign fine-tuning, by reporting a single leaderboard score to facilitate comparisons. We conduct multilingual, domain-specific, and out-of-distribution analyses, along with category-wise breakdowns, to identify when safety alignment fails after benign fine-tuning, ultimately showcasing SPQR as a concise yet comprehensive benchmark for T2I safety alignment techniques for T2I models.

</details>


### [8] [Accuracy and Efficiency Trade-Offs in LLM-Based Malware Detection and Explanation: A Comparative Study of Parameter Tuning vs. Full Fine-Tuning](https://arxiv.org/abs/2511.19654)
*Stephen C. Gravereaux,Sheikh Rabiul Islam*

Main category: cs.CR

TL;DR: 本研究探讨LoRA微调的LLMs在恶意软件分类的解释性能是否能媲美全微调模型。结果显示全微调整体最优，但某些LoRA配置在性能接近的同时大幅降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 实现可信的恶意软件检测（尤其涉及LLMs时）是重要挑战，需要平衡性能与资源效率。

Method: 使用BLEU、ROUGE和语义相似度指标构建评估框架，对比五种LoRA配置和全微调基准。

Result: 全微调在BLEU和ROUGE上比LoRA变体最高提升10%，但中等LoRA模型在两个指标上超越全微调，同时模型大小减少约81%，训练时间降低超80%。

Conclusion: LoRA在可解释性和资源效率间提供了实用平衡，适合资源受限环境部署，通过自然语言解释增强恶意软件检测系统的透明度和可扩展性。

Abstract: This study examines whether Low-Rank Adaptation (LoRA) fine-tuned Large Language Models (LLMs) can approximate the performance of fully fine-tuned models in generating human-interpretable decisions and explanations for malware classification. Achieving trustworthy malware detection, particularly when LLMs are involved, remains a significant challenge. We developed an evaluation framework using Bilingual Evaluation Understudy (BLEU), Recall-Oriented Understudy for Gisting Evaluation (ROUGE), and Semantic Similarity Metrics to benchmark explanation quality across five LoRA configurations and a fully fine-tuned baseline. Results indicate that full fine-tuning achieves the highest overall scores, with BLEU and ROUGE improvements of up to 10% over LoRA variants. However, mid-range LoRA models deliver competitive performance exceeding full fine-tuning on two metrics while reducing model size by approximately 81% and training time by over 80% on a LoRA model with 15.5% trainable parameters. These findings demonstrate that LoRA offers a practical balance of interpretability and resource efficiency, enabling deployment in resource-constrained environments without sacrificing explanation quality. By providing feature-driven natural language explanations for malware classifications, this approach enhances transparency, analyst confidence, and operational scalability in malware detection systems.

</details>


### [9] [BASICS: Binary Analysis and Stack Integrity Checker System for Buffer Overflow Mitigation](https://arxiv.org/abs/2511.19670)
*Luis Ferreirinha,Iberia Medeiros*

Main category: cs.CR

TL;DR: 提出了一种结合模型检测和执行驱动分析的自动化二进制程序漏洞检测与修复方法，针对CPS系统中的C程序缓冲区溢出漏洞。


<details>
  <summary>Details</summary>
Motivation: 传统漏洞发现技术在二进制代码层面存在可扩展性和精确性问题，导致程序持续存在漏洞风险。

Method: 利用模型检测和执行驱动分析技术构建内存状态空间，通过LTL定义安全属性识别漏洞，采用蹦床技术进行自动修复。

Result: 在Juliet C/C++和SARD数据集上实现87%以上的检测和修正准确率，优于CWE Checker工具。

Conclusion: 该方法能有效提升CPS系统关键软件的可靠性和安全性，为二进制代码安全验证提供了可行方案。

Abstract: Cyber-Physical Systems have played an essential role in our daily lives, providing critical services such as power and water, whose operability, availability, and reliability must be ensured. The C programming language, prevalent in CPS development, is crucial for system control where reliability is critical. However, it is also commonly susceptible to vulnerabilities, particularly buffer overflows. Traditional vulnerability discovery techniques often struggle with scalability and precision when applied directly to the binary code of C programs, which can thereby keep programs vulnerable. This work introduces a novel approach designed to overcome these limitations by leveraging model checking and concolic execution techniques to automatically verify security properties of a program's stack memory in binary code, trampoline techniques to perform automated repair of the issues, and crash-inducing inputs to verify if they were successfully removed. The approach constructs a Memory State Space - MemStaCe- from the binary program's control flow graph and simulations, provided by concolic execution, of C function calls and loop constructs. The security properties, defined in LTL, model the correct behaviour of functions associated with vulnerabilities and allow the approach to identify vulnerabilities in MemStaCe by analysing counterexample traces that are generated when a security property is violated. These vulnerabilities are then addressed with a trampoline-based binary patching method, and the effectiveness of the patches is checked with crash-inducing inputs extracted during concolic execution. We implemented the approach in the BASICS tool for BO mitigation and evaluated using the Juliet C/C++ and SARD datasets and real applications, achieving an accuracy and precision above 87%, both in detection and correction. Also, we compared it with CWE Checker, outperforming it.

</details>


### [10] [CrypTorch: PyTorch-based Auto-tuning Compiler for Machine Learning with Multi-party Computation](https://arxiv.org/abs/2511.19711)
*Jinyu Liu,Gang Tan,Kiwan Maeng*

Main category: cs.CR

TL;DR: 提出了CrypTorch编译器，通过自动选择近似方法解决MPC-based ML中近似计算成为性能瓶颈的问题，相比现有框架实现了显著加速。


<details>
  <summary>Details</summary>
Motivation: MPC-based ML框架中，由于MPC无法原生运行Softmax、GELU等ML操作，现有框架使用近似方法但这些近似往往成为性能瓶颈，且难以优化。

Method: 开发了基于PyTorch 2编译器的CrypTorch，将近似计算与MPC运行时解耦，提供编程接口便于添加新近似方法，并自动选择最优近似以平衡性能与精度。

Result: 仅通过自动调优即可实现1.20-1.7倍加速（无精度损失）或1.31-1.8倍加速（允许精度损失）；结合工程优化后，相比CrypTen实现3.22-8.6倍端到端加速。

Conclusion: CrypTorch通过模块化设计和自动近似选择有效解决了MPC-based ML的性能瓶颈问题，为隐私保护机器学习提供了高效解决方案。

Abstract: Machine learning (ML) involves private data and proprietary model parameters. MPC-based ML allows multiple parties to collaboratively run an ML workload without sharing their private data or model parameters using multi-party computing (MPC). Because MPC cannot natively run ML operations such as Softmax or GELU, existing frameworks use different approximations. Our study shows that, on a well-optimized framework, these approximations often become the dominating bottleneck. Popular approximations are often insufficiently accurate or unnecessarily slow, and these issues are hard to identify and fix in existing frameworks. To tackle this issue, we propose a compiler for MPC-based ML, CrypTorch. CrypTorch disentangles these approximations with the rest of the MPC runtime, allows easily adding new approximations through its programming interface, and automatically selects approximations to maximize both performance and accuracy. Built as an extension to PyTorch 2's compiler, we show that CrypTorch's auto-tuning alone provides 1.20--1.7$\times$ immediate speedup without sacrificing accuracy, and 1.31--1.8$\times$ speedup when some accuracy degradation is allowed, compared to our well-optimized baseline. Combined with better engineering and adoption of state-of-the-art practices, the entire framework brings 3.22--8.6$\times$ end-to-end speedup compared to the popular framework, CrypTen.

</details>


### [11] [Prompt Fencing: A Cryptographic Approach to Establishing Security Boundaries in Large Language Model Prompts](https://arxiv.org/abs/2511.19727)
*Steven Peh*

Main category: cs.CR

TL;DR: 提出Prompt Fencing方法，通过加密认证架构为LLM提示建立安全边界，实验中将注入攻击成功率从86.7%降至0%


<details>
  <summary>Details</summary>
Motivation: LLM在生成部署中容易受到提示注入攻击，这是最重要的安全威胁

Method: 使用加密签名元数据装饰提示段，包括信任评级和内容类型，通过模拟围栏意识实现注入攻击防护

Result: 在300个测试案例中完全预防注入攻击，验证总开销为0.224秒

Conclusion: 该方法平台无关且可增量部署，期待未来模型具备原生围栏意识

Abstract: Large Language Models (LLMs) remain vulnerable to prompt injection attacks, representing the most significant security threat in production deployments. We present Prompt Fencing, a novel architectural approach that applies cryptographic authentication and data architecture principles to establish explicit security boundaries within LLM prompts. Our approach decorates prompt segments with cryptographically signed metadata including trust ratings and content types, enabling LLMs to distinguish between trusted instructions and untrusted content. While current LLMs lack native fence awareness, we demonstrate that simulated awareness through prompt instructions achieved complete prevention of injection attacks in our experiments, reducing success rates from 86.7% (260/300 successful attacks) to 0% (0/300 successful attacks) across 300 test cases with two leading LLM providers. We implement a proof-of-concept fence generation and verification pipeline with a total overhead of 0.224 seconds (0.130s for fence generation, 0.094s for validation) across 100 samples. Our approach is platform-agnostic and can be incrementally deployed as a security layer above existing LLM infrastructure, with the expectation that future models will be trained with native fence awareness for optimal security.

</details>


### [12] [Improving the Identification of Real-world Malware's DNS Covert Channels Using Locality Sensitive Hashing](https://arxiv.org/abs/2511.20229)
*Pascal Ruffing,Denis Petrov,Sebastian Zillien,Steffen Wendzel*

Main category: cs.CR

TL;DR: 本文首次应用局部敏感哈希技术检测DNS隐蔽信道恶意软件，通过统计相似性特征和随机森林分类器实现高精度识别和恶意行为分类。


<details>
  <summary>Details</summary>
Motivation: 当前恶意软件普遍利用DNS隐蔽信道逃避检测，但现有方法难以从变化的DNS流量中准确识别特定恶意软件家族及其行为。

Method: 将DNS子域序列编码为统计相似性特征，结合随机森林分类器进行检测和分类。

Result: 相比现有方法，本方法准确率更高、误报率更低，对未知或变种恶意软件样本具有更强鲁棒性和泛化能力。

Conclusion: 该方法仅基于DNS子域即可可靠分类恶意软件行为（如文件上传/下载），为DNS隐蔽信道检测提供了有效新方案。

Abstract: Nowadays, malware increasingly uses DNS-based covert channels in order to evade detection and maintain stealthy communication with its command-and-control servers. While prior work has focused on detecting such activity, identifying specific malware families and their behaviors from captured network traffic remains challenging due to the variability of DNS. In this paper, we present the first application of Locality Sensitive Hashing to the detection and identification of real-world malware utilizing DNS covert channels. Our approach encodes DNS subdomain sequences into statistical similarity features that effectively capture anomalies indicative of malicious activity. Combined with a Random Forest classifier, our method achieves higher accuracy and reduced false positive rates than prior approaches, while demonstrating improved robustness and generalization to previously unseen or modified malware samples. We further demonstrate that our approach enables reliable classification of malware behavior (e.g., uploading or downloading of files), based solely on DNS subdomains.

</details>


### [13] [Hey there! You are using WhatsApp: Enumerating Three Billion Accounts for Security and Privacy](https://arxiv.org/abs/2511.20252)
*Gabriel K. Gegenhuber,Philipp É. Frenzel,Maximilian Günther,Johanna Ullrich,Aljosha Judmayer*

Main category: cs.CR

TL;DR: WhatsApp存在严重的电话号码枚举漏洞，攻击者每小时可探测上亿号码且不受阻挡，并通过Facebook数据泄露重现了近半数号码仍活跃，还发现了密钥重用等安全问题。


<details>
  <summary>Details</summary>
Motivation: 鉴于WhatsApp作为全球最大即时通讯平台的庞大用户基础，其安全架构中的电话号码查询机制可能被滥用进行大规模枚举攻击，这对用户隐私构成严重威胁。

Method: 通过模拟正常用户查询联系人可用性的过程，大规模探测电话号码是否注册WhatsApp，测试平台的频率限制机制有效性。

Result: 研究发现WhatsApp的速率限制防护无效，每小时可成功探测超过1亿个电话号码；2021年Facebook泄露数据中近半数号码在WhatsApp上仍活跃；发现X25519加密密钥在不同设备和号码间重用现象。

Conclusion: 即时通讯平台的设计需在便利性和隐私保护间取得平衡，仅依赖速率限制不足以防止枚举攻击；密钥重用暴露出实施或欺诈问题；通过与平台合作确认漏洞已修复。

Abstract: WhatsApp, with 3.5 billion active accounts as of early 2025, is the world's largest instant messaging platform. Given its massive user base, WhatsApp plays a critical role in global communication.
  To initiate conversations, users must first discover whether their contacts are registered on the platform. This is achieved by querying WhatsApp's servers with mobile phone numbers extracted from the user's address book (if they allowed access). This architecture inherently enables phone number enumeration, as the service must allow legitimate users to query contact availability. While rate limiting is a standard defense against abuse, we revisit the problem and show that WhatsApp remains highly vulnerable to enumeration at scale. In our study, we were able to probe over a hundred million phone numbers per hour without encountering blocking or effective rate limiting.
  Our findings demonstrate not only the persistence but the severity of this vulnerability. We further show that nearly half of the phone numbers disclosed in the 2021 Facebook data leak are still active on WhatsApp, underlining the enduring risks associated with such exposures. Moreover, we were able to perform a census of WhatsApp users, providing a glimpse on the macroscopic insights a large messaging service is able to generate even though the messages themselves are end-to-end encrypted. Using the gathered data, we also discovered the re-use of certain X25519 keys across different devices and phone numbers, indicating either insecure (custom) implementations, or fraudulent activity.
  In this updated version of the paper, we also provide insights into the collaborative remediation process through which we confirmed that the underlying rate-limiting issue had been resolved.

</details>


### [14] [Can LLMs Make (Personalized) Access Control Decisions?](https://arxiv.org/abs/2511.20284)
*Friederike Groschupp,Daniele Lain,Aritra Dhar,Lara Magdalena Lazier,Srdjan Čapkun*

Main category: cs.CR

TL;DR: 利用大语言模型进行上下文感知的访问控制决策，在减轻用户认知负担的同时达到86%的准确率，但个性化偏好与安全最佳实践之间存在权衡


<details>
  <summary>Details</summary>
Motivation: 传统应用中，复杂的访问控制决策给用户带来巨大认知负担，导致决策质量低下。随着系统复杂性和自动化程度提高，需要更智能的决策方案

Method: 通过用户研究收集307条自然语言隐私声明和14,682个用户访问控制决策，比较通用LLM和个性化LLM的决策准确性，并收集1,446个决策的用户反馈

Result: LLM总体上能很好反映用户偏好，与多数用户决策相比达到86%准确率。个性化LLM能提高与个体用户决策的一致性，但可能违反安全最佳实践

Conclusion: 设计和实施基于自然语言的访问控制系统时，需要在个性化、安全性和实用性之间取得平衡，并考虑相关的设计风险和考量

Abstract: Precise access control decisions are crucial to the security of both traditional applications and emerging agent-based systems. Typically, these decisions are made by users during app installation or at runtime. Due to the increasing complexity and automation of systems, making these access control decisions can add a significant cognitive load on users, often overloading them and leading to suboptimal or even arbitrary access control decisions. To address this problem, we propose to leverage the processing and reasoning capabilities of large language models (LLMs) to make dynamic, context-aware decisions aligned with the user's security preferences. For this purpose, we conducted a user study, which resulted in a dataset of 307 natural-language privacy statements and 14,682 access control decisions made by users. We then compare these decisions against those made by two versions of LLMs: a general and a personalized one, for which we also gathered user feedback on 1,446 of its decisions.
  Our results show that in general, LLMs can reflect users' preferences well, achieving up to 86\% accuracy when compared to the decision made by the majority of users. Our study also reveals a crucial trade-off in personalizing such a system: while providing user-specific privacy preferences to the LLM generally improves agreement with individual user decisions, adhering to those preferences can also violate some security best practices. Based on our findings, we discuss design and risk considerations for implementing a practical natural-language-based access control system that balances personalization, security, and utility.

</details>


### [15] [APT-CGLP: Advanced Persistent Threat Hunting via Contrastive Graph-Language Pre-Training](https://arxiv.org/abs/2511.20290)
*Xuebo Qiu,Mingqi Lv,Yimei Zhang,Tieming Chen,Tiantian Zhu,Qijie Song,Shouling Ji*

Main category: cs.CR

TL;DR: APT-CGLP系统通过对比图语言预训练实现端到端的溯源图与威胁情报语义匹配，无需人工干预


<details>
  <summary>Details</summary>
Motivation: 解决现有基于图匹配的威胁狩猎方法存在的信息丢失问题和人工干预需求

Method: 利用大语言模型合成高质量数据对，采用多目标训练算法结合对比学习和跨模态掩码建模

Result: 在四个真实APT数据集上实验表明，APT-CGLP在准确性和效率上均优于现有最先进方法

Conclusion: APT-CGLP为解决模态鸿沟问题提供了有效的端到端解决方案，显著提升了威胁狩猎的实用性

Abstract: Provenance-based threat hunting identifies Advanced Persistent Threats (APTs) on endpoints by correlating attack patterns described in Cyber Threat Intelligence (CTI) with provenance graphs derived from system audit logs. A fundamental challenge in this paradigm lies in the modality gap--the structural and semantic disconnect between provenance graphs and CTI reports. Prior work addresses this by framing threat hunting as a graph matching task: 1) extracting attack graphs from CTI reports, and 2) aligning them with provenance graphs. However, this pipeline incurs severe \textit{information loss} during graph extraction and demands intensive manual curation, undermining scalability and effectiveness.
  In this paper, we present APT-CGLP, a novel cross-modal APT hunting system via Contrastive Graph-Language Pre-training, facilitating end-to-end semantic matching between provenance graphs and CTI reports without human intervention. First, empowered by the Large Language Model (LLM), APT-CGLP mitigates data scarcity by synthesizing high-fidelity provenance graph-CTI report pairs, while simultaneously distilling actionable insights from noisy web-sourced CTIs to improve their operational utility. Second, APT-CGLP incorporates a tailored multi-objective training algorithm that synergizes contrastive learning with inter-modal masked modeling, promoting cross-modal attack semantic alignment at both coarse- and fine-grained levels. Extensive experiments on four real-world APT datasets demonstrate that APT-CGLP consistently outperforms state-of-the-art threat hunting baselines in terms of accuracy and efficiency.

</details>


### [16] [A Reality Check on SBOM-based Vulnerability Management: An Empirical Study and A Path Forward](https://arxiv.org/abs/2511.20313)
*Li Zhou,Marc Dacier,Charalambos Konstantinou*

Main category: cs.CR

TL;DR: 本文通过大规模实证研究发现，SBOM生成不准确和漏洞扫描高误报率是软件供应链安全的主要问题，并提出基于锁文件和函数调用分析的两阶段解决方案。


<details>
  <summary>Details</summary>
Motivation: 软件物料清单（SBOM）对软件供应链安全至关重要，但实际应用中因生成不准确和漏洞扫描的高误报率而效果受限。

Method: 对2414个开源仓库进行大规模实证研究：1）使用强包管理器的锁文件生成准确SBOM；2）分析下游漏洞扫描器的误报问题；3）引入函数调用分析来削减误报。

Result: 锁文件能生成高精度SBOM；漏洞扫描器误报率高达97.5%，主因是标记了不可达代码的漏洞；函数调用分析可削减63.3%的误报。

Conclusion: 验证了软件供应链安全的两阶段实践方法：先用锁文件生成准确SBOM，再结合函数调用分析生成低噪音漏洞报告，有效缓解开发者的警报疲劳。

Abstract: The Software Bill of Materials (SBOM) is a critical tool for securing the software supply chain (SSC), but its practical utility is undermined by inaccuracies in both its generation and its application in vulnerability scanning. This paper presents a large-scale empirical study on 2,414 open-source repositories to address these issues from a practical standpoint. First, we demonstrate that using lock files with strong package managers enables the generation of accurate and consistent SBOMs, establishing a reliable foundation for security analysis. Using this high-fidelity foundation, however, we expose a more fundamental flaw in practice: downstream vulnerability scanners produce a staggering 97.5\% false positive rate. We pinpoint the primary cause as the flagging of vulnerabilities within unreachable code. We then demonstrate that function call analysis can effectively prune 63.3\% of these false alarms. Our work validates a practical, two-stage approach for SSC security: first, generate an accurate SBOM using lock files and strong package managers, and second, enrich it with function call analysis to produce actionable, low-noise vulnerability reports that alleviate developers' alert fatigue.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [Using Wearable Devices to Improve Chronic PainTreatment among Patients with Opioid Use Disorder](https://arxiv.org/abs/2511.19577)
*Abhay Goyal,Navin Kumar,Kimberly DiMeola,Rafael Trujillo,Soorya Ram Shimgekar,Christian Poellabauer,Pi Zonooz,Ermonda Gjoni-Markaj,Declan Barry,Lynn Madden*

Main category: cs.AI

TL;DR: 本研究探讨了如何利用可穿戴设备和AI技术监测慢性疼痛患者的疼痛波动，发现机器学习模型预测疼痛峰值准确率较高，但大语言模型在此领域应用有限。


<details>
  <summary>Details</summary>
Motivation: 慢性疼痛和阿片类药物使用障碍常并存且相互影响，但目前缺乏针对这两种疾病的整合治疗方案。可穿戴设备能够监测复杂患者信息，但LLMs在疼痛波动分析中的应用尚无研究。

Method: 采用一系列AI方法（包括机器学习和LLMs）分析可穿戴设备数据，研究疼痛峰值的临床相关性。

Result: 机器学习模型预测疼痛峰值的准确率相对较高（>0.7），而LLMs在提供疼痛峰值洞察方面表现有限。

Conclusion: 可穿戴设备实时监测结合先进AI模型有助于早期发现疼痛峰值，支持个性化干预。由于LLMs整体表现有限，需要开发能够在此领域提供可行洞察的LLMs。

Abstract: Chronic pain (CP) and opioid use disorder (OUD) are common and interrelated chronic medical conditions. Currently, there is a paucity of evidence-based integrated treatments for CP and OUD among individuals receiving medication for opioid use disorder (MOUD). Wearable devices have the potential to monitor complex patient information and inform treatment development for persons with OUD and CP, including pain variability (e.g., exacerbations of pain or pain spikes) and clinical correlates (e.g., perceived stress). However, the application of large language models (LLMs) with wearable data for understanding pain spikes, remains unexplored. Consequently, the aim of this pilot study was to examine the clinical correlates of pain spikes using a range of AI approaches. We found that machine learning models achieved relatively high accuracy (>0.7) in predicting pain spikes, while LLMs were limited in providing insights on pain spikes. Real-time monitoring through wearable devices, combined with advanced AI models, could facilitate early detection of pain spikes and support personalized interventions that may help mitigate the risk of opioid relapse, improve adherence to MOUD, and enhance the integration of CP and OUD care. Given overall limited LLM performance, these findings highlight the need to develop LLMs which can provide actionable insights in the OUD/CP context.

</details>


### [18] [HeaRT: A Hierarchical Circuit Reasoning Tree-Based Agentic Framework for AMS Design Optimization](https://arxiv.org/abs/2511.19669)
*Souradip Poddar,Chia-Tung Ho,Ziming Wei,Weidong Cao,Haoxing Ren,David Z. Pan*

Main category: cs.AI

TL;DR: HeaRT是一种用于AMS设计自动化的基础推理引擎，在40电路基准测试中实现>97%推理准确率和>98% Pass@1性能，且运行效率比SOTA基线高2倍


<details>
  <summary>Details</summary>
Motivation: 传统AI驱动的AMS设计自动化算法受限于对高质量数据集的依赖、跨架构可移植性差以及缺乏自适应机制

Method: 提出HeaRT基础推理引擎，作为实现智能自适应人类风格设计优化的第一步

Result: 在40电路基准测试中保持>97%推理准确率和>98% Pass@1性能，电路复杂度增加时性能稳定，运行效率比SOTA基线高2倍

Conclusion: 实验表明HeaRT在不同优化方法的尺寸和拓扑设计自适应任务中实现>3倍收敛速度提升，同时保留先前的设计意图

Abstract: Conventional AI-driven AMS design automation algorithms remain constrained by their reliance on high-quality datasets to capture underlying circuit behavior, coupled with poor transferability across architectures, and a lack of adaptive mechanisms. This work proposes HeaRT, a foundational reasoning engine for automation loops and a first step toward intelligent, adaptive, human-style design optimization. HeaRT consistently demonstrates reasoning accuracy >97% and Pass@1 performance >98% across our 40-circuit benchmark repository, even as circuit complexity increases, while operating at <0.5x real-time token budget of SOTA baselines. Our experiments show that HeaRT yields >3x faster convergence in both sizing and topology design adaptation tasks across diverse optimization approaches, while preserving prior design intent.

</details>


### [19] [FISCAL: Financial Synthetic Claim-document Augmented Learning for Efficient Fact-Checking](https://arxiv.org/abs/2511.19671)
*Rishab Sharma,Iman Saberi,Elham Alipour,Jie JW Wu,Fatemeh Fard*

Main category: cs.AI

TL;DR: 提出了FISCAL框架，通过生成针对性合成数据训练MiniCheck-FISCAL轻量验证器，在金融事实核查任务上超越GPT-3.5 Turbo等模型，接近大型系统性能。


<details>
  <summary>Details</summary>
Motivation: 金融领域的大语言模型需要事实可靠性和计算效率，但现有系统常产生幻觉且依赖过大模型。

Method: 开发FISCAL模块化框架生成金融事实核查专用合成数据，并用其训练轻量验证器MiniCheck-FISCAL。

Result: MiniCheck-FISCAL超越基线及GPT-3.5 Turbo等同类模型，在外部数据集上媲美GPT-4o和Claude-3.5，优于Gemini-1.5 Flash。

Conclusion: 领域专用合成数据结合高效微调可使紧凑模型实现最先进的准确性、鲁棒性和可扩展性。

Abstract: Financial applications of large language models (LLMs) require factual reliability and computational efficiency, yet current systems often hallucinate details and depend on prohibitively large models. We propose FISCAL (Financial Synthetic Claim-Document Augmented Learning), a modular framework for generating synthetic data tailored to financial fact-checking. Using FISCAL, we generate a dataset called FISCAL-data and use it to train MiniCheck-FISCAL, a lightweight verifier for numerical financial claims. MiniCheck-FISCAL outperforms its baseline, surpasses GPT-3.5 Turbo and other open-source peers of similar size, and approaches the accuracy of much larger systems (20x), such as Mixtral-8x22B and Command R+. On external datasets FinDVer and Fin-Fact, it rivals GPT-4o and Claude-3.5 while outperforming Gemini-1.5 Flash. These results show that domain-specific synthetic data, combined with efficient fine-tuning, enables compact models to achieve state-of-the-art accuracy, robustness, and scalability for practical financial AI. The dataset and scripts are available in the project repository (link provided in the paper).

</details>


### [20] [Scaling Item-to-Standard Alignment with Large Language Models: Accuracy, Limits, and Solutions](https://arxiv.org/abs/2511.19749)
*Farzan Karimi-Malekabadi,Pooya Razavi,Sonya Powers*

Main category: cs.AI

TL;DR: 本研究探讨了大型语言模型在教育评估项目与内容标准对齐中的应用潜力，通过在三个真实任务场景下的测试，证明LLMs能在保持准确性的同时显著减少人工审核负担。


<details>
  <summary>Details</summary>
Motivation: 传统的评估项目人工对齐评审虽然准确但耗时费力，难以应对大规模项目库的需求，需要寻找更高效的自动化解决方案。

Method: 使用超过12,000个K-5年级项目-技能对，测试了三种LLM模型在三个任务上的表现：识别错位项目、从完整标准集中选择正确技能、在分类前筛选候选技能列表。

Result: GPT-4o-mini在识别对齐状态方面达到83-94%准确率；数学领域表现强劲但阅读领域较低；候选技能预筛选将正确技能出现在前五建议的概率提升至95%以上。

Conclusion: LLMs特别是结合候选筛选策略后，可以显著减少项目审核的人工负担同时保持对齐准确性，建议开发结合LLM筛查和人工审核的混合流程。

Abstract: As educational systems evolve, ensuring that assessment items remain aligned with content standards is essential for maintaining fairness and instructional relevance. Traditional human alignment reviews are accurate but slow and labor-intensive, especially across large item banks. This study examines whether Large Language Models (LLMs) can accelerate this process without sacrificing accuracy. Using over 12,000 item-skill pairs in grades K-5, we tested three LLMs (GPT-3.5 Turbo, GPT-4o-mini, and GPT-4o) across three tasks that mirror real-world challenges: identifying misaligned items, selecting the correct skill from the full set of standards, and narrowing candidate lists prior to classification. In Study 1, GPT-4o-mini correctly identified alignment status in approximately 83-94% of cases, including subtle misalignments. In Study 2, performance remained strong in mathematics but was lower for reading, where standards are more semantically overlapping. Study 3 demonstrated that pre-filtering candidate skills substantially improved results, with the correct skill appearing among the top five suggestions more than 95% of the time. These findings suggest that LLMs, particularly when paired with candidate filtering strategies, can significantly reduce the manual burden of item review while preserving alignment accuracy. We recommend the development of hybrid pipelines that combine LLM-based screening with human review in ambiguous cases, offering a scalable solution for ongoing item validation and instructional alignment.

</details>


### [21] [Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs](https://arxiv.org/abs/2511.19773)
*Meng Lu,Ran Xu,Yi Fang,Wenxuan Zhang,Yue Yu,Gaurav Srivastava,Yuchen Zhuang,Mohamed Elhoseiny,Charles Fleming,Carl Yang,Zhengzhong Tu,Yang Xie,Guanghua Xiao,Hanrui Wang,Di Jin,Wenqi Shi,Xuan Wang*

Main category: cs.AI

TL;DR: VISTA-Gym is a scalable training environment that enables vision-language models to learn multi-step visual reasoning through tool integration and reinforcement learning, achieving significant performance improvements on reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models lack the ability to perform multi-step visual reasoning through tool interactions, limiting their practical reasoning capabilities.

Method: Developed VISTA-Gym with standardized interfaces for visual tools, executable interaction loops, and reinforcement learning. Trained VISTA-R1 model using trajectory sampling and end-to-end reinforcement learning.

Result: VISTA-R1-8B outperforms state-of-the-art baselines by 9.51%-18.72% across 11 reasoning-intensive VQA benchmarks.

Conclusion: VISTA-Gym effectively unlocks tool-integrated reasoning capabilities in VLMs, demonstrating substantial improvements in visual reasoning performance.

Abstract: While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to "think with images", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.

</details>


### [22] [KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA)](https://arxiv.org/abs/2511.19798)
*Weizhi Liu,Xi Chen,Zekun Jiang,Liang Zhao,Kunyuan Jiang,Ruisi Tang,Li Wang,Mingke You,Hanyu Zhou,Hongyu Chen,Qiankun Xiong,Yong Nie,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: 开发用于膝骨关节炎自动化管理的多智能体系统KOM，在临床工作流中提升诊疗效率


<details>
  <summary>Details</summary>
Motivation: 膝骨关节炎影响全球6亿多人，个性化多学科干预需要大量医疗资源，在资源有限环境中难以实施

Method: 开发KOM多智能体系统，自动化膝骨关节炎评估、风险预测和治疗方案制定，支持基于患者个体特征的定制化管理计划

Result: 基准实验显示KOM在影像分析和处方生成方面优于通用大语言模型；三臂随机模拟研究表明KOM与临床医生协作减少诊疗时间38.5%，提升治疗质量

Conclusion: KOM有助于促进膝骨关节炎自动化管理，其模块化架构可为其他慢性病AI辅助管理系统开发提供参考

Abstract: Knee osteoarthritis (KOA) affects more than 600 million individuals globally and is associated with significant pain, functional impairment, and disability. While personalized multidisciplinary interventions have the potential to slow disease progression and enhance quality of life, they typically require substantial medical resources and expertise, making them difficult to implement in resource-limited settings. To address this challenge, we developed KOM, a multi-agent system designed to automate KOA evaluation, risk prediction, and treatment prescription. This system assists clinicians in performing essential tasks across the KOA care pathway and supports the generation of tailored management plans based on individual patient profiles, disease status, risk factors, and contraindications. In benchmark experiments, KOM demonstrated superior performance compared to several general-purpose large language models in imaging analysis and prescription generation. A randomized three-arm simulation study further revealed that collaboration between KOM and clinicians reduced total diagnostic and planning time by 38.5% and resulted in improved treatment quality compared to each approach used independently. These findings indicate that KOM could help facilitate automated KOA management and, when integrated into clinical workflows, has the potential to enhance care efficiency. The modular architecture of KOM may also offer valuable insights for developing AI-assisted management systems for other chronic conditions.

</details>


### [23] [A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization](https://arxiv.org/abs/2511.19829)
*Ke Chen,Yifeng Wang,Hassan Almosapeeh,Haohan Wang*

Main category: cs.AI

TL;DR: 提出了一种基于评估引导的提示优化框架，通过性能导向的评估器和指标感知的优化器，实现稳定、可解释的提示改进。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法依赖静态模板或不稳定的文本反馈，缺乏系统化的质量定义和可靠的评估信号。

Method: 建立系统化的提示评估框架，训练免执行的评估器预测多维质量分数，并开发指标感知的优化器进行可解释的提示重写。

Result: 评估器在预测提示性能方面达到最高准确率，优化方法在8个数据集和3个骨干模型上均超越基线。

Conclusion: 提出了统一的提示质量评估视角，证明评估引导的优化流程能实现稳定、可解释且模型无关的改进。

Abstract: Most prompt-optimization methods refine a single static template, making them ineffective in complex and dynamic user scenarios. Existing query-dependent approaches rely on unstable textual feedback or black-box reward models, providing weak and uninterpretable optimization signals. More fundamentally, prompt quality itself lacks a unified, systematic definition, resulting in fragmented and unreliable evaluation signals. Our approach first establishes a performance-oriented, systematic, and comprehensive prompt evaluation framework. Furthermore, we develop and finetune an execution-free evaluator that predicts multi-dimensional quality scores directly from text. The evaluator then instructs a metric-aware optimizer that diagnoses failure modes and rewrites prompts in an interpretable, query-dependent manner. Our evaluator achieves the strongest accuracy in predicting prompt performance, and the evaluation-instructed optimization consistently surpass both static-template and query-dependent baselines across eight datasets and on three backbone models. Overall, we propose a unified, metric-grounded perspective on prompt quality, and demonstrated that our evaluation-instructed optimization pipeline delivers stable, interpretable, and model-agnostic improvements across diverse tasks.

</details>


### [24] [Reinforcement Learning with $ω$-Regular Objectives and Constraints](https://arxiv.org/abs/2511.19849)
*Dominik Wagner,Leon Witzman,Luke Ong*

Main category: cs.AI

TL;DR: 该论文提出了一种结合ω-正则目标和显式约束的强化学习方法，通过线性规划确保策略在满足安全约束的同时最大化目标概率。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖标量奖励，表达能力有限且容易导致奖励破解；同时单一性能指标掩盖了安全与性能的权衡。

Method: 开发基于线性规划的模型强化学习算法，将ω-正则目标与约束分离处理，确保策略在约束阈值内优化目标。

Result: 算法在极限情况下生成满足ω-正则约束并最大化目标概率的策略，并建立了与约束极限平均问题的等价转换。

Conclusion: 该方法解决了强化学习中目标表达和安全约束的分离问题，为复杂环境下的安全强化学习提供了理论保证。

Abstract: Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $ω$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.
  We address both limitations simultaneously by combining $ω$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $ω$-regular objective while also adhering to $ω$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.

</details>


### [25] [MicroSims: A Framework for AI-Generated, Scalable Educational Simulations with Universal Embedding and Adaptive Learning Support](https://arxiv.org/abs/2511.19864)
*Valerie Lockhart,Dan McCreary,Troy A. Peterson*

Main category: cs.AI

TL;DR: MicroSims是一个由AI驱动的轻量级教育仿真框架，通过标准化设计模式、iframe架构和可修改代码实现快速生成、跨平台嵌入和无代码定制，提升学习效果并降低技术门槛。


<details>
  <summary>Details</summary>
Motivation: 传统教育仿真创建需要大量资源和技术专长，限制了其广泛应用。为解决成本高、技术复杂和平台依赖等问题，需要开发更易用的仿真工具。

Method: 提出包含设计原则、技术架构、元数据标准和开发流程的完整框架，结合AI辅助生成、iframe嵌入式安全和透明可修改代码三大创新。

Result: 实证研究表明互动仿真比传统教学能提高30-40%的概念理解率，MicroSims在保持效果的同时显著降低了成本和复杂性。

Conclusion: MicroSims推动了教育公平，为全球教师提供低成本定制化仿真工具，并为AI驱动的自适应学习系统奠定基础。

Abstract: Educational simulations have long been recognized as powerful tools for enhancing learning outcomes, yet their creation has traditionally required substantial resources and technical expertise. This paper introduces MicroSims a novel framework for creating lightweight, interactive educational simulations that can be rapidly generated using artificial intelligence, universally embedded across digital learning platforms, and easily customized without programming knowledge. MicroSims occupy a unique position at the intersection of three key innovations: (1) standardized design patterns that enable AI-assisted generation, (2) iframe-based architecture that provides universal embedding and sandboxed security, and (3) transparent, modifiable code that supports customization and pedagogical transparency. We present a comprehensive framework encompassing design principles, technical architecture, metadata standards, and development workflows. Drawing on empirical research from physics education studies and meta-analyses across STEM disciplines, we demonstrate that interactive simulations can improve conceptual understanding by up to 30-40\% compared to traditional instruction. MicroSims extend these benefits while addressing persistent barriers of cost, technical complexity, and platform dependence. This work has significant implications for educational equity, and low-cost intelligent interactive textbooks that enabling educators worldwide to create customized, curriculum-aligned simulations on demand. We discuss implementation considerations, present evidence of effectiveness, and outline future directions for AI-powered adaptive learning systems built on the MicroSim foundation.

</details>


### [26] [Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy](https://arxiv.org/abs/2511.19872)
*Daniel I Jackson,Emma L Jensen,Syed-Amad Hussain,Emre Sezgin*

Main category: cs.AI

TL;DR: 本研究使用改良的自我效能感量表评估大语言模型的自我评估能力，发现模型在不同任务条件下的自我效能感存在显著差异，但与实际任务表现不完全一致，且普遍低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型的评估主要关注任务准确性，而忽视了模型自我评估这一可靠智能的关键方面。研究旨在系统考察LLMs的自我效能感表现及其与实际能力的关系。

Method: 研究采用改良的10项一般自我效能感量表，对10个LLMs在四种条件（无任务、计算推理、社会推理和文本摘要）下进行模拟自我评估测试，并分析反应的稳定性与任务表现的关系。

Result: 模型自我评估在不同条件下差异显著，总体得分低于人类常模；所有模型在计算和社会推理任务上准确率达到100%，但摘要任务表现参差不齐；自我评估与能力不相关，部分低分模型表现优异而高分模型表现较弱；后续信心提示导致轻微向下修正。

Conclusion: 心理测量提示能结构化洞察LLM的沟通行为，但无法提供校准的性能估计；高自我效能感对应更自信、拟人化的推理风格，低分则反映谨慎、去拟人化的解释。

Abstract: Self-assessment is a key aspect of reliable intelligence, yet evaluations of large language models (LLMs) focus mainly on task accuracy. We adapted the 10-item General Self-Efficacy Scale (GSES) to elicit simulated self-assessments from ten LLMs across four conditions: no task, computational reasoning, social reasoning, and summarization. GSES responses were highly stable across repeated administrations and randomized item orders. However, models showed significantly different self-efficacy levels across conditions, with aggregate scores lower than human norms. All models achieved perfect accuracy on computational and social questions, whereas summarization performance varied widely. Self-assessment did not reliably reflect ability: several low-scoring models performed accurately, while some high-scoring models produced weaker summaries. Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments. Qualitative analysis showed that higher self-efficacy corresponded to more assertive, anthropomorphic reasoning styles, whereas lower scores reflected cautious, de-anthropomorphized explanations. Psychometric prompting provides structured insight into LLM communication behavior but not calibrated performance estimates.

</details>


### [27] [RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation](https://arxiv.org/abs/2511.19895)
*Yuanyuan Lin,Xiangyu Ouyang,Teng Zhang,Kaixin Sui*

Main category: cs.AI

TL;DR: RPM-MCTS是一种基于蒙特卡洛树搜索的代码生成方法，通过知识检索作为过程奖励模型评估中间算法步骤，无需复杂训练，减少计算成本并提高准确性


<details>
  <summary>Details</summary>
Motivation: 传统树搜索方法在代码生成中难以有效评估中间步骤，无法及时定位和纠正错误，导致生成错误代码和计算成本增加

Method: 使用基于知识检索的过程奖励模型（无需复杂训练），在扩展阶段采用相似性过滤去除冗余节点，利用沙箱执行反馈定位错误步骤

Result: 在四个公共代码生成基准测试中优于当前最先进方法，令牌消耗减少约15%

Conclusion: RPM-MCTS显著提升了代码生成性能，且使用其构建的数据对基础模型进行全微调可进一步增强代码能力

Abstract: Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.

</details>


### [28] [Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity](https://arxiv.org/abs/2511.19925)
*Qiyao Wei,Edward Morrell,Lea Goetz,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 论文提出了一种基于知识图谱生成语义相似度评测基准的新方法，解决了现有基准依赖人工标注、领域受限和等价性定义模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 现有语义相似度评测方法存在过度关注语法/词法形式而非语义内容的问题，且现有基准存在生成成本高、领域可用性有限和等价性定义不明确等局限。

Method: 利用知识图谱生成语义相似/不相似的自然语言语句对，其中不相似对分为四种子类型。在四个领域生成基准数据集，并比较传统NLP评分和LLM作为评判者的方法。

Result: 发现语义变化子类型和基准领域都会影响语义相似度方法的性能，没有一种方法始终最优。LLM作为评判者在检测文本语义内容方面存在重要影响。

Conclusion: 提出的基于知识图谱的基准生成方法能有效评估语义相似度方法，为LLM输出语义评估提供了新的解决方案。

Abstract: Evaluating the open-form textual responses generated by Large Language Models (LLMs) typically requires measuring the semantic similarity of the response to a (human generated) reference. However, there is evidence that current semantic similarity methods may capture syntactic or lexical forms over semantic content. While benchmarks exist for semantic equivalence, they often suffer from high generation costs due to reliance on subjective human judgment, limited availability for domain-specific applications, and unclear definitions of equivalence. This paper introduces a novel method for generating benchmarks to evaluate semantic similarity methods for LLM outputs, specifically addressing these limitations. Our approach leverages knowledge graphs (KGs) to generate pairs of natural-language statements that are semantically similar or dissimilar, with dissimilar pairs categorized into one of four sub-types. We generate benchmark datasets in four different domains (general knowledge, biomedicine, finance, biology), and conduct a comparative study of semantic similarity methods including traditional natural language processing scores and LLM-as-a-judge predictions. We observe that the sub-type of semantic variation, as well as the domain of the benchmark impact the performance of semantic similarity methods, with no method being consistently superior. Our results present important implications for the use of LLM-as-a-judge in detecting the semantic content of text. Code is available at https://github.com/QiyaoWei/semantic-kg and the dataset is available at https://huggingface.co/datasets/QiyaoWei/Semantic-KG.

</details>


### [29] [A System-Level Taxonomy of Failure Modes in Large Language Model Applications](https://arxiv.org/abs/2511.19933)
*Vaishali Vinay*

Main category: cs.AI

TL;DR: 这篇论文提出了一个系统级分类法来描述LLM在真实应用中的15种隐藏故障模式，分析了评估与监控实践的差距，并提供了构建可靠LLM系统的设计原则。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型被快速集成到各种应用中，但其在生产环境中的行为未被充分理解，且故障模式与传统机器学习模型有本质不同。

Method: 通过提出系统级分类法来识别和分析LLM的真实故障模式，并考察部署挑战和现有评估实践的局限性。

Result: 识别了15种隐藏故障模式，揭示了评估与监控实践的差距，并为构建可靠LLM系统提供了设计原则。

Conclusion: 将LLM可靠性视为系统工程问题而非纯模型中心问题，为未来研究提供了分析基础。

Abstract: Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.

</details>


### [30] [M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2511.19969)
*Weizi Shao,Taolin Zhang,Zijie Zhou,Chen Chen,Chengyu Wang,Xiaofeng He*

Main category: cs.AI

TL;DR: 提出M³Prune框架，通过多模态层次化通信图剪枝减少多智能体系统的令牌开销，在保持性能的同时显著降低成本


<details>
  <summary>Details</summary>
Motivation: 现有多智能体mRAG系统存在显著的令牌开销和计算成本过高问题，难以大规模部署

Method: 开发多模态多智能体层次化通信图剪枝框架：1) 模态内图稀疏化识别关键边；2) 构造动态通信拓扑；3) 渐进式剪枝冗余边

Result: 在通用和专业mRAG基准测试中持续优于单智能体和鲁棒多智能体系统，同时显著降低令牌消耗

Conclusion: M³Prune框架有效解决了多智能体mRAG系统的效率瓶颈，实现了性能与成本的最佳平衡

Abstract: Recent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelligence of multiple agents can significantly outperform a single model through effective communication. Despite impressive performance, existing multi-agent systems inherently incur substantial token overhead and increased computational costs, posing challenges for large-scale deployment. To address these issues, we propose a novel Multi-Modal Multi-agent hierarchical communication graph PRUNING framework, termed M$^3$Prune. Our framework eliminates redundant edges across different modalities, achieving an optimal balance between task performance and token overhead. Specifically, M$^3$Prune first applies intra-modal graph sparsification to textual and visual modalities, identifying the edges most critical for solving the task. Subsequently, we construct a dynamic communication topology using these key edges for inter-modal graph sparsification. Finally, we progressively prune redundant edges to obtain a more efficient and hierarchical topology. Extensive experiments on both general and domain-specific mRAG benchmarks demonstrate that our method consistently outperforms both single-agent and robust multi-agent mRAG systems while significantly reducing token consumption.

</details>


### [31] [VICoT-Agent: A Vision-Interleaved Chain-of-Thought Framework for Interpretable Multimodal Reasoning and Scalable Remote Sensing Analysis](https://arxiv.org/abs/2511.20085)
*Chujie Wang,Zhiyuan Luo,Ruiqi Liu,Can Ran,Shenghua Fan,Xi Chen,Chu He*

Main category: cs.AI

TL;DR: 提出了VICoT框架，通过视觉工具与思维链的动态交织实现多轮推理，在遥感图像分析中优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 遥感图像分析任务正从简单物体识别转向复杂智能推理，需要更强的推理能力和灵活的工具调用机制。

Method: 采用基于栈的推理结构和模块化MCP兼容工具套件，使大语言模型能高效执行多轮视觉-语言交织推理；并提出推理栈蒸馏方法将复杂Agent行为迁移到轻量模型中。

Result: 在多个遥感基准测试上，VICoT在推理透明度、执行效率和生成质量方面显著优于现有最先进框架。

Conclusion: VICoT框架通过动态整合视觉工具到思维链中，为复杂遥感推理任务提供了有效的解决方案，同时通过蒸馏技术实现了能力与效率的平衡。

Abstract: The current remote sensing image analysis task is increasingly evolving from traditional object recognition to complex intelligence reasoning, which places higher requirements on the model's reasoning ability and the flexibility of tool invocation. To this end, we propose a new multimodal agent framework, Vision-Interleaved Chain-of-Thought Framework (VICoT), which implements explicit multi-round reasoning by dynamically incorporating visual tools into the chain of thought. Through a stack-based reasoning structure and a modular MCP-compatible tool suite, VICoT enables LLMs to efficiently perform multi-round, interleaved vision-language reasoning tasks with strong generalization and flexibility.We also propose the Reasoning Stack distillation method to migrate complex Agent behaviors to small, lightweight models, which ensures the reasoning capability while significantly reducing complexity. Experiments on multiple remote sensing benchmarks demonstrate that VICoT significantly outperforms existing SOTA frameworks in reasoning transparency, execution efficiency, and generation quality.

</details>


### [32] [From data to concepts via wiring diagrams](https://arxiv.org/abs/2511.20138)
*Jason Lo,Mohammadnima Jafari*

Main category: cs.AI

TL;DR: 本文提出了拟骨架接线图的概念，证明了其与Hasse图的对应关系，并设计了从时序数据中提取接线图的算法。算法在分析自主智能体游戏行为中成功识别了获胜策略。


<details>
  <summary>Details</summary>
Motivation: 时序数据通常难以直观理解，需要将抽象概念转换为可视化的接线图表示，以便更好地分析复杂系统的行为模式。

Method: 结合范畴论、图论和聚类技术，提出了拟骨架接线图理论，并设计了基于该理论的算法，与DBSCAN和凝聚层次聚类算法进行了对比。

Result: 算法能够正确识别自主智能体在电脑游戏中的获胜策略，且在数据扰动情况下保持稳健性能。

Conclusion: 拟骨架接线图为时序数据分析提供了新的理论框架和实用工具，融合了多个学科的技术，具有广泛的应用潜力。

Abstract: A wiring diagram is a labeled directed graph that represents an abstract concept such as a temporal process. In this article, we introduce the notion of a quasi-skeleton wiring diagram graph, and prove that quasi-skeleton wiring diagram graphs correspond to Hasse diagrams. Using this result, we designed algorithms that extract wiring diagrams from sequential data. We used our algorithms in analyzing the behavior of an autonomous agent playing a computer game, and the algorithms correctly identified the winning strategies. We compared the performance of our main algorithm with two other algorithms based on standard clustering techniques (DBSCAN and agglomerative hierarchical), including when some of the data was perturbed. Overall, this article brings together techniques in category theory, graph theory, clustering, reinforcement learning, and data engineering.

</details>


### [33] [Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025](https://arxiv.org/abs/2511.20200)
*Yitian Huang,Yuxuan Lei,Jianxun Lian,Hao Liao*

Main category: cs.AI

TL;DR: MSRA_SC团队在CPDC 2025竞赛中提出的统一框架，通过上下文工程和GRPO训练在API和GPU赛道取得了优异成绩


<details>
  <summary>Details</summary>
Motivation: 解决常识人物对话挑战中的工具调用稳定性和对话性能优化问题

Method: 1.上下文工程：动态工具剪枝、人物裁剪、参数归一化、函数合并；2.GPU赛道：GRPO训练替代监督微调

Result: 团队在Task 2 API排名第1，Task 1 API排名第2，Task 3 API和GPU赛道均排名第3

Conclusion: 提出的简单有效框架在常识人物对话任务中表现出色，证明了方法的有效性

Abstract: This report presents the solution and results of our team MSRA\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution

</details>


### [34] [CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents](https://arxiv.org/abs/2511.20216)
*Haebin Seong,Sungmin Kim,Minchan Kim,Yongjun Cho,Myunchul Joe,Suhwan Choi,Jaeyoon Jung,Jiyong Youn,Yoonshik Kim,Samwoo Seong,Yubeen Park,Youngjae Yu,Yunsung Lee*

Main category: cs.AI

TL;DR: CostNav是首个将导航研究指标与商业可行性进行定量比较的微导航经济测试平台，通过全面的成本收益分析评估自动驾驶配送机器人的经济可行性


<details>
  <summary>Details</summary>
Motivation: 现有的导航基准只关注任务成功指标，而忽略了经济可行性这一对自动驾驶配送机器人商业部署至关重要的因素

Method: CostNav模拟完整的经济生命周期，包括硬件、训练、能源、维护成本和配送收入等服务级别协议，使用行业衍生参数。通过从缩减规模模拟到现实配送的投影进行评估

Result: 基线方法实现了43.0%的SLA合规性，但不可商业可行：每次运行损失30.009美元，没有有限的盈亏平衡点。运行成本主要由碰撞引起的维护主导，占总成本的99.7%

Conclusion: CostNav弥合了导航研究和商业部署之间的差距，使研究人员能够在不同导航范式之间做出数据驱动的经济权衡决策。优化任务成功与经济优化部署存在根本性差异

Abstract: Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \emph{CostNav}, a \textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\% SLA compliance but is \emph{not} commercially viable: yielding a loss of \$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.

</details>


### [35] [Improving Language Agents through BREW](https://arxiv.org/abs/2511.20297)
*Shashank Kirtania,Param Biyani,Priyanshu Gupta,Yasharth Bajpai,Roshni Iyer,Sumit Gulwani,Gustavo Soares*

Main category: cs.AI

TL;DR: BREW是一种通过构建和优化知识库来优化LLM智能体的框架，替代了传统重量优化的训练方法，提高了任务精度和效率。


<details>
  <summary>Details</summary>
Motivation: 当前优化LLM智能体的方法（如PPO和GRPO）计算开销大，且生成的策略难以解释和适应。

Method: 引入了BREW框架，通过任务评分和行为准则构建和优化经验学习知识库，使用状态空间搜索确保鲁棒性。

Result: 在多个基准测试中，BREW实现了任务精度提升10-20%，API/工具调用减少10-15%，执行时间更短，计算效率与基础模型相当。

Conclusion: BREW确立了知识库作为模块化、可控的优化基板，实现了透明、可解释和可扩展的行为塑造。

Abstract: Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $τ^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\%$ improvement in task precision, $10-15\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.

</details>


### [36] [Active Inference in Discrete State Spaces from First Principles](https://arxiv.org/abs/2511.20321)
*Patrick Kenny*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We seek to clarify the concept of active inference by disentangling it from the Free Energy Principle. We show how the optimizations that need to be carried out in order to implement active inference in discrete state spaces can be formulated as constrained divergence minimization problems which can be solved by standard mean field methods that do not appeal to the idea of expected free energy. When it is used to model perception, the perception/action divergence criterion that we propose coincides with variational free energy. When it is used to model action, it differs from an expected free energy functional by an entropy regularizer.

</details>


### [37] [VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning](https://arxiv.org/abs/2511.20422)
*Bo Pang,Chenxi Xu,Jierui Ren,Guoping Wang,Sheng Li*

Main category: cs.AI

TL;DR: VibraVerse：一个结合3D几何与物理属性的声学数据集，通过因果链建模实现物理一致性多模态学习


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习框架缺乏物理一致性，忽视了物体几何、材料属性与产生声音之间的内在因果关系

Method: 构建VibraVerse数据集，包含3D几何模型、物理属性参数和合成声学信号；开发CLASP对比学习框架进行跨模态对齐

Result: 基于VibraVerse训练的模型在不同任务上展现出更高的准确性、可解释性和跨模态泛化能力

Conclusion: VibraVerse为物理一致性和因果可解释的多模态学习建立了基准，为声音引导的具身感知提供基础

Abstract: Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -> physical attributes -> modal parameters -> acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.

</details>
