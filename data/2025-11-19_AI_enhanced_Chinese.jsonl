{"id": "2511.13972", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13972", "abs": "https://arxiv.org/abs/2511.13972", "authors": ["Jeremiah Bohr"], "title": "Show and Tell: Prompt Strategies for Style Control in Multi-Turn LLM Code Generation", "comment": "23 pages, 2 figures, 3 tables. Under review", "summary": "Language models generate functionally correct code that tends toward excessive verbosity, with elaborate documentation and defensive patterns that diverge from human baselines. Two prompting mechanisms have emerged for stylistic control: instruction based prompts that articulate abstract directives, and example based prompts that provide concrete code demonstrations. The core problem is whether stylistic constraints persist when models enhance initial implementations with additional features while maintaining high functional accuracy. Here we show that instruction-based, example-based, and combined prompts produce distinct patterns of initial control and expansion discipline over one enhancement turn. We manipulated system prompts across four conditions in a paired two-turn protocol where models first generated solutions to an intermediate Python task, then revised their code under general improvement directives, holding the user task fixed (N = 160 paired programs). Combined prompts produced the strongest initial compression and greatest expansion discipline. Instructions showed large initial effects and moderate expansion discipline. Examples showed modest initial effects with no expansion discipline. These results show that initial prompt effectiveness and expansion discipline are separate aspects of prompt design, and that combined approaches provide the most stable stylistic control in this two-turn workflow.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u6307\u4ee4\u63d0\u793a\u3001\u793a\u4f8b\u63d0\u793a\u548c\u7ec4\u5408\u63d0\u793a\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u98ce\u683c\u63a7\u5236\u6548\u679c\uff0c\u53d1\u73b0\u7ec4\u5408\u63d0\u793a\u5728\u521d\u59cb\u538b\u7f29\u548c\u6269\u5c55\u63a7\u5236\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\u901a\u5e38\u8fc7\u4e8e\u5197\u957f\uff0c\u4e0e\u4eba\u7c7b\u57fa\u51c6\u4e0d\u7b26\uff0c\u9700\u8981\u7814\u7a76\u63d0\u793a\u673a\u5236\u5982\u4f55\u5728\u4fdd\u6301\u529f\u80fd\u51c6\u786e\u6027\u7684\u540c\u65f6\u63a7\u5236\u4ee3\u7801\u98ce\u683c\u3002", "method": "\u91c7\u7528\u56db\u7ec4\u7cfb\u7edf\u63d0\u793a\u6761\u4ef6\uff0c\u5728\u53cc\u8f6e\u534f\u8bae\u4e2d\u8ba9\u6a21\u578b\u9996\u5148\u751f\u6210Python\u4efb\u52a1\u89e3\u51b3\u65b9\u6848\uff0c\u7136\u540e\u5728\u901a\u7528\u6539\u8fdb\u6307\u4ee4\u4e0b\u4fee\u8ba2\u4ee3\u7801\uff08N=160\u5bf9\u7a0b\u5e8f\uff09\u3002", "result": "\u7ec4\u5408\u63d0\u793a\u4ea7\u751f\u6700\u5f3a\u7684\u521d\u59cb\u538b\u7f29\u548c\u6700\u5927\u7684\u6269\u5c55\u63a7\u5236\uff1b\u6307\u4ee4\u63d0\u793a\u6709\u8f83\u5927\u7684\u521d\u59cb\u6548\u679c\u548c\u4e2d\u7b49\u6269\u5c55\u63a7\u5236\uff1b\u793a\u4f8b\u63d0\u793a\u521d\u59cb\u6548\u679c\u4e00\u822c\u4e14\u65e0\u6269\u5c55\u63a7\u5236\u3002", "conclusion": "\u521d\u59cb\u63d0\u793a\u6548\u679c\u548c\u6269\u5c55\u63a7\u5236\u662f\u63d0\u793a\u8bbe\u8ba1\u7684\u4e24\u4e2a\u72ec\u7acb\u65b9\u9762\uff0c\u7ec4\u5408\u65b9\u6cd5\u5728\u53cc\u8f6e\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u63d0\u4f9b\u6700\u7a33\u5b9a\u7684\u98ce\u683c\u63a7\u5236\u3002"}}
{"id": "2511.13996", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.13996", "abs": "https://arxiv.org/abs/2511.13996", "authors": ["Daihan Xu", "Diana Martin"], "title": "Exploring the Use of ChatGPT by Computer Science Students in Software Development: Applications, Ethical Considerations, and Insights for Engineering Education", "comment": "Full paper oral presentation at the European Society for Engineering Education (SEFI) 2025 Annual Conference (September 2025)", "summary": "ChatGPT has been increasingly used in computer science, offering efficient support across software development tasks. While it helps students navigate programming challenges, its use also raises concerns about academic integrity and overreliance. Despite growing interest in this topic, prior research has largely relied on surveys, emphasizing trends over in-depth analysis of students' strategies and ethical awareness. This study complements existing work through a qualitative investigation of how computer science students in one UK institution strategically and ethically engage with ChatGPT in software development projects. Drawing on semi-structured interviews, it explores two key questions: How do computer science students ethically and strategically report using ChatGPT in software development projects? How do students understand and perceive the ethical issues associated with using ChatGPT in academic and professional contexts? Findings reveal a shift in students' learning models, moving from traditional \"independent thinking-manual coding-iterative debugging\" to \"AI-assisted ideation-interactive programming-collaborative optimization.\" Importantly, many use ChatGPT conversationally to deepen understanding, while consciously reserving creative and high-level decision-making tasks for themselves. Students tend to cap ChatGPT's contribution to roughly 30%, and evaluate its output to mitigate overreliance. However, only a minority thoroughly analyze AI-generated code, raising concerns about reduced critical engagement. Meanwhile, students reject uncredited use, highlight risks such as privacy breaches and skill degradation, and call for clear usage guidelines set by their teachers. This research offers novel insights into the evolving learner-AI dynamic and highlights the need for explicit guidance to support responsible and pedagogically sound use of such tools.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5b9a\u6027\u8bbf\u8c08\u63a2\u8ba8\u82f1\u56fd\u8ba1\u7b97\u673a\u79d1\u5b66\u5b66\u751f\u5982\u4f55\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7b56\u7565\u6027\u548c\u4f26\u7406\u6027\u5730\u4f7f\u7528ChatGPT\uff0c\u63ed\u793a\u4e86\u5b66\u4e60\u6a21\u5f0f\u4ece\u4f20\u7edf\u7f16\u7a0b\u8f6c\u5411AI\u8f85\u52a9\u534f\u4f5c\u7684\u53d8\u5316\uff0c\u5b66\u751f\u666e\u904d\u9650\u5236AI\u8d21\u732e\u7387\u5e76\u91cd\u89c6\u4f26\u7406\u51c6\u5219\uff0c\u4f46\u7f3a\u4e4f\u6df1\u5165\u4ee3\u7801\u5ba1\u67e5\uff0c\u547c\u5401\u5236\u5b9a\u660e\u786e\u4f7f\u7528\u6307\u5357\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u4f9d\u8d56\u8c03\u67e5\u6570\u636e\uff0c\u4fa7\u91cd\u4e8e\u8d8b\u52bf\u800c\u975e\u5b66\u751f\u5177\u4f53\u7b56\u7565\u548c\u4f26\u7406\u610f\u8bc6\u7684\u6df1\u5ea6\u5206\u6790\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5b9a\u6027\u8c03\u67e5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u65e8\u5728\u6df1\u5165\u7406\u89e3\u8ba1\u7b97\u673a\u79d1\u5b66\u5b66\u751f\u5728\u8f6f\u4ef6\u5f00\u53d1\u9879\u76ee\u4e2d\u5982\u4f55\u7b56\u7565\u6027\u548c\u4f26\u7406\u6027\u5730\u4f7f\u7528ChatGPT\u3002", "method": "\u91c7\u7528\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u8fdb\u884c\u5b9a\u6027\u7814\u7a76\uff0c\u805a\u7126\u4e8e\u82f1\u56fd\u4e00\u6240\u9ad8\u6821\u7684\u8ba1\u7b97\u673a\u79d1\u5b66\u5b66\u751f\uff0c\u63a2\u8ba8\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\u5b66\u751f\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5982\u4f55\u7b56\u7565\u6027\u548c\u4f26\u7406\u6027\u5730\u4f7f\u7528ChatGPT\uff0c\u4ee5\u53ca\u4ed6\u4eec\u5bf9\u76f8\u5173\u4f26\u7406\u95ee\u9898\u7684\u8ba4\u77e5\u548c\u770b\u6cd5\u3002", "result": "\u53d1\u73b0\u5b66\u751f\u5b66\u4e60\u6a21\u5f0f\u4ece\u201c\u72ec\u7acb\u601d\u8003-\u624b\u52a8\u7f16\u7801-\u8fed\u4ee3\u8c03\u8bd5\u201d\u8f6c\u53d8\u4e3a\u201cAI\u8f85\u52a9\u6784\u601d-\u4ea4\u4e92\u7f16\u7a0b-\u534f\u4f5c\u4f18\u5316\u201d\u3002\u591a\u6570\u5b66\u751f\u5bf9\u8bdd\u5f0f\u4f7f\u7528ChatGPT\u4ee5\u52a0\u6df1\u7406\u89e3\uff0c\u4f46\u81ea\u89c9\u4fdd\u7559\u521b\u610f\u548c\u9ad8\u7ea7\u51b3\u7b56\u4efb\u52a1\uff0c\u9650\u5236AI\u8d21\u732e\u7387\u7ea630%\u5e76\u8bc4\u4f30\u8f93\u51fa\u4ee5\u907f\u514d\u8fc7\u5ea6\u4f9d\u8d56\u3002\u7136\u800c\uff0c\u4ec5\u5c11\u6570\u6df1\u5165\u5206\u6790AI\u751f\u6210\u4ee3\u7801\uff0c\u5b58\u5728\u6279\u5224\u6027\u53c2\u4e0e\u4e0d\u8db3\u7684\u62c5\u5fe7\u3002\u5b66\u751f\u666e\u904d\u53cd\u5bf9\u672a\u6ce8\u660e\u4f7f\u7528\u7684\u884c\u4e3a\uff0c\u5f3a\u8c03\u9690\u79c1\u6cc4\u9732\u548c\u6280\u80fd\u9000\u5316\u98ce\u9669\uff0c\u5e76\u547c\u5401\u6559\u5e08\u5236\u5b9a\u660e\u786e\u4f7f\u7528\u6307\u5357\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5b66\u4e60\u8005\u4e0eAI\u52a8\u6001\u5173\u7cfb\u7684\u65b0\u89c1\u89e3\uff0c\u5f3a\u8c03\u9700\u8981\u660e\u786e\u6307\u5bfc\u4ee5\u652f\u6301\u8d1f\u8d23\u4efb\u548c\u6559\u5b66\u5408\u7406\u7684\u5de5\u5177\u4f7f\u7528\u3002\u5b66\u751f\u5728\u9002\u5e94AI\u8f85\u52a9\u65f6\u8868\u73b0\u51fa\u7b56\u7565\u6027\u548c\u4f26\u7406\u610f\u8bc6\uff0c\u4f46\u9700\u52a0\u5f3a\u4ee3\u7801\u5ba1\u67e5\u7b49\u6279\u5224\u6027\u5b9e\u8df5\uff0c\u6559\u80b2\u8005\u5e94\u63d0\u4f9b\u7ed3\u6784\u5316\u6846\u67b6\u4ee5\u5e73\u8861\u521b\u65b0\u4e0e\u5b66\u672f\u8bda\u4fe1\u3002"}}
{"id": "2511.13998", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13998", "abs": "https://arxiv.org/abs/2511.13998", "authors": ["Jielin Qiu", "Zuxin Liu", "Zhiwei Liu", "Rithesh Murthy", "Jianguo Zhang", "Haolin Chen", "Shiyu Wang", "Ming Zhu", "Liangwei Yang", "Juntao Tan", "Roshan Ram", "Akshara Prabhakar", "Tulika Awalgaonkar", "Zixiang Chen", "Zhepeng Cen", "Cheng Qian", "Shelby Heinecke", "Weiran Yao", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "title": "LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering", "comment": "54-pages", "summary": "As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \\textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.", "AI": {"tldr": "LoCoBench-Agent is a new evaluation framework for LLM agents that extends the original LoCoBench to assess multi-turn interactions, tool usage, and adaptive reasoning in realistic software engineering workflows with context lengths up to 1M tokens.", "motivation": "Existing benchmarks like LoCoBench focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents.", "method": "Extends LoCoBench's 8,000 scenarios into interactive agent environments with 8 specialized tools (file operations, search, code analysis) and evaluates across context lengths from 10K to 1M tokens using 9 metrics across comprehension and efficiency dimensions.", "result": "Evaluation revealed: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation; (3) conversation efficiency varies dramatically across models with strategic tool usage patterns differentiating high-performing agents.", "conclusion": "LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale as the first long-context LLM agent benchmark for software engineering."}}
{"id": "2511.14002", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.14002", "abs": "https://arxiv.org/abs/2511.14002", "authors": ["Chengpeng Li", "Farnaz Behrang", "August Shi", "Peng Liu"], "title": "FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale", "comment": "To appear in ASE 2025", "summary": "Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.", "AI": {"tldr": "FlakyGuard is a new approach that fixes flaky tests using graph-based code analysis to provide optimal context to LLMs, achieving 47.6% repair rate with high developer acceptance.", "motivation": "Flaky tests waste developer time but existing LLM repair tools fail in industrial settings due to providing either too little or too much context.", "method": "Treats code as a graph structure and uses selective graph exploration to find only the most relevant context for LLM-based repair.", "result": "Repairs 47.6% of reproducible flaky tests with 51.8% developer acceptance, outperforming state-of-the-art by at least 22%.", "conclusion": "FlakyGuard effectively solves the context problem in flaky test repair and developers find its root cause explanations highly useful."}}
{"id": "2511.13782", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13782", "abs": "https://arxiv.org/abs/2511.13782", "authors": ["Xiaoxing Lian", "Aidong Yang", "Jun Zhu", "Peng Wang", "Yue Zhang"], "title": "Imagine in Space: Exploring the Frontier of Spatial Intelligence and Reasoning Efficiency in Vision Language Models", "comment": "10 pages,a detail and effective benchmark for spatial reasoning", "summary": "Large language models (LLMs) and vision language models (VLMs), such as DeepSeek R1,OpenAI o3, and Gemini 2.5 Pro, have demonstrated remarkable reasoning capabilities across logical inference, problem solving, and decision making. However, spatial reasoning:a fundamental component of human cognition that includes mental rotation, navigation, and spatial relationship comprehension remains a significant challenge for current advanced VLMs. We hypothesize that imagination, the internal simulation of spatial states, is the dominant reasoning mechanism within a spatial world model. To test this hypothesis and systematically probe current VLM spatial reasoning mechanisms, we introduce SpatiaLite, a fully synthetic benchmark that jointly measures spatial reasoning accuracy and reasoning efficiency. Comprehensive experiments reveal three key findings. First, advanced VLMs predominantly rely on linguistic representations for reasoning and imagination, resulting in significant deficiencies on visual centric tasks that demand perceptual spatial relations and 3D geometry transformations such as mental rotation or projection prediction. Second, advanced VLMs exhibit severe inefficiency in their current spatial reasoning mechanisms, with token usage growing rapidly as transformation complexity increases. Third, we propose an Imagery Driven Framework (IDF) for data synthesis and training, which can implicitly construct an internal world model that is critical for spatial reasoning in VLMs. Building on SpatiaLite, this work delineates the spatial reasoning limits and patterns of advanced VLMs, identifies key shortcomings, and informs future advances", "AI": {"tldr": "SpatiaLite is a synthetic benchmark that reveals VLMs rely more on linguistic representations than visual-spatial reasoning, show inefficiency in complex spatial tasks, and proposes an Imagery Driven Framework to improve spatial reasoning.", "motivation": "Current VLMs excel in many reasoning tasks but struggle with spatial reasoning like mental rotation and navigation, which requires internal simulation of spatial states.", "method": "Introduced SpatiaLite benchmark to measure spatial reasoning accuracy and efficiency, revealing VLMs' reliance on linguistic representations and proposing Imagery Driven Framework for improvement.", "result": "VLMs show significant deficiencies in visual-centric spatial tasks, inefficiency with complex transformations, but IDF framework shows promise for developing spatial reasoning capabilities.", "conclusion": "SpatiaLite identifies key limitations in VLM spatial reasoning and provides a pathway for developing more effective spatial reasoning mechanisms through world model construction."}}
{"id": "2511.13723", "categories": ["cs.CE", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.13723", "abs": "https://arxiv.org/abs/2511.13723", "authors": ["Abhishek Arora", "Caglar Oskay"], "title": "Variational multiscale enrichment method for dynamic response of hyperelastic materials at finite deformation", "comment": null, "summary": "In this manuscript, we extend the variational multiscale enrichment (VME) method to model the dynamic response of hyperelastic materials undergoing large deformations. This approach enables the simulation of wave propagation under scale-inseparable conditions, including short-wavelength regimes, while accounting for material and geometric nonlinearities that lead to wave steepening or flattening. By employing an additive decomposition of the displacement field, we derive multiscale governing equations for the coarse- and fine-scale problems, which naturally incorporate micro-inertial effects. The framework allows the discretization of each unit cell with a patch of coarse-scale elements, which is essential to accurately capture wave propagation in short-wavelength regimes. An operator-split procedure is used to iteratively solve the semi-discrete equations at both scales until convergence is achieved. The coarse-scale problem is integrated explicitly, while the fine-scale problem is solved using either explicit or implicit time integration schemes, including both dissipative and non-dissipative methods. Numerical examples demonstrate that multiscale dissipative schemes effectively suppress spurious oscillations. The multiscale framework was applied to investigate how material and geometric nonlinearities, along with elastic stiffness contrast in heterogeneous microstructures, influence key wave characteristics such as dispersion, attenuation, and steepening. This multiscale computational framework provides a foundation for studying the dynamic response of architected materials.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u53d8\u5206\u591a\u5c3a\u5ea6\u5bcc\u96c6\u65b9\u6cd5\u7528\u4e8e\u6a21\u62df\u8d85\u5f39\u6027\u6750\u6599\u5728\u5927\u53d8\u5f62\u4e0b\u7684\u52a8\u6001\u54cd\u5e94\uff0c\u80fd\u591f\u6a21\u62df\u5c3a\u5ea6\u4e0d\u53ef\u5206\u79bb\u6761\u4ef6\u4e0b\u7684\u6ce2\u4f20\u64ad\uff0c\u5305\u62ec\u77ed\u6ce2\u8303\u56f4\u5185\u7684\u975e\u7ebf\u6027\u6548\u5e94\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u6a21\u62df\u5728\u5c3a\u5ea6\u4e0d\u53ef\u5206\u79bb\u6761\u4ef6\u4e0b\u5305\u542b\u6750\u6599\u548c\u51e0\u4f55\u975e\u7ebf\u6027\u7684\u6ce2\u4f20\u64ad\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u77ed\u6ce2\u8303\u56f4\u5185\u9700\u8981\u8003\u8651\u5fae\u89c2\u60ef\u6027\u6548\u5e94\u548c\u6ce2\u5f62\u7684\u9661\u5316\u6216\u5e73\u5766\u5316\u73b0\u8c61\u3002", "method": "\u91c7\u7528\u4f4d\u79fb\u573a\u7684\u52a0\u6027\u5206\u89e3\uff0c\u63a8\u5bfc\u7c97\u5c3a\u5ea6\u548c\u7ec6\u5c3a\u5ea6\u95ee\u9898\u7684\u591a\u5c3a\u5ea6\u63a7\u5236\u65b9\u7a0b\uff0c\u4f7f\u7528\u7b97\u5b50\u5206\u88c2\u7a0b\u5e8f\u8fed\u4ee3\u6c42\u89e3\u534a\u79bb\u6563\u65b9\u7a0b\u3002\u7c97\u5c3a\u5ea6\u95ee\u9898\u663e\u5f0f\u79ef\u5206\uff0c\u7ec6\u5c3a\u5ea6\u95ee\u9898\u53ef\u91c7\u7528\u663e\u5f0f\u6216\u9690\u5f0f\u65f6\u95f4\u79ef\u5206\u65b9\u6848\u3002", "result": "\u6570\u503c\u7b97\u4f8b\u8868\u660e\u591a\u5c3a\u5ea6\u8017\u6563\u65b9\u6848\u80fd\u6709\u6548\u6291\u5236\u865a\u5047\u632f\u8361\u3002\u7814\u7a76\u53d1\u73b0\u6750\u6599\u548c\u51e0\u4f55\u975e\u7ebf\u6027\u4ee5\u53ca\u5f02\u8d28\u5fae\u7ed3\u6784\u4e2d\u7684\u5f39\u6027\u521a\u5ea6\u5bf9\u6bd4\u5bf9\u6ce2\u7684\u8272\u6563\u3001\u8870\u51cf\u548c\u9661\u5316\u7b49\u5173\u952e\u7279\u6027\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u8be5\u591a\u5c3a\u5ea6\u8ba1\u7b97\u6846\u67b6\u4e3a\u7814\u7a76\u6784\u7b51\u6750\u6599\u7684\u52a8\u6001\u54cd\u5e94\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u80fd\u591f\u51c6\u786e\u6355\u6349\u77ed\u6ce2\u8303\u56f4\u5185\u7684\u6ce2\u4f20\u64ad\u884c\u4e3a\uff0c\u540c\u65f6\u8003\u8651\u590d\u6742\u7684\u975e\u7ebf\u6027\u6548\u5e94\u3002"}}
{"id": "2511.14022", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14022", "abs": "https://arxiv.org/abs/2511.14022", "authors": ["Pradeep Kumar Sharma", "Ishaan Puri", "Mantinder Jit Singh", "Swapnil Shivaprasad", "Hritvik Shrivastava"], "title": "Keeping Code-Aware LLMs Fresh: Full Refresh, In-Context Deltas, and Incremental Fine-Tuning", "comment": null, "summary": "Modern codebases evolve continuously: files are renamed or deleted; public APIs drift; behavior shifts within otherwise familiar modules. A model trained yesterday to map a developer's natural-language question to the exact set of repository file paths that matter will degrade tomorrow, even if the questions themselves look unchanged. In this paper we study, at system scale and across several widely used repositories, how to keep such a model fresh without surrendering retention on earlier code. We frame freshness as a form of domain drift between a base snapshot and the current HEAD, and we compare three families of update strategies: (A) Full Refresh, retraining the entire model at the new snapshot; (B) In-Context Learning (ICL) that injects recent deltas (raw git diffs or concise English summaries) at inference; and (C) Incremental Fine-Tuning (Inc-FT) on delta-derived training sets, with carefully controlled NEW:OLD mixing to mitigate catastrophic forgetting. We contribute an alias-aware evaluation protocol that credits rename while never rewarding deleted paths, and a practical Forgetting Probe that quantifies residual emissions of obsolete paths. Across Flask, SQLAlchemy, Pandas, and Poetry, Inc-FT with old-aware mixes delivers the best overall balance on mixed sets, ICL with English delta summaries delivers the fastest new-code lift when training is not feasible, and Full Refresh remains the ceiling when maximum NEW accuracy matters. We also compare Git-diff Inc-FT to full-file Inc-FT, showing that diffs excel in rename/delete-heavy windows while full-file context wins in behavior-change-heavy windows.", "AI": {"tldr": "The paper compares three strategies (Full Refresh, In-Context Learning, and Incremental Fine-Tuning) for maintaining code search model freshness as codebases evolve, finding that Incremental Fine-Tuning with old-aware mixing provides the best overall balance, while ICL with English delta summaries offers the fastest new-code improvement.", "motivation": "Codebases continuously evolve, causing models trained to map natural-language questions to relevant code files to degrade over time due to domain drift between codebase snapshots.", "method": "The study compares three update strategies: Full Refresh (retraining entire model), In-Context Learning (injecting recent deltas at inference), and Incremental Fine-Tuning (fine-tuning on delta-derived training sets with controlled mixing to prevent catastrophic forgetting).", "result": "Inc-FT with old-aware mixing delivered the best overall balance across Flask, SQLAlchemy, Pandas, and Poetry repositories. ICL with English delta summaries provided fastest new-code improvement when training isn't feasible. Full Refresh remained the accuracy ceiling when maximum new-code accuracy is critical.", "conclusion": "Inc-FT with careful NEW:OLD mixing is optimal for maintaining model freshness while preserving knowledge of older code, with ICL as a practical alternative when retraining is not possible."}}
{"id": "2511.13771", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13771", "abs": "https://arxiv.org/abs/2511.13771", "authors": ["Shaowei Guan", "Yu Zhai", "Zhengyu Zhang", "Yanze Wang", "Hin Chi Kwok"], "title": "ExplainableGuard: Interpretable Adversarial Defense for Large Language Models Using Chain-of-Thought Reasoning", "comment": "9 pages, 2 figures", "summary": "Large Language Models (LLMs) are increasingly vulnerable to adversarial attacks that can subtly manipulate their outputs. While various defense mechanisms have been proposed, many operate as black boxes, lacking transparency in their decision-making. This paper introduces ExplainableGuard, an interpretable adversarial defense framework leveraging the chain-of-thought (CoT) reasoning capabilities of DeepSeek-Reasoner. Our approach not only detects and neutralizes adversarial perturbations in text but also provides step-by-step explanations for each defense action. We demonstrate how tailored CoT prompts guide the LLM to perform a multi-faceted analysis (character, word, structural, and semantic) and generate a purified output along with a human-readable justification. Preliminary results on the GLUE Benchmark and IMDB Movie Reviews dataset show promising defense efficacy. Additionally, a human evaluation study reveals that ExplainableGuard's explanations outperform ablated variants in clarity, specificity, and actionability, with a 72.5% deployability-trust rating, underscoring its potential for more trustworthy LLM deployments.", "AI": {"tldr": "ExplainableGuard is an interpretable adversarial defense framework using DeepSeek-Reasoner's chain-of-thought reasoning to detect and neutralize text perturbations while providing step-by-step explanations.", "motivation": "LLMs are increasingly vulnerable to adversarial attacks, and existing defense mechanisms lack transparency in decision-making.", "method": "Leverages tailored CoT prompts to guide LLM through multi-faceted analysis (character, word, structural, semantic) for adversarial defense.", "result": "Promising defense efficacy on GLUE Benchmark and IMDB Movie Reviews dataset; human evaluation shows 72.5% deployability-trust rating with superior explanation clarity.", "conclusion": "ExplainableGuard demonstrates potential for more trustworthy LLM deployments through interpretable adversarial defense with actionable explanations."}}
{"id": "2511.13905", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2511.13905", "abs": "https://arxiv.org/abs/2511.13905", "authors": ["Amin Heyrani Nobari", "Faez Ahmed"], "title": "PGD-TO: A Scalable Alternative to MMA Using Projected Gradient Descent for Multi-Constraint Topology Optimization", "comment": null, "summary": "Projected Gradient Descent (PGD) methods offer a simple and scalable approach to topology optimization (TO), yet they often struggle with nonlinear and multi-constraint problems due to the complexity of active-set detection. This paper introduces PGD-TO, a framework that reformulates the projection step into a regularized convex quadratic problem, eliminating the need for active-set search and ensuring well-posedness even when constraints are infeasible. The framework employs a semismooth Newton solver for general multi-constraint cases and a binary search projection for single or independent constraints, achieving fast and reliable convergence. It further integrates spectral step-size adaptation and nonlinear conjugate-gradient directions for improved stability and efficiency. We evaluate PGD-TO on four benchmark families representing the breadth of TO problems: (i) minimum compliance with a linear volume constraint, (ii) minimum volume under a nonlinear compliance constraint, (iii) multi-material minimum compliance with four independent volume constraints, and (iv) minimum compliance with coupled volume and center-of-mass constraints. Across these single- and multi-constraint, linear and nonlinear cases, PGD-TO achieves convergence and final compliance comparable to the Method of Moving Asymptotes (MMA) and Optimality Criteria (OC), while reducing per-iteration computation time by 10-43x on general problems and 115-312x when constraints are independent. Overall, PGD-TO establishes a fast, robust, and scalable alternative to MMA, advancing topology optimization toward practical large-scale, multi-constraint, and nonlinear design problems. Public code available at: https://github.com/ahnobari/pyFANTOM", "AI": {"tldr": "PGD-TO\u662f\u4e00\u79cd\u65b0\u7684\u62d3\u6251\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6295\u5f71\u6b65\u91cd\u65b0\u8868\u8ff0\u4e3a\u6b63\u5219\u5316\u51f8\u4e8c\u6b21\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfPGD\u65b9\u6cd5\u5728\u591a\u7ea6\u675f\u548c\u975e\u7ebf\u6027\u95ee\u9898\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u6bd4MMA\u548cOC\u65b9\u6cd5\u5feb10-312\u500d\u7684\u8ba1\u7b97\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\uff08PGD\uff09\u65b9\u6cd5\u5728\u62d3\u6251\u4f18\u5316\u4e2d\u9762\u4e34\u591a\u7ea6\u675f\u548c\u975e\u7ebf\u6027\u95ee\u9898\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6709\u6548\u96c6\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5927\u89c4\u6a21\u8bbe\u8ba1\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5c06\u6295\u5f71\u6b65\u91cd\u65b0\u8868\u8ff0\u4e3a\u6b63\u5219\u5316\u51f8\u4e8c\u6b21\u95ee\u9898\uff0c\u6d88\u9664\u6709\u6548\u96c6\u641c\u7d22\u9700\u6c42\uff1b\u9488\u5bf9\u591a\u7ea6\u675f\u60c5\u51b5\u4f7f\u7528\u534a\u5149\u6ed1\u725b\u987f\u6c42\u89e3\u5668\uff0c\u9488\u5bf9\u5355\u7ea6\u675f\u6216\u72ec\u7acb\u7ea6\u675f\u4f7f\u7528\u4e8c\u5206\u641c\u7d22\u6295\u5f71\uff1b\u96c6\u6210\u8c31\u6b65\u957f\u81ea\u9002\u5e94\u548c\u975e\u7ebf\u6027\u5171\u8f6d\u68af\u5ea6\u65b9\u5411\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "result": "\u5728\u56db\u79cd\u62d3\u6251\u4f18\u5316\u57fa\u51c6\u95ee\u9898\u4e0a\u6d4b\u8bd5\uff08\u5305\u62ec\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u3001\u5355\u7ea6\u675f\u548c\u591a\u7ea6\u675f\u60c5\u51b5\uff09\uff0cPGD-TO\u7684\u6536\u655b\u6027\u548c\u6700\u7ec8\u67d4\u5ea6\u4e0eMMA\u548cOC\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u6bcf\u8f6e\u8fed\u4ee3\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c1110-43\u500d\uff08\u4e00\u822c\u95ee\u9898\uff09\u548c115-312\u500d\uff08\u7ea6\u675f\u72ec\u7acb\u65f6\uff09\u3002", "conclusion": "PGD-TO\u5efa\u7acb\u4e86\u4e00\u4e2a\u5feb\u901f\u3001\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684MMA\u66ff\u4ee3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u62d3\u6251\u4f18\u5316\u5411\u5b9e\u9645\u5927\u89c4\u6a21\u3001\u591a\u7ea6\u675f\u548c\u975e\u7ebf\u6027\u8bbe\u8ba1\u95ee\u9898\u7684\u5e94\u7528\u3002"}}
{"id": "2511.14062", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14062", "abs": "https://arxiv.org/abs/2511.14062", "authors": ["Shenglin Zhang", "Ziang Chen", "Zijing Que", "Yilun Liu", "Yongqian Sun", "Sicheng Wei", "Dan Pei", "Hailin Li"], "title": "LogPurge: Log Data Purification for Anomaly Detection via Rule-Enhanced Filtering", "comment": null, "summary": "Log anomaly detection, which is critical for identifying system failures and preempting security breaches, detects irregular patterns within large volumes of log data, and impacts domains such as service reliability, performance optimization, and database log analysis. Modern log anomaly detection methods rely on training deep learning models on clean, anomaly-free log sequences. However, obtaining such clean log data requires costly and tedious human labeling, and existing automatic cleaning methods fail to fully integrate the specific characteristics and actual semantics of logs in their purification process. In this paper, we propose a cost-aware, rule-enhanced purification framework, LogPurge, that automatically selects a sufficient subset of normal log sequences from contamination log sequences to train a anomaly detection model. Our approach involves a two-stage filtering algorithm: In the first stage, we use a large language model (LLM) to remove clustered anomalous patterns and enhance system rules to improve LLM's understanding of system logs; in the second stage, we utilize a divide-and-conquer strategy that decomposes the remaining contaminated regions into smaller subproblems, allowing each to be effectively purified through the first stage procedure. Our experiments, conducted on two public datasets and one industrial dataset, show that our method significantly removes an average of 98.74% of anomalies while retaining 82.39% of normal samples. Compared to the latest unsupervised log sample selection algorithms, our method achieves F-1 score improvements of 35.7% and 84.11% on the public datasets, and an impressive 149.72% F-1 improvement on the private dataset, demonstrating the effectiveness of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLogPurge\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u6ee4\u7b97\u6cd5\u81ea\u52a8\u4ece\u53d7\u6c61\u67d3\u7684\u65e5\u5fd7\u5e8f\u5217\u4e2d\u7b5b\u9009\u6b63\u5e38\u65e5\u5fd7\u6837\u672c\uff0c\u7528\u4e8e\u8bad\u7ec3\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u6548\u679c\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u5e72\u51c0\u7684\u65e0\u5f02\u5e38\u65e5\u5fd7\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u83b7\u53d6\u8fd9\u7c7b\u6570\u636e\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u73b0\u6709\u81ea\u52a8\u6e05\u6d17\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6574\u5408\u65e5\u5fd7\u7684\u7279\u5b9a\u7279\u5f81\u548c\u5b9e\u9645\u8bed\u4e49\u3002", "method": "\u63d0\u51fa\u6210\u672c\u611f\u77e5\u3001\u89c4\u5219\u589e\u5f3a\u7684\u51c0\u5316\u6846\u67b6LogPurge\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8fc7\u6ee4\u7b97\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528LLM\u53bb\u9664\u805a\u96c6\u5f02\u5e38\u6a21\u5f0f\u5e76\u589e\u5f3a\u7cfb\u7edf\u89c4\u5219\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u5206\u6cbb\u7b56\u7565\u5c06\u5269\u4f59\u6c61\u67d3\u533a\u57df\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u79fb\u966498.74%\u7684\u5f02\u5e38\u540c\u65f6\u4fdd\u755982.39%\u7684\u6b63\u5e38\u6837\u672c\uff0cF-1\u5206\u6570\u76f8\u6bd4\u6700\u65b0\u65e0\u76d1\u7763\u65b9\u6cd5\u63d0\u534735.7%-149.72%\u3002", "conclusion": "LogPurge\u6846\u67b6\u80fd\u6709\u6548\u81ea\u52a8\u51c0\u5316\u65e5\u5fd7\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u83b7\u53d6\u5e72\u51c0\u8bad\u7ec3\u6570\u636e\u7684\u6280\u672f\u96be\u9898\u3002"}}
{"id": "2511.13777", "categories": ["cs.CR", "math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.13777", "abs": "https://arxiv.org/abs/2511.13777", "authors": ["Pierre-Olivier Goffard", "Hansjoerg Albrecher", "Jean-Pierre Fouque"], "title": "Hashpower allocation in Pay-per-Share blockchain mining pools", "comment": null, "summary": "Mining blocks in a blockchain using the \\textit{Proof-of-Work} consensus protocol involves significant risk, as network participants face continuous operational costs while earning infrequent capital gains upon successfully mining a block. A common risk mitigation strategy is to join a mining pool, which combines the computing resources of multiple miners to provide a more stable income. This article examines a Pay-per-Share (PPS) reward system, where the pool manager can adjust both the share difficulty and the management fee. Using a simplified wealth model for miners, we explore how miners should allocate their computing resources among different mining pools, considering the trade-off between risk transfer to the manager and management fees.", "AI": {"tldr": "Analysis of how miners should allocate computing resources between mining pools in a PPS system, considering risk transfer vs. management fees.", "motivation": "Miners face financial risk due to high operational costs and infrequent rewards in Proof-of-Work mining. Mining pools help mitigate risk but involve fees.", "method": "Simplified wealth model analyzing PPS reward system with adjustable share difficulty and management fees.", "result": "Provides framework for optimal resource allocation decisions between pools based on risk tolerance and fee structures.", "conclusion": "Miners must balance risk transfer benefits against management fees when choosing pool allocation strategies."}}
{"id": "2511.13983", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2511.13983", "abs": "https://arxiv.org/abs/2511.13983", "authors": ["Peng Shu", "Junhao Chen", "Zhengliang Liu", "Hanqi Jiang", "Yi Pan", "Khanh Nhu Nguyen", "Zihao Wu", "Huaqin Zhao", "Yiwei Li", "Enze Shi", "ShaoChen Xu"], "title": "MoMoE: A Mixture of Expert Agent Model for Financial Sentiment Analysis", "comment": null, "summary": "We present a novel approach called Mixture of Mixture of Expert (MoMoE) that combines the strengths of Mixture-of-Experts (MoE) architectures with collaborative multi-agent frameworks. By modifying the LLaMA 3.1 8B architecture to incorporate MoE layers in each agent of a layered collaborative structure, we create an ensemble of specialized expert agents that iteratively refine their outputs. Each agent leverages an MoE layer in its final attention block, enabling efficient task decomposition while maintaining computational feasibility. This hybrid approach creates specialized pathways through both the model architecture and the agent collaboration layers. Experimental results demonstrate significant improvements across multiple language understanding and generation benchmarks, highlighting the synergistic benefits of combining expert routing at both the neural and agent levels.", "AI": {"tldr": "\u63d0\u51faMoMoE\u65b9\u6cd5\uff0c\u7ed3\u5408MoE\u67b6\u6784\u4e0e\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5728LLaMA 3.1 8B\u4e2d\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u52a0\u5165MoE\u5c42\uff0c\u5f62\u6210\u534f\u540c\u4e13\u5bb6\u7cfb\u7edf", "motivation": "\u7ed3\u5408MoE\u67b6\u6784\u7684\u9ad8\u6548\u4efb\u52a1\u5206\u89e3\u80fd\u529b\u4e0e\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7684\u534f\u540c\u4f18\u52bf\uff0c\u63d0\u5347\u8bed\u8a00\u4efb\u52a1\u7684\u6027\u80fd", "method": "\u4fee\u6539LLaMA 3.1 8B\u67b6\u6784\uff0c\u5728\u5206\u5c42\u534f\u4f5c\u7ed3\u6784\u7684\u6bcf\u4e2a\u667a\u80fd\u4f53\u6700\u7ec8\u6ce8\u610f\u529b\u5757\u4e2d\u52a0\u5165MoE\u5c42\uff0c\u521b\u5efa\u4e13\u4e1a\u5316\u4e13\u5bb6\u667a\u80fd\u4f53 ensemble", "result": "\u5728\u591a\u9879\u8bed\u8a00\u7406\u89e3\u4e0e\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u63d0\u5347", "conclusion": "\u795e\u7ecf\u5c42\u9762\u548c\u667a\u80fd\u4f53\u5c42\u9762\u7684\u4e13\u5bb6\u8def\u7531\u534f\u540c\u7ed3\u5408\u80fd\u4ea7\u751f\u663e\u8457\u589e\u6548\u4f5c\u7528"}}
{"id": "2511.14215", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14215", "abs": "https://arxiv.org/abs/2511.14215", "authors": ["Malik Muhammad Umer"], "title": "A Practical Implementation of Customized Scrum-Based Agile Framework in Aerospace Software Development Under DO-178C Constraints", "comment": null, "summary": "The increasing complexity of aerospace systems requires development processes that balance agility with stringent safety and certification demands. This study presents an empirically validated Scrum-based Agile framework tailored for DO-178C compliant, safety-critical aerospace software. The framework adapts core Scrum roles, artifacts, and events to meet certification, verification, and independence objectives. Key enhancements include a multi-disciplinary product ownership model, dual compliance-and-functionality acceptance criteria, independent testing and documentation teams, and dedicated certification liaisons. The approach was evaluated through two comparable aerospace projects-one using the customized Agile process and the other a traditional Waterfall model. Results showed significant improvements: a 76% reduction in Total Effort per Requirement, 75% faster Defect Detection, 78% faster Defect Resolution, and over 50% lower Defect Density, while maintaining full compliance with DO-178C Design Assurance Level A. These findings demonstrate that Agile practices and regulatory compliance can coexist effectively when supported by disciplined tailoring and proactive engagement with certification authorities. The study also notes challenges, including increased V&V effort due to recurring Sprint activities and refactoring inherent to iterative development. Nonetheless, it identifies substantial opportunities for further gains through workflow automation, CI/CD practices, and automated documentation, verification, and configuration management. Future research should expand validation of this framework across the aerospace domain and other safety-critical industries with similar certification requirements.", "AI": {"tldr": "Study presents a Scrum-based Agile framework for DO-178C compliant aerospace software development, showing significant improvements in efficiency and defect metrics compared to Waterfall while maintaining full compliance.", "motivation": "The increasing complexity of aerospace systems requires development processes that balance agility with stringent safety and certification demands like DO-178C.", "method": "Customized Scrum framework with multi-disciplinary product ownership, dual acceptance criteria, independent testing teams, and certification liaisons. Evaluated through comparative analysis of Agile vs Waterfall projects.", "result": "76% reduction in Total Effort per Requirement, 75% faster Defect Detection, 78% faster Defect Resolution, over 50% lower Defect Density, while maintaining DO-178C Level A compliance.", "conclusion": "Agile practices and regulatory compliance can coexist effectively with disciplined tailoring. Future opportunities include workflow automation and CI/CD practices for further gains."}}
{"id": "2511.13852", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13852", "abs": "https://arxiv.org/abs/2511.13852", "authors": ["Anna Rodum Bj\u00f8ru", "Rafael Caba\u00f1as", "Helge Langseth", "Antonio Salmer\u00f3n"], "title": "Causal computations in Semi Markovian Structural Causal Models using divide and conquer", "comment": "36 pages, 7 figures, 1 appendix", "summary": "Recently, Bj\u00f8ru et al. proposed a novel divide-and-conquer algorithm for bounding counterfactual probabilities in structural causal models (SCMs). They assumed that the SCMs were learned from purely observational data, leading to an imprecise characterization of the marginal distributions of exogenous variables. Their method leveraged the canonical representation of structural equations to decompose a general SCM with high-cardinality exogenous variables into a set of sub-models with low-cardinality exogenous variables. These sub-models had precise marginals over the exogenous variables and therefore admitted efficient exact inference. The aggregated results were used to bound counterfactual probabilities in the original model. The approach was developed for Markovian models, where each exogenous variable affects only a single endogenous variable. In this paper, we investigate extending the methodology to \\textit{semi-Markovian} SCMs, where exogenous variables may influence multiple endogenous variables. Such models are capable of representing confounding relationships that Markovian models cannot. We illustrate the challenges of this extension using a minimal example, which motivates a set of alternative solution strategies. These strategies are evaluated both theoretically and through a computational study.", "AI": {"tldr": "This paper extends Bj\u00f8ru et al.'s divide-and-conquer algorithm for bounding counterfactual probabilities from Markovian to semi-Markovian SCMs, identifies challenges via a minimal example, and proposes and evaluates alternative solution strategies.", "motivation": "To extend a recent algorithm for bounding counterfactual probabilities in Markovian Structural Causal Models (SCMs) to semi-Markovian SCMs, which can represent confounding relationships not possible in Markovian models.", "method": "The paper investigates the extension by illustrating the challenges with a minimal example. Based on these challenges, it motivates and develops a set of alternative solution strategies.", "result": "The proposed strategies are evaluated both theoretically and through a computational study.", "conclusion": "The research presents and assesses methods to adapt the bounding algorithm for the more complex semi-Markovian setting."}}
{"id": "2511.13781", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.13781", "abs": "https://arxiv.org/abs/2511.13781", "authors": ["Warda Usman", "Yixin Zou", "Daniel Zappala"], "title": "Human-Centered Threat Modeling in Practice: Lessons, Challenges, and Paths Forward", "comment": null, "summary": "Human-centered threat modeling (HCTM) is an emerging area within security and privacy research that focuses on how people define and navigate threats in various social, cultural, and technological contexts. While researchers increasingly approach threat modeling from a human-centered perspective, little is known about how they prepare for and engage with HCTM in practice. In this work, we conduct 23 semi-structured interviews with researchers to examine the state of HCTM, including how researchers design studies, elicit threats, and navigate values, constraints, and long-term goals. We find that HCTM is not a prescriptive process but a set of evolving practices shaped by relationships with participants, disciplinary backgrounds, and institutional structures. Researchers approach threat modeling through sustained groundwork and participant-centered inquiry, guided by values such as care, justice, and autonomy. They also face challenges including emotional strain, ethical dilemmas, and structural barriers that complicate efforts to translate findings into real-world impact. We conclude by identifying opportunities to advance HCTM through shared infrastructure, broader recognition of diverse contributions, and stronger mechanisms for translating findings into policy, design, and societal change.", "AI": {"tldr": "Interview study on human-centered threat modeling practices, revealing it as evolving practices shaped by relationships and values, with challenges in emotional strain and impact translation.", "motivation": "To understand how researchers practically engage with human-centered threat modeling (HCTM) in various contexts, as little is known about their actual practices.", "method": "Conducted 23 semi-structured interviews with researchers to examine HCTM study design, threat elicitation, and navigation of values and constraints.", "result": "HCTM is not prescriptive but evolves through relationships, disciplinary backgrounds, and institutional structures. Researchers use groundwork and participant-centered inquiry guided by care, justice, and autonomy values, while facing emotional strain and structural barriers.", "conclusion": "Opportunities exist to advance HCTM through shared infrastructure, recognition of diverse contributions, and better mechanisms for translating findings into policy and societal change."}}
{"id": "2511.14548", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2511.14548", "abs": "https://arxiv.org/abs/2511.14548", "authors": ["Elisabeth Vogel", "Peter Langend\u00f6rfer"], "title": "Enhancing Cyber-Resilience in Cyber-Physical Systems of Systems:A Methodical Approach", "comment": "11 pages, 3 figures", "summary": "Cyber-physical Systems of Systems (CPSoS) are becoming increasingly prevalent across sectors such as Industry 4.0 and smart homes, where they play a critical role in enabling intelligent, interconnected functionality. Addressing the challenges and resilience requirements of these complex environments, we propose a modified Cyber-Resilience Life-Cycle as a practical framework for sustainable risk mitigation. Our approach enhances the adaptability of CPSoS and supports resilience against evolving system complexities and potential disruptions. We conclude by outlining application scenarios for the modified life-cycle and highlighting its relevance in fostering cyber-resilience in operational systems.", "AI": {"tldr": "Proposes a modified Cyber-Resilience Life-Cycle framework for Cyber-physical Systems of Systems (CPSoS) to enhance adaptability and resilience.", "motivation": "Address challenges and resilience requirements of increasingly prevalent CPSoS in sectors like Industry 4.0 and smart homes.", "method": "Enhanced Cyber-Resilience Life-Cycle framework designed for sustainable risk mitigation.", "result": "Framework improves CPSoS adaptability and resilience against system complexities and disruptions.", "conclusion": "Outlines application scenarios and emphasizes the framework's relevance for fostering cyber-resilience in operational systems."}}
{"id": "2511.14224", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14224", "abs": "https://arxiv.org/abs/2511.14224", "authors": ["Anji Li", "Mingwei Liu", "Zhenxi Chen", "Zheng Pei", "Zike Li", "Dekun Dai", "Yanlin Wang", "Zibin Zheng"], "title": "KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation", "comment": "13 pages, 11 figures", "summary": "Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.", "AI": {"tldr": "KTester integrates project-specific and testing domain knowledge with LLMs to generate more correct and maintainable unit tests, outperforming existing methods in multiple metrics.", "motivation": "LLM-based test generation struggles with producing tests that are both correct and maintainable in real-world projects. Current approaches lack integration of project-specific context and systematic testing knowledge.", "method": "Extracts project structure and usage knowledge through static analysis, employs testing-domain-knowledge-guided separation of test case design and method generation, and uses multi-perspective prompting with testing heuristics.", "result": "Significantly outperforms SOTA baselines: 5.69% higher execution pass rate, 8.83% higher line coverage, generates fewer tests in less time. Human evaluators rate tests higher for correctness, readability, and maintainability.", "conclusion": "KTester's knowledge-driven approach effectively enhances LLM-based test generation, demonstrating practical advantages for real-world software testing through its structured, context-aware methodology."}}
{"id": "2511.13892", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13892", "abs": "https://arxiv.org/abs/2511.13892", "authors": ["Badhan Chandra Das", "Md Tasnim Jawad", "Md Jueal Mia", "M. Hadi Amini", "Yanzhao Wu"], "title": "Jailbreaking Large Vision Language Models in Intelligent Transportation Systems", "comment": null, "summary": "Large Vision Language Models (LVLMs) demonstrate strong capabilities in multimodal reasoning and many real-world applications, such as visual question answering. However, LVLMs are highly vulnerable to jailbreaking attacks. This paper systematically analyzes the vulnerabilities of LVLMs integrated in Intelligent Transportation Systems (ITS) under carefully crafted jailbreaking attacks. First, we carefully construct a dataset with harmful queries relevant to transportation, following OpenAI's prohibited categories to which the LVLMs should not respond. Second, we introduce a novel jailbreaking attack that exploits the vulnerabilities of LVLMs through image typography manipulation and multi-turn prompting. Third, we propose a multi-layered response filtering defense technique to prevent the model from generating inappropriate responses. We perform extensive experiments with the proposed attack and defense on the state-of-the-art LVLMs (both open-source and closed-source). To evaluate the attack method and defense technique, we use GPT-4's judgment to determine the toxicity score of the generated responses, as well as manual verification. Further, we compare our proposed jailbreaking method with existing jailbreaking techniques and highlight severe security risks involved with jailbreaking attacks with image typography manipulation and multi-turn prompting in the LVLMs integrated in ITS.", "AI": {"tldr": "This paper analyzes vulnerabilities of Large Vision Language Models (LVLMs) in Intelligent Transportation Systems (ITS) to jailbreaking attacks, proposing a new attack method using image typography manipulation and multi-turn prompting, and a multi-layered defense technique.", "motivation": "LVLMs are increasingly used in ITS applications but are vulnerable to jailbreaking attacks that could lead to inappropriate responses, highlighting the need for systematic security analysis and robust defenses.", "method": "The paper presents three main contributions: 1) Construction of a harmful query dataset for transportation, 2) A novel jailbreaking attack combining image typography manipulation and multi-turn prompting, 3) A multi-layered response filtering defense technique.", "result": "Extensive experiments on state-of-the-art LVLMs show the effectiveness of the proposed attack method, which outperforms existing techniques, and the defense method's ability to mitigate risks. Evaluation uses GPT-4 judgment and manual verification.", "conclusion": "The study reveals severe security risks in LVLMs integrated in ITS and demonstrates that the proposed defense can effectively protect against sophisticated jailbreaking attacks."}}
{"id": "2511.13789", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13789", "abs": "https://arxiv.org/abs/2511.13789", "authors": ["Haotian Jin", "Yang Li", "Haihui Fan", "Lin Shen", "Xiangfang Li", "Bo Li"], "title": "Uncovering and Aligning Anomalous Attention Heads to Defend Against NLP Backdoor Attacks", "comment": null, "summary": "Backdoor attacks pose a serious threat to the security of large language models (LLMs), causing them to exhibit anomalous behavior under specific trigger conditions. The design of backdoor triggers has evolved from fixed triggers to dynamic or implicit triggers. This increased flexibility in trigger design makes it challenging for defenders to identify their specific forms accurately. Most existing backdoor defense methods are limited to specific types of triggers or rely on an additional clean model for support. To address this issue, we propose a backdoor detection method based on attention similarity, enabling backdoor detection without prior knowledge of the trigger. Our study reveals that models subjected to backdoor attacks exhibit unusually high similarity among attention heads when exposed to triggers. Based on this observation, we propose an attention safety alignment approach combined with head-wise fine-tuning to rectify potentially contaminated attention heads, thereby effectively mitigating the impact of backdoor attacks. Extensive experimental results demonstrate that our method significantly reduces the success rate of backdoor attacks while preserving the model's performance on downstream tasks.", "AI": {"tldr": "Proposes a backdoor detection method using attention similarity and a mitigation approach via attention safety alignment and head-wise fine-tuning, effective against dynamic/implicit triggers without needing trigger knowledge or clean model.", "motivation": "Backdoor attacks with dynamic/implicit triggers are hard to detect as defenders lack trigger knowledge; existing defenses are trigger-specific or require clean models.", "method": "Detects backdoors via high attention head similarity when triggers are present; mitigates via attention safety alignment and head-wise fine-tuning of contaminated heads.", "result": "Method significantly reduces backdoor attack success rates while maintaining downstream task performance, validated extensively.", "conclusion": "The attention-based approach effectively detects and mitigates backdoor attacks without prior trigger knowledge, offering a versatile defense."}}
{"id": "2511.14052", "categories": ["cs.AI", "cs.CE", "stat.AP", "stat.OT"], "pdf": "https://arxiv.org/pdf/2511.14052", "abs": "https://arxiv.org/abs/2511.14052", "authors": ["Amirreza Mehrabi", "Jason W. Morphew", "Breejha Quezada", "N. Sanjay Rebello"], "title": "Making Evidence Actionable in Adaptive Learning", "comment": null, "summary": "Adaptive learning often diagnoses precisely yet intervenes weakly, yielding help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted micro-interventions. The adaptive learning algorithm contains three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted constraint for time and redundancy, and diversity as protection against overfitting to a single resource. We formalize intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows informed by ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy enforced through diversity. Greedy selection serves low-richness and tight-latency regimes, gradient-based relaxation serves rich repositories, and a hybrid method transitions along a richness-latency frontier. In simulation and in an introductory physics deployment with one thousand two hundred four students, both solvers achieved full skill coverage for essentially all learners within bounded watch time. The gradient-based method reduced redundant coverage by approximately twelve percentage points relative to greedy and harmonized difficulty across slates, while greedy delivered comparable adequacy with lower computational cost in scarce settings. Slack variables localized missing content and supported targeted curation, sustaining sufficiency across subgroups. The result is a tractable and auditable controller that closes the diagnostic-pedagogical loop and delivers equitable, load-aware personalization at classroom scale.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6559\u5e08\u4e3b\u5bfc\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u4fdd\u969c\u673a\u5236\uff08\u5145\u5206\u6027\u3001\u6ce8\u610f\u529b\u3001\u591a\u6837\u6027\uff09\u5c06\u6982\u5ff5\u8bc4\u4f30\u8f6c\u5316\u4e3a\u7cbe\u51c6\u7684\u5fae\u5e72\u9884\uff0c\u5728\u7269\u7406\u8bfe\u7a0b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027", "motivation": "\u89e3\u51b3\u81ea\u9002\u5e94\u5b66\u4e60\u4e2d\u8bca\u65ad\u51c6\u786e\u4f46\u5e72\u9884\u8584\u5f31\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5e38\u5e38\u63d0\u4f9b\u65f6\u673a\u4e0d\u5f53\u6216\u5185\u5bb9\u4e0d\u5339\u914d\u7684\u5e2e\u52a9", "method": "\u5c06\u5e72\u9884\u5206\u914d\u5efa\u6a21\u4e3a\u5e26\u7ea6\u675f\u7684\u4e8c\u5143\u6574\u6570\u89c4\u5212\u95ee\u9898\uff0c\u5305\u542b\u8986\u76d6\u7387\u3001\u65f6\u95f4\u3001\u96be\u5ea6\u7a97\u53e3\u7b49\u7ea6\u675f\uff0c\u63d0\u51fa\u8d2a\u5a6a\u9009\u62e9\u3001\u68af\u5ea6\u677e\u5f1b\u548c\u6df7\u5408\u65b9\u6cd5\u4e09\u79cd\u6c42\u89e3\u7b56\u7565", "result": "\u57281204\u540d\u5b66\u751f\u7684\u7269\u7406\u8bfe\u7a0b\u90e8\u7f72\u4e2d\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u5728\u6709\u9650\u65f6\u95f4\u5185\u5b9e\u73b0\u51e0\u4e4e\u6240\u6709\u5b66\u4e60\u8005\u7684\u5168\u6280\u80fd\u8986\u76d6\uff0c\u68af\u5ea6\u65b9\u6cd5\u6bd4\u8d2a\u5a6a\u65b9\u6cd5\u51cf\u5c11\u5197\u4f59\u8986\u76d6\u7ea612%", "conclusion": "\u8be5\u7b97\u6cd5\u5b9e\u73b0\u4e86\u53ef\u8ffd\u6eaf\u3001\u53ef\u5ba1\u8ba1\u7684\u63a7\u5236\u7cfb\u7edf\uff0c\u95ed\u5408\u4e86\u8bca\u65ad-\u6559\u5b66\u5faa\u73af\uff0c\u5728\u8bfe\u5802\u89c4\u6a21\u4e0b\u5b9e\u73b0\u4e86\u516c\u5e73\u4e14\u8d1f\u8f7d\u611f\u77e5\u7684\u4e2a\u6027\u5316\u5b66\u4e60"}}
{"id": "2511.14367", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14367", "abs": "https://arxiv.org/abs/2511.14367", "authors": ["Dulaji Hidellaarachchi", "Sebastian Baltes", "John Grundy"], "title": "How Does Cognitive Capability and Personality Influence Problem-Solving in Coding Interview Puzzles?", "comment": "11 pages, 8 figures, 7 tables", "summary": "Software engineering is a deeply cognitive activity shaped by individual differences that extend beyond technical skill. This study investigates how cognitive capability and personality traits jointly relate to software problem solving among 80 participants (40 software practitioners, 40 software engineering students). Cognitive capability was measured using Baddeleys three minute grammatical reasoning test, while personality was assessed using the IPIP NEO 50 test. Participants further completed nine interview style problem solving questions. Six questions were related to coding and three were related to logical reasoning. Descriptive and correlational analyses show that practitioners achieved slightly higher grammatical reasoning accuracy and overall task performance than students. Grammatical-reasoning accuracy correlated positively with problem solving performance, indicating that stronger cognitive capability is associated with better performance in coding and logical tasks. Personality performance links were systematic. We identified that the conscientiousness trait correlated most strongly with problem solving and with reasoning accuracy, while the openness to experience trait was positively related to both outcomes. Neuroticism showed small, negative associations with accuracy and performance. Taken together, our results suggest that conscientiousness and openness to experience characteristics complement reasoning accuracy to support software problem solving, whereas elevated negative affect may hinder precision under time pressure. Our findings suggest practical implications for education and industry such as integrating structured reasoning tasks in curricula, and considering personality cognition in recruitment and role allocation. We highlight directions for future research such as longitudinal and task diverse replications with larger samples.", "AI": {"tldr": "Study examines cognitive capability and personality traits in software problem solving, finding practitioners slightly outperform students, with conscientiousness and reasoning accuracy being key predictors of performance.", "motivation": "To understand how individual differences in cognitive capability and personality traits influence software problem solving performance.", "method": "Surveyed 80 participants (40 practitioners, 40 students) using Baddeley's reasoning test, IPIP NEO personality test, and 9 problem-solving questions (6 coding, 3 logical).", "result": "Practitioners had higher reasoning accuracy and task performance; reasoning accuracy positively correlated with performance; conscientiousness and openness positively related to performance.", "conclusion": "Conscientiousness and openness complement reasoning ability in software problem solving, while neuroticism may hinder precision under pressure. Implications for education and recruitment."}}
{"id": "2511.13942", "categories": ["cs.AI", "cs.DS", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13942", "abs": "https://arxiv.org/abs/2511.13942", "authors": ["Daniel Weitekamp"], "title": "CORGI: Efficient Pattern Matching With Quadratic Guarantees", "comment": null, "summary": "Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $\u03b2$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.", "AI": {"tldr": "\u5f15\u5165CORGI\u7b97\u6cd5\u89e3\u51b3\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\u4e2d\u6a21\u5f0f\u5339\u914d\u7684\u65f6\u95f4\u7a7a\u95f4\u95ee\u9898\uff0c\u76f8\u6bd4RETE\u7b97\u6cd5\u6709\u66f4\u597d\u7684\u6027\u80fd\u4fdd\u8bc1", "motivation": "\u5b9e\u65f6AI\u7cfb\u7edf\u548c\u6570\u636e\u5e93\u67e5\u8be2\u9700\u8981\u9ad8\u6548\u7684\u6a21\u5f0f\u5339\u914d\uff0c\u4f46\u81ea\u52a8\u751f\u6210\u7684\u89c4\u5219\u5bb9\u6613\u5bfc\u81f4\u6307\u6570\u7ea7\u590d\u6742\u5ea6", "method": "CORGI\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a\u524d\u5411\u6784\u5efa\u5173\u7cfb\u56fe\uff0c\u540e\u5411\u8fed\u4ee3\u751f\u6210\u5339\u914d\uff0c\u65e0\u9700\u4f20\u7edf\u03b2\u5185\u5b58", "result": "\u5728\u7ec4\u5408\u5339\u914d\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8eSOAR\u548cOPS5\u7684RETE\u5b9e\u73b0", "conclusion": "CORGI\u4e3a\u81ea\u52a8\u751f\u6210\u7684\u89c4\u5219\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5339\u914d\u65b9\u6848\uff0c\u907f\u514d\u5185\u5b58\u6ea2\u51fa"}}
{"id": "2511.13808", "categories": ["cs.CR", "cs.LG", "cs.MS"], "pdf": "https://arxiv.org/pdf/2511.13808", "abs": "https://arxiv.org/abs/2511.13808", "authors": ["Edward Raff", "Ryan R. Curtin", "Derek Everett", "Robert J. Joyce", "James Holt"], "title": "Zipf-Gramming: Scaling Byte N-Grams Up to Production Sized Malware Corpora", "comment": "Published in CIKM 2025", "summary": "A classifier using byte n-grams as features is the only approach we have found fast enough to meet requirements in size (sub 2 MB), speed (multiple GB/s), and latency (sub 10 ms) for deployment in numerous malware detection scenarios. However, we've consistently found that 6-8 grams achieve the best accuracy on our production deployments but have been unable to deploy regularly updated models due to the high cost of finding the top-k most frequent n-grams over terabytes of executable programs. Because the Zipfian distribution well models the distribution of n-grams, we exploit its properties to develop a new top-k n-gram extractor that is up to $35\\times$ faster than the previous best alternative. Using our new Zipf-Gramming algorithm, we are able to scale up our production training set and obtain up to 30\\% improvement in AUC at detecting new malware. We show theoretically and empirically that our approach will select the top-k items with little error and the interplay between theory and engineering required to achieve these results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aZipf-Gramming\u7684\u65b0\u578btop-k n-gram\u63d0\u53d6\u7b97\u6cd5\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb35\u500d\uff0c\u80fd\u591f\u5728\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u66f4\u9891\u7e41\u7684\u6a21\u578b\u66f4\u65b0\u548c30%\u7684AUC\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u5b57\u8282n-gram\u5206\u7c7b\u5668\u5728\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u4e2d\u6548\u679c\u826f\u597d\uff0c\u4f46\u66f4\u65b0\u6a21\u578b\u8017\u65f6\u592a\u957f\uff0c\u65e0\u6cd5\u4ece\u592a\u5b57\u8282\u7ea7\u6570\u636e\u4e2d\u5feb\u901f\u63d0\u53d6\u6700\u9891\u7e41\u7684n-gram\u3002", "method": "\u5229\u7528n-gram\u7684Zipf\u5206\u5e03\u7279\u6027\u5f00\u53d1\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5de5\u7a0b\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u63d0\u53d6\u3002", "result": "\u65b0\u7b97\u6cd5\u6bd4\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5\u5feb35\u500d\uff0c\u751f\u4ea7\u8bad\u7ec3\u96c6\u6269\u5c55\u5230\u66f4\u5927\u89c4\u6a21\uff0c\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4bAUC\u63d0\u534730%\u3002", "conclusion": "Zipf-Gramming\u7b97\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21n-gram\u63d0\u53d6\u7684\u6548\u7387\u74f6\u9888\uff0c\u4e3a\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u7cfb\u7edf\u7684\u5b9e\u65f6\u66f4\u65b0\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.14435", "categories": ["cs.SE", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.14435", "abs": "https://arxiv.org/abs/2511.14435", "authors": ["Angelo Ferrando"], "title": "Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems", "comment": "In Proceedings FMAS 2025, arXiv:2511.13245", "summary": "Assuring the safety and trustworthiness of autonomous systems is particularly difficult when learning-enabled components and open environments are involved. Formal methods provide strong guarantees but depend on complete models and static assumptions. Runtime verification (RV) complements them by monitoring executions at run time and, in its predictive variants, by anticipating potential violations. Large language models (LLMs), meanwhile, excel at translating natural language into formal artefacts and recognising patterns in data, yet they remain error-prone and lack formal guarantees. This vision paper argues for a symbiotic integration of RV and LLMs. RV can serve as a guardrail for LLM-driven autonomy, while LLMs can extend RV by assisting specification capture, supporting anticipatory reasoning, and helping to handle uncertainty. We outline how this mutual reinforcement differs from existing surveys and roadmaps, discuss challenges and certification implications, and identify future research directions towards dependable autonomy.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.13970", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13970", "abs": "https://arxiv.org/abs/2511.13970", "authors": ["Sanjay Acharjee", "Abir Khan Ratul", "Diego Patino", "Md Nazmus Sakib"], "title": "Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios", "comment": null, "summary": "Training vision models to detect workplace hazards accurately requires realistic images of unsafe conditions that could lead to accidents. However, acquiring such datasets is difficult because capturing accident-triggering scenarios as they occur is nearly impossible. To overcome this limitation, this study presents a novel scene graph-guided generative AI framework that synthesizes photorealistic images of hazardous scenarios grounded in historical Occupational Safety and Health Administration (OSHA) accident reports. OSHA narratives are analyzed using GPT-4o to extract structured hazard reasoning, which is converted into object-level scene graphs capturing spatial and contextual relationships essential for understanding risk. These graphs guide a text-to-image diffusion model to generate compositionally accurate hazard scenes. To evaluate the realism and semantic fidelity of the generated data, a visual question answering (VQA) framework is introduced. Across four state-of-the-art generative models, the proposed VQA Graph Score outperforms CLIP and BLIP metrics based on entropy-based validation, confirming its higher discriminative sensitivity.", "AI": {"tldr": "A framework using scene graphs and AI to generate realistic workplace hazard images from OSHA reports, with a novel VQA-based evaluation metric.", "motivation": "Difficulty in obtaining real images of workplace accidents necessitates synthetic data generation for training safety detection models.", "method": "GPT-4o analyzes OSHA reports to create scene graphs, which guide a diffusion model to generate hazard images. A VQA framework evaluates realism.", "result": "The proposed VQA Graph Score outperformed CLIP and BLIP metrics in evaluating generated image quality across four generative models.", "conclusion": "The framework successfully generates realistic hazard images and introduces a more sensitive evaluation metric for synthetic safety data."}}
{"id": "2511.13939", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.13939", "abs": "https://arxiv.org/abs/2511.13939", "authors": ["Paul Staat", "Christof Paar", "Swarun Kumar"], "title": "The Battle of Metasurfaces: Understanding Security in Smart Radio Environments", "comment": null, "summary": "Metasurfaces, or Reconfigurable Intelligent Surfaces (RISs), have emerged as a transformative technology for next-generation wireless systems, enabling digitally controlled manipulation of electromagnetic wave propagation. By turning the traditionally passive radio environment into a smart, programmable medium, metasurfaces promise advances in communication and sensing. However, metasurfaces also present a new security frontier: both attackers and defenders can exploit them to alter wireless propagation for their own advantage. While prior security research has primarily explored unilateral metasurface applications - empowering either attackers or defenders - this work investigates symmetric scenarios, where both sides possess comparable metasurface capabilities. Using both theoretical modeling and real-world experiments, we analyze how competing metasurfaces interact for diverse objectives, including signal power and sensing perception. Thereby, we present the first systematic study of context-agnostic metasurface-to-metasurface interactions and their implications for wireless security. Our results reveal that the outcome of metasurface \"battles\" depends on an interplay of timing, placement, algorithmic strategy, and hardware scale. Across multiple case studies in Wi-Fi environments, including wireless jamming, channel obfuscation for sensing and communication, and sensing spoofing, we demonstrate that opposing metasurfaces can substantially or fully negate each other's effects. By undermining previously proposed security and privacy schemes, our findings open new opportunities for designing resilient and high-assurance physical-layer systems in smart radio environments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5bf9\u79f0\u573a\u666f\u4e0b\u53cc\u65b9\u90fd\u62e5\u6709\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762(RIS)\u80fd\u529b\u7684\u65e0\u7ebf\u5b89\u5168\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u5206\u6790\u53d1\u73b0\u5bf9\u7acb\u7684RIS\u53ef\u80fd\u4e92\u76f8\u62b5\u6d88\u6548\u679c\uff0c\u8fd9\u5bf9\u73b0\u6709\u5b89\u5168\u65b9\u6848\u63d0\u51fa\u4e86\u6311\u6218", "motivation": "\u4f20\u7edf\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u65b9\u9762\u7684RIS\u5e94\u7528(\u653b\u51fb\u8005\u6216\u9632\u5fa1\u8005)\uff0c\u672c\u6587\u9996\u6b21\u7814\u7a76\u53cc\u65b9\u90fd\u5177\u6709RIS\u80fd\u529b\u7684\u5bf9\u79f0\u573a\u666f\u53ca\u76f8\u4e92\u4f5c\u7528", "method": "\u91c7\u7528\u7406\u8bba\u5efa\u6a21\u548c\u73b0\u5b9e\u5b9e\u9a8c\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u7ade\u4e89\u6027RIS\u5728\u4e0d\u540c\u76ee\u6807(\u4fe1\u53f7\u529f\u7387\u3001\u611f\u77e5\u6027\u80fd)\u4e0b\u7684\u76f8\u4e92\u4f5c\u7528", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793aRIS\u5bf9\u6297\u7684\u7ed3\u679c\u53d6\u51b3\u4e8e\u65f6\u673a\u3001\u4f4d\u7f6e\u3001\u7b97\u6cd5\u7b56\u7565\u548c\u786c\u4ef6\u89c4\u6a21\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5728Wi-Fi\u73af\u5883\u4e2d\u7684\u6848\u4f8b\u8868\u660e\u5bf9\u7acb\u7684RIS\u53ef\u80fd\u5927\u5e45\u6216\u5b8c\u5168\u62b5\u6d88\u5f7c\u6b64\u6548\u679c", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7834\u574f\u4e86\u5148\u524d\u63d0\u51fa\u7684\u5b89\u5168\u548c\u9690\u79c1\u65b9\u6848\uff0c\u4e3a\u8bbe\u8ba1\u667a\u80fd\u65e0\u7ebf\u7535\u73af\u5883\u4e2d\u5177\u6709\u5f39\u6027\u548c\u9ad8\u4fdd\u969c\u7684\u7269\u7406\u5c42\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u673a\u4f1a"}}
{"id": "2511.14528", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14528", "abs": "https://arxiv.org/abs/2511.14528", "authors": ["Tatiane Ornelas", "Allysson Allex Ara\u00fajo", "J\u00falia Ara\u00fajo", "Marina Ara\u00fajo", "Bianca Trinkenreich", "Marcos Kalinowski"], "title": "LLM-Assisted Thematic Analysis: Opportunities, Limitations, and Recommendations", "comment": null, "summary": "[Context] Large Language Models (LLMs) are increasingly used to assist qualitative research in Software Engineering (SE), yet the methodological implications of this usage remain underexplored. Their integration into interpretive processes such as thematic analysis raises fundamental questions about rigor, transparency, and researcher agency. [Objective] This study investigates how experienced SE researchers conceptualize the opportunities, risks, and methodological implications of integrating LLMs into thematic analysis. [Method] A reflective workshop with 25 ISERN researchers guided participants through structured discussions of LLM-assisted open coding, theme generation, and theme reviewing, using color-coded canvases to document perceived opportunities, limitations, and recommendations. [Results] Participants recognized potential efficiency and scalability gains, but highlighted risks related to bias, contextual loss, reproducibility, and the rapid evolution of LLMs. They also emphasized the need for prompting literacy and continuous human oversight. [Conclusion] Findings portray LLMs as tools that can support, but not substitute, interpretive analysis. The study contributes to ongoing community reflections on how LLMs can responsibly enhance qualitative research in SE.", "AI": {"tldr": "This study explores how experienced Software Engineering researchers view the use of Large Language Models (LLMs) in thematic analysis, highlighting both potential benefits and significant risks.", "motivation": "To investigate the methodological implications, including opportunities, risks, and the impact on rigor and transparency, when integrating LLMs into qualitative research processes like thematic analysis in Software Engineering.", "method": "A reflective workshop with 25 ISERN researchers, using structured discussions and color-coded canvases to document insights on LLM-assisted open coding, theme generation, and theme reviewing.", "result": "Participants acknowledged potential efficiency gains but raised concerns about bias, loss of context, reproducibility issues, and the need for human oversight and prompting literacy due to LLMs' rapid evolution.", "conclusion": "LLMs are seen as supportive tools that cannot replace human interpretive analysis. The study contributes to discussions on responsibly enhancing qualitative SE research with LLMs."}}
{"id": "2511.13987", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13987", "abs": "https://arxiv.org/abs/2511.13987", "authors": ["Antonio Manuel Mart\u00ednez-Heredia", "Dolores Godrid Rodr\u00edguez", "Andr\u00e9s Ortiz Garc\u00eda"], "title": "Artificial Intelligence Agents in Music Analysis: An Integrative Perspective Based on Two Use Cases", "comment": "Extended version of the conference paper presented at SATMUS 2025", "summary": "This paper presents an integrative review and experimental validation of artificial intelligence (AI) agents applied to music analysis and education. We synthesize the historical evolution from rule-based models to contemporary approaches involving deep learning, multi-agent architectures, and retrieval-augmented generation (RAG) frameworks. The pedagogical implications are evaluated through a dual-case methodology: (1) the use of generative AI platforms in secondary education to foster analytical and creative skills; (2) the design of a multiagent system for symbolic music analysis, enabling modular, scalable, and explainable workflows.\n  Experimental results demonstrate that AI agents effectively enhance musical pattern recognition, compositional parameterization, and educational feedback, outperforming traditional automated methods in terms of interpretability and adaptability. The findings highlight key challenges concerning transparency, cultural bias, and the definition of hybrid evaluation metrics, emphasizing the need for responsible deployment of AI in educational environments.\n  This research contributes to a unified framework that bridges technical, pedagogical, and ethical considerations, offering evidence-based guidance for the design and application of intelligent agents in computational musicology and music education.", "AI": {"tldr": "Review and experimental validation of AI agents for music analysis and education, covering evolution from rule-based to deep learning models and their pedagogical applications.", "motivation": "To synthesize AI's historical progression in music and evaluate its educational implications through case studies.", "method": "Integrative review and dual-case methodology: generative AI in secondary education and multi-agent system for symbolic music analysis.", "result": "AI agents enhance musical pattern recognition, composition, and education feedback, outperforming traditional methods in interpretability and adaptability.", "conclusion": "Research provides a framework bridging technical, pedagogical, and ethical aspects for AI in musicology and education, highlighting challenges like transparency and bias."}}
{"id": "2511.14005", "categories": ["cs.CR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.14005", "abs": "https://arxiv.org/abs/2511.14005", "authors": ["Kaiyuan Hu", "Hong Kang", "Yili Jin", "Junhua Liu", "Chengming Hu", "Haolun Wu", "Xue Liu"], "title": "Privis: Towards Content-Aware Secure Volumetric Video Delivery", "comment": null, "summary": "Volumetric video has emerged as a key paradigm in eXtended Reality (XR) and immersive multimedia because it enables highly interactive, spatially consistent 3D experiences. However, the transport-layer security for such 3D content remains largely unaddressed. Existing volumetric streaming pipelines inherit uniform encryption schemes from 2D video, overlooking the heterogeneous privacy sensitivity of different geometry and the strict motion-to-photon latency constraints of real-time XR.\n  We take an initial step toward content-aware secure volumetric video delivery by introducing Privis, a saliency-guided transport framework that (i) partitions volumetric assets into independent units, (ii) applies lightweight authenticated encryption with adaptive key rotation, and (iii) employs selective traffic shaping to balance confidentiality and low latency. Privis specifies a generalized transport-layer security architecture for volumetric media, defining core abstractions and adaptive protection mechanisms. We further explore a prototype implementation and present initial latency measurements to illustrate feasibility and design tradeoffs, providing early empirical guidance toward future work on real-time, saliency-conditioned secure delivery.", "AI": {"tldr": "Privis is a content-aware security framework for volumetric video that uses saliency-guided encryption and selective traffic shaping to balance privacy protection with low latency requirements in XR applications.", "motivation": "Volumetric video streaming lacks tailored security solutions. Current encryption schemes from 2D video don't address the heterogeneous privacy sensitivity of 3D geometry content or meet XR's strict latency constraints.", "method": "Partitions volumetric assets into independent units, applies lightweight authenticated encryption with adaptive key rotation, and uses selective traffic shaping based on content saliency.", "result": "A generalized transport-layer security architecture for volumetric media with core abstractions and adaptive protection mechanisms. Prototype implementation shows feasibility through initial latency measurements.", "conclusion": "Privis provides an initial step toward real-time, saliency-conditioned secure delivery of volumetric video, offering design tradeoffs for future work in XR security."}}
{"id": "2511.14618", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14618", "abs": "https://arxiv.org/abs/2511.14618", "authors": ["Severin Kohler", "Jordi Piera Jim\u00e9nez", "Michael Anywar", "Lars Fuhrmann", "Heather Leslie", "Maximilian Meixner", "Julian Sa\u00df", "Florian K\u00e4rcher", "Diego Bosc\u00e1", "Birger Haarbrandt", "Michael Marschollek", "Roland Eils"], "title": "FHIRconnect: Towards a seamless integration of openEHR and FHIR", "comment": "27 pages, 4 figures", "summary": "Healthcare interoperability between openEHR and HL7 FHIR remains challenging due to fundamental differences in their data modeling approaches and the absence of standardized transformation mechanisms. This paper presents FHIRconnect, a novel domain-specific language and open-source transformation engine that enables standardized, bidirectional data exchange between openEHR and FHIR. Our approach addresses critical interoperability gaps through a triple-layered architecture that achieves 65% mapping reuse across projects by leveraging international archetype-based foundations while supporting local customizations. Using this framework, FHIRconnect successfully mapped 24 international archetypes to 15 FHIR profiles across seven clinical domains. Key contributions include the first comprehensive DSL for openEHR-FHIR transformation with a formal specification, an open-source execution engine (openFHIR), and an accessible mapping library covering high-impact clinical archetypes. Together, these components establish the technical basis for community-driven mapping standardization, reducing reliance on custom ETL solutions and advancing syntactic and semantic interoperability in healthcare IT systems built on open standards.", "AI": {"tldr": "FHIRconnect is a novel DSL and open-source engine enabling standardized bidrectional data exchange between openEHR and HL7 FHIR, addressing interoperability challenges through a triple-layered architecture.", "motivation": "Healthcare interoperability between openEHR and HL7 FHIR remains challenging due to fundamental differences in data modeling approaches and lack of standardized transformation mechanisms.", "method": "A triple-layered architecture using a domain-specific language and open-source transformation engine that leverages international archetype-based foundations while supporting local customizations.", "result": "Achieved 65% mapping reuse across projects and successfully mapped 24 international archetypes to 15 FHIR profiles across seven clinical domains.", "conclusion": "FHIRconnect establishes the technical basis for community-driven mapping standardization, reducing reliance on custom ETL solutions and advancing syntactic and semantic interoperability in healthcare IT systems."}}
{"id": "2511.14018", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14018", "abs": "https://arxiv.org/abs/2511.14018", "authors": ["Minghu Wang", "Shuliang Zhao", "Yuanyuan Zhao", "Hongxia Xu"], "title": "ALEX:A Light Editing-knowledge Extractor", "comment": null, "summary": "The static nature of knowledge within Large Language Models (LLMs) makes it difficult for them to adapt to evolving information, rendering knowledge editing a critical task. However, existing methods struggle with challenges of scalability and retrieval efficiency, particularly when handling complex, multi-hop questions that require multi-step reasoning. To address these challenges, this paper introduces ALEX (A Light Editing-knowledge Extractor), a lightweight knowledge editing framework. The core innovation of ALEX is its hierarchical memory architecture, which organizes knowledge updates (edits) into semantic clusters. This design fundamentally reduces retrieval complexity from a linear O(N) to a highly scalable O(K+N/C). Furthermore, the framework integrates an Inferential Query Synthesis (IQS) module to bridge the semantic gap between queries and facts , and a Dynamic Evidence Adjudication (DEA) engine that executes an efficient two-stage retrieval process. Experiments on the MQUAKE benchmark demonstrate that ALEX significantly improves both the accuracy of multi-hop answers (MultiHop-ACC) and the reliability of reasoning paths (HopWise-ACC). It also reduces the required search space by over 80% , presenting a promising path toward building scalable, efficient, and accurate knowledge editing systems.", "AI": {"tldr": "This paper introduces ALEX, a lightweight knowledge editing framework that improves LLM adaptability through hierarchical memory architecture and two-stage retrieval, achieving better accuracy and efficiency on multi-hop questions.", "motivation": "Static knowledge in LLMs makes them struggle with evolving information. Current knowledge editing methods face scalability and retrieval efficiency issues, especially for complex multi-hop questions requiring multi-step reasoning.", "method": "Hierarchical memory architecture organizing knowledge edits into semantic clusters; Inferential Query Synthesis module to bridge semantic gaps; Dynamic Evidence Adjudication engine for two-stage retrieval.", "result": "Significantly improves multi-hop answer accuracy (MultiHop-ACC) and reasoning reliability (HopWise-ACC) on MQUAKE benchmark while reducing search space by over 80%.", "conclusion": "ALEX presents a promising path toward building scalable, efficient, and accurate knowledge editing systems by fundamentally reducing retrieval complexity from O(N) to O(K+N/C)."}}
{"id": "2511.14032", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.14032", "abs": "https://arxiv.org/abs/2511.14032", "authors": ["Kunal Mukherjee"], "title": "Location-Dependent Cryptosystem", "comment": null, "summary": "Digital content distribution and proprietary research-driven industries face persistent risks from intellectual property theft and unauthorized redistribution. Conventional encryption schemes such as AES, TDES, ECC, and ElGamal provide strong cryptographic guarantees, but they remain fundamentally agnostic to where decryption takes place.In practice, this means that once a decryption key is leaked or intercepted, any adversary can misuse the key to decrypt the protected content from any location. We present a location-dependent cryptosystem in which the decryption key is not transmitted as human- or machine-readable data, but implicitly encoded in precise time-of-flight differences of ultra-wideband (UWB) data transmission packets. The system leverages precise timing hardware and a custom JMTK protocol to map a SHA-256 hashed AES key onto scheduled transmission timestamps. Only receivers located within a predefined spatial region can observe the packet timings that align with the intended \"time slot\" pattern, enabling them to reconstruct the key and decrypt the secret. Receivers outside the authorized region observe incorrect keys. We implement a complete prototype that encrypts and transmits audio data using our cryptosystem, and only when the receiver is within the authorized data, they are able to decrypt the data. Our evaluation demonstrates that the system (i) removes the need to share decryption passwords electronically or physically, (ii) ensures the decryption key cannot be recovered by the eavesdropper, and (iii) provides a non-trivial spatial tolerance for legitimate users.", "AI": {"tldr": "This paper presents a location-dependent cryptosystem that encodes decryption keys in UWB packet timings rather than transmitting them directly, ensuring only receivers within a specific geographic area can decrypt content.", "motivation": "Conventional encryption schemes are vulnerable to key leakage as decryption keys can be misused from any location once intercepted. There is a need for a system that binds decryption capability to physical location.", "method": "The system uses ultra-wideband (UWB) data transmission with precise timing hardware and a custom JMTK protocol to map a SHA-256 hashed AES key onto packet transmission timestamps, creating location-dependent key reconstruction.", "result": "The prototype successfully encrypts and transmits audio data, with decryption only possible when the receiver is within the authorized spatial region. Eavesdroppers outside this region observe incorrect keys.", "conclusion": "The location-dependent cryptosystem eliminates the need for electronic key sharing, prevents key recovery by eavesdroppers, and provides spatial tolerance for legitimate users while maintaining security."}}
{"id": "2511.14023", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14023", "abs": "https://arxiv.org/abs/2511.14023", "authors": ["Chiharu Hagiwara", "Naoki Nonaka", "Yuhta Hashimoto", "Ryu Uchimido", "Jun Seita"], "title": "Syn-STARTS: Synthesized START Triage Scenario Generation Framework for Scalable LLM Evaluation", "comment": "Introducing an open dataset", "summary": "Triage is a critically important decision-making process in mass casualty incidents (MCIs) to maximize victim survival rates. While the role of AI in such situations is gaining attention for making optimal decisions within limited resources and time, its development and performance evaluation require benchmark datasets of sufficient quantity and quality. However, MCIs occur infrequently, and sufficient records are difficult to accumulate at the scene, making it challenging to collect large-scale realworld data for research use. Therefore, we developed Syn-STARTS, a framework that uses LLMs to generate triage cases, and verified its effectiveness. The results showed that the triage cases generated by Syn-STARTS were qualitatively indistinguishable from the TRIAGE open dataset generated by manual curation from training materials. Furthermore, when evaluating the LLM accuracy using hundreds of cases each from the green, yellow, red, and black categories defined by the standard triage method START, the results were found to be highly stable. This strongly indicates the possibility of synthetic data in developing high-performance AI models for severe and critical medical situations.", "AI": {"tldr": "Syn-STARTS is an LLM-based framework for generating synthetic triage cases that are indistinguishable from real datasets, enabling AI model development for mass casualty incidents where real data is scarce.", "motivation": "Mass casualty incidents (MCIs) are rare but require efficient triage systems. AI can optimize triage decisions but lacks sufficient real-world training data due to the infrequency of MCIs.", "method": "Developed Syn-STARTS framework using Large Language Models (LLMs) to generate synthetic triage cases. Compared quality against manually curated TRIAGE dataset and evaluated LLM accuracy across different triage categories (green, yellow, red, black) using START method.", "result": "Synthetic cases were qualitatively indistinguishable from real datasets. LLM accuracy remained highly stable across all triage categories, demonstrating reliable performance.", "conclusion": "Syn-STARTS enables creation of high-quality synthetic data for developing AI triage systems, addressing data scarcity in critical medical scenarios."}}
{"id": "2511.14045", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14045", "abs": "https://arxiv.org/abs/2511.14045", "authors": ["Yule Liu", "Heyi Zhang", "Jinyi Zheng", "Zhen Sun", "Zifan Peng", "Tianshuo Cong", "Yilong Yang", "Xinlei He", "Zhuo Ma"], "title": "GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards", "comment": null, "summary": "Membership inference attacks (MIAs) on large language models (LLMs) pose significant privacy risks across various stages of model training. Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have brought a profound paradigm shift in LLM training, particularly for complex reasoning tasks. However, the on-policy nature of RLVR introduces a unique privacy leakage pattern: since training relies on self-generated responses without fixed ground-truth outputs, membership inference must now determine whether a given prompt (independent of any specific response) is used during fine-tuning. This creates a threat where leakage arises not from answer memorization.\n  To audit this novel privacy risk, we propose Divergence-in-Behavior Attack (DIBA), the first membership inference framework specifically designed for RLVR. DIBA shifts the focus from memorization to behavioral change, leveraging measurable shifts in model behavior across two axes: advantage-side improvement (e.g., correctness gain) and logit-side divergence (e.g., policy drift). Through comprehensive evaluations, we demonstrate that DIBA significantly outperforms existing baselines, achieving around 0.8 AUC and an order-of-magnitude higher TPR@0.1%FPR. We validate DIBA's superiority across multiple settings--including in-distribution, cross-dataset, cross-algorithm, black-box scenarios, and extensions to vision-language models. Furthermore, our attack remains robust under moderate defensive measures.\n  To the best of our knowledge, this is the first work to systematically analyze privacy vulnerabilities in RLVR, revealing that even in the absence of explicit supervision, training data exposure can be reliably inferred through behavioral traces.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9RLVR\u8bad\u7ec3\u6846\u67b6\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5DIBA\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u884c\u4e3a\u53d8\u5316\u800c\u975e\u8bb0\u5fc6\u6548\u5e94\u6765\u68c0\u6d4b\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u9690\u79c1\u5ba1\u8ba1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "RLVR\u8bad\u7ec3\u7684\u5728\u7ebf\u6027\u8d28\u5e26\u6765\u4e86\u72ec\u7279\u7684\u9690\u79c1\u6cc4\u9732\u6a21\u5f0f\uff1a\u7531\u4e8e\u8bad\u7ec3\u4f9d\u8d56\u81ea\u751f\u6210\u54cd\u5e94\u800c\u65e0\u56fa\u5b9a\u771f\u503c\u8f93\u51fa\uff0c\u6210\u5458\u63a8\u7406\u9700\u8981\u5224\u65ad\u63d0\u793a\u662f\u5426\u7528\u4e8e\u5fae\u8c03\uff0c\u800c\u975e\u57fa\u4e8e\u7b54\u6848\u8bb0\u5fc6\u3002", "method": "DIBA\u6846\u67b6\u5c06\u7126\u70b9\u4ece\u8bb0\u5fc6\u8f6c\u5411\u884c\u4e3a\u53d8\u5316\uff0c\u5229\u7528\u6a21\u578b\u5728\u4e24\u4e2a\u8f74\u4e0a\u7684\u53ef\u6d4b\u91cf\u504f\u79fb\uff1a\u4f18\u52bf\u4fa7\u6539\u8fdb\uff08\u5982\u6b63\u786e\u6027\u63d0\u5347\uff09\u548c\u5bf9\u6570\u4fa7\u5206\u6b67\uff08\u5982\u7b56\u7565\u6f02\u79fb\uff09\u3002", "result": "DIBA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u8fbe\u5230\u7ea60.8\u7684AUC\u548c\u6570\u91cf\u7ea7\u66f4\u9ad8\u7684TPR@0.1%FPR\uff0c\u5728\u591a\u573a\u666f\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5305\u62ec\u8de8\u6570\u636e\u96c6\u3001\u8de8\u7b97\u6cd5\u548c\u9ed1\u76d2\u8bbe\u7f6e\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u7cfb\u7edf\u5206\u6790RLVR\u9690\u79c1\u6f0f\u6d1e\u7684\u5de5\u4f5c\uff0c\u8868\u660e\u5373\u4f7f\u6ca1\u6709\u663e\u5f0f\u76d1\u7763\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u884c\u4e3a\u8f68\u8ff9\u53ef\u9760\u63a8\u65ad\u8bad\u7ec3\u6570\u636e\u66b4\u9732\uff0c\u63ed\u793a\u4e86RLVR\u8303\u5f0f\u4e0b\u7684\u65b0\u578b\u9690\u79c1\u98ce\u9669\u3002"}}
{"id": "2511.14074", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14074", "abs": "https://arxiv.org/abs/2511.14074", "authors": ["Ajesh Koyatan Chathoth", "Stephen Lee"], "title": "Dynamic Black-box Backdoor Attacks on IoT Sensory Data", "comment": null, "summary": "Sensor data-based recognition systems are widely used in various applications, such as gait-based authentication and human activity recognition (HAR). Modern wearable and smart devices feature various built-in Inertial Measurement Unit (IMU) sensors, and such sensor-based measurements can be fed to a machine learning-based model to train and classify human activities. While deep learning-based models have proven successful in classifying human activity and gestures, they pose various security risks. In our paper, we discuss a novel dynamic trigger-generation technique for performing black-box adversarial attacks on sensor data-based IoT systems. Our empirical analysis shows that the attack is successful on various datasets and classifier models with minimal perturbation on the input data. We also provide a detailed comparative analysis of performance and stealthiness to various other poisoning techniques found in backdoor attacks. We also discuss some adversarial defense mechanisms and their impact on the effectiveness of our trigger-generation technique.", "AI": {"tldr": "Proposes a dynamic trigger-generation technique for black-box adversarial attacks on sensor-based IoT systems, showing effectiveness across datasets with minimal perturbations, comparing with other poisoning methods, and discussing defense mechanisms.", "motivation": "Sensor data-based systems like gait authentication and HAR using deep learning are vulnerable to security risks, necessitating exploration of adversarial attack techniques.", "method": "Development of a novel dynamic trigger-generation method for black-box adversarial attacks on IMU sensor data, tested on various datasets and classifier models.", "result": "The attack successfully perturbs input data minimally while compromising system integrity across different models, with comparative analysis showing its stealthiness and performance advantages over other poisoning techniques.", "conclusion": "The technique poses significant security threats to sensor-based IoT applications, highlighting the need for robust adversarial defense mechanisms to mitigate such attacks."}}
{"id": "2511.14088", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.14088", "abs": "https://arxiv.org/abs/2511.14088", "authors": ["Adam Caulfield", "Muhammad Wasif Kamran", "N. Asokan"], "title": "Resolving Availability and Run-time Integrity Conflicts in Real-Time Embedded Systems", "comment": null, "summary": "Run-time integrity enforcement in real-time systems presents a fundamental conflict with availability. Existing approaches in real- time systems primarily focus on minimizing the execution-time overhead of monitoring. After a violation is detected, prior works face a trade-off: (1) prioritize availability and allow a compromised system to continue to ensure applications meet their deadlines, or (2) prioritize security by generating a fault to abort all execution. In this work, we propose PAIR, an approach that offers a middle ground between the stark extremes of this trade-off. PAIR monitors real-time tasks for run-time integrity violations and maintains an Availability Region (AR) of all tasks that are safe to continue. When a task causes a violation, PAIR triggers a non-maskable interrupt to kill the task and continue executing a non-violating task within AR. Thus, PAIR ensures only violating tasks are prevented from execution, while granting availability to remaining tasks. With its hardware approach, PAIR does not cause any run-time overhead to the executing tasks, integrates with real-time operating systems (RTOSs), and is affordable to low-end microcontroller units (MCUs) by incurring +2.3% overhead in memory and hardware usage.", "AI": {"tldr": "PAIR\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u7cfb\u7edf\u8fd0\u884c\u65f6\u5b8c\u6574\u6027\u76d1\u63a7\u65b9\u6cd5\uff0c\u901a\u8fc7\u786c\u4ef6\u65b9\u5f0f\u5728\u68c0\u6d4b\u5230\u8fdd\u89c4\u65f6\u4ec5\u4e2d\u6b62\u8fdd\u89c4\u4efb\u52a1\uff0c\u4fdd\u8bc1\u5176\u4ed6\u4efb\u52a1\u7ee7\u7eed\u6267\u884c\uff0c\u5b9e\u73b0\u5b89\u5168\u6027\u548c\u53ef\u7528\u6027\u7684\u5e73\u8861\u3002", "motivation": "\u5b9e\u65f6\u7cfb\u7edf\u4e2d\u8fd0\u884c\u65f6\u5b8c\u6574\u6027\u68c0\u67e5\u4e0e\u53ef\u7528\u6027\u5b58\u5728\u51b2\u7a81\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u727a\u7272\u5b89\u5168\u6027\u4fdd\u8bc1\u53ef\u7528\u6027\uff0c\u8981\u4e48\u4e2d\u6b62\u6240\u6709\u6267\u884c\u786e\u4fdd\u5b89\u5168\uff0c\u9700\u8981\u4e2d\u95f4\u89e3\u51b3\u65b9\u6848\u3002", "method": "PAIR\u76d1\u63a7\u5b9e\u65f6\u4efb\u52a1\u7684\u8fd0\u884c\u65f6\u5b8c\u6574\u6027\u8fdd\u89c4\uff0c\u7ef4\u62a4\u5b89\u5168\u4efb\u52a1\u53ef\u7528\u533a\u57df(AR)\uff0c\u68c0\u6d4b\u5230\u8fdd\u89c4\u65f6\u901a\u8fc7\u4e0d\u53ef\u5c4f\u853d\u4e2d\u65ad\u6740\u6b7b\u8fdd\u89c4\u4efb\u52a1\uff0c\u7ee7\u7eed\u6267\u884cAR\u5185\u975e\u8fdd\u89c4\u4efb\u52a1\u3002", "result": "PAIR\u91c7\u7528\u786c\u4ef6\u65b9\u6cd5\uff0c\u5bf9\u6267\u884c\u4efb\u52a1\u4e0d\u4ea7\u751f\u8fd0\u884c\u65f6\u5f00\u9500\uff0c\u4e0e\u5b9e\u65f6\u64cd\u4f5c\u7cfb\u7edf\u96c6\u6210\uff0c\u5185\u5b58\u548c\u786c\u4ef6\u4f7f\u7528\u5f00\u9500\u4ec5\u589e\u52a02.3%\uff0c\u9002\u7528\u4e8e\u4f4e\u7aef\u5fae\u63a7\u5236\u5668\u3002", "conclusion": "PAIR\u5728\u5b9e\u65f6\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u5b89\u5168\u6027\u548c\u53ef\u7528\u6027\u7684\u6709\u6548\u5e73\u8861\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u4e2d\u6b62\u8fdd\u89c4\u4efb\u52a1\uff0c\u65e2\u4fdd\u8bc1\u4e86\u7cfb\u7edf\u5b89\u5168\u53c8\u4e0d\u5f71\u54cd\u5176\u4ed6\u4efb\u52a1\u7684\u6267\u884c\u65f6\u6548\u6027\u3002"}}
{"id": "2511.14098", "categories": ["cs.AI", "cs.MA", "cs.SI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.14098", "abs": "https://arxiv.org/abs/2511.14098", "authors": ["Adit Jain", "Vikram Krishnamurthy", "Yiming Zhang"], "title": "Collaborative QA using Interacting LLMs. Impact of Network Structure, Node Capability and Distributed Data", "comment": null, "summary": "In this paper, we model and analyze how a network of interacting LLMs performs collaborative question-answering (CQA) in order to estimate a ground truth given a distributed set of documents. This problem is interesting because LLMs often hallucinate when direct evidence to answer a question is lacking, and these effects become more pronounced in a network of interacting LLMs. The hallucination spreads, causing previously accurate LLMs to hallucinate. We study interacting LLMs and their hallucination by combining novel ideas of mean-field dynamics (MFD) from network science and the randomized utility model from economics to construct a useful generative model. We model the LLM with a latent state that indicates if it is truthful or not with respect to the ground truth, and extend a tractable analytical model considering an MFD to model the diffusion of information in a directed network of LLMs. To specify the probabilities that govern the dynamics of the MFD, we propose a randomized utility model. For a network of LLMs, where each LLM has two possible latent states, we posit sufficient conditions for the existence and uniqueness of a fixed point and analyze the behavior of the fixed point in terms of the incentive (e.g., test-time compute) given to individual LLMs. We experimentally study and analyze the behavior of a network of $100$ open-source LLMs with respect to data heterogeneity, node capability, network structure, and sensitivity to framing on multiple semi-synthetic datasets.", "AI": {"tldr": "This paper models how networks of LLMs collaboratively answer questions, analyzing how hallucinations spread through the network using mean-field dynamics and randomized utility models.", "motivation": "Large Language Models (LLMs) often hallucinate when evidence is lacking, and these hallucinations become more pronounced and spread in networks of interacting LLMs, affecting previously accurate models.", "method": "Combines mean-field dynamics from network science and randomized utility models from economics to create a generative model. Each LLM has a latent state (truthful/hallucinating), and the model analyzes information diffusion in directed networks.", "result": "For networks where each LLM has two latent states, sufficient conditions for fixed point existence/uniqueness are established. Behavior is analyzed concerning individual LLM incentives (e.g., test-time compute). Experimental analysis on 100 open-source LLMs examines data heterogeneity, node capability, network structure, and framing sensitivity.", "conclusion": "The framework provides a tractable way to model and analyze hallucination propagation in LLM networks, with implications for understanding and controlling collaborative AI system behavior."}}
{"id": "2511.14101", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14101", "abs": "https://arxiv.org/abs/2511.14101", "authors": ["Xinpeng Chen", "Xiaofeng Han", "Kaihao Zhang", "Guochao Ren", "Yujie Wang", "Wenhao Cao", "Yang Zhou", "Jianfeng Lu", "Zhenbo Song"], "title": "APD-Agents: A Large Language Model-Driven Multi-Agents Collaborative Framework for Automated Page Design", "comment": null, "summary": "Layout design is a crucial step in developing mobile app pages. However, crafting satisfactory designs is time-intensive for designers: they need to consider which controls and content to present on the page, and then repeatedly adjust their size, position, and style for better aesthetics and structure. Although many design software can now help to perform these repetitive tasks, extensive training is needed to use them effectively. Moreover, collaborative design across app pages demands extra time to align standards and ensure consistent styling. In this work, we propose APD-agents, a large language model (LLM) driven multi-agent framework for automated page design in mobile applications. Our framework contains OrchestratorAgent, SemanticParserAgent, PrimaryLayoutAgent, TemplateRetrievalAgent, and RecursiveComponentAgent. Upon receiving the user's description of the page, the OrchestratorAgent can dynamically can direct other agents to accomplish users' design task. To be specific, the SemanticParserAgent is responsible for converting users' descriptions of page content into structured data. The PrimaryLayoutAgent can generate an initial coarse-grained layout of this page. The TemplateRetrievalAgent can fetch semantically relevant few-shot examples and enhance the quality of layout generation. Besides, a RecursiveComponentAgent can be used to decide how to recursively generate all the fine-grained sub-elements it contains for each element in the layout. Our work fully leverages the automatic collaboration capabilities of large-model-driven multi-agent systems. Experimental results on the RICO dataset show that our APD-agents achieve state-of-the-art performance.", "AI": {"tldr": "APD-agents\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u79fb\u52a8\u5e94\u7528\u9875\u9762\u81ea\u52a8\u5316\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u591f\u6839\u636e\u7528\u6237\u63cf\u8ff0\u81ea\u52a8\u751f\u6210\u79fb\u52a8\u5e94\u7528\u9875\u9762\u5e03\u5c40\u8bbe\u8ba1", "motivation": "\u79fb\u52a8\u5e94\u7528\u9875\u9762\u5e03\u5c40\u8bbe\u8ba1\u9700\u8981\u8bbe\u8ba1\u5e08\u82b1\u8d39\u5927\u91cf\u65f6\u95f4\u8003\u8651\u63a7\u4ef6\u5e03\u5c40\u3001\u8c03\u6574\u5c3a\u5bf8\u4f4d\u7f6e\u548c\u6837\u5f0f\uff0c\u73b0\u6709\u8bbe\u8ba1\u8f6f\u4ef6\u9700\u8981\u4e13\u4e1a\u57f9\u8bad\uff0c\u8de8\u9875\u9762\u534f\u4f5c\u8bbe\u8ba1\u9700\u8981\u989d\u5916\u65f6\u95f4\u7edf\u4e00\u6807\u51c6", "method": "\u63d0\u51faAPD-agents\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u667a\u80fd\u4f53\uff1aOrchestratorAgent\u52a8\u6001\u534f\u8c03\u4efb\u52a1\uff0cSemanticParserAgent\u89e3\u6790\u7528\u6237\u63cf\u8ff0\u4e3a\u7ed3\u6784\u5316\u6570\u636e\uff0cPrimaryLayoutAgent\u751f\u6210\u7c97\u7c92\u5ea6\u5e03\u5c40\uff0cTemplateRetrievalAgent\u83b7\u53d6\u76f8\u5173\u793a\u4f8b\uff0cRecursiveComponentAgent\u9012\u5f52\u751f\u6210\u7ec6\u7c92\u5ea6\u5b50\u5143\u7d20", "result": "\u5728RICO\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAPD-agents\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0", "conclusion": "\u8be5\u5de5\u4f5c\u5145\u5206\u5229\u7528\u4e86\u5927\u6a21\u578b\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u81ea\u52a8\u534f\u4f5c\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u81ea\u52a8\u5316\u79fb\u52a8\u5e94\u7528\u9875\u9762\u8bbe\u8ba1\u6d41\u7a0b"}}
{"id": "2511.14132", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.14132", "abs": "https://arxiv.org/abs/2511.14132", "authors": ["Kavya Bhand", "Payal Khubchandani", "Jyoti Khubchandani"], "title": "A Fuzzy Logic-Based Cryptographic Framework For Real-Time Dynamic Key Generation For Enhanced Data Encryption", "comment": null, "summary": "With the ever-growing demand for cybersecurity, static key encryption mechanisms are increasingly vulnerable to adversarial attacks due to their deterministic and non-adaptive nature. Brute-force attacks, key compromise, and unauthorized access have become highly common cyber threats. This research presents a novel fuzzy logic-based cryptographic framework that dynamically generates encryption keys in real-time by accessing system-level entropy and hardware-bound trust. The proposed system leverages a Fuzzy Inference System (FIS) to evaluate system parameters that include CPU utilization, process count, and timestamp variation. It assigns entropy level based on linguistically defined fuzzy rules which are fused with hardware-generated randomness and then securely sealed using a Trusted Platform Module (TPM). The sealed key is incorporated in an AES-GCM encryption scheme to ensure both confidentiality and integrity of the data. This system introduces a scalable solution for adaptive encryption in high-assurance computing, zero-trust environments, and cloud-based infrastructure.", "AI": {"tldr": "Novel fuzzy logic-based cryptographic framework for dynamic real-time key generation using system entropy and hardware trust for adaptive encryption in high-security environments.", "motivation": "Static key encryption is vulnerable to brute-force attacks and key compromise; need for adaptive, non-deterministic encryption solutions.", "method": "Uses Fuzzy Inference System to evaluate CPU utilization, process count, and timestamp, combined with hardware randomness sealed by TPM, integrated with AES-GCM.", "result": "Framework enables dynamic key generation from system parameters, enhancing security against traditional attacks.", "conclusion": "Provides scalable, adaptive encryption solution suitable for zero-trust and cloud environments, improving resilience."}}
{"id": "2511.14140", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.14140", "abs": "https://arxiv.org/abs/2511.14140", "authors": ["Hajun Kim", "Hyunsik Na", "Daeseon Choi"], "title": "Beyond Fixed and Dynamic Prompts: Embedded Jailbreak Templates for Advancing LLM Security", "comment": null, "summary": "As the use of large language models (LLMs) continues to expand, ensuring their safety and robustness has become a critical challenge. In particular, jailbreak attacks that bypass built-in safety mechanisms are increasingly recognized as a tangible threat across industries, driving the need for diverse templates to support red-teaming efforts and strengthen defensive techniques. However, current approaches predominantly rely on two limited strategies: (i) substituting harmful queries into fixed templates, and (ii) having the LLM generate entire templates, which often compromises intent clarity and reproductibility. To address this gap, this paper introduces the Embedded Jailbreak Template, which preserves the structure of existing templates while naturally embedding harmful queries within their context. We further propose a progressive prompt-engineering methodology to ensure template quality and consistency, alongside standardized protocols for generation and evaluation. Together, these contributions provide a benchmark that more accurately reflects real-world usage scenarios and harmful intent, facilitating its application in red-teaming and policy regression testing.", "AI": {"tldr": "Introduces Embedded Jailbreak Template for safer LLM testing by embedding harmful queries in existing templates, with protocols for generation/evaluation.", "motivation": "Current jailbreak attack templates are limited (fixed templates or LLM-generated ones lacking clarity), needing better methods for red-teaming and defense.", "method": "Proposes Embedded Jailbreak Template that retains template structure while embedding harmful queries, using progressive prompt-engineering for quality control.", "result": "Provides a benchmark reflecting real-world scenarios for improved red-teaming and policy testing.", "conclusion": "The approach enhances safety evaluation accuracy and supports robust defensive measures against jailbreak attacks."}}
{"id": "2511.14131", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14131", "abs": "https://arxiv.org/abs/2511.14131", "authors": ["Yu Zhong", "Zihao Zhang", "Rui Zhang", "Lingdong Huang", "Haihan Gao", "Shuo Wang", "Da Li", "Ruijian Han", "Jiaming Guo", "Shaohui Peng", "Di Huang", "Yunji Chen"], "title": "Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation", "comment": null, "summary": "Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.", "AI": {"tldr": "R3\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a(VLN)\u7684\u53cc\u8fc7\u7a0b\u601d\u7ef4\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u8f7b\u91cf\u7ea7\u4e13\u5bb6\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u901a\u8fc7Runner\u3001Ruminator\u548cRegulator\u4e09\u4e2a\u6a21\u5757\u534f\u540c\u5de5\u4f5c\uff0c\u5728REVERIE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684VLN\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09LLM\u96be\u4ee5\u7cbe\u786e\u7406\u89e3\u771f\u5b9e\u4e16\u754c\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u4e0e\u9886\u57df\u4e13\u5bb6\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff1b2\uff09LLM\u5e26\u6765\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa\u4e86R3\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1aRunner\uff08\u8f7b\u91cf\u7ea7Transformer\u4e13\u5bb6\u6a21\u578b\uff0c\u8d1f\u8d23\u5e38\u89c4\u5bfc\u822a\uff09\u3001Ruminator\uff08\u57fa\u4e8e\u591a\u6a21\u6001LLM\uff0c\u91c7\u7528\u601d\u7ef4\u94fe\u63d0\u793a\u8fdb\u884c\u7ed3\u6784\u5316\u63a8\u7406\uff09\u3001Regulator\uff08\u6839\u636e\u4e09\u4e2a\u6807\u51c6\u76d1\u63a7\u5bfc\u822a\u8fdb\u5ea6\u5e76\u63a7\u5236\u601d\u7ef4\u6a21\u5f0f\uff09\u3002", "result": "\u5728REVERIE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cR3\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\uff0cSPL\u548cRGSPL\u5206\u522b\u63d0\u9ad8\u4e863.28%\u548c3.30%\u3002", "conclusion": "R3\u6846\u67b6\u901a\u8fc7\u6574\u5408LLM\u7684\u6cdb\u5316\u80fd\u529b\u548cVLN\u7279\u5b9a\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u6709\u6548\u89e3\u51b3\u4e86VLN\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u8bc1\u660e\u4e86\u53cc\u8fc7\u7a0b\u601d\u7ef4\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.14301", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14301", "abs": "https://arxiv.org/abs/2511.14301", "authors": ["Eric Xue", "Ruiyi Zhang", "Zijun Zhang", "Pengtao Xie"], "title": "Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion", "comment": null, "summary": "Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.", "AI": {"tldr": "SteganoBackdoor is a stealthy backdoor attack method that uses semantic triggers optimized via steganography techniques to embed hidden behaviors in Transformer models, achieving high attack success with low poisoning rates while evading detection.", "motivation": "Current backdoor research focuses on artificial triggers, neglecting the realistic threat of semantic triggers (e.g., specific names/entities) that could manipulate real-world systems. SteganoBackdoor addresses this gap by aligning stealth techniques with practical threats.", "method": "Leverages natural-language steganography and gradient-guided data optimization to transform semantic triggers into steganographic carriers that embed backdoor payloads while maintaining fluency and avoiding detectable patterns.", "result": "Achieves >99% attack success rate with significantly lower data-poisoning rates than prior methods, while effectively evading various data-level defenses in diverse experimental settings.", "conclusion": "SteganoBackdoor exposes a critical vulnerability in current defenses, highlighting the need for improved adversarial data defenses and more realistic threat modeling focused on semantic triggers."}}
{"id": "2511.14422", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14422", "abs": "https://arxiv.org/abs/2511.14422", "authors": ["Zhengchunmin Dai", "Jiaxiong Tang", "Peng Sun", "Honglong Chen", "Liantao Wu"], "title": "Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection", "comment": "18 pages,8 figures", "summary": "In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels.\n  To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.", "AI": {"tldr": "Sigil is a mandatory watermarking framework for capability-limited servers in decentralized ML (like SFL/U-shaped SFL) that embeds watermarks via gradient injection as statistical constraints on activation space, without requiring data knowledge or client cooperation.", "motivation": "Capability-limited servers in decentralized ML are vulnerable to model theft by malicious clients, but existing watermarking methods are either unreliable (client-dependent) or infeasible (requires server capabilities).", "method": "Defines watermark as statistical constraint on server-visible activation space, embeds via gradient injection with adaptive gradient clipping for stealth and mandatory enforcement.", "result": "Experimental results on multiple datasets/models show Sigil maintains model fidelity, provides robustness against attacks (including adaptive subspace removal), and remains stealthy against detection methods.", "conclusion": "Sigil effectively protects server IP in capability-limited decentralized ML settings through mandatory, stealthy watermarking that requires no client cooperation or server data access."}}
{"id": "2511.14199", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14199", "abs": "https://arxiv.org/abs/2511.14199", "authors": ["Jiazhuo Tian", "Yachao Yuan"], "title": "HFL-FlowLLM: Large Language Models for Network Traffic Flow Classification in Heterogeneous Federated Learning", "comment": null, "summary": "In modern communication networks driven by 5G and the Internet of Things (IoT), effective network traffic flow classification is crucial for Quality of Service (QoS) management and security. Traditional centralized machine learning struggles with the distributed data and privacy concerns in these heterogeneous environments, while existing federated learning approaches suffer from high costs and poor generalization. To address these challenges, we propose HFL-FlowLLM, which to our knowledge is the first framework to apply large language models to network traffic flow classification in heterogeneous federated learning. Compared to state-of-the-art heterogeneous federated learning methods for network traffic flow classification, the proposed approach improves the average F1 score by approximately 13%, demonstrating compelling performance and strong robustness. When compared to existing large language models federated learning frameworks, as the number of clients participating in each training round increases, the proposed method achieves up to a 5% improvement in average F1 score while reducing the training costs by about 87%. These findings prove the potential and practical value of HFL-FlowLLM in modern communication networks security.", "AI": {"tldr": "HFL-FlowLLM is a novel framework that applies large language models to network traffic flow classification in heterogeneous federated learning, improving F1 scores by 13% over SOTA methods while reducing training costs by 87%.", "motivation": "Address limitations of traditional centralized ML (privacy issues) and existing federated learning (high costs, poor generalization) for network traffic classification in 5G/IoT environments.", "method": "Proposes HFL-FlowLLM, the first framework to apply large language models to network traffic flow classification within a heterogeneous federated learning setting.", "result": "Achieves approximately 13% improvement in average F1 score compared to state-of-the-art heterogeneous FL methods, and up to 5% improvement with 87% reduction in training costs compared to existing LLM FL frameworks.", "conclusion": "HFL-FlowLLM demonstrates strong potential and practical value for security applications in modern communication networks, showing compelling performance and robustness."}}
{"id": "2511.14611", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14611", "abs": "https://arxiv.org/abs/2511.14611", "authors": ["Charles Cheng Ji", "Brandon Kong"], "title": "SecureSign: Bridging Security and UX in Mobile Web3 through Emulated EIP-6963 Sandboxing", "comment": "19 pages, 11 figures", "summary": "Mobile Web3 faces catastrophic retention (< 5%) yielding effective acquisition costs of \\$500 - \\$1,000 per retained user. Existing solutions force an impossible tradeoff: embedded wallets achieve moderate usability but suffer inherent click-jacking vulnerabilities; app wallets maintain security at the cost of 2 - 3% retention due to download friction and context-switching penalties. We present SecureSign, a PWA-based architecture that adapts desktop browser extension security to mobile via EIP-6963 provider sandboxing. SecureSign isolates dApp execution in iframes within a trusted parent application, achieving click-jacking immunity and transaction integrity while enabling native mobile capabilities (push notifications, home screen installation, zero context-switching). Our drop-in SDK requires no codebase changes for existing Web3 applications. Threat model analysis demonstrates immunity to click-jacking, overlay, and skimming attacks while maintaining wallet interoperability across dApps.", "AI": {"tldr": "SecureSign is a PWA-based mobile Web3 architecture that eliminates the tradeoff between security and usability by using EIP-6963 provider sandboxing to isolate dApps in iframes, achieving click-jacking immunity while maintaining native mobile features.", "motivation": "Mobile Web3 faces catastrophic user retention (<5%) due to security-usability tradeoffs: embedded wallets are vulnerable to click-jacking, while app wallets suffer from 2-3% retention loss due to download friction.", "method": "Adapts desktop browser extension security to mobile via EIP-6963 provider sandboxing, isolating dApp execution in iframes within a trusted parent PWA application with a drop-in SDK requiring no code changes.", "result": "Achieves immunity to click-jacking, overlay, and skimming attacks while maintaining wallet interoperability and native mobile capabilities like push notifications and zero context-switching.", "conclusion": "SecureSign provides a viable solution to mobile Web3's retention problem by delivering both security and usability without compromising on native mobile experience or requiring dApp modifications."}}
{"id": "2511.14214", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14214", "abs": "https://arxiv.org/abs/2511.14214", "authors": ["Pattaraphon Kenny Wongchamcharoen", "Paul Glasserman"], "title": "Do Large Language Models (LLMs) Understand Chronology?", "comment": "47 pages", "summary": "Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.", "AI": {"tldr": "This study evaluates LLMs' ability to understand chronology through ordering tasks, finding performance declines with complexity but improves with explicit reasoning budgets, especially with GPT-5 and Claude-3.7 Sonnet with Extended Thinking.", "motivation": "To test if LLMs fundamentally understand chronology, which is crucial for finance applications where look-ahead bias is a concern.", "method": "Evaluated GPT-4.1, Claude-3.7 Sonnet (with/without Extended Thinking), and GPT-5 on chronological ordering, conditional sorting, and anachronism detection tasks with varying complexity.", "result": "LLMs struggle with globally consistent timelines as sequence length increases; conditional sorting failures mainly from filtering; anachronism detection is easiest but declines with complexity. GPT-5 with medium/high reasoning effort achieved perfect performance.", "conclusion": "Explicit reasoning allocation improves chronological understanding, revealing current LLM limitations and providing insights for financial applications."}}
{"id": "2511.14219", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.14219", "abs": "https://arxiv.org/abs/2511.14219", "authors": ["Kumud Tripathi", "Aditya Srinivas Menon", "Aman Gaurav", "Raj Prakash Gohil", "Pankaj Wasnik"], "title": "Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation", "comment": "Accepted at AAAI 2026 - Main Technical Track", "summary": "The Whisper model, an open-source automatic speech recognition system, is widely adopted for its strong performance across multilingual and zero-shot settings. However, it frequently suffers from hallucination errors, especially under noisy acoustic conditions. Previous works to reduce hallucinations in Whisper-style ASR systems have primarily focused on audio preprocessing or post-processing of transcriptions to filter out erroneous content. However, modifications to the Whisper model itself remain largely unexplored to mitigate hallucinations directly. To address this challenge, we present a two-stage architecture that first enhances encoder robustness through Adaptive Layer Attention (ALA) and further suppresses hallucinations using a multi-objective knowledge distillation (KD) framework. In the first stage, ALA groups encoder layers into semantically coherent blocks via inter-layer correlation analysis. A learnable multi-head attention module then fuses these block representations, enabling the model to jointly exploit low- and high-level features for more robust encoding. In the second stage, our KD framework trains the student model on noisy audio to align its semantic and attention distributions with a teacher model processing clean inputs. Our experiments on noisy speech benchmarks show notable reductions in hallucinations and word error rates, while preserving performance on clean speech. Together, ALA and KD offer a principled strategy to improve Whisper's reliability under real-world noisy conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u67b6\u6784\u6765\u51cf\u5c11Whisper ASR\u7cfb\u7edf\u4e2d\u7684\u5e7b\u89c9\u9519\u8bef\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u5c42\u6ce8\u610f\u529b\u548c\u591a\u76ee\u6807\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u3002", "motivation": "Whisper\u6a21\u578b\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u7ecf\u5e38\u51fa\u73b0\u5e7b\u89c9\u9519\u8bef\uff0c\u4e4b\u524d\u7684\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9884\u5904\u7406\u6216\u540e\u5904\u7406\uff0c\u800c\u5bf9\u6a21\u578b\u672c\u8eab\u7684\u4fee\u6539\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u81ea\u9002\u5e94\u5c42\u6ce8\u610f\u529b\u589e\u5f3a\u7f16\u7801\u5668\u9c81\u68d2\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u591a\u76ee\u6807\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u6291\u5236\u5e7b\u89c9\u3002", "result": "\u5728\u566a\u58f0\u8bed\u97f3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u5e7b\u89c9\u9519\u8bef\u548c\u8bcd\u9519\u8bef\u7387\u663e\u8457\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u5e72\u51c0\u8bed\u97f3\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u6539\u5584Whisper\u5728\u771f\u5b9e\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u539f\u5219\u7684\u7b56\u7565\u3002"}}
{"id": "2511.14227", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14227", "abs": "https://arxiv.org/abs/2511.14227", "authors": ["Yuxiang Wang", "Siwen Wang", "Haowei Han", "Ao Wang", "Boya Liu", "Yong Zhao", "Chengbo Wu", "Bin Zhu", "Bin Qin", "Xiaokai Zhou", "Xiao Yan", "Jiawei Jiang", "Bo Du"], "title": "DevPiolt: Operation Recommendation for IoT Devices at Xiaomi Home", "comment": null, "summary": "Operation recommendation for IoT devices refers to generating personalized device operations for users based on their context, such as historical operations, environment information, and device status. This task is crucial for enhancing user satisfaction and corporate profits. Existing recommendation models struggle with complex operation logic, diverse user preferences, and sensitive to suboptimal suggestions, limiting their applicability to IoT device operations. To address these issues, we propose DevPiolt, a LLM-based recommendation model for IoT device operations. Specifically, we first equip the LLM with fundamental domain knowledge of IoT operations via continual pre-training and multi-task fine-tuning. Then, we employ direct preference optimization to align the fine-tuned LLM with specific user preferences. Finally, we design a confidence-based exposure control mechanism to avoid negative user experiences from low-quality recommendations. Extensive experiments show that DevPiolt significantly outperforms baselines on all datasets, with an average improvement of 69.5% across all metrics. DevPiolt has been practically deployed in Xiaomi Home app for one quarter, providing daily operation recommendations to 255,000 users. Online experiment results indicate a 21.6% increase in unique visitor device coverage and a 29.1% increase in page view acceptance rates.", "AI": {"tldr": "DevPiolt\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684IoT\u8bbe\u5907\u64cd\u4f5c\u63a8\u8350\u6a21\u578b\uff0c\u901a\u8fc7\u9886\u57df\u77e5\u8bc6\u9884\u8bad\u7ec3\u3001\u7528\u6237\u504f\u597d\u5bf9\u9f50\u548c\u7f6e\u4fe1\u5ea6\u63a7\u5236\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u6548\u679c\uff0c\u5df2\u5728\u5c0f\u7c73\u5bb6\u5ead\u5e94\u7528\u4e2d\u90e8\u7f72\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u63a8\u8350\u6a21\u578b\u5728\u5904\u7406IoT\u8bbe\u5907\u64cd\u4f5c\u65f6\u9762\u4e34\u64cd\u4f5c\u903b\u8f91\u590d\u6742\u3001\u7528\u6237\u504f\u597d\u591a\u6837\u4ee5\u53ca\u5bf9\u6b21\u4f18\u5efa\u8bae\u654f\u611f\u7b49\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5728IoT\u573a\u666f\u7684\u9002\u7528\u6027\u3002", "method": "\u9996\u5148\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u591a\u4efb\u52a1\u5fae\u8c03\u4e3aLLM\u63d0\u4f9bIoT\u64cd\u4f5c\u9886\u57df\u77e5\u8bc6\uff0c\u7136\u540e\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5bf9\u9f50\u7528\u6237\u504f\u597d\uff0c\u6700\u540e\u8bbe\u8ba1\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u66dd\u5149\u63a7\u5236\u673a\u5236\u907f\u514d\u4f4e\u8d28\u91cf\u63a8\u8350\u3002", "result": "\u5b9e\u9a8c\u663e\u793aDevPiolt\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5404\u9879\u6307\u6807\u5e73\u5747\u63d0\u534769.5%\u3002\u5728\u7ebf\u90e8\u7f72\u7ed3\u679c\u663e\u793a\u552f\u4e00\u8bbf\u5ba2\u8bbe\u5907\u8986\u76d6\u7387\u589e\u52a021.6%\uff0c\u9875\u9762\u67e5\u770b\u63a5\u53d7\u7387\u589e\u52a029.1%\u3002", "conclusion": "DevPiolt\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u9884\u8bad\u7ec3\u3001\u504f\u597d\u5bf9\u9f50\u548c\u667a\u80fd\u66dd\u5149\u63a7\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86IoT\u8bbe\u5907\u64cd\u4f5c\u63a8\u8350\u7684\u7279\u6b8a\u6311\u6218\uff0c\u8bc1\u660e\u4e86LLM\u5728\u8be5\u9886\u57df\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.14248", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14248", "abs": "https://arxiv.org/abs/2511.14248", "authors": ["Hongju Lee", "Youngjun Park", "Jisun An", "Dongman Lee"], "title": "Enhancing Regional Airbnb Trend Forecasting Using LLM-Based Embeddings of Accessibility and Human Mobility", "comment": "Accepted at ASONAM 2025", "summary": "The expansion of short-term rental platforms, such as Airbnb, has significantly disrupted local housing markets, often leading to increased rental prices and housing affordability issues. Accurately forecasting regional Airbnb market trends can thus offer critical insights for policymakers and urban planners aiming to mitigate these impacts. This study proposes a novel time-series forecasting framework to predict three key Airbnb indicators -- Revenue, Reservation Days, and Number of Reservations -- at the regional level. Using a sliding-window approach, the model forecasts trends 1 to 3 months ahead. Unlike prior studies that focus on individual listings at fixed time points, our approach constructs regional representations by integrating listing features with external contextual factors such as urban accessibility and human mobility. We convert structured tabular data into prompt-based inputs for a Large Language Model (LLM), producing comprehensive regional embeddings. These embeddings are then fed into advanced time-series models (RNN, LSTM, Transformer) to better capture complex spatio-temporal dynamics. Experiments on Seoul's Airbnb dataset show that our method reduces both average RMSE and MAE by approximately 48% compared to conventional baselines, including traditional statistical and machine learning models. Our framework not only improves forecasting accuracy but also offers practical insights for detecting oversupplied regions and supporting data-driven urban policy decisions.", "AI": {"tldr": "Novel LLM-based time-series framework for forecasting regional Airbnb metrics (Revenue, Reservation Days, Number of Reservations) 1-3 months ahead, integrating listing features with urban context, achieving 48% lower error than baselines.", "motivation": "Airbnb expansion disrupts housing markets, causing affordability issues, requiring accurate regional forecasts for policy interventions.", "method": "Sliding-window approach converts structured tabular data into LLM prompts for regional embeddings, fed into RNN/LSTM/Transformer models to capture spatio-temporal dynamics.", "result": "Experiments on Seoul data show 48% reduction in RMSE and MAE compared to traditional statistical and ML baselines.", "conclusion": "Framework improves forecasting accuracy and provides practical insights for detecting oversupply and supporting data-driven urban policy decisions."}}
{"id": "2511.14256", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.14256", "abs": "https://arxiv.org/abs/2511.14256", "authors": ["Yu Liu", "Xixun Lin", "Yanmin Shang", "Yangxi Li", "Shi Wang", "Yanan Cao"], "title": "PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models", "comment": "AAAI 2026, Long Paper, Oral", "summary": "Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a \"Retrieve-Prioritize-Reason\" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.", "AI": {"tldr": "PathMind\u662f\u4e00\u4e2a\u65b0\u7684\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7'\u68c0\u7d22-\u4f18\u5148\u7ea7-\u63a8\u7406'\u8303\u5f0f\uff0c\u4f7f\u7528\u8bed\u4e49\u611f\u77e5\u8def\u5f84\u4f18\u5148\u7ea7\u673a\u5236\u7b5b\u9009\u91cd\u8981\u63a8\u7406\u8def\u5f84\uff0c\u51cf\u5c11\u566a\u58f0\u5e76\u964d\u4f4eLLM\u8c03\u7528\u9700\u6c42\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a1\uff09\u65e0\u5dee\u522b\u63d0\u53d6\u63a8\u7406\u8def\u5f84\u53ef\u80fd\u5f15\u5165\u65e0\u5173\u566a\u58f0\u8bef\u5bfcLLM\uff1b2\uff09\u52a8\u6001\u63a2\u7d22\u63a8\u7406\u8def\u5f84\u9700\u8981\u9ad8\u68c0\u7d22\u9700\u6c42\u548c\u9891\u7e41LLM\u8c03\u7528\u3002", "method": "PathMind\u91c7\u7528'\u68c0\u7d22-\u4f18\u5148\u7ea7-\u63a8\u7406'\u4e09\u9636\u6bb5\uff1a\u9996\u5148\u68c0\u7d22\u67e5\u8be2\u5b50\u56fe\uff0c\u7136\u540e\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u8def\u5f84\u4f18\u5148\u7ea7\u51fd\u6570\u8bc6\u522b\u91cd\u8981\u63a8\u7406\u8def\u5f84\uff08\u7efc\u5408\u8003\u8651\u7d2f\u79ef\u6210\u672c\u548c\u9884\u4f30\u672a\u6765\u6210\u672c\uff09\uff0c\u6700\u540e\u901a\u8fc7\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u751f\u6210\u51c6\u786e\u54cd\u5e94\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPathMind consistently outperforms competitive baselines\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u4f7f\u7528\u66f4\u5c11\u7684\u8f93\u5165token\u3002", "conclusion": "PathMind\u901a\u8fc7\u9009\u62e9\u6027\u5f15\u5bfcLLM\u4f7f\u7528\u91cd\u8981\u63a8\u7406\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u66f4\u5fe0\u5b9e\u548c\u53ef\u89e3\u91ca\u7684\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.14299", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.14299", "abs": "https://arxiv.org/abs/2511.14299", "authors": ["Xiaochuan Liu", "Yuanfeng Song", "Xiaoming Yin", "Xing Chen"], "title": "DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning", "comment": null, "summary": "In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.", "AI": {"tldr": "\u63d0\u51faDataSage\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u89e3\u51b3\u5f53\u524d\u6570\u636e\u6d1e\u5bdfAgent\u5728\u9886\u57df\u77e5\u8bc6\u5229\u7528\u3001\u5206\u6790\u6df1\u5ea6\u548c\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u4e0d\u8db3", "motivation": "\u73b0\u6709\u6570\u636e\u6d1e\u5bdf\u667a\u80fd\u4f53\u5b58\u5728\u9886\u57df\u77e5\u8bc6\u5229\u7528\u4e0d\u8db3\u3001\u5206\u6790\u6df1\u5ea6\u6d45\u3001\u4ee3\u7801\u751f\u6210\u6613\u51fa\u9519\u7684\u95ee\u9898", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u3001\u591a\u89d2\u8272\u8fa9\u8bba\u673a\u5236\u548c\u591a\u8def\u5f84\u63a8\u7406\u4e09\u5927\u521b\u65b0\u7279\u6027", "result": "\u5728InsightBench\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793aDataSage\u5728\u6240\u6709\u96be\u5ea6\u7ea7\u522b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u6d1e\u5bdf\u667a\u80fd\u4f53", "conclusion": "DataSage\u4e3a\u81ea\u52a8\u5316\u6570\u636e\u6d1e\u5bdf\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2511.14334", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14334", "abs": "https://arxiv.org/abs/2511.14334", "authors": ["Alessio Pellegrino", "Jacopo Mauro"], "title": "When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling", "comment": null, "summary": "One of the long-standing goals in optimisation and constraint programming is to describe a problem in natural language and automatically obtain an executable, efficient model. Large language models appear to bring this vision closer, showing impressive results in automatically generating models for classical benchmarks. However, much of this apparent success may derive from data contamination rather than genuine reasoning: many standard CP problems are likely included in the training data of these models. To examine this hypothesis, we systematically rephrased and perturbed a set of well-known CSPLib problems to preserve their structure while modifying their context and introducing misleading elements. We then compared the models produced by three representative LLMs across original and modified descriptions. Our qualitative analysis shows that while LLMs can produce syntactically valid and semantically plausible models, their performance drops sharply under contextual and linguistic variation, revealing shallow understanding and sensitivity to wording.", "AI": {"tldr": "LLMs struggle with contextual variations in constraint programming problems, showing performance declines when problem descriptions are rephrased or perturbed despite preserving structure.", "motivation": "To investigate whether LLMs' success in generating constraint programming models stems from genuine reasoning or data contamination from training on standard benchmarks.", "method": "Systematically rephrased and perturbed well-known CSPLib problems while preserving structure but changing context and adding misleading elements, then compared model outputs from three representative LLMs.", "result": "LLMs produce syntactically valid models but performance drops significantly under contextual/linguistic variation, revealing shallow understanding and wording sensitivity.", "conclusion": "LLMs' apparent success in constraint programming model generation may derive more from data contamination than genuine reasoning capabilities."}}
{"id": "2511.14476", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14476", "abs": "https://arxiv.org/abs/2511.14476", "authors": ["Dalia Ali", "Dora Zhao", "Allison Koenecke", "Orestis Papakyriakopoulos"], "title": "Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior", "comment": null, "summary": "Although large language models (LLMs) are increasingly trained using human feedback for safety and alignment with human values, alignment decisions often overlook human social diversity. This study examines how incorporating pluralistic values affects LLM behavior by systematically evaluating demographic variation and design parameters in the alignment pipeline. We collected alignment data from US and German participants (N = 1,095, 27,375 ratings) who rated LLM responses across five dimensions: Toxicity, Emotional Awareness (EA), Sensitivity, Stereotypical Bias, and Helpfulness. We fine-tuned multiple Large Language Models and Large Reasoning Models using preferences from different social groups while varying rating scales, disagreement handling methods, and optimization techniques. The results revealed systematic demographic effects: male participants rated responses 18% less toxic than female participants; conservative and Black participants rated responses 27.9% and 44% more emotionally aware than liberal and White participants, respectively. Models fine-tuned on group-specific preferences exhibited distinct behaviors. Technical design choices showed strong effects: the preservation of rater disagreement achieved roughly 53% greater toxicity reduction than majority voting, and 5-point scales yielded about 22% more reduction than binary formats; and Direct Preference Optimization (DPO) consistently outperformed Group Relative Policy Optimization (GRPO) in multi-value optimization. These findings represent a preliminary step in answering a critical question: How should alignment balance expert-driven and user-driven signals to ensure both safety and fair representation?", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728LLM\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u7eb3\u5165\u591a\u5143\u5316\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u4eba\u53e3\u7edf\u8ba1\u5dee\u5f02\u548c\u6280\u672f\u8bbe\u8ba1\u9009\u62e9\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u5f53\u524dLLM\u5bf9\u9f50\u51b3\u7b56\u5f80\u5f80\u5ffd\u89c6\u4eba\u7c7b\u793e\u4f1a\u7684\u591a\u6837\u6027\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u516c\u5e73\u4ee3\u8868\u6027\u3002", "method": "\u4ece\u7f8e\u5fb7\u53c2\u4e0e\u8005\u6536\u96c627,375\u6761\u8bc4\u7ea7\u6570\u636e\uff0c\u5728\u4e0d\u540c\u793e\u4f1a\u7fa4\u4f53\u504f\u597d\u57fa\u7840\u4e0a\u5fae\u8c03\u591a\u4e2aLLM/LRM\u6a21\u578b\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u8bc4\u5206\u5c3a\u5ea6\u3001\u5206\u6b67\u5904\u7406\u65b9\u6cd5\u548c\u4f18\u5316\u6280\u672f\u3002", "result": "\u53d1\u73b0\u7cfb\u7edf\u6027\u4eba\u53e3\u7edf\u8ba1\u6548\u5e94\uff08\u5982\u7537\u6027\u8bc4\u5206\u6bd2\u6027\u4f4e18%\uff09\uff0c\u6280\u672f\u9009\u62e9\u5f71\u54cd\u663e\u8457\uff085\u70b9\u91cf\u8868\u6bd4\u4e8c\u5206\u683c\u5f0f\u6548\u679c\u63d0\u534722%\uff0cDPO\u4f18\u4e8eGRPO\uff09\u3002", "conclusion": "\u8fd9\u662f\u89e3\u51b3\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u5982\u4f55\u5e73\u8861\u4e13\u5bb6\u9a71\u52a8\u4e0e\u7528\u6237\u9a71\u52a8\u4fe1\u53f7\u7684\u91cd\u8981\u521d\u6b65\u63a2\u7d22\u3002"}}
{"id": "2511.14533", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14533", "abs": "https://arxiv.org/abs/2511.14533", "authors": ["Jiahao Wu", "Shengwen Yu"], "title": "A Neuro-Symbolic Framework for Reasoning under Perceptual Uncertainty: Bridging Continuous Perception and Discrete Symbolic Planning", "comment": "29 pages, 10 figures, 12 tables", "summary": "Bridging continuous perceptual signals and discrete symbolic reasoning is a fundamental challenge in AI systems that must operate under uncertainty. We present a neuro-symbolic framework that explicitly models and propagates uncertainty from perception to planning, providing a principled connection between these two abstraction levels. Our approach couples a transformer-based perceptual front-end with graph neural network (GNN) relational reasoning to extract probabilistic symbolic states from visual observations, and an uncertainty-aware symbolic planner that actively gathers information when confidence is low. We demonstrate the framework's effectiveness on tabletop robotic manipulation as a concrete application: the translator processes 10,047 PyBullet-generated scenes (3--10 objects) and outputs probabilistic predicates with calibrated confidences (overall F1=0.68). When embedded in the planner, the system achieves 94\\%/90\\%/88\\% success on Simple Stack, Deep Stack, and Clear+Stack benchmarks (90.7\\% average), exceeding the strongest POMDP baseline by 10--14 points while planning within 15\\,ms. A probabilistic graphical-model analysis establishes a quantitative link between calibrated uncertainty and planning convergence, providing theoretical guarantees that are validated empirically. The framework is general-purpose and can be applied to any domain requiring uncertainty-aware reasoning from perceptual input to symbolic planning.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u7b26\u53f7\u72b6\u6001\u5efa\u6a21\u5c06\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u5230\u89c4\u5212\u4e2d\uff0c\u5728\u684c\u9762\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u5747\u6210\u529f\u738790.7%\uff0c\u4f18\u4e8e\u4f20\u7edfPOMDP\u65b9\u6cd5\u3002", "motivation": "\u5728AI\u7cfb\u7edf\u4e2d\uff0c\u8fde\u63a5\u8fde\u7eed\u6027\u611f\u77e5\u4fe1\u53f7\u548c\u79bb\u6563\u7b26\u53f7\u63a8\u7406\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u539f\u5219\u6027\u5730\u5728\u611f\u77e5\u548c\u89c4\u5212\u4e24\u4e2a\u62bd\u8c61\u5c42\u6b21\u4e4b\u95f4\u4f20\u64ad\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408transformer\u611f\u77e5\u524d\u7aef\u548cGNN\u5173\u7cfb\u63a8\u7406\u6765\u4ece\u89c6\u89c9\u89c2\u5bdf\u4e2d\u63d0\u53d6\u6982\u7387\u7b26\u53f7\u72b6\u6001\uff0c\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7b26\u53f7\u89c4\u5212\u5668\u5728\u7f6e\u4fe1\u5ea6\u4f4e\u65f6\u4e3b\u52a8\u6536\u96c6\u4fe1\u606f\u3002", "result": "\u572810,047\u4e2aPyBullet\u751f\u6210\u573a\u666f(3-10\u4e2a\u7269\u4f53)\u4e0a\u6d4b\u8bd5\uff0c\u6982\u7387\u8c13\u8bcd\u63d0\u53d6F1=0.68\u3002\u5728Simple Stack\u3001Deep Stack\u548cClear+Stack\u4efb\u52a1\u4e2d\u5206\u522b\u8fbe\u523094%/90%/88%\u6210\u529f\u7387\uff0c\u89c4\u5212\u65f6\u95f4\u4ec515ms\uff0c\u6bd4\u6700\u5f3aPOMDP\u57fa\u7ebf\u9ad810-14\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u4e0e\u89c4\u5212\u6536\u655b\u4e4b\u95f4\u7684\u5b9a\u91cf\u8054\u7cfb\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002\u8be5\u901a\u7528\u6846\u67b6\u53ef\u5e94\u7528\u4e8e\u4efb\u4f55\u9700\u8981\u4ece\u611f\u77e5\u8f93\u5165\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7b26\u53f7\u89c4\u5212\u7684\u9886\u57df\u3002"}}
{"id": "2511.14595", "categories": ["cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.14595", "abs": "https://arxiv.org/abs/2511.14595", "authors": ["Yuan An", "Ruhma Hashmi", "Michelle Rogers", "Jane Greenberg", "Brian K. Smith"], "title": "Rate-Distortion Guided Knowledge Graph Construction from Lecture Notes Using Gromov-Wasserstein Optimal Transport", "comment": "Accepted in the 5th Workshop on Knowledge Graphs and Big Data in Conjunction with IEEE Big Data 2025", "summary": "Task-oriented knowledge graphs (KGs) enable AI-powered learning assistant systems to automatically generate high-quality multiple-choice questions (MCQs). Yet converting unstructured educational materials, such as lecture notes and slides, into KGs that capture key pedagogical content remains difficult. We propose a framework for knowledge graph construction and refinement grounded in rate-distortion (RD) theory and optimal transport geometry. In the framework, lecture content is modeled as a metric-measure space, capturing semantic and relational structure, while candidate KGs are aligned using Fused Gromov-Wasserstein (FGW) couplings to quantify semantic distortion. The rate term, expressed via the size of KG, reflects complexity and compactness. Refinement operators (add, merge, split, remove, rewire) minimize the rate-distortion Lagrangian, yielding compact, information-preserving KGs. Our prototype applied to data science lectures yields interpretable RD curves and shows that MCQs generated from refined KGs consistently surpass those from raw notes on fifteen quality criteria. This study establishes a principled foundation for information-theoretic KG optimization in personalized and AI-assisted education.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u7387\u5931\u771f\u7406\u8bba\u548c\u6700\u4f18\u4f20\u8f93\u51e0\u4f55\u7684\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u4e0e\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6559\u5b66\u6750\u6599\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u9009\u62e9\u9898", "motivation": "\u5c06\u975e\u7ed3\u6784\u5316\u6559\u5b66\u6750\u6599\uff08\u5982\u8bb2\u4e49\u548c\u5e7b\u706f\u7247\uff09\u8f6c\u5316\u4e3a\u80fd\u591f\u6355\u6349\u5173\u952e\u6559\u5b66\u5185\u5bb9\u7684\u77e5\u8bc6\u56fe\u8c31\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027", "method": "\u5c06\u8bb2\u5ea7\u5185\u5bb9\u5efa\u6a21\u4e3a\u5ea6\u91cf-\u6d4b\u5ea6\u7a7a\u95f4\uff0c\u4f7f\u7528Fused Gromov-Wasserstein\u8026\u5408\u91cf\u5316\u8bed\u4e49\u5931\u771f\uff0c\u901a\u8fc7\u7cbe\u70bc\u64cd\u4f5c\uff08\u6dfb\u52a0\u3001\u5408\u5e76\u3001\u62c6\u5206\u3001\u5220\u9664\u3001\u91cd\u8fde\uff09\u6700\u5c0f\u5316\u7387\u5931\u771f\u62c9\u683c\u6717\u65e5\u91cf", "result": "\u5728\u6570\u636e\u79d1\u5b66\u8bb2\u5ea7\u4e0a\u7684\u5e94\u7528\u663e\u793a\uff0c\u4ece\u4f18\u5316\u540e\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u7684\u9009\u62e9\u9898\u572815\u4e2a\u8d28\u91cf\u6807\u51c6\u4e0a consistently \u4f18\u4e8e\u539f\u59cb\u7b14\u8bb0\u751f\u6210\u7684\u9009\u62e9\u9898", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e2a\u6027\u5316AI\u8f85\u52a9\u6559\u80b2\u4e2d\u7684\u4fe1\u606f\u8bba\u77e5\u8bc6\u56fe\u8c31\u4f18\u5316\u5efa\u7acb\u4e86\u7406\u8bba\u57fa\u7840"}}
{"id": "2509.12443", "categories": ["cs.SE", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12443", "abs": "https://arxiv.org/abs/2509.12443", "authors": ["Sparsh Gupta", "Kamalavasan Kamalakkannan", "Maxim Moraru", "Galen Shipman", "Patrick Diehl"], "title": "From Legacy Fortran to Portable Kokkos: An Autonomous Agentic AI Workflow", "comment": "12 pages, 6 figures, 7 tables", "summary": "Scientific applications continue to rely on legacy Fortran codebases originally developed for homogeneous, CPU-based systems. As High-Performance Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many accelerators lack native Fortran bindings, creating an urgent need to modernize legacy codes for portability. Frameworks like Kokkos provide performance portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos porting demands significant expertise and time. Large language models (LLMs) have shown promise in source-to-source code generation, yet their use in fully autonomous workflows for translating and optimizing parallel code remains largely unexplored, especially for performance portability across diverse hardware. This paper presents an agentic AI workflow where specialized LLM \"agents\" collaborate to translate, validate, compile, run, test, debug, and optimize Fortran kernels into portable Kokkos C++ programs. Results show the pipeline modernizes a range of benchmark kernels, producing performance-portable Kokkos codes across hardware partitions. Paid OpenAI models such as GPT-5 and o4-mini-high executed the workflow for only a few U.S. dollars, generating optimized codes that surpassed Fortran baselines, whereas open-source models like Llama4-Maverick often failed to yield functional codes. This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos transformation and offers a pathway for autonomously modernizing legacy scientific applications to run portably and efficiently on diverse supercomputers. It further highlights the potential of LLM-driven agentic systems to perform structured, domain-specific reasoning tasks in scientific and systems-oriented applications.", "AI": {"tldr": "This paper presents an AI-driven workflow using specialized LLM agents to automatically translate legacy Fortran code into performance-portable Kokkos C++ programs for GPU-accelerated HPC systems, achieving successful modernization with commercial AI models.", "motivation": "Legacy Fortran codes face challenges in GPU-accelerated HPC due to lack of native Fortran bindings, requiring manual porting expertise. There's a need for automated approaches to modernize scientific applications for heterogeneous architectures.", "method": "An agentic AI workflow employing specialized LLM agents that collaborate to translate, validate, compile, run, test, debug, and optimize Fortran kernels into Kokkos C++ programs across multiple hardware platforms.", "result": "The pipeline successfully modernized benchmark kernels, producing portable Kokkos codes. Commercial models (GPT-5, o4-mini-high) generated optimized codes surpassing Fortran baselines cost-effectively, while open-source models often failed to produce functional codes.", "conclusion": "Agentic AI is feasible for Fortran-to-Kokkos transformation, offering an autonomous pathway to modernize legacy scientific applications for diverse supercomputers, demonstrating LLMs' potential for structured reasoning in scientific applications."}}
