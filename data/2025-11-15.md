<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 1]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.SE](#cs.SE) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Querying Labeled Time Series Data with Scenario Programs](https://arxiv.org/abs/2511.10627)
*Edward Kim,Devan Shanker,Varun Bharadwaj,Hongbeen Park,Jinkyu Kim,Hazem Torfah,Daniel J Fremont,Sanjit A Seshia*

Main category: cs.AI

TL;DR: The paper presents a method to validate simulation-identified failure scenarios in cyber-physical systems by matching them against real-world datasets using Scenic scenario programs, showing improved accuracy and speed over existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the sim-to-real gap in CPS testing by ensuring failure scenarios found in simulation are reproducible in real-world conditions.

Method: Developed a formal definition for matching labeled time series sensor data to abstract scenarios using Scenic programming language, with an efficient querying algorithm.

Result: The algorithm is more accurate and significantly faster than state-of-the-art commercial vision large language models, scaling well with data duration.

Conclusion: This approach effectively validates simulated failure scenarios against real-world data, bridging the sim-to-real gap in CPS safety testing.

Abstract: Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [2] [Revisit to the Bai-Galbraith signature scheme](https://arxiv.org/abs/2511.09582)
*Banhirup Sengupta,Peenal Gupta,Souvik Sengupta*

Main category: cs.CR

TL;DR: 本文比较了Dilithium和Bai-Galbraith两种基于格的签名方案，重点指出BG14方案不使用公钥压缩的特点。


<details>
  <summary>Details</summary>
Motivation: 研究不同的基于格的签名方案，特别是比较NIST批准的Dilithium方案与BG14提出的Bai-Galbraith方案的差异。

Method: 基于Learning with Errors (LWE)问题构建签名方案，与Dilithium的主要区别在于不使用公钥压缩技术。

Result: 描述了Bai-Galbraith签名方案的基本结构和特点，突出了其与Dilithium在公钥处理方式上的差异。

Conclusion: Bai-Galbraith签名方案提供了基于LWE的另一种设计选择，其无公钥压缩的特性可能在某些应用场景中具有优势。

Abstract: Dilithium is one of the NIST approved lattice-based signature schemes. In this short note we describe the Bai-Galbraith signature scheme proposed in BG14, which differs to Dilithium, due to the fact that there is no public key compression. This lattice-based signature scheme is based on Learning with Errors (LWE).

</details>


### [3] [An explainable Recursive Feature Elimination to detect Advanced Persistent Threats using Random Forest classifier](https://arxiv.org/abs/2511.09603)
*Noor Hazlina Abdul Mutalib,Aznul Qalid Md Sabri,Ainuddin Wahid Abdul Wahab,Erma Rahayu Mohd Faizal Abdullah,Nouar AlDahoul*

Main category: cs.CR

TL;DR: 论文提出基于递归特征消除和随机森林的可解释入侵检测框架，用于检测高级持续威胁，在CICIDS2017数据集上取得99.9%准确率并降低误报和计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前入侵检测系统在面对高级持续威胁时存在检测精度不足、缺乏可解释性以及高误报率的问题，需要开发兼具高精度和透明度的解决方案。

Method: 结合递归特征消除进行特征选择，使用随机森林构建检测模型，并采用SHAP方法对模型预测结果进行可解释性分析。

Result: 在CICIDS2017数据集上的实验表明，该框架达到99.9%的检测准确率，相比传统分类器显著降低了误报率和计算开销。

Conclusion: 将可解释人工智能与特征选择相结合，可以有效开发出鲁棒、透明且可部署的入侵检测系统解决方案。

Abstract: Intrusion Detection Systems (IDS) play a vital role in modern cybersecurity frameworks by providing a primary defense mechanism against sophisticated threat actors. In this paper, we propose an explainable intrusion detection framework that integrates Recursive Feature Elimination (RFE) with Random Forest (RF) to enhance detection of Advanced Persistent Threats (APTs). By using CICIDS2017 dataset, the approach begins with comprehensive data preprocessing and narrows down the most significant features via RFE. A Random Forest (RF) model was trained on the refined feature set, with SHapley Additive exPlanations (SHAP) used to interpret the contribution of each selected feature. Our experiment demonstrates that the explainable RF-RFE achieved a detection accuracy of 99.9%, reducing false positive and computational cost in comparison to traditional classifiers. The findings underscore the effectiveness of integrating explainable AI and feature selection to develop a robust, transparent, and deployable IDS solution.

</details>


### [4] [How Can We Effectively Use LLMs for Phishing Detection?: Evaluating the Effectiveness of Large Language Model-based Phishing Detection Models](https://arxiv.org/abs/2511.09606)
*Fujiao Ji,Doowon Kim*

Main category: cs.CR

TL;DR: LLM在钓鱼检测中表现出潜力，特别是在品牌识别方面。商业LLM表现优于开源模型，截图输入效果最佳，零温度设置可最大化准确性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习钓鱼检测器泛化能力差且缺乏可解释性。LLM作为有前景的检测机制，但其有效性尚未被探索，因此研究如何有效利用LLM进行钓鱼检测。

Method: 使用19,131个真实钓鱼网站和243个良性网站数据集，评估7个LLM（2个商业模型和5个开源模型）及2个深度学习基线。研究输入模态（截图、logo、HTML、URL）、温度设置和提示工程策略的影响。

Result: 商业LLM在钓鱼检测中总体优于开源模型，而DL模型在良性样本上表现更好。截图输入在品牌识别中达到最佳效果（商业LLM 93-95%准确率，开源模型如Qwen达92%）。多模态输入或单样本提示未持续提升性能，较高温度降低性能。

Conclusion: 建议使用截图输入和零温度设置来最大化LLM检测器准确性，当截图信息不足时可将HTML作为辅助上下文。

Abstract: Large language models (LLMs) have emerged as a promising phishing detection mechanism, addressing the limitations of traditional deep learning-based detectors, including poor generalization to previously unseen websites and a lack of interpretability. However, LLMs' effectiveness for phishing detection remains unexplored. This study investigates how to effectively leverage LLMs for phishing detection (including target brand identification) by examining the impact of input modalities (screenshots, logos, HTML, and URLs), temperature settings, and prompt engineering strategies. Using a dataset of 19,131 real-world phishing websites and 243 benign sites, we evaluate seven LLMs -- two commercial models (GPT 4.1 and Gemini 2.0 flash) and five open-source models (Qwen, Llama, Janus, DeepSeek-VL2, and R1) -- alongside two deep learning (DL)-based baselines (PhishIntention and Phishpedia).
  Our findings reveal that commercial LLMs generally outperform open-source models in phishing detection, while DL models demonstrate better performance on benign samples. For brand identification, screenshot inputs achieve optimal results, with commercial LLMs reaching 93-95% accuracy and open-source models, particularly Qwen, achieving up to 92%. However, incorporating multiple input modalities simultaneously or applying one-shot prompts does not consistently enhance performance and may degrade results. Furthermore, higher temperature values reduce performance. Based on these results, we recommend using screenshot inputs with zero temperature to maximize accuracy for LLM-based detectors with HTML serving as auxiliary context when screenshot information is insufficient.

</details>


### [5] [Slice-Aware Spoofing Detection in 5G Networks Using Lightweight Machine Learning](https://arxiv.org/abs/2511.09610)
*Daniyal Ganiuly,Nurzhau Bolatbek*

Main category: cs.CR

TL;DR: A lightweight ML framework for detecting spoofing attacks in 5G network slices using Logistic Regression and Random Forest trained on slice-specific traffic data.


<details>
  <summary>Details</summary>
Motivation: 5G network virtualization expands the attack surface, making spoofing a threat to slice integrity and service reliability.

Method: Implemented on Open5GS/srsRAN testbed simulating eMBB, URLLC, and mMTC services; trained classifiers using statistical flow features from mirrored user-plane traffic.

Result: Slice-aware training improved detection accuracy by up to 5%, achieving F1-scores of 0.93-0.96 while maintaining real-time operation on edge hardware.

Conclusion: Aligning security intelligence with slice boundaries enhances detection reliability and enables scalable, privacy-preserving spoofing defense without high computational costs.

Abstract: The increasing virtualization of fifth generation (5G) networks expands the attack surface of the user plane, making spoofing a persistent threat to slice integrity and service reliability. This study presents a slice-aware lightweight machine-learning framework for detecting spoofing attacks within 5G network slices. The framework was implemented on a reproducible Open5GS and srsRAN testbed emulating three service classes such as enhanced Mobile Broadband (eMBB), Ultra-Reliable Low-Latency Communication (URLLC), and massive Machine-Type Communication (mMTC) under controlled benign and adversarial traffic. Two efficient classifiers, Logistic Regression and Random Forest, were trained independently for each slice using statistical flow features derived from mirrored user-plane traffic. Slice-aware training improved detection accuracy by up to 5% and achieved F1-scores between 0.93 and 0.96 while maintaining real-time operation on commodity edge hardware. The results demonstrate that aligning security intelligence with slice boundaries enhances detection reliability and preserves operational isolation, enabling practical deployment in 5G network-security environments. Conceptually, the work bridges network-security architecture and adaptive machine learning by showing that isolation-aware intelligence can achieve scalable, privacy-preserving spoofing defense without high computational cost.

</details>


### [6] [Cooperative Local Differential Privacy: Securing Time Series Data in Distributed Environments](https://arxiv.org/abs/2511.09696)
*Bikash Chandra Singh,Md Jakir Hossain,Rafael Diaz,Sandip Roy,Ravi Mukkamala,Sachin Shetty*

Main category: cs.CR

TL;DR: 提出了一种协作式局部差分隐私机制，通过在多个用户之间分布噪声向量来增强时间序列数据的隐私保护。


<details>
  <summary>Details</summary>
Motivation: 智能设备的快速增长导致时间序列数据爆炸式增长，但传统局部差分隐私方法存在漏洞，噪声在聚合时可能被抵消，导致隐私泄露风险。

Method: 引入协作式局部差分隐私机制，通过多个用户协作生成和分配噪声向量，使得在聚合数据时噪声相互抵消。

Result: 该机制能够有效对抗基于时间窗口方法的脆弱性，保护个体隐私的同时保持整体统计特性。

Conclusion: 协作式策略在多用户环境中实现了数据效用和隐私之间更好的平衡，适用于大规模实时数据集。

Abstract: The rapid growth of smart devices such as phones, wearables, IoT sensors, and connected vehicles has led to an explosion of continuous time series data that offers valuable insights in healthcare, transportation, and more. However, this surge raises significant privacy concerns, as sensitive patterns can reveal personal details. While traditional differential privacy (DP) relies on trusted servers, local differential privacy (LDP) enables users to perturb their own data. However, traditional LDP methods perturb time series data by adding user-specific noise but exhibit vulnerabilities. For instance, noise applied within fixed time windows can be canceled during aggregation (e.g., averaging), enabling adversaries to infer individual statistics over time, thereby eroding privacy guarantees.
  To address these issues, we introduce a Cooperative Local Differential Privacy (CLDP) mechanism that enhances privacy by distributing noise vectors across multiple users. In our approach, noise is collaboratively generated and assigned so that when all users' perturbed data is aggregated, the noise cancels out preserving overall statistical properties while protecting individual privacy. This cooperative strategy not only counters vulnerabilities inherent in time-window-based methods but also scales effectively for large, real-time datasets, striking a better balance between data utility and privacy in multiuser environments.

</details>


### [7] [Privacy-Preserving Explainable AIoT Application via SHAP Entropy Regularization](https://arxiv.org/abs/2511.09775)
*Dilli Prasad Sharma,Xiaowei Sun,Liang Xue,Xiaodong Lin,Pulei Xiong*

Main category: cs.CR

TL;DR: 提出基于SHAP熵正则化的隐私保护方法，减少可解释AI在智能家居中的隐私泄露风险，同时保持预测准确性和解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法（如SHAP、LIME）可能会泄露敏感用户信息和行为模式，造成新的隐私风险，需要解决这一问题。

Method: 提出SHAP熵正则化方法，通过熵正则化惩罚项迫使特征贡献分布更均匀，降低隐私泄露的可能性。开发了一套基于SHAP的隐私攻击作为评估基准。

Result: 在智能家居能耗数据集上验证表明，所提出的方法显著降低了隐私泄露，同时保持了较高的预测准确性和解释保真度。

Conclusion: 该方法为安全可靠的AIoT应用中的隐私保护可解释AI技术发展做出了贡献，实现了隐私与可解释性的平衡。

Abstract: The widespread integration of Artificial Intelligence of Things (AIoT) in smart home environments has amplified the demand for transparent and interpretable machine learning models. To foster user trust and comply with emerging regulatory frameworks, the Explainable AI (XAI) methods, particularly post-hoc techniques such as SHapley Additive exPlanations (SHAP), and Local Interpretable Model-Agnostic Explanations (LIME), are widely employed to elucidate model behavior. However, recent studies have shown that these explanation methods can inadvertently expose sensitive user attributes and behavioral patterns, thereby introducing new privacy risks. To address these concerns, we propose a novel privacy-preserving approach based on SHAP entropy regularization to mitigate privacy leakage in explainable AIoT applications. Our method incorporates an entropy-based regularization objective that penalizes low-entropy SHAP attribution distributions during training, promoting a more uniform spread of feature contributions. To evaluate the effectiveness of our approach, we developed a suite of SHAP-based privacy attacks that strategically leverage model explanation outputs to infer sensitive information. We validate our method through comparative evaluations using these attacks alongside utility metrics on benchmark smart home energy consumption datasets. Experimental results demonstrate that SHAP entropy regularization substantially reduces privacy leakage compared to baseline models, while maintaining high predictive accuracy and faithful explanation fidelity. This work contributes to the development of privacy-preserving explainable AI techniques for secure and trustworthy AIoT applications.

</details>


### [8] [DP-GENG : Differentially Private Dataset Distillation Guided by DP-Generated Data](https://arxiv.org/abs/2511.09876)
*Shuo Shi,Jinghuai Zhang,Shijie Jiang,Chunyi Zhou,Yuyuan Li,Mengying Zhu,Yangyang Wu,Tianyu Du*

Main category: cs.CR

TL;DR: 本文提出了一种名为Libn的新型差分隐私数据集蒸馏框架，通过使用DP生成的数据增强现实性，改进特征匹配技术，并引入专家模型优化类别分布对齐，显著提升了隐私保护下的数据集效用。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私数据集蒸馏方法因注入噪声导致数据现实性和效用下降，无法充分保护原始数据集隐私。

Method: Libn利用DP生成数据初始化蒸馏数据集；改进DP特征匹配技术在小隐私预算下蒸馏原始数据；训练专家模型对齐蒸馏样本与类别分布；设计隐私预算分配策略。

Result: 实验表明Libn在数据集效用和对成员推理攻击的鲁棒性上显著优于现有DP-DD方法。

Conclusion: Libn为隐私保护数据集蒸馏建立了新范式，有效平衡了隐私保护与数据效用。

Abstract: Dataset distillation (DD) compresses large datasets into smaller ones while preserving the performance of models trained on them. Although DD is often assumed to enhance data privacy by aggregating over individual examples, recent studies reveal that standard DD can still leak sensitive information from the original dataset due to the lack of formal privacy guarantees. Existing differentially private (DP)-DD methods attempt to mitigate this risk by injecting noise into the distillation process. However, they often fail to fully leverage the original dataset, resulting in degraded realism and utility. This paper introduces \libn, a novel framework that addresses the key limitations of current DP-DD by leveraging DP-generated data. Specifically, \lib initializes the distilled dataset with DP-generated data to enhance realism. Then, generated data refines the DP-feature matching technique to distill the original dataset under a small privacy budget, and trains an expert model to align the distilled examples with their class distribution. Furthermore, we design a privacy budget allocation strategy to determine budget consumption across DP components and provide a theoretical analysis of the overall privacy guarantees. Extensive experiments show that \lib significantly outperforms state-of-the-art DP-DD methods in terms of both dataset utility and robustness against membership inference attacks, establishing a new paradigm for privacy-preserving dataset distillation.

</details>


### [9] [Taught by the Flawed: How Dataset Insecurity Breeds Vulnerable AI Code](https://arxiv.org/abs/2511.09879)
*Catherine Xia,Manar H. Alalfi*

Main category: cs.CR

TL;DR: Training AI models on vulnerability-free code datasets reduces security issues in generated code while maintaining functional correctness.


<details>
  <summary>Details</summary>
Motivation: AI programming assistants often generate code with security vulnerabilities due to training on datasets containing vulnerable code.

Method: Created a secure dataset by filtering Python code with static analysis, trained transformer models on both curated and original datasets.

Result: Model trained on curated dataset produced code with fewer security issues while maintaining comparable correctness.

Conclusion: Secure training data is crucial for reliable AI programming assistants, though further improvements in model architecture are needed.

Abstract: AI programming assistants have demonstrated a tendency to generate code containing basic security vulnerabilities. While developers are ultimately responsible for validating and reviewing such outputs, improving the inherent quality of these generated code snippets remains essential. A key contributing factor to insecure outputs is the presence of vulnerabilities in the training datasets used to build large language models (LLMs). To address this issue, we propose curating training data to include only code that is free from detectable vulnerabilities. In this study, we constructed a secure dataset by filtering an existing Python corpus using a static analysis tool to retain only vulnerability-free functions. We then trained two transformer-based models: one on the curated dataset and one on the original, unfiltered dataset. The models were evaluated on both the correctness and security of the code they generated in response to natural language function descriptions. Our results show that the model trained on the curated dataset produced outputs with fewer security issues, while maintaining comparable functional correctness. These findings highlight the importance of secure training data in improving the reliability of AI-based programming assistants, though further enhancements to model architecture and evaluation are needed to reinforce these outcomes.

</details>


### [10] [Trapped by Their Own Light: Deployable and Stealth Retroreflective Patch Attacks on Traffic Sign Recognition Systems](https://arxiv.org/abs/2511.10050)
*Go Tsuruoka,Takami Sato,Qi Alfred Chen,Kazuki Nomoto,Ryunosuke Kobayashi,Yuna Tanaka,Tatsuya Mori*

Main category: cs.CR

TL;DR: 论文提出了一种对抗性反光贴片（ARP）攻击，通过反光材料和黑盒优化实现对交通标志识别系统的高效隐蔽攻击，并提出DPR Shield偏振滤光片防御方案。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性攻击（如贴纸或激光投影）存在视觉可检测性或实施限制，需要探索更隐蔽且易部署的TSR系统漏洞。

Method: 开发反射模拟方法，利用反光材料（仅在车辆头灯照射下激活），结合黑盒优化最大化攻击效果。

Result: ARP在35米动态场景中成功率≥93.4%，对商业TSR系统真实场景成功率≥60%；用户研究显示隐蔽性接近正常标志，比以往贴片攻击高≥1.9%。

Conclusion: ARP结合了贴片攻击的高部署性和激光投射的隐蔽性，揭示了TSR系统新漏洞；提出的DPR Shield防御对特定标志防御成功率≥75%。

Abstract: Traffic sign recognition plays a critical role in ensuring safe and efficient transportation of autonomous vehicles but remain vulnerable to adversarial attacks using stickers or laser projections. While existing attack vectors demonstrate security concerns, they suffer from visual detectability or implementation constraints, suggesting unexplored vulnerability surfaces in TSR systems. We introduce the Adversarial Retroreflective Patch (ARP), a novel attack vector that combines the high deployability of patch attacks with the stealthiness of laser projections by utilizing retroreflective materials activated only under victim headlight illumination. We develop a retroreflection simulation method and employ black-box optimization to maximize attack effectiveness. ARP achieves $\geq$93.4\% success rate in dynamic scenarios at 35 meters and $\geq$60\% success rate against commercial TSR systems in real-world conditions. Our user study demonstrates that ARP attacks maintain near-identical stealthiness to benign signs while achieving $\geq$1.9\% higher stealthiness scores than previous patch attacks. We propose the DPR Shield defense, employing strategically placed polarized filters, which achieves $\geq$75\% defense success rates for stop signs and speed limit signs against micro-prism patches.

</details>


### [11] [An In-Depth Systematic Analysis of the Security, Usability, and Automation Capabilities of Password Update Processes on Top-Ranked Websites](https://arxiv.org/abs/2511.10111)
*Alexander Krause,Jacques Suray,Lea Schmüser,Marten Oltrogge,Oliver Wiese,Maximilian Golla,Sascha Fahl*

Main category: cs.CR

TL;DR: This paper analyzes 111 password update processes on top websites, finding they are often insecure, difficult to use, and incompatible with password managers, offering recommendations for improvement.


<details>
  <summary>Details</summary>
Motivation: To perform the first systematic analysis of password update processes to understand their security, usability, and automation problems.

Method: In-depth systematic analysis of 111 password update processes from top-ranked websites.

Result: Found highly diverse, complex, and confusing processes that lack password manager support and hinder user experience.

Conclusion: Recommends improvements for web developers, standardization bodies, and researchers to enhance password update security and usability.

Abstract: Password updates are a critical account security measure and an essential part of the password lifecycle. Service providers and common security recommendations advise users to update their passwords in response to incidents or as a critical cyber hygiene measure. However, password update processes are often cumbersome and require manual password creation. Inconsistent and complex workflows and a lack of automation capabilities for password managers further negatively impact overall password security.
  In this work, we perform the first in-depth systematic analysis of 111 password update processes deployed on top-ranked websites. We provide novel insights into their overall security, usability, and automation capabilities and contribute to authentication security research through a better understanding of password update processes. Websites deploy highly diverse, often complex, confusing password update processes and lack the support of password managers. Processes are often hard to use, and end-users can barely transfer experiences and knowledge across websites. Notably, protective measures designed to enhance security frequently obstruct password manager automation. We conclude our work by discussing our findings and giving recommendations for web developers, the web standardization community, and security researchers.

</details>


### [12] [Enhanced Anonymous Credentials for E-Voting Systems](https://arxiv.org/abs/2511.10265)
*Tomasz Truderung*

Main category: cs.CR

TL;DR: 通过完美隐藏承诺增强匿名凭证机制，在保持永久隐私的同时加强选民与凭证的绑定关系


<details>
  <summary>Details</summary>
Motivation: 解决简单匿名凭证方法与二次设备验证等安全功能结合时可能出现的挑战

Method: 使用完美隐藏承诺将匿名凭证与选民身份关联

Result: 确保已发布选票无法与选民身份关联，同时支持投票和审核过程中的必要一致性检查

Conclusion: 该方法在不依赖高级密码技术的情况下实现了电子投票系统的永久隐私保护

Abstract: A simple and practical method for achieving everlasting privacy in e-voting systems, without relying on advanced cryptographic techniques, is to use anonymous voter credentials. The simplicity of this approach may, however, create some challenges, when combined with other security features, such as cast-as-intended verifiability with second device and second-factor authentication.
  This paper considers a simple augmentation to the anonymous credential mechanism, using perfectly hiding commitments to link such credentials to the voter identities. This solution strengthens the binding between voters and their credentials while preserving everlasting privacy. It ensures that published ballots remain unlinkable to voter identities, yet enables necessary consistency checks during ballot casting and ballot auditing

</details>


### [13] [Enhanced Privacy Leakage from Noise-Perturbed Gradients via Gradient-Guided Conditional Diffusion Models](https://arxiv.org/abs/2511.10423)
*Jiayang Meng,Tao Huang,Hong Chen,Chen Hou,Guolong Zheng*

Main category: cs.CR

TL;DR: 提出GG-CDM方法，利用梯度引导条件扩散模型从泄露的梯度中重建私有图像，绕过噪声扰动的防御机制。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的梯度传输存在隐私风险，现有梯度反演攻击在梯度被噪声扰动时性能显著下降。

Method: 采用梯度引导条件扩散模型，利用其去噪能力规避噪声扰动保护，无需目标数据分布先验知识。

Result: 理论分析了重建误差界和攻击损失收敛性，实验证明在高斯噪声扰动梯度下具有优越的重建性能。

Conclusion: GG-CDM能有效提升噪声防御下的攻击效果，为联邦学习隐私保护机制设计提供重要参考。

Abstract: Federated learning synchronizes models through gradient transmission and aggregation. However, these gradients pose significant privacy risks, as sensitive training data is embedded within them. Existing gradient inversion attacks suffer from significantly degraded reconstruction performance when gradients are perturbed by noise-a common defense mechanism. In this paper, we introduce Gradient-Guided Conditional Diffusion Models (GG-CDMs) for reconstructing private images from leaked gradients without prior knowledge of the target data distribution. Our approach leverages the inherent denoising capability of diffusion models to circumvent the partial protection offered by noise perturbation, thereby improving attack performance under such defenses. We further provide a theoretical analysis of the reconstruction error bounds and the convergence properties of attack loss, characterizing the impact of key factors-such as noise magnitude and attacked model architecture-on reconstruction quality. Extensive experiments demonstrate our attack's superior reconstruction performance with Gaussian noise-perturbed gradients, and confirm our theoretical findings.

</details>


### [14] [On the Detectability of Active Gradient Inversion Attacks in Federated Learning](https://arxiv.org/abs/2511.10502)
*Vincenzo Carletti,Pasquale Foggia,Carlo Mazzocca,Giuseppe Parrella,Mario Vento*

Main category: cs.CR

TL;DR: 这篇论文分析了联邦学习中的梯度反演攻击，特别是新型主动攻击的隐蔽性，并提出了轻量级客户端检测技术。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然不共享原始数据，但交换的梯度仍面临隐私风险。新型主动梯度反演攻击声称更隐蔽，但其可检测性尚未得到系统评估。

Method: 提出了基于统计异常权重结构和损失/梯度动态的轻量级客户端检测方法，无需修改联邦学习协议。

Result: 在多种配置下的广泛评估表明，所提方法能有效检测主动梯度反演攻击。

Conclusion: 现有"隐蔽"攻击仍可被检测，客户端防御是可行的，打破了攻击者关于完全隐蔽性的说法。

Abstract: One of the key advantages of Federated Learning (FL) is its ability to collaboratively train a Machine Learning (ML) model while keeping clients' data on-site. However, this can create a false sense of security. Despite not sharing private data increases the overall privacy, prior studies have shown that gradients exchanged during the FL training remain vulnerable to Gradient Inversion Attacks (GIAs). These attacks allow reconstructing the clients' local data, breaking the privacy promise of FL. GIAs can be launched by either a passive or an active server. In the latter case, a malicious server manipulates the global model to facilitate data reconstruction. While effective, earlier attacks falling under this category have been demonstrated to be detectable by clients, limiting their real-world applicability. Recently, novel active GIAs have emerged, claiming to be far stealthier than previous approaches. This work provides the first comprehensive analysis of these claims, investigating four state-of-the-art GIAs. We propose novel lightweight client-side detection techniques, based on statistically improbable weight structures and anomalous loss and gradient dynamics. Extensive evaluation across several configurations demonstrates that our methods enable clients to effectively detect active GIAs without any modifications to the FL training protocol.

</details>


### [15] [How Worrying Are Privacy Attacks Against Machine Learning?](https://arxiv.org/abs/2511.10516)
*Josep Domingo-Ferrer*

Main category: cs.CR

TL;DR: 本文探讨了机器学习模型发布中的隐私风险，分析了成员推断攻击、属性推断攻击和重建攻击等主要隐私攻击方式，认为这些攻击在实际中的效果通常低于文献表面上的预期。


<details>
  <summary>Details</summary>
Motivation: 随着监管框架将个人数据保护扩展到机器学习领域，需要评估发布训练好的ML模型是否真的会带来与直接发布训练数据相当的隐私风险。

Method: 作为概念性论文，本文通过审视和讨论针对预测性和生成性ML的主要隐私攻击家族来进行分析。

Result: 分析表明，大多数隐私攻击在现实世界条件下的有效性低于相关文献表面暗示的水平。

Conclusion: 虽然存在隐私风险，但实际的隐私攻击效果可能被高估，需要更准确地评估ML模型发布的真实隐私影响。

Abstract: In several jurisdictions, the regulatory framework on the release and sharing of personal data is being extended to machine learning (ML). The implicit assumption is that disclosing a trained ML model entails a privacy risk for any personal data used in training comparable to directly releasing those data. However, given a trained model, it is necessary to mount a privacy attack to make inferences on the training data. In this concept paper, we examine the main families of privacy attacks against predictive and generative ML, including membership inference attacks (MIAs), property inference attacks, and reconstruction attacks. Our discussion shows that most of these attacks seem less effective in the real world than what a prima face interpretation of the related literature could suggest.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [16] [Localized kernel gradient correction for SPH simulations of water wave propagation](https://arxiv.org/abs/2511.10064)
*Lennart Justin Schulze,Vito Zago,Giuseppe Bilotta,Robert Anthony Dalrymple*

Main category: cs.CE

TL;DR: This paper addresses excessive numerical dissipation in SPH water wave simulations by implementing a selective higher-order kernel gradient correction method only where needed, based on wave mechanics criteria, reducing computational cost while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Basic SPH models suffer from excessive numerical dissipation in water wave propagation simulations, requiring computationally expensive higher-order correction schemes throughout the entire domain.

Method: Proposes a selective kernel gradient correction approach that only applies the higher-order scheme to particles in critical regions of the water wave, determined by wave mechanics criteria, with special treatment for free surface issues.

Result: The method achieves satisfying results for both standing waves in a basin and progressive wave trains in long tanks while potentially saving significant computational effort, especially for deep water waves.

Conclusion: Selective application of higher-order correction based on wave mechanics criteria provides an efficient solution to SPH dissipation problems in water wave simulations, with proposed remedies for free surface issues.

Abstract: Basic Smoothed Particle Hydrodynamics (SPH) models exhibit excessive, numerical dissipation in the simulation of water wave propagation. This can be remedied using higher-order approaches such as kernel gradient correction, which introduce additional computational effort. The present work demonstrates, that the higher-order scheme is only required in a limited part of the water wave in order to obtain satisfying results. The criterion for distinguishing particles in need of special treatment from those that do not is motivated by water wave mechanics. Especially for deep water waves, the approach potentially spares large amounts of computational effort. The present paper also proposes a remedy for issues of the kernel gradient correction occurring at the free surface. Satisfying results for the proposed approach are shown for a standing wave in a basin and a progressive wave train in a long wave tank.

</details>


### [17] [Phase field modelling of cracking and capacity fade in core-shell cathode particles for lithium-ion batteries](https://arxiv.org/abs/2511.10355)
*Y. Tu,B. Wu,E. Martínez-Pañeda*

Main category: cs.CE

TL;DR: 开发了一种计算框架预测核壳电极颗粒的机械失效（开裂和脱粘）及其对锂电池性能的影响，通过相场法模拟发现界面应力导致快速脱粘和超过10%容量损失。


<details>
  <summary>Details</summary>
Motivation: 尽管核壳电极颗粒是高性能锂离子电池的有前景策略，但实验观察到单次充电后易发生机械失效（壳破裂和核壳脱粘），需要深入理解失效机制。

Method: 提出了一个完全耦合的化学-机械-损伤计算框架，利用相场法捕捉机械损伤与电化学行为的相互作用，量化颗粒开裂和容量衰减。

Result: 在NMC811@NMC532系统中，表面裂纹由核内锂浓度显著高于壳层引发；界面脱粘源于壳层膨胀导致的局部环向应力，快速脱粘阻碍锂离子传输，单次放电可导致超过10%容量损失；大颗粒可能出现裂纹分枝导致整体破碎。

Conclusion: 该框架为核壳颗粒降解机制提供了新见解，可用于设计性能更优、寿命更长的电极材料。

Abstract: Core-shell electrode particles are a promising morphology control strategy for high-performance lithium-ion batteries. However, experimental observations reveal that these structures remain prone to mechanical failure, with shell fractures and core-shell debonding occurring after a single charge. In this work, we present a novel, comprehensive computational framework to predict and gain insight into the failure of core-shell morphologies and the associated degradation in battery performance. The fully coupled chemo-mechano-damage model presented captures the interplay between mechanical damage and electrochemical behaviours, enabling the quantification of particle cracking and capacity fade. Both bulk material fracture and interface debonding are captured by utilising the phase field method. We quantify the severity of particle cracking and capacity loss through case studies on a representative core-shell system (NMC811@NMC532). The results bring valuable insights into cracking patterns, underlying mechanisms, and their impact on capacity loss. Surface cracks are found to initiate when a significantly higher lithium concentration accumulates in the core compared to the shell. Interfacial debonding is shown to arise from localised hoop stresses near the core-shell interface, due to greater shell expansion. This debonding develops rapidly, impedes lithium-ion transport, and can lead to more than 10\% capacity loss after a single discharge. Furthermore, larger particles may experience crack branching driven by extensive tensile zones, potentially fragmenting the entire particle. The framework developed can not only bring new insight into the degradation mechanisms of core-shell particles but also be used to design electrode materials with improved performance and extended lifetime.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [Evaluating Software Process Models for Multi-Agent Class-Level Code Generation](https://arxiv.org/abs/2511.09794)
*Wasique Islam Shafin,Md Nakhla Rafi,Zhenhao Li,Tse-Hsun Chen*

Main category: cs.SE

TL;DR: 本研究探讨多智能体LLM工作流中流程结构和角色专门化对类级别代码生成的影响，发现在瀑布式开发流程下，代码维护性提升但功能性下降，不同模型表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单智能体函数级代码生成，而现代软件系统需要更复杂、可维护的代码结构，因此需要探索多智能体LLM在类级别代码生成中的协作效果。

Method: 使用三种LLM（GPT-4o-mini、DeepSeek-Chat、Claude-3.5-Haiku）在ClassEval基准的100个Python任务上模拟瀑布式开发周期（需求、设计、实现、测试）。

Result: 多智能体工作流重组而非一致提升模型性能：瀑布式协作产生更清晰、可维护的代码，但普遍降低功能正确性（GPT-4o-mini下降37.8%，DeepSeek-Chat下降39.8%），Claude-3.5-Haiku例外（提升9.5%）。流程约束改变错误特征：结构问题减少，语义和验证错误增加。测试阶段影响最大。

Conclusion: 软件流程结构从根本上改变LLM的推理、协作和失败方式，揭示了多智能体代码生成中严格工作流纪律与灵活问题解决之间的内在权衡。

Abstract: Modern software systems require code that is not only functional but also maintainable and well-structured. Although Large Language Models (LLMs) are increasingly used to automate software development, most studies focus on isolated, single-agent function-level generation. This work examines how process structure and role specialization shape multi-agent LLM workflows for class-level code generation. We simulate a Waterfall-style development cycle covering Requirement, Design, Implementation, and Testing using three LLMs (GPT-4o-mini, DeepSeek-Chat, and Claude-3.5-Haiku) on 100 Python tasks from the ClassEval benchmark. Our findings show that multi-agent workflows reorganize, rather than consistently enhance, model performance. Waterfall-style collaboration produces cleaner and more maintainable code but often reduces functional correctness (-37.8\% for GPT-4o-mini and -39.8\% for DeepSeek-Chat), with Claude-3.5-Haiku as a notable exception (+9.5\%). Importantly, process constraints shift failure characteristics: structural issues such as missing code decrease, while semantic and validation errors become more frequent. Among all stages, Testing exerts the strongest influence by improving verification coverage but also introducing new reasoning failures, whereas Requirement and Design have comparatively modest effects. Overall, this study provides empirical evidence that software process structure fundamentally alters how LLMs reason, collaborate, and fail, revealing inherent trade-offs between rigid workflow discipline and flexible problem-solving in multi-agent code generation.

</details>


### [19] [EnvTrace: Simulation-Based Semantic Evaluation of LLM Code via Execution Trace Alignment -- Demonstrated at Synchrotron Beamlines](https://arxiv.org/abs/2511.09964)
*Noah van der Vleuten,Anthony Flores,Shray Mathur,Max Rakitin,Thomas Hopkins,Kevin G. Yager,Esther H. R. Tsai*

Main category: cs.SE

TL;DR: EnvTrace是一个基于仿真的LLM评估方法，通过执行轨迹评估代码语义等价性，在光束线控制逻辑数字孪生中测试，30多个LLM在控制代码生成方面接近人类水平。


<details>
  <summary>Details</summary>
Motivation: 传统静态算法基准无法全面评估LLMs在仪器控制中的表现，因为物理系统行为无法仅通过单元测试捕捉。

Method: 引入EnvTrace方法，通过仿真评估执行轨迹来分析语义代码等价性，使用光束线控制逻辑数字孪生进行验证。

Result: 超过30个LLM通过轨迹对齐评估，在关键行为维度上获得多功能正确性评分，多个顶级模型在快速控制代码生成方面接近人类水平。

Conclusion: 这是实现LLMs与数字孪生共生协作的第一步：LLMs提供直观控制和智能编排，数字孪生提供安全高保真环境，为自主具身AI铺平道路。

Abstract: Evaluating large language models (LLMs) for instrument control requires methods that go beyond standard, stateless algorithmic benchmarks, since the behavior of physical systems cannot be fully captured by unit tests alone. Here we introduce EnvTrace, a simulation-based method that evaluates execution traces to assess semantic code equivalence. EnvTrace is demonstrated with a beamline control-logic digital twin to facilitate the evaluation of instrument control code, with the digital twin itself also enabling the pre-execution validation of live experiments. Over 30 LLMs were evaluated using trace alignment to generate a multi-faceted score for functional correctness across key behavioral dimensions, showing that many top-tier models can approach human-level performance in rapid control-code generation. This is a first step toward a broader vision where LLMs and digital twins work symbiotically: LLMs providing intuitive control and agentic orchestration, and digital twins offering safe and high-fidelity environments, paving the way towards autonomous embodied AI.

</details>


### [20] [Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics](https://arxiv.org/abs/2511.10271)
*Xin Sun,Daniel Ståhl,Kristian Sandahl,Christoph Kessler*

Main category: cs.SE

TL;DR: 论文系统性评估了LLM生成代码的非功能质量，发现学术研究与行业关注存在错配，且不同质量维度存在权衡关系，需要将质量保证机制整合到代码生成流程中。


<details>
  <summary>Details</summary>
Motivation: 当前LLM已广泛集成到软件工程中支持代码生成，但缺乏对其非功能质量的系统理解和评估。现有研究主要关注代码是否能通过测试而非代码质量。

Method: 基于ISO/IEC 25010质量模型进行三项互补研究：108篇论文的系统综述、多组织从业者的行业研讨会、三种LLM修复实际软件问题的实证分析。

Result: 学术研究重点关注安全性和性能效率，而行业优先考虑可维护性和可读性。实证评估显示功能正确补丁的质量改进往往以牺牲其他质量为代价，运行时和内存结果在不同模型和优化策略间差异显著。

Conclusion: 学术关注、行业优先级和模型性能之间存在错配，迫切需要将质量保证机制整合到LLM代码生成流程中，确保生成的代码不仅通过测试，更具备高质量。

Abstract: In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality.

</details>


### [21] [A Large-Scale Collection Of (Non-)Actionable Static Code Analysis Reports](https://arxiv.org/abs/2511.10323)
*Dávid Kószó,Tamás Aladics,Rudolf Ferenc,Péter Hegedűs*

Main category: cs.SE

TL;DR: 论文提出了一种收集和分类静态代码分析警告的新方法，并能区分可操作和不可操作警告，创建了包含超过100万条Java源代码警告的大规模数据集NASCAR。


<details>
  <summary>Details</summary>
Motivation: 静态代码分析工具会产生大量警告，导致开发者产生警报疲劳，可能忽略关键问题。目前缺乏足够的数据集来训练机器学习模型以改进这些工具。

Method: 引入了一种新颖的方法论来收集和分类SCA警告，有效区分可操作和不可操作警告。

Result: 生成了包含超过100万条Java源代码警告的大规模数据集NASCAR，并将数据集和生成工具公开提供。

Conclusion: 该方法解决了数据稀缺问题，有助于改进SCA工具的准确性和可用性，减轻警报疲劳的影响。

Abstract: Static Code Analysis (SCA) tools, while invaluable for identifying potential coding problems, functional bugs, or vulnerabilities, often generate an overwhelming number of warnings, many of which are non-actionable. This overload of alerts leads to ``alert fatigue'', a phenomenon where developers become desensitized to warnings, potentially overlooking critical issues and ultimately hindering productivity and code quality. Analyzing these warnings and training machine learning models to identify and filter them requires substantial datasets, which are currently scarce, particularly for Java. This scarcity impedes efforts to improve the accuracy and usability of SCA tools and mitigate the effects of alert fatigue. In this paper, we address this gap by introducing a novel methodology for collecting and categorizing SCA warnings, effectively distinguishing actionable from non-actionable ones. We further leverage this methodology to generate a large-scale dataset of over 1 million entries of Java source code warnings, named NASCAR: (Non-)Actionable Static Code Analysis Reports. To facilitate follow-up research in this domain, we make both the dataset and the tools used to generate it publicly available.

</details>
