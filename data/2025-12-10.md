<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 26]
- [cs.CR](#cs.CR) [Total: 13]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs](https://arxiv.org/abs/2512.07841)
*Gabriel M. Arantes,Richard F. Pinto,Bruno L. Dalmazo,Eduardo N. Borges,Giancarlo Lucca,Viviane L. D. de Mattos,Fabian C. Cardoso,Rafael A. Berri*

Main category: cs.AI

TL;DR: 比较数据导向设计(DOD)与面向对象设计(OOD)在多线程环境下的性能表现，发现DOD在处理数据密集型操作时具有显著性能优势


<details>
  <summary>Details</summary>
Motivation: 多核CPU与主内存之间日益增长的性能差距需要硬件感知的软件设计范式

Method: 开发并比较了A*搜索算法的四个版本：单线程OOD、单线程DOD、多线程OOD和多线程DOD，基于执行时间、内存使用和CPU缓存未命中等指标进行评估

Result: 在多线程测试中，DOD实现表现出显著的性能提升，执行时间更快，原始系统调用和缓存未命中次数更少。对于细粒度任务如A*算法，单线程版本在多线程环境中表现更好

Conclusion: 即使在简单算法中性能差异看似微小，但DOD在关键指标上的一致优势突出了其基础架构的优越性，表明在复杂的大规模AI和并行计算任务中，DOD是最大化硬件效率的更有效方法

Abstract: The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks.

</details>


### [2] [SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models](https://arxiv.org/abs/2512.07993)
*Jiayi Tian,Seyedarmin Azizi,Yequan Zhao,Erfan Baghaei Potraghloo,Sean McPherson,Sharath Nittur Sridhar,Zhengyang Wang,Zheng Zhang,Massoud Pedram,Souvik Kundu*

Main category: cs.AI

TL;DR: SkipKV是一种无需训练的KV缓存压缩方法，通过句子级选择和动态向量调整，在保持推理准确性的同时显著减少生成长度和提高吞吐量


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在处理思维链推理时存在显著的KV缓存开销问题，现有基于令牌级的KV缓存驱逐方法在多批次设置下无法保持准确性且会导致生成长度增加

Method: 提出句子级评分机制识别和移除高度相似的句子，同时引入动态引导向量调整隐藏激活状态以抑制冗余生成

Result: 在多推理基准测试中，SkipKV在相似压缩预算下保持高达26.7%的准确性提升，相比最先进方法减少1.6倍生成长度并提高1.7倍吞吐量

Conclusion: SkipKV通过粗粒度句子级序列移除和动态生成控制，有效解决了CoT推理中的KV缓存效率问题

Abstract: Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.

</details>


### [3] [Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching](https://arxiv.org/abs/2512.08026)
*Caroline N. Leach,Mitchell A. Klusty,Samuel E. Armstrong,Justine C. Pickarski,Kristen L. Hankins,Emily B. Collier,Maya Shah,Aaron D. Mullen,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 提出一个基于大语言模型的AI系统，用于自动化临床试验患者匹配，通过可解释的推理链支持人工审查，旨在提高效率并减少协调员负担。


<details>
  <summary>Details</summary>
Motivation: 筛选患者是否符合临床试验资格仍然是一个手动、耗时且资源密集的过程，需要解决关键实施挑战：整合异构电子健康记录（EHR）数据、促进专家审查和保持严格的安全标准。

Method: 利用开源、支持推理的大语言模型（LLMs），系统超越二元分类，生成具有可解释推理链的结构化资格评估，支持人工在环审查。该系统将资格表示为动态状态而非固定判定。

Result: 开发了一个安全、可扩展的概念验证系统，用于AI增强的患者-试验匹配，能够识别可用匹配并提供可操作建议，使患者未来可能符合资格。

Conclusion: 该系统旨在通过AI辅助的患者-试验匹配，减少协调员负担，智能扩展每个患者考虑的试验范围，并保证所有AI生成输出的全面可审计性。

Abstract: Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.

</details>


### [4] [Large Language Models for Education and Research: An Empirical and User Survey-based Analysis](https://arxiv.org/abs/2512.08057)
*Md Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe,Lu Peng*

Main category: cs.AI

TL;DR: 本研究对比ChatGPT与DeepSeek在教育研究场景的性能：ChatGPT擅通用文本任务，DeepSeek长于编程效率；二者均能胜任专业问题求解，用户调查佐证其实用性。


<details>
  <summary>Details</summary>
Motivation: 随着预训练大语言模型在教育与研究领域影响力日益增强，需系统评估主流模型（如ChatGPT和DeepSeek）在实际应用中的性能差异与适用性，为领域内用户提供选型参考。

Method: 本研究采用背景技术分析、实证实验及真实用户调查相结合的综合评估方法，通过文本生成、编程任务和专业问题解决三大基准测试对比模型表现。

Result: 实验表明：ChatGPT在通用语言理解与文本生成任务中领先，DeepSeek凭借效率优化设计在编程任务中表现更优；两者均能准确处理医学诊断与复杂数学问题。用户调查进一步揭示了模型在实际应用中的具体价值与局限。

Conclusion: 研究总结显示，ChatGPT与DeepSeek在不同领域各具优势——前者在通用语言任务中表现优异，后者在编程效率上更胜一筹；两者均能有效支持教育研究应用，但需根据具体场景权衡性能与资源需求。

Abstract: Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency- focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.

</details>


### [5] [Scalable Back-End for an AI-Based Diabetes Prediction Application](https://arxiv.org/abs/2512.08147)
*Henry Anand Septian Radityo,Bernardus Willson,Reynard Tanadi,Latifa Dwiyanti,Saiful Akbar*

Main category: cs.AI

TL;DR: 本文开发了一个用于移动糖尿病预测应用的可扩展后端系统，实现了低于5%的故障率和1000毫秒的平均延迟目标，能够处理10,000名并发用户。


<details>
  <summary>Details</summary>
Motivation: 全球糖尿病患病率上升需要早期检测，AI预测应用需要可扩展的后端架构来有效服务大量用户。

Method: 架构采用水平扩展、数据库分片和通过消息队列的异步通信。

Result: 83%的系统功能（24个中的20个）达到了性能目标，用户档案管理、活动跟踪和读取密集型预测操作等关键功能表现良好。

Conclusion: 使用RabbitMQ的异步通信对于最小化计算密集型预测请求的错误率至关重要，确保系统在高负载下的可靠性。

Abstract: The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.

</details>


### [6] [Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes](https://arxiv.org/abs/2512.08261)
*Yibowen Zhao,Yinan Zhang,Zhixiang Su,Lizhen Cui,Chunyan Miao*

Main category: cs.AI

TL;DR: 提出了名为KPI的知识图谱增强、原型感知和可解释框架，通过引入医学知识图谱和对比学习来解决疾病预测中的不平衡分布和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 基于患者信息（如人口统计数据和自我报告症状）进行疾病预测面临疾病分布不平衡和缺乏可解释性的挑战，导致预测存在偏差或不可靠。

Method: 构建统一的疾病知识图谱，整合结构化可信医学知识；创建临床有意义的疾病原型；采用对比学习提升预测准确性；利用大语言模型生成患者特异性医学解释。

Result: 在真实数据集上的广泛实验表明，KPI在预测准确性上优于现有最优方法，并提供与患者叙述高度一致的临床有效解释。

Conclusion: KPI框架通过增强预测准确性和可解释性，展示了在以患者为中心的医疗保健服务中的实际价值。

Abstract: Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.

</details>


### [7] [Reasoning Models Ace the CFA Exams](https://arxiv.org/abs/2512.08270)
*Jaisal Patel,Yunzhe Chen,Kaiwen He,Keyi Wang,David Li,Kairong Xiao,Xiao-Yang Liu*

Main category: cs.AI

TL;DR: 最新推理模型在CFA各级考试中表现优异，超越了之前LLMs的弱势表现


<details>
  <summary>Details</summary>
Motivation: 评评估先进推理模型在CFA模拟考试中的表现，验证其专业能力

Method: 使用980道题组成的模拟CFA考试集，按CFA标准评估多个推理模型

Result: 多数模型通过所有三个级别考试，Gemini 3.0 Pro整体最优，Level I达97.6%，GPT-5 Level II达94.3%，Gemini系列在Level III表现突出

Conclusion: 现代推理模型已能胜任CFA等专业性考试，展示较强的推理能力

Abstract: Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.

</details>


### [8] [AgentEval: Generative Agents as Reliable Proxies for Human Evaluation of AI-Generated Content](https://arxiv.org/abs/2512.08273)
*Thanh Vu,Richi Nayak,Thiru Balasubramaniam*

Main category: cs.AI

TL;DR: 本研究提出使用生成智能体来高效、低成本地评估AI生成内容的质量，模拟人类判断多个维度，帮助企业减少对昂贵人工评估的依赖，提升内容质量一致性。


<details>
  <summary>Details</summary>
Motivation: 现代企业在生成和评估高质量内容时面临时间和成本的双重挑战。人工撰写受时间限制，外部评估成本高昂。尽管大语言模型（LLMs）在内容创作方面具有潜力，但人们对AI生成内容的质量仍存疑虑。传统的评估方法（如人工调查）进一步增加了运营成本，因此需要高效、自动化的解决方案。

Method: 研究引入了生成智能体（Generative Agents）作为评估AI生成内容的方法。这些智能体能够快速、经济地模拟人类判断，对内容的连贯性、趣味性、清晰度、公平性和相关性等方面进行评分。

Result: 生成智能体能够有效评估AI生成内容，模拟人类判断，对多个质量维度进行评分。通过整合这些智能体，企业可以优化内容生成流程，确保输出内容的高质量和一致性，同时大幅降低对成本高昂的人工评估的依赖。

Conclusion: 本研究通过引入生成智能体，为企业在内容生成和评估方面提供了高效、低成本的解决方案，显著提升了AI生成内容的质量一致性，并减少了对昂贵人工评估的依赖。

Abstract: Modern businesses are increasingly challenged by the time and expense required to generate and assess high-quality content. Human writers face time constraints, and extrinsic evaluations can be costly. While Large Language Models (LLMs) offer potential in content creation, concerns about the quality of AI-generated content persist. Traditional evaluation methods, like human surveys, further add operational costs, highlighting the need for efficient, automated solutions. This research introduces Generative Agents as a means to tackle these challenges. These agents can rapidly and cost-effectively evaluate AI-generated content, simulating human judgment by rating aspects such as coherence, interestingness, clarity, fairness, and relevance. By incorporating these agents, businesses can streamline content generation and ensure consistent, high-quality output while minimizing reliance on costly human evaluations. The study provides critical insights into enhancing LLMs for producing business-aligned, high-quality content, offering significant advancements in automated content generation and evaluation.

</details>


### [9] [Towards a Science of Scaling Agent Systems](https://arxiv.org/abs/2512.08296)
*Yubin Kim,Ken Gu,Chanwoo Park,Chunjong Park,Samuel Schmidgall,A. Ali Heydari,Yao Yan,Zhihan Zhang,Yuchen Zhuang,Mark Malhotra,Paul Pu Liang,Hae Won Park,Yuzhe Yang,Xuhai Xu,Yilun Du,Shwetak Patel,Tim Althoff,Daniel McDuff,Xin Liu*

Main category: cs.AI

TL;DR: 本文提出了基于语言模型的智能体系统的量化扩展原则，通过五个标准架构在四个基准测试上的实验，建立了预测模型来指导智能体协调策略的选择。


<details>
  <summary>Details</summary>
Motivation: 尽管基于语言模型的智能体系统在现实AI应用中日益普及，但其性能决定原则尚未充分探索，实践中仍依赖启发式而非原则性设计选择。

Method: 在四个不同基准（Finance-Agent等）上评估五种标准架构（Single、Independent等），使用三种LLM家族的180个标准化配置，构建基于经验协调指标的预测模型。

Result: 识别出三个主导效应：工具-协调权衡、能力饱和和拓扑依赖错误放大。预测框架对87%的保留配置预测最优协调策略。

Conclusion: 该研究提供了基于可测量任务属性的智能体扩展预测原则，能够指导实际应用中的智能体系统设计。

Abstract: Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.

</details>


### [10] [rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection](https://arxiv.org/abs/2512.08300)
*Sijia Chen,Baochun Li,Di Niu*

Main category: cs.AI

TL;DR: 提出了增强策略注入机制(rSIM)，通过小型规划器指导LLM的思维链，使其具备高级推理能力


<details>
  <summary>Details</summary>
Motivation: 观察到LLM通过强化学习训练后出现推理策略运用的"aha"时刻，希望将其系统化

Method: 采用领导者-跟随者多智能体强化学习框架，联合训练规划器与LLM

Result: rSIM使0.5B参数模型性能超越14B模型，规划器具备通用性和持续学习能力

Conclusion: rSIM机制能有效提升LLM的推理能力，且具有很好的泛化性和扩展性

Abstract: Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.

</details>


### [11] [Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from Türkiye](https://arxiv.org/abs/2512.08340)
*Abdullah Hulusi Kökçam,Uğur Dağdeviren,Talas Fikret Kurnaz,Alparslan Serhat Demir,Caner Erden*

Main category: cs.AI

TL;DR: 本研究开发了一个机器学习框架，使用382个土耳其土壤样本预测加州承载比(CBR)。随机森林模型表现最佳，R2分数达0.83，为传统岩土测试提供了高效替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统的CBR测定依赖于实验室贯入试验，尽管准确，但通常耗时、昂贵，且对于大规模或多样化的土壤剖面可能不实用。人工智能特别是机器学习的进展使得数据驱动方法能够以更高的速度和精度建模复杂的土壤行为。

Method: 研究引入了一个全面的机器学习框架，使用从土耳其不同地理气候区域收集的382个土壤样本数据集。测试了12种机器学习算法，包括决策树、随机森林、额外树、梯度提升、xgboost、k近邻、支持向量回归、多层感知器、adaboost、bagging、投票和堆叠回归器。

Result: 随机森林回归器表现最佳，实现了强大的R2分数：训练集0.95，验证集0.76，测试集0.83。这些结果突显了模型强大的非线性映射能力。

Conclusion: 该研究支持将智能、数据驱动的模型集成到岩土工程中，为传统方法提供了有效的替代方案，并促进了基础设施分析和设计的数字化转型。

Abstract: The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in Türkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.

</details>


### [12] [Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach](https://arxiv.org/abs/2512.08343)
*Caner Erden,Alparslan Serhat Demir,Abdullah Hulusi Kokcam,Talas Fikret Kurnaz,Ugur Dagdeviren*

Main category: cs.AI

TL;DR: 本研究提出了一种自动化机器学习（AutoML）方法，用于预测土壤的最佳含水率（OMC）和最大干密度（MDD）。通过自动化算法选择和超参数优化，AutoML在异质数据集上表现优于传统方法，其中XGBoost算法取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统的OMC和MDD测定方法依赖劳动密集型的实验室实验，而现有的机器学习模型在预测准确性和泛化能力方面存在局限，尤其是面对不同土壤类型的异质数据集时。

Method: 采用AutoML方法，自动进行算法选择和超参数优化，以预测OMC和MDD。通过大量实验评估不同算法的性能。

Result: XGBoost算法在独立数据集上表现最佳，MDD的R平方值为80.4%，OMC的R平方值为89.1%。

Conclusion: AutoML方法能有效预测不同土壤类型的压实参数，异质数据集对提升ML模型的泛化能力和性能至关重要，该研究有助于提高施工实践的效率和可靠性。

Abstract: Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.

</details>


### [13] [DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals](https://arxiv.org/abs/2512.08379)
*Kaiwei Liu,Yuting He,Bufang Yang,Mu Yuan,Chun Man Victor Wong,Ho Pong Andrew Sze,Zhenyu Yan,Hongkai Chen*

Main category: cs.AI

TL;DR: 本文提出了DeepFeature，首个基于LLM的上下文感知特征生成框架，用于可穿戴生物信号，通过多源特征生成、迭代精炼和多层过滤验证，在多项任务中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有特征提取方法缺乏任务特定上下文知识，难以在高维特征空间中找到最优设置，且易出现代码生成错误。

Method: DeepFeature采用多源特征生成机制整合专家知识与任务设定，使用基于特征评估的反馈进行迭代特征精炼，并通过多层过滤验证确保特征到代码的稳健转换。

Result: 实验显示，DeepFeature在八项任务中平均AUROC提升4.21-9.67%，在五项任务中超越最先进方法，其余任务性能相当。

Conclusion: DeepFeature框架有效解决了特征提取的上下文不足和自动化错误问题，提升了可穿戴生物信号分析的性能和鲁棒性。

Abstract: Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.

</details>


### [14] [Using reinforcement learning to probe the role of feedback in skill acquisition](https://arxiv.org/abs/2512.08463)
*Antonio Terpin,Raffaello D'Andrea*

Main category: cs.AI

TL;DR: 研究人员开发了一个强化学习系统来控制水槽中旋转圆柱体的阻力，发现了对于学习高性能技能而言，训练期间的信息丰富度比执行技能更重要。


<details>
  <summary>Details</summary>
Motivation: 研究在没有外部反馈的情况下人类如何获得高性能技能（如花样滑冰的三周跳），但由于直接研究人类受试者困难，转而使用可控的物理系统来探究技能学习过程。

Method: 将通用强化学习智能体与桌面循环水槽中的旋转圆柱体连接，通过高维流动反馈来最大化或最小化阻力，并与无反馈训练进行比较。

Result: 有流动反馈时，智能体在几分钟内就发现了高性能的阻力控制策略；无反馈时训练失败（阻力最大化）或效果较差（阻力最小化）。但学习到的策略在执行时无需反馈。

Conclusion: 学习高性能技能可能需要比执行该技能更丰富的信息，且学习条件的难易程度仅取决于目标，而非动力学或策略的复杂性。

Abstract: Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.

</details>


### [15] [Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance](https://arxiv.org/abs/2512.08492)
*Aliaksei Kaliutau*

Main category: cs.AI

TL;DR: 本文提出了一种从控制流为中心转向数据流为中心的程序自动修复新范式，使用数据转换图替代传统的代码属性图，通过多智能体框架实现了87.1%的缺陷修复率。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的代码生成在函数级别已取得进展，但仓库级的自动程序修复仍面临挑战。现有方法采用控制流为中心的范式，导致智能体需要处理复杂的目录结构和不相关的控制逻辑。

Method: 提出数据转换图（DTG）概念，将数据状态作为节点、函数作为边，通过数据血缘而非控制流来追踪逻辑缺陷。设计了包含神经符号推理的多智能体框架AIR（自主问题解决器）。

Result: 在多个软件工程基准测试中表现出色，特别是在SWE-Verified基准上达到了87.1%的修复率。

Conclusion: 该方法成功解决了现代编码智能体中RAG系统固有的“语义陷阱”问题，为软件依赖日益增长的世界提供了更强大的自动代码维护基础。

Abstract: Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the "Semantic Trap" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.

</details>


### [16] [Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans](https://arxiv.org/abs/2512.08536)
*Tammy Zhong,Yang Song,Maurice Pagnucco*

Main category: cs.AI

TL;DR: Principles2Plan是一个交互式研究原型，展示人类与大语言模型如何合作生成情境敏感的伦理规则，以指导自动规划。


<details>
  <summary>Details</summary>
Motivation: 机器人在人类环境中操作需要伦理意识，但现有的自动规划工具对此支持不足。手动指定伦理规则既费力又高度依赖具体情境。

Method: 领域专家提供规划域、问题细节和相关高级原则（如仁慈和隐私）。系统生成与这些原则一致的可操作伦理规则，用户可以审查、优先排序并提供给规划器。

Result: 该系统能够生成基于原则的伦理规则，支持经典规划情境下的伦理自动规划。

Conclusion: Principles2Plan展示了人类与LLM合作在使伦理自动规划更实用和可行方面的潜力。

Abstract: Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.

</details>


### [17] [CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models](https://arxiv.org/abs/2512.08609)
*Hui Wang,Yang Liu,Xiaoyu Zhang,Chaoxu Mu*

Main category: cs.AI

TL;DR: 本文提出认知引导蒙特卡洛树搜索框架(CogMCTS)，通过多轮认知反馈和双轨节点扩展优化LLM的启发式生成，在自动启发式设计中实现更好的探索-利用平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的进化方法容易陷入局部最优，MCTS与LLM结合时多轮认知整合有限且搜索多样性不足。

Method: CogMCTS将LLM认知引导机制与MCTS深度集成，采用多轮认知反馈整合历史经验/节点信息/负面结果，通过双轨节点扩展和精英启发式管理平衡探索与利用，辅以策略突变增强多样性。

Result: 实验表明CogMCTS在稳定性、效率和求解质量上优于现有LLM-based AHD方法。

Conclusion: CogMCTS框架有效解决了LLM启发式生成的局限性，为复杂优化问题提供了更高效的自动启发式设计解决方案。

Abstract: Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.

</details>


### [18] [Protein Secondary Structure Prediction Using Transformers](https://arxiv.org/abs/2512.08613)
*Manzi Kevin Maxime*

Main category: cs.AI

TL;DR: 使用Transformer模型和注意力机制预测蛋白质二级结构，在CB513数据集上取得良好效果


<details>
  <summary>Details</summary>
Motivation: 蛋白质二级结构（如α螺旋、β折叠和无规则卷曲）的预测对于理解蛋白质功能至关重要

Method: 提出了一种基于Transformer的模型，应用注意力机制处理蛋白质序列数据来预测结构基序。在CB513数据集上使用滑动窗口数据增强技术扩展训练样本

Result: Transformer模型在处理可变长度序列时表现出强大的泛化能力，有效捕捉了局部和长程残基相互作用

Conclusion: 基于Transformer的模型是预测蛋白质二级结构的有效方法

Abstract: Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.

</details>


### [19] [See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm](https://arxiv.org/abs/2512.08629)
*Haoyu Zhao,Weizhong Ding,Yuhao Yang,Zheng Tian,Linyi Yang,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: See-Control框架通过低自由度机械臂实现平台无关的智能手机操作，包含基准任务集、基于MLLM的具身智能体和操作数据集


<details>
  <summary>Details</summary>
Motivation: 现有智能手机操作方案依赖ADB，仅适用于Android设备，限制了应用范围

Method: 提出具身智能手机操作任务，开发包含155个任务的基准测试集，构建基于MLLM的具身智能体直接生成机器人控制命令

Result: 创建了包含操作记录的数据集，为未来研究提供资源

Conclusion: 该框架弥合了数字智能体与物理世界之间的差距，推动家庭机器人执行智能手机相关任务

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.

</details>


### [20] [Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology](https://arxiv.org/abs/2512.08674)
*Rongzhao Zhang,Junqiao Wang,Shuyun Yang,Mouxiao Bian,Chao Ding,Yuwei Bai,Chihao Zhang,Yuguang Shen,Lei Wang,Lei Zheng,Qiujuan Yan,Yun Zhong,Meiling Liu,Jiwei Yu,Zheng Wang,Jie Xu,Meng Luo*

Main category: cs.AI

TL;DR: 提出了一个分层多智能体框架来模拟人类多学科团队协作，用于胃肠道肿瘤的多模态临床推理。该系统在专家评估中获得4.60/5.00的高分，显著优于单体基线。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在处理复杂异构医疗数据时面临上下文稀释和幻觉问题，需要更可靠的临床决策支持系统。

Method: 开发了分层多Agent框架，模拟多学科团队协作的工作流程，整合内窥镜图像、放射学数据和生物标志物的解读。

Result: 系统在专家评估中获得4.60/5.00的高分，在推理逻辑和医学准确性方面获得最大改进。

Conclusion: 基于智能体的模仿协作为肿瘤学自动化决策支持提供了可扩展、可解释且临床稳健的范式。

Abstract: Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.

</details>


### [21] [Towards Foundation Models with Native Multi-Agent Intelligence](https://arxiv.org/abs/2512.08743)
*Shuyue Hu,Haoyang Yan,Yiqun Zhang,Yang Chen,Dongzhan Zhou,Lei Bai*

Main category: cs.AI

TL;DR: 论文主张基础模型需要在多智能体环境中具备原生智能，并确定了四个核心能力。实证研究表明当前大语言模型的单智能体能力不足以自动实现多智能体智能，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型主要发展单智能体能力（如GUI交互、工具使用），但论文认为多智能体智能是下一个前沿领域，需要系统性地赋予基础模型原生多智能体能力。

Method: 通过分析41个大语言模型的实证研究，验证了单智能体性能与多智能体智能的非必然关联性。

Result: 研究表明强大的单智能体性能不会自动转化为稳健的多智能体智能，揭示了现有基础模型在多智能体环境中的能力局限。

Conclusion: 需要专门的研究方向（数据集构建、评估方法、训练范式和安全考量）来系统性开发具备原生多智能体智能的基础模型。

Abstract: Foundation models (FMs) are increasingly assuming the role of the "brain" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.

</details>


### [22] [A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows](https://arxiv.org/abs/2512.08769)
*Eranga Bandara,Ross Gore,Peter Foytik,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Xueping Liang,Safdar H. Bouk,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: 本文提供了构建生产级Agentic AI工作流的端到端实用指南，包含工程生命周期、最佳实践和案例研究


<details>
  <summary>Details</summary>
Motivation: 随着Agentic AI在行业和研究中的加速采用，组织面临如何设计、工程和操作满足可靠性、可观察性、可维护性及安全治理要求的生产级工作流的挑战

Method: 引入结构化工程生命周期，包括工作流分解、多智能体设计模式、MCP协议、工具集成、确定性编排、负责任AI考量和环境感知部署策略

Result: 提出了九条核心最佳实践，并通过多模态新闻分析和媒体生成工作流的案例研究展示了这些原则的实际应用

Conclusion: 通过结合架构指导、操作模式和实践实施洞察，本文为构建稳健、可扩展且生产就绪的Agentic AI工作流提供了基础参考

Abstract: Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.

</details>


### [23] [CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale](https://arxiv.org/abs/2512.08826)
*Shahar Sarfaty,Adi Haviv,Uri Hacohen,Niva Elkin-Koren,Roi Livni,Amit H. Bermano*

Main category: cs.AI

TL;DR: CARLoS框架通过分析LoRA生成图像的特征，构建了方向、强度和一致性三个维度的表征，实现了无需元数据的LoRA检索，并在自动化评估和人工评估中优于文本基线。


<details>
  <summary>Details</summary>
Motivation: 现有生成组件（如LoRAs）生态系统庞大但缺乏结构化，现有发现方法依赖不可靠的用户描述或有偏见的流行度指标，影响了可用性。

Method: 通过分析650多个LoRAs，在不同提示和种子下生成图像，使用CLIP嵌入及其与基础模型生成的差异，定义了方向（语义偏移）、强度（效果显著性）和一致性（效果稳定性）的三部分表征。

Result: 开发了高效的检索框架，语义匹配文本查询到相关LoRAs，同时过滤过强或不稳定的LoRAs，在自动和人工评估中优于文本基线。

Conclusion: CARLoS不仅实现了有效的LoRA检索，其表征还可支持将强度和一致性与版权中的实质性和意愿性等法律概念联系起来分析，展示了其在LoRA分析中的广泛实用性。

Abstract: The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.

</details>


### [24] [Interpolation in Knowledge Representation](https://arxiv.org/abs/2512.08833)
*Jean Christoph Jung,Patrick Koopmann,Matthias Knorr*

Main category: cs.AI

TL;DR: 克雷格插值和均匀插值在知识表示中有广泛应用，但实际计算插值项具有挑战性。本文重点研究了描述逻辑和逻辑编程这两种知识表示形式，讨论了计算插值项的理论结果和实用方法。


<details>
  <summary>Details</summary>
Motivation: 克雷格插值和均匀插值在知识表示领域具有重要作用，但在实际应用中面临诸多挑战，特别是在某些知识表示形式中缺乏有效的插值计算方法。

Method: 通过深入研究描述逻辑和逻辑编程这两种知识表示形式，分析和讨论计算插值项的理论框架和实际计算方法。

Result: 提供了两种知识表示形式下计算克雷格插值和均匀插值的理论支撑和实践方法。

Conclusion: 本文的系统性研究为知识表示中插值计算的挑战提供了理论指导和实践解决方案，对促进相关应用具有重要意义。

Abstract: Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.

</details>


### [25] [EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce](https://arxiv.org/abs/2512.08868)
*Rui Min,Zile Qiao,Ze Xu,Jiawen Zhai,Wenyu Gao,Xuanzhong Chen,Haozhen Sun,Zhen Zhang,Xinyu Wang,Hong Zhou,Wenbiao Yin,Xuan Zhou,Yong Jiang,Haicheng Liu,Liang Ding,Ling Zou,Yi R.,Fung,Yalong Li,Pengjun Xie*

Main category: cs.AI

TL;DR: 引入EcomBench电商基准测试，评估智能体在真实电商环境中的表现能力


<details>
  <summary>Details</summary>
Motivation: 现有基准主要集中在学术设置或人工设计场景，忽视了真实应用中的挑战

Method: 基于全球领先电商生态的真实用户需求构建，经过专家精心标注，涵盖多个任务类别和三个难度级别

Result: EcomBench为评估智能体在现代电商中的实际能力提供了严谨动态的测试平台

Conclusion: 该基准通过真实电商场景评估，有效衡量智能体的深度信息检索、多步推理和跨源知识整合等关键能力

Abstract: Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.

</details>


### [26] [Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs](https://arxiv.org/abs/2512.08923)
*Angela van Sprang,Laurens Samson,Ana Lucic,Erman Acar,Sennay Ghebreab,Yuki M. Asano*

Main category: cs.AI

TL;DR: 提出了REST和REST+基准来系统评估多模态大语言模型的跨模态不一致性，发现即使OCR正确，视觉特征和视觉标记数量仍影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型训练用于在相同嵌入空间表示视觉和语言，但无法在两种模态中执行相同任务，需要评估其跨模态一致性。

Method: 创建包含图像、文本和混合三种模态相同语义信息的样本，评估15个最先进的多模态大语言模型在不同模态下的一致性。

Result: 最先进的多模态大语言模型无法在不同模态间保持一致性推理；视觉特征和视觉标记数量影响模型性能。

Conclusion: 跨模态不一致性与文本和图像之间的模态差距相关，为多模态大语言模型的机制解释提供了见解。

Abstract: We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [27] [AgentCrypt: Advancing Privacy and (Secure) Computation in AI Agent Collaboration](https://arxiv.org/abs/2512.08104)
*Harish Karthikeyan,Yue Guo,Leo de Castro,Antigoni Polychroniadou,Leo Ardon,Udari Madhushani Sehwag,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As AI agents increasingly operate in real-world, multi-agent environments, ensuring reliable and context-aware privacy in agent communication is critical, especially to comply with evolving regulatory requirements. Traditional access controls are insufficient, as privacy risks often arise after access is granted; agents may use information in ways that compromise privacy, such as messaging humans, sharing context with other agents, making tool calls, persisting data, or generating derived private information. Existing approaches often treat privacy as a binary constraint, whether data is shareable or not, overlooking nuanced, role-specific, and computation-dependent privacy needs essential for regulatory compliance.
  Agents, including those based on large language models, are inherently probabilistic and heuristic. There is no formal guarantee of how an agent will behave for any query, making them ill-suited for operations critical to security. To address this, we introduce AgentCrypt, a four-tiered framework for fine-grained, encrypted agent communication that adds a protection layer atop any AI agent platform. AgentCrypt spans unrestricted data exchange (Level 1) to fully encrypted computation using techniques such as homomorphic encryption (Level 4). Crucially, it guarantees the privacy of tagged data is always maintained, prioritizing privacy above correctness.
  AgentCrypt ensures privacy across diverse interactions and enables computation on otherwise inaccessible data, overcoming barriers such as data silos. We implemented and tested it with Langgraph and Google ADK, demonstrating versatility across platforms. We also introduce a benchmark dataset simulating privacy-critical tasks at all privacy levels, enabling systematic evaluation and fostering the development of regulatable machine learning systems for secure agent communication and computation.

</details>


### [28] [Security Analysis of Integer Learning with Errors with Rejection Sampling](https://arxiv.org/abs/2512.08172)
*Kyle Yates,Antsa Pierrottet,Abdullah Al Mamun,Ryann Cartor,Mashrur Chowdhury,Shuhong Gao*

Main category: cs.CR

TL;DR: 本文提出了对ILWE问题的数字攻击方法，通过实验验证了基于ILWE的签名方案（如CRYSTALS-Dilithium）的安全性，并讨论了在智能交通系统等实际应用中的影响。


<details>
  <summary>Details</summary>
Motivation: 研究直接针对小参数ILWE实例的攻击有效性，特别是用于流行数字签名方案（如CRYSTALS-Dilithium）的情况，以验证其安全性。

Method: 使用线性最小二乘法进行数字攻击，通过签名直接构建ILWE实例，引入了基于实数矩阵的模多项式算术和高效处理大样本量的算法。

Result: 实验结果表明，基于ILWE的签名方案具有宣称的安全性，攻击方法在实际应用中效果有限。

Conclusion: 基于ILWE的签名方案在现实世界中是安全的，但需要进一步研究其在实际应用如智能交通系统中的长期安全性。

Abstract: At ASIACRYPT 2018, a digital attack based on linear least squares was introduced for a variant of the learning with errors (LWE) problem which omits modular reduction known as the integer learning with errors problem (ILWE). In this paper, we present a theoretical and experimental study of the effectiveness of the attack when applied directly to small parameter ILWE instances found in popular digital signature schemes such as CRYSTALS-Dilithium which utilize rejection sampling. Unlike other studies which form ILWE instances based on additional information obtained from side-channel attacks, we take a more direct approach to the problem by constructing our ILWE instance from only the obtained signatures. We outline and introduce novel techniques in our simulation designs such as modular polynomial arithmetic via matrices in $\mathbb{R}$, as well as algorithms for handling large sample sizes efficiently. Our experimental results reinforce the proclaimed security of signature schemes based on ILWE. We additionally discuss the implications of our work and digital signatures as a whole in regards to real-world applications such as in Intelligent Transportation Systems (ITS).

</details>


### [29] [A Practical Framework for Evaluating Medical AI Security: Reproducible Assessment of Jailbreaking and Privacy Vulnerabilities Across Clinical Specialties](https://arxiv.org/abs/2512.08185)
*Jinghao Wang,Ping Zhang,Carter Yagemann*

Main category: cs.CR

TL;DR: 提出一个可完全复现的医疗AI安全评估框架，能够在消费级CPU硬件上运行，无需GPU集群或商业API访问


<details>
  <summary>Details</summary>
Motivation: 当前医疗大语言模型的安全评估存在资源门槛高、难以复现的问题，限制了社区参与这一重要研究领域

Method: 开发包含多种医疗专科、分层临床风险的安全评估框架，使用合成患者记录避免IRB审批，涵盖越狱攻击和隐私提取攻击

Result: 建立了医疗专家模型和安全防御机制的比较评估基础

Conclusion: 该框架为实现安全可信的医疗AI系统提供了实践基础

Abstract: Medical Large Language Models (LLMs) are increasingly deployed for clinical decision support across diverse specialties, yet systematic evaluation of their robustness to adversarial misuse and privacy leakage remains inaccessible to most researchers. Existing security benchmarks require GPU clusters, commercial API access, or protected health data -- barriers that limit community participation in this critical research area. We propose a practical, fully reproducible framework for evaluating medical AI security under realistic resource constraints. Our framework design covers multiple medical specialties stratified by clinical risk -- from high-risk domains such as emergency medicine and psychiatry to general practice -- addressing jailbreaking attacks (role-playing, authority impersonation, multi-turn manipulation) and privacy extraction attacks. All evaluation utilizes synthetic patient records requiring no IRB approval. The framework is designed to run entirely on consumer CPU hardware using freely available models, eliminating cost barriers. We present the framework specification including threat models, data generation methodology, evaluation protocols, and scoring rubrics. This proposal establishes a foundation for comparative security assessment of medical-specialist models and defense mechanisms, advancing the broader goal of ensuring safe and trustworthy medical AI systems.

</details>


### [30] [Secure Audio Embedding in Images using Nature-Inspired Optimization](https://arxiv.org/abs/2512.08299)
*Aman Kumar,Ankit Chaudhary*

Main category: cs.CR

TL;DR: 提出了一种基于HHO算法优化的LSB音频隐写方法，在图像质量和嵌入容量方面优于现有方法


<details>
  <summary>Details</summary>
Motivation: 在数字世界中保护敏感数据传输，通过隐藏数据存在而非内容来增强多媒体通信安全

Method: 使用Harris Hawks Optimization算法优化的LSB方法，将音频文件隐藏到图像中

Result: 实验评估显示HHO方法在PSNR、SSIM和MSE指标上表现优于现有方法

Conclusion: HHO优化的LSB隐写技术能提供更好的图像质量、鲁棒性和嵌入容量，适用于安全多媒体通信

Abstract: In todays digital world, protecting sensitive data is very essential. Steganography hides the existence of secret data instead of its content, providing better security for multimedia communication. This paper proposes a new technique for hiding audio files inside images using the Least Significant Bit (LSB) method optimized by the Harris Hawks Optimization (HHO) algorithm. HHO is a nature-inspired metaheuristic that imitates the hunting behavior of Harris hawks to find optimal pixel positions for embedding data. The proposed method is evaluated using Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Mean Square Error (MSE). Experimental results show that HHO achieves better image quality, robustness, and embedding capacity compared to existing methods.

</details>


### [31] [Developing a Strong CPS Defender: An Evolutionary Approach](https://arxiv.org/abs/2512.08320)
*Qingyuan Hu,Christopher M. Poskitt,Jun Sun,Yuqi Chen*

Main category: cs.CR

TL;DR: Evo-Defender是一种通过攻击者-防御者动态交互进化增强CPS防御的框架，相比传统一次性训练方法显著提升对未知攻击的检测性能


<details>
  <summary>Details</summary>
Motivation: 传统CPS异常检测方法依赖专家或模糊测试生成的数据集训练，难以泛化到更隐蔽的攻击策略，缺乏动态对抗机制

Method: 提出进化框架Evo-Defender：智能攻击者使用引导式模糊测试探索多样攻击策略，自进化防御者通过增量学习适应新攻击模式

Result: 在田纳西伊士曼流程和机器人装配工作站两个CPS测试平台上注入600+攻击场景，对未知场景的检测性能比基线最高提升2.7%

Conclusion: 动态对抗训练能更高效利用数据实现快速鲁棒检测，为关键基础设施安全提供新范式

Abstract: Cyber-physical systems (CPSs) are used extensively in critical infrastructure, underscoring the need for anomaly detection systems that are able to catch even the most motivated attackers. Traditional anomaly detection techniques typically do `one-off' training on datasets crafted by experts or generated by fuzzers, potentially limiting their ability to generalize to unseen and more subtle attack strategies. Stopping at this point misses a key opportunity: a defender can actively challenge the attacker to find more nuanced attacks, which in turn can lead to more effective detection capabilities. Building on this concept, we propose Evo-Defender, an evolutionary framework that iteratively strengthens CPS defenses through a dynamic attacker-defender interaction. Evo-Defender includes a smart attacker that employs guided fuzzing to explore diverse, non-redundant attack strategies, while the self-evolving defender uses incremental learning to adapt to new attack patterns. We implement Evo-Defender on two realistic CPS testbeds: the Tennessee Eastman process and a Robotic Arm Assembly Workstation, injecting over 600 attack scenarios. In end-to-end attack detection experiments, Evo-Defender achieves up to 2.7% higher performance than state-of-the-art baselines on unseen scenarios, while utilizing training data more efficiently for faster and more robust detection.

</details>


### [32] [Argus: A Multi-Agent Sensitive Information Leakage Detection Framework Based on Hierarchical Reference Relationships](https://arxiv.org/abs/2512.08326)
*Bin Wang,Hui Li,Liyang Zhang,Qijia Zhuang,Ao Yang,Dong Zhang,Xijun Luo,Bing Lin*

Main category: cs.CR

TL;DR: 提出了名为Argus的多智能体协作框架，通过三层检测机制识别代码仓库中的敏感信息泄漏，在真实环境中达到94.86%的检测准确率且成本低廉。


<details>
  <summary>Details</summary>
Motivation: 传统敏感信息检测方法（如正则表达式、指纹特征）误报率高，降低检测效率并增加人工筛查负担，而大语言模型和多智能体架构为这一问题提供了新思路。

Method: 设计Argus多智能体协作框架，集成关键内容、文件上下文和项目引用关系的三层检测机制，并创建两个新基准评估真实泄漏检测能力和误报过滤性能。

Result: 实验显示Argus在泄漏检测中准确率最高达94.86%，精确率96.36%，召回率94.64%，F1分数0.955；分析97个真实仓库总成本仅2.2美元。

Conclusion: Argus有效降低了敏感信息检测的误报率，提升了检测准确性，且具备低成本优势，代码和数据集已开源供进一步研究应用。

Abstract: Sensitive information leakage in code repositories has emerged as a critical security challenge. Traditional detection methods that rely on regular expressions, fingerprint features, and high-entropy calculations often suffer from high false-positive rates. This not only reduces detection efficiency but also significantly increases the manual screening burden on developers. Recent advances in large language models (LLMs) and multi-agent collaborative architectures have demonstrated remarkable potential for tackling complex tasks, offering a novel technological perspective for sensitive information detection. In response to these challenges, we propose Argus, a multi-agent collaborative framework for detecting sensitive information. Argus employs a three-tier detection mechanism that integrates key content, file context, and project reference relationships to effectively reduce false positives and enhance overall detection accuracy. To comprehensively evaluate Argus in real-world repository environments, we developed two new benchmarks, one to assess genuine leak detection capabilities and another to evaluate false-positive filtering performance. Experimental results show that Argus achieves up to 94.86% accuracy in leak detection, with a precision of 96.36%, recall of 94.64%, and an F1 score of 0.955. Moreover, the analysis of 97 real repositories incurred a total cost of only 2.2$. All code implementations and related datasets are publicly available at https://github.com/TheBinKing/Argus-Guard for further research and application.

</details>


### [33] [USCSA: Evolution-Aware Security Analysis for Proxy-Based Upgradeable Smart Contracts](https://arxiv.org/abs/2512.08372)
*Xiaoqi Li,Lei Xie,Wenkai Li,Zongwei Li*

Main category: cs.CR

TL;DR: USCSA：基于AST差分分析的可升级智能合约安全分析器，针对合约升级过程中引入的漏洞进行风险评估，实验显示准确率达92.3%。


<details>
  <summary>Details</summary>
Motivation: 区块链系统升级智能合约时，升级操作常引入新漏洞，需关注升级和维护的连续性。

Method: 采用AST差分分析评估升级过程风险，分析了3546个可升级合约漏洞案例，覆盖重入、访问控制缺陷等常见漏洞。

Result: USCSA检测升级引发的漏洞准确率92.3%，召回率89.7%，F1分数91.0%；高风险变更映射效率较传统方法提升30%。

Conclusion: USCSA显著提升可升级智能合约的安全性与完整性，为区块链应用安全审计提供新颖高效的解决方案。

Abstract: In the case of upgrading smart contracts on blockchain systems, it is essential to consider the continuity of upgrade and subsequent maintenance. In practice, upgrade operations often introduce new vulnerabilities. To address this, we propose an Upgradable Smart Contract Security Analyzer, USCSA, which evaluates the risks associated with the upgrade process using the Abstract Syntax Tree (AST) differential analysis. We collected and analyzed 3,546 cases of vulnerabilities in upgradable contracts,covering common vulnerability categories such as reentrancy, access control flaws, and integer overflow. Experimental results show that USCSA achieves an accuracy of 92.3%, recall of 89.7%, and F1-score of 91.0% in detecting upgrade-induced vulnerabilities.
  In addition, the efficiency of mapping high-risk changes has achieved a 30% improvement over the conventional approach. As a result, USCSA provides a significant advantage to improve the security and integrity of upgradable smart contracts, providing a novel and efficient solution to secure audits on blockchain applications.

</details>


### [34] [Attention is All You Need to Defend Against Indirect Prompt Injection Attacks in LLMs](https://arxiv.org/abs/2512.08417)
*Yinan Zhong,Qianhao Miao,Yanjiao Chen,Jiangyi Deng,Yushi Cheng,Wenyuan Xu*

Main category: cs.CR

TL;DR: 论文介绍了Rennervate防御框架，用于检测和防止间接提示注入攻击，通过细粒度令牌级检测和净化保护LLM应用。


<details>
  <summary>Details</summary>
Motivation: LLM应用易受通过不可信外部数据源注入指令的间接提示注入攻击，现有防御方法效果有限。

Method: 利用注意力特征通过2步注意力池化机制在令牌级检测隐蔽注入，实现精确净化同时保持LLM功能。

Result: 在5个LLM和6个数据集上优于15种商业和学术防御方法，对未见攻击具有可迁移性且能抵抗自适应对抗。

Conclusion: Rennervate能有效防御IPI攻击，建立了细粒度IPI数据集FIPI开源以支持进一步研究。

Abstract: Large Language Models (LLMs) have been integrated into many applications (e.g., web agents) to perform more sophisticated tasks. However, LLM-empowered applications are vulnerable to Indirect Prompt Injection (IPI) attacks, where instructions are injected via untrustworthy external data sources. This paper presents Rennervate, a defense framework to detect and prevent IPI attacks. Rennervate leverages attention features to detect the covert injection at a fine-grained token level, enabling precise sanitization that neutralizes IPI attacks while maintaining LLM functionalities. Specifically, the token-level detector is materialized with a 2-step attentive pooling mechanism, which aggregates attention heads and response tokens for IPI detection and sanitization. Moreover, we establish a fine-grained IPI dataset, FIPI, to be open-sourced to support further research. Extensive experiments verify that Rennervate outperforms 15 commercial and academic IPI defense methods, achieving high precision on 5 LLMs and 6 datasets. We also demonstrate that Rennervate is transferable to unseen attacks and robust against adaptive adversaries.

</details>


### [35] [LLM-based Vulnerable Code Augmentation: Generate or Refactor?](https://arxiv.org/abs/2512.08493)
*Dyna Soumhane Ouchebara,Stéphane Dupont*

Main category: cs.CR

TL;DR: 本文研究了基于LLM的数据增强方法来解决漏洞代码库中的类别不平衡问题，通过对比生成新漏洞样本和重构现有样本两种策略，发现混合方法能最有效地提升漏洞分类器性能。


<details>
  <summary>Details</summary>
Motivation: 漏洞代码库存在严重的类别不平衡问题，这限制了基于深度学习的漏洞分类器的效果。数据增强可以通过缓解代表性不足的CWE类别样本来解决这一问题。

Method: 使用Qwen2.5-Coder生成增强数据，采用两种策略：控制生成新的漏洞样本和对现有样本进行语义保留的重构。在SVEN数据集上使用CodeBERT作为漏洞分类器进行评估。

Result: 研究发现这两种方法都能通过简单流程和合理质量有效丰富漏洞代码库，混合策略最能提升漏洞分类器的性能。

Conclusion: LLM-based的数据增强是解决漏洞检测中类别不平衡问题的有效方法，特别是在采用混合增强策略时能取得最佳效果。

Abstract: Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented CWEs. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a vulnerability classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance.

</details>


### [36] [Democratizing ML for Enterprise Security: A Self-Sustained Attack Detection Framework](https://arxiv.org/abs/2512.08802)
*Sadegh Momeni,Ge Zhang,Birkett Huber,Hamza Harkous,Sam Lipton,Benoit Seguin,Yanis Pavlidis*

Main category: cs.CR

TL;DR: 本文提出了一种新颖的两阶段混合框架，结合了规则和机器学习，用于威胁检测。第一阶段使用宽松的YARA规则进行粗粒度过滤，第二阶段使用ML分类器减少误报。该系统采用合成数据生成和主动学习，以解决数据稀缺和模型退化问题。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在安全领域有所进展，但由于资源密集性和技能差距，基于规则的检测仍在安全运营中心中占主导地位。传统规则方法效率高但灵活性差，导致高误报或漏报，且需要持续手动维护。

Method: 提出了一个两阶段混合框架：第一阶段使用故意宽松的YARA规则进行高召回率的粗粒度过滤；第二阶段应用机器学习分类器过滤第一阶段的误报。利用Simula框架生成合成数据以克服数据稀缺，并通过持续反馈循环自适应调整ML模型。

Result: 该模型在生产环境中经过长时间严格测试，覆盖数万台系统。每日处理约2500亿原始日志事件，通过过滤和ML推理大幅减少至少量每日工单供人工调查。长期实验显示，由于主动学习特性，模型精度随时间逐步提升。

Conclusion: 该方法提供了一个自持、低开销、低维护的解决方案，使安全专业人员能够作为专家'教师'指导模型学习，有效 democratize 基于ML的威胁检测。

Abstract: Despite advancements in machine learning for security, rule-based detection remains prevalent in Security Operations Centers due to the resource intensiveness and skill gap associated with ML solutions. While traditional rule-based methods offer efficiency, their rigidity leads to high false positives or negatives and requires continuous manual maintenance. This paper proposes a novel, two-stage hybrid framework to democratize ML-based threat detection. The first stage employs intentionally loose YARA rules for coarse-grained filtering, optimized for high recall. The second stage utilizes an ML classifier to filter out false positives from the first stage's output. To overcome data scarcity, the system leverages Simula, a seedless synthetic data generation framework, enabling security analysts to create high-quality training datasets without extensive data science expertise or pre-labeled examples. A continuous feedback loop incorporates real-time investigation results to adaptively tune the ML model, preventing rule degradation.
  This proposed model with active learning has been rigorously tested for a prolonged time in a production environment spanning tens of thousands of systems. The system handles initial raw log volumes often reaching 250 billion events per day, significantly reducing them through filtering and ML inference to a handful of daily tickets for human investigation. Live experiments over an extended timeline demonstrate a general improvement in the model's precision over time due to the active learning feature. This approach offers a self-sustained, low-overhead, and low-maintenance solution, allowing security professionals to guide model learning as expert ``teachers''.

</details>


### [37] [PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration](https://arxiv.org/abs/2512.08809)
*Yi Liu,Weixiang Han,Chengjun Cai,Xingliang Yuan,Cong Wang*

Main category: cs.CR

TL;DR: PrivTune框架通过Split Learning和优化的噪声注入机制，在保护敏感数据隐私的同时保持模型调优性能，显著降低推理攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型服务的普及，用户通过上传私有数据进行模型微调时面临敏感数据泄露风险。现有基于差分隐私的方法难以平衡隐私保护和模型效用。

Method: 提出PrivTune框架，核心是通过Split Learning在底层模型注入经过优化的噪声，使每个token表征接近其n跳间接邻居，并通过调整d_χ-隐私噪声分布的参数来最小化失真。

Result: 在5个数据集上的实验表明，针对3类嵌入反演和3类属性推理攻击，PrivTune能将攻击成功率降至10%，效用损失仅3.33%，优于现有基线方法。

Conclusion: PrivTune有效解决了隐私保护与模型效用之间的平衡问题，为基于云服务的语言模型微调提供了可行的隐私保护方案。

Abstract: With the rise of large language models, service providers offer language models as a service, enabling users to fine-tune customized models via uploaded private datasets. However, this raises concerns about sensitive data leakage. Prior methods, relying on differential privacy within device-cloud collaboration frameworks, struggle to balance privacy and utility, exposing users to inference attacks or degrading fine-tuning performance. To address this, we propose PrivTune, an efficient and privacy-preserving fine-tuning framework via Split Learning (SL). The key idea of PrivTune is to inject crafted noise into token representations from the SL bottom model, making each token resemble the $n$-hop indirect neighbors. PrivTune formulates this as an optimization problem to compute the optimal noise vector, aligning with defense-utility goals. On this basis, it then adjusts the parameters (i.e., mean) of the $d_χ$-Privacy noise distribution to align with the optimization direction and scales the noise according to token importance to minimize distortion. Experiments on five datasets (covering both classification and generation tasks) against three embedding inversion and three attribute inference attacks show that, using RoBERTa on the Stanford Sentiment Treebank dataset, PrivTune reduces the attack success rate to 10% with only a 3.33% drop in utility performance, outperforming state-of-the-art baselines.

</details>


### [38] [Decentralized Trust for Space AI: Blockchain-Based Federated Learning Across Multi-Vendor LEO Satellite Networks](https://arxiv.org/abs/2512.08882)
*Mohamed Elmahallawy,Asma Jodeiri Akbarfam*

Main category: cs.CR

TL;DR: OrbitChain is a blockchain-based framework that addresses trust and efficiency issues in federated satellite learning (FSL) by offloading consensus to high-altitude platforms, ensuring transparent model update provenance, and preventing manipulated contributions.


<details>
  <summary>Details</summary>
Motivation: FSL enables collaborative model training across satellites without sharing raw data, but suffers from slow convergence and trust challenges due to intermittent connectivity and potential biased/falsified updates from cyberattacks.

Method: The proposed OrbitChain framework (i) offloads consensus to high-altitude platforms (HAPs) for computational efficiency, (ii) ensures transparent, auditable provenance of model updates from different orbits/vendors, and (iii) prevents manipulated/incomplete contributions from affecting global FSL aggregation.

Result: Simulations show OrbitChain reduces computational/communication overhead while improving privacy, security, and model accuracy. It achieves sub-second latency (0.16-0.35s) for block finalization and reduces convergence time by up to 30 hours on real satellite datasets compared to single-vendor approaches.

Conclusion: OrbitChain effectively enables trustworthy multi-vendor collaboration in LEO networks, demonstrating practical benefits for real-time federated learning in space applications.

Abstract: The rise of space AI is reshaping government and industry through applications such as disaster detection, border surveillance, and climate monitoring, powered by massive data from commercial and governmental low Earth orbit (LEO) satellites. Federated satellite learning (FSL) enables joint model training without sharing raw data, but suffers from slow convergence due to intermittent connectivity and introduces critical trust challenges--where biased or falsified updates can arise across satellite constellations, including those injected through cyberattacks on inter-satellite or satellite-ground communication links. We propose OrbitChain, a blockchain-backed framework that empowers trustworthy multi-vendor collaboration in LEO networks. OrbitChain (i) offloads consensus to high-altitude platforms (HAPs) with greater computational capacity, (ii) ensures transparent, auditable provenance of model updates from different orbits owned by different vendors, and (iii) prevents manipulated or incomplete contributions from affecting global FSL model aggregation. Extensive simulations show that OrbitChain reduces computational and communication overhead while improving privacy, security, and global model accuracy. Its permissioned proof-of-authority ledger finalizes over 1000 blocks with sub-second latency (0.16,s, 0.26,s, 0.35,s for 1-of-5, 3-of-5, and 5-of-5 quorums). Moreover, OrbitChain reduces convergence time by up to 30 hours on real satellite datasets compared to single-vendor, demonstrating its effectiveness for real-time, multi-vendor learning. Our code is available at https://github.com/wsu-cyber-security-lab-ai/OrbitChain.git

</details>


### [39] [Improved Pseudorandom Codes from Permuted Puzzles](https://arxiv.org/abs/2512.08918)
*Miranda Christ,Noah Golowich,Sam Gunn,Ankur Moitra,Daniel Wichs*

Main category: cs.CR

TL;DR: 本文提出了首个同时实现亚指数伪随机性、二进制字母表编辑鲁棒性和抗已知密钥攻击的伪随机码构造，解决了现有水印技术的多个安全缺陷。


<details>
  <summary>Details</summary>
Motivation: 解决现有伪随机码构造在准多项式时间区分攻击、常数大小字母表编辑鲁棒性以及对抗已知检测密钥攻击方面的不足，实现更强大的水印安全性。

Method: 基于新形式化的置换码猜想构建伪随机码，该猜想认为置换后的噪声码字分布是伪随机的，并通过证明其被之前用于构建双高效私有信息检索的置换谜题猜想所蕴含来提供证据支持。

Result: 成功构造了具备亚指数伪随机性安全性、二进制字母表最坏情况编辑鲁棒性以及对抗已知检测密钥攻击能力的伪随机码，为水印技术提供了更全面的安全保障。

Conclusion: 本文提出了首个同时实现亚指数伪随机性安全性、二进制字母表上最坏情况编辑鲁棒性以及对抗已知检测密钥攻击的伪随机码构造，为AI生成内容水印提供了更强的安全保障。

Abstract: Watermarks are an essential tool for identifying AI-generated content. Recently, Christ and Gunn (CRYPTO '24) introduced pseudorandom error-correcting codes (PRCs), which are equivalent to watermarks with strong robustness and quality guarantees. A PRC is a pseudorandom encryption scheme whose decryption algorithm tolerates a high rate of errors. Pseudorandomness ensures quality preservation of the watermark, and error tolerance of decryption translates to the watermark's ability to withstand modification of the content.
  In the short time since the introduction of PRCs, several works (NeurIPS '24, RANDOM '25, STOC '25) have proposed new constructions. Curiously, all of these constructions are vulnerable to quasipolynomial-time distinguishing attacks. Furthermore, all lack robustness to edits over a constant-sized alphabet, which is necessary for a meaningfully robust LLM watermark. Lastly, they lack robustness to adversaries who know the watermarking detection key. Until now, it was not clear whether any of these properties was achievable individually, let alone together.
  We construct pseudorandom codes that achieve all of the above: plausible subexponential pseudorandomness security, robustness to worst-case edits over a binary alphabet, and robustness against even computationally unbounded adversaries that have the detection key. Pseudorandomness rests on a new assumption that we formalize, the permuted codes conjecture, which states that a distribution of permuted noisy codewords is pseudorandom. We show that this conjecture is implied by the permuted puzzles conjecture used previously to construct doubly efficient private information retrieval. To give further evidence, we show that the conjecture holds against a broad class of simple distinguishers, including read-once branching programs.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [40] [Broadband Thermoelectric Energy Harvesting for Wearable Biosensors Using Plasmonic Field-Enhancement and Machine-Learning-Guided Device Optimization](https://arxiv.org/abs/2512.08103)
*Hamidreza Moradi,Melika Filvantorkaman*

Main category: cs.CE

TL;DR: 机器学习优化的热电偶-等离子体混合能量采集器，可将柔性可穿戴设备的功率密度提高4-6倍


<details>
  <summary>Details</summary>
Motivation: 传统皮肤贴附热电发电机受限於实际环境中较小的温差，无法满足可穿戴生物传感器对持续无电池电源的需求

Method: 结合宽带等离子体超表面吸收红外辐射产生局部加热，通过机器学习多物理场优化器件几何结构

Result: 将有效温差从3-4°C提升至约13°C，室内红外辐射下功率密度达0.15 mW/cm²

Conclusion: 该混合框架为自主长期可穿戴生理监测提供了高效紧凑的能量采集解决方案

Abstract: Wearable biosensors increasingly require continuous and battery-free power sources, but conventional skin-mounted thermoelectric generators are limited by the small temperature differences available in real environments. This work introduces a hybrid thermoplasmonic and thermoelectric energy harvester that combines multiband plasmonic absorption with machine-learning-guided optimization to improve on-body energy conversion. A broadband metasurface made of cross-bowtie nanoantennas is designed to absorb infrared radiation across the 2 to 12 micron range, capturing human body emission, ambient infrared radiation, and near-infrared sunlight. Electromagnetic simulations show strong field enhancement in nanoscale antenna gaps, producing localized thermoplasmonic heating directly above flexible Bi2Te3 thermoelectric junctions. Coupled optical, thermal, and electrical modeling indicates that this localized heating increases the effective temperature difference from the typical 3 to 4 degrees C of standard wearable thermoelectric generators to approximately 13 degrees C. This results in a power density of about 0.15 mW per cm^2 under indoor-relevant infrared flux, representing a four- to six-fold improvement over existing flexible devices. A machine-learning surrogate model trained on multiphysics data predicts temperature rise and electrical output with high accuracy (R2 greater than 0.92) and identifies optimal device geometries through Pareto-front analysis. The proposed hybrid thermoplasmonic, thermoelectric, and machine-learning framework provides a scalable route toward more efficient, compact, and flexible energy harvesters for autonomous and long-term wearable physiological monitoring.

</details>


### [41] [Dflow-SUR: Enhancing Generative Aerodynamic Inverse Design using Differentiation Throughout Flow Matching](https://arxiv.org/abs/2512.08336)
*Aobo Yang,Zhen Wei,Rhea Liem,Pascal Fua*

Main category: cs.CE

TL;DR: 论文提出了Dflow-SUR方法解决生成式反设计中物理损失优化的异步问题，显著提升了精度和效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于能量的生成式反设计方法存在物理损失优化与流匹配推理过程异步的问题，限制了设计可靠性和准确性。

Method: 引入Dflow-SUR策略，将物理损失优化与流匹配推理分离，实现更有效的微分优化。

Result: 在翼型案例中物理损失降低4个数量级，计算时间减少74%；机翼设计中升阻比均值提升11.8%，并具备更好的控制性、不确定性和鲁棒性。

Conclusion: Dflow-SUR是一个高可扩展性和高保真度的生成式气动设计框架，具有重要应用前景。

Abstract: Generative inverse design requires incorporating physical constraints to ensure that generated designs are both reliable and accurate. However, we observe that current state-of-the-art energy-based methods suffer from an asynchronous phenomenon, where the optimization of the physical loss is constrained by the flow matching inference process. To overcome this limitation, we introduce Dflow-SUR, a differentiation strategy that separates the optimization of the physical loss from the flow matching inference.
  Compared to the most advanced energy-based baseline, Dflow-SUR achieves a reduction in physical loss by four orders of magnitude, while also cutting wall-clock time by 74% on the airfoil case. Additionally, it increases the mean lift-to-drag ratio by 11.8% over traditional Latin-hypercube sampling in wing design. Beyond improvements in accuracy and efficiency, Dflow-SUR offers three additional practical advantages: (i) enhanced control over guidance, (ii) lower surrogate uncertainty, and (iii) greater robustness to hyper-parameter tuning.
  Together, these results demonstrate that Dflow-SUR is a highly promising framework, providing both scalability and high fidelity for generative aerodynamic design.

</details>


### [42] [Mechanical behaviour of brain-skull interface (meninges) under shear loading through experiment and finite element modelling: Preliminary results](https://arxiv.org/abs/2512.08425)
*Sajjad Arzemanzadeh,Karol Miller,Tim Rosenow,Sjoerd B. Vos,Adam Wittek*

Main category: cs.CE

TL;DR: 该研究开发了一种结合实验和计算建模的改进方法，用于确定脑-颅骨界面的力学特性，为改进头部计算模型的生物保真度提供了基础。


<details>
  <summary>Details</summary>
Motivation: 脑-颅骨界面（脑膜）在头部撞击过程中对脑运动起着关键作用，但由于实验数据有限，计算模型通常使用理想化的接触条件来简化这一界面。

Method: 研究采用实验测试和计算建模相结合的方法。从绵羊尸体头部提取脑组织和脑-颅骨复合样本进行剪切加载实验，使用磁共振成像获取样本的精确3D几何形状，并创建有限元网格进行模拟。脑组织采用二阶Ogden超弹性模型，脑-颅骨界面采用粘聚层模型。

Result: 结果表明粘聚层能够捕捉脑-颅骨界面的力-位移和损伤起始特性。校准的粘聚特性在不同样本间表现出一致的模式，最大法向牵引力范围为2.8-3.4 kPa，最大切向牵引力范围为1.8-2.1 kPa。

Conclusion: 该研究通过实验测试和计算模拟相结合的方法，确定了脑-颅骨界面在剪切载荷下的力学特性，并提出了一种改进的建模框架，可用于提高计算头部模型的生物保真度。

Abstract: The brain-skull interface (meninges) plays a critical role in governing brain motion during head impacts, yet computational models often simplify this interface using idealized contact conditions due to limited experimental data. This study presents an improved protocol combining experimental testing and computational modelling to determine the mechanical properties of the brain-skull interface under shear loading. Brain tissue and brain-skull complex samples were extracted from sheep cadaver heads and subjected to shear loading. Magnetic resonance imaging (MRI) was used to obtain accurate 3D geometries of the samples, which were then used to create computational grids (meshes) for simulation of the experiments using finite element (FE) models to determine subject-specific properties of the brain tissue and brain-skull interface. A second-order Ogden hyperelastic model was used for the brain tissue, and a cohesive layer was employed to model the brain-skull interface. Our results indicate that a cohesive layer captures the force-displacement and damage initiation of the brain-skull interface. The calibrated cohesive properties showed consistent patterns across samples, with maximum normal tractions ranging from 2.8-3.4 kPa and maximum tangential tractions from 1.8-2.1 kPa. This framework provides a foundation for improving the biofidelity of computational head models used in injury prediction and neurosurgical planning by replacing arbitrary boundary conditions with formulations derived from experimental data on brain-skull interface (meninges) biomechanical behaviour.

</details>


### [43] [Financial News Summarization: Can extractive methods still offer a true alternative to LLMs?](https://arxiv.org/abs/2512.08764)
*Nicolas Reche,Elvys Linhares-Pontes,Juan-Manuel Torres-Moreno*

Main category: cs.CE

TL;DR: 本研究评估了从简单抽取方法到大型语言模型的金融新闻摘要方法，发现在FinLLMs数据集上FT-Mistral-7B模型获得了最佳ROUGE分数，但需谨慎解读结果


<details>
  <summary>Details</summary>
Motivation: 金融市场变化迅速，每日有超过5万篇金融文章发布，投资者需要简洁的摘要来快速决策，因此自动化摘要技术十分必要

Method: 使用FinLLMs Challenge数据集比较了抽取式摘要方法和基于LLM的摘要方法

Result: LLM生成的摘要更连贯和信息丰富但资源密集且易产生幻觉错误；抽取方法在结构良好的短文本上表现高效；FT-Mistral-7B模型获得最佳ROUGE结果

Conclusion: LLM在金融摘要中表现出潜力但需注意幻觉问题，抽取方法对结构化文本是高效替代；数据集可靠性有限需谨慎解读结果

Abstract: Financial markets change rapidly due to news, economic shifts, and geopolitical events. Quick reactions are vital for investors to avoid losses or capture short-term gains. As a result, concise financial news summaries are critical for decision-making. With over 50,000 financial articles published daily, automation in summarization is necessary. This study evaluates a range of summarization methods, from simple extractive techniques to advanced large language models (LLMs), using the FinLLMs Challenge dataset. LLMs generated more coherent and informative summaries, but they are resource-intensive and prone to hallucinations, which can introduce significant errors into financial summaries. In contrast, extractive methods perform well on short, well-structured texts and offer a more efficient alternative for this type of article. The best ROUGE results come from fine-tuned LLM model like FT-Mistral-7B, although our data corpus has limited reliability, which calls for cautious interpretation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [44] [CFD-copilot: leveraging domain-adapted large language model and model context protocol to enhance simulation automation](https://arxiv.org/abs/2512.07917)
*Zhehao Dong,Shanghai Du,Zhen Lu,Yue Yang*

Main category: cs.SE

TL;DR: CFD-copilot是一个专用于CFD仿真的LLM框架，能够通过自然语言实现从设置到后处理的端到端CFD工作流自动化。


<details>
  <summary>Details</summary>
Motivation: CFD仿真配置需要丰富的物理建模和数值方法专业知识，这为非专业人士设置了障碍。当前LLM在完整的端到端CFD工作流应用中仍面临挑战。

Method: 采用微调LLM直接将用户描述转换为可执行CFD设置，通过多智能体系统集成仿真执行、自动错误校正和结果分析，后处理使用模型上下文协议实现LLM推理与外部工具的分离。

Result: 在NACA 0012翼型和30P-30N三元素翼型基准测试中进行了评估，结果表明领域特定适应和MCP的结合提高了LLM驱动工程工作流的可靠性和效率。

Conclusion: 特定领域适应和模型上下文协议的引入共同增强了LLM驱动工程工作流的自动化能力，为非专家用户提供了更易用的CFD仿真解决方案。

Abstract: Configuring computational fluid dynamics (CFD) simulations requires significant expertise in physics modeling and numerical methods, posing a barrier to non-specialists. Although automating scientific tasks with large language models (LLMs) has attracted attention, applying them to the complete, end-to-end CFD workflow remains a challenge due to its stringent domain-specific requirements. We introduce CFD-copilot, a domain-specialized LLM framework designed to facilitate natural language-driven CFD simulation from setup to post-processing. The framework employs a fine-tuned LLM to directly translate user descriptions into executable CFD setups. A multi-agent system integrates the LLM with simulation execution, automatic error correction, and result analysis. For post-processing, the framework utilizes the model context protocol (MCP), an open standard that decouples LLM reasoning from external tool execution. This modular design allows the LLM to interact with numerous specialized post-processing functions through a unified and scalable interface, improving the automation of data extraction and analysis. The framework was evaluated on benchmarks including the NACA~0012 airfoil and the three-element 30P-30N airfoil. The results indicate that domain-specific adaptation and the incorporation of the MCP jointly enhance the reliability and efficiency of LLM-driven engineering workflows.

</details>


### [45] [An Empirical Framework for Evaluating Semantic Preservation Using Hugging Face](https://arxiv.org/abs/2512.07983)
*Nan Jia,Anita Raja,Raffi Khatchadourian*

Main category: cs.SE

TL;DR: 该论文提出了一个评估机器学习系统中语义保持性的实证框架，通过分析HuggingFace上的模型演化数据来检测语义漂移


<details>
  <summary>Details</summary>
Motivation: 随着机器学习成为高自治系统的组成部分，确保学习赋能软件系统的可信性至关重要。但ML的非确定性和运行时定义语义使得传统软件重构变得复杂

Method: 通过从HuggingFace挖掘模型演化数据，提取提交历史、模型卡和性能指标，并在三个领域进行案例研究，追踪跨版本性能变化

Result: 建立了大规模ML模型演化数据集（170万个条目），开发了语义保持性评估流程（536个模型，4000+指标），并通过案例研究揭示了语义漂移检测方法

Conclusion: 该框架为定义社区接受的语义保持性边界提供了基础，推动了更可维护和可信的ML系统的发展

Abstract: As machine learning (ML) becomes an integral part of high-autonomy systems, it is critical to ensure the trustworthiness of learning-enabled software systems (LESS). Yet, the nondeterministic and run-time-defined semantics of ML complicate traditional software refactoring. We define semantic preservation in LESS as the property that optimizations of intelligent components do not alter the system's overall functional behavior. This paper introduces an empirical framework to evaluate semantic preservation in LESS by mining model evolution data from HuggingFace. We extract commit histories, $\textit{Model Cards}$, and performance metrics from a large number of models. To establish baselines, we conducted case studies in three domains, tracing performance changes across versions. Our analysis demonstrates how $\textit{semantic drift}$ can be detected via evaluation metrics across commits and reveals common refactoring patterns based on commit message analysis. Although API constraints limited the possibility of estimating a full-scale threshold, our pipeline offers a foundation for defining community-accepted boundaries for semantic preservation. Our contributions include: (1) a large-scale dataset of ML model evolution, curated from 1.7 million Hugging Face entries via a reproducible pipeline using the native HF hub API, (2) a practical pipeline for the evaluation of semantic preservation for a subset of 536 models and 4000+ metrics and (3) empirical case studies illustrating semantic drift in practice. Together, these contributions advance the foundations for more maintainable and trustworthy ML systems.

</details>


### [46] [A Gray Literature Study on Fairness Requirements in AI-enabled Software Engineering](https://arxiv.org/abs/2512.07990)
*Thanh Nguyen,Chaima Boufaied,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 论文回顾灰色文献，分析AI系统中公平性要求的定义、管理和违反后果，强调需要将公平性融入AI软件的框架


<details>
  <summary>Details</summary>
Motivation: 当前AI应用过度关注模型有效性而忽视公平性，需要系统研究AI系统中的公平性要求

Method: 通过灰色文献综述方法，分析公平性要求在不同应用领域的定义、在软件开发生命周期中的管理实践

Result: 揭示了公平性要求的多样性定义，识别了公平性违反的主要原因（数据偏见、算法设计等）和后果（社会危害、信任损失等）

Conclusion: 需要建立一致的框架和实践，将公平性要求与有效性同等重视地集成到AI系统中

Abstract: Today, with the growing obsession with applying Artificial Intelligence (AI), particularly Machine Learning (ML), to software across various contexts, much of the focus has been on the effectiveness of AI models, often measured through common metrics such as F1- score, while fairness receives relatively little attention. This paper presents a review of existing gray literature, examining fairness requirements in AI context, with a focus on how they are defined across various application domains, managed throughout the Software Development Life Cycle (SDLC), and the causes, as well as the corresponding consequences of their violation by AI models. Our gray literature investigation shows various definitions of fairness requirements in AI systems, commonly emphasizing non-discrimination and equal treatment across different demographic and social attributes. Fairness requirement management practices vary across the SDLC, particularly in model training and bias mitigation, fairness monitoring and evaluation, and data handling practices. Fairness requirement violations are frequently linked, but not limited, to data representation bias, algorithmic and model design bias, human judgment, and evaluation and transparency gaps. The corresponding consequences include harm in a broad sense, encompassing specific professional and societal impacts as key examples, stereotype reinforcement, data and privacy risks, and loss of trust and legitimacy in AI-supported decisions. These findings emphasize the need for consistent frameworks and practices to integrate fairness into AI software, paying as much attention to fairness as to effectiveness.

</details>


### [47] [What Pulls the Strings? Understanding the Characteristics and Role of Argumentation in Open-Source Software Usability Discussions](https://arxiv.org/abs/2512.08032)
*Arghavan Sanei,Chaima Amiri,Atefeh Shokrizadeh,Jinghui Cheng*

Main category: cs.SE

TL;DR: 分析了开源软件可用性讨论中的论证特征和质量，发现尽管讨论以论证为主，但质量参差不齐，且评论质量低于帖子质量，影响了社区集体智慧的形成。


<details>
  <summary>Details</summary>
Motivation: 开源软件的可用性常被技术复杂性所忽视，而论证在可用性讨论中是各方表达意见和说服他人的关键手段，但目前对这些讨论中的论证特征和质量缺乏了解，导致难以有效支持参与者。

Method: 通过对五个开源软件项目中的论证话语和质量进行全面分析，探讨了可用性讨论的特点。

Result: 可用性讨论主要以论证驱动，但质量不一；问题评论的论证质量低于问题帖子，表明开源社区在可用性方面的集体智慧不足；论证话语和质量对参与者后续行为有不同程度的影响。

Conclusion: 研究为开源软件利益相关者提供了构建更有效论证的见解，有助于最终提升开源软件的可用性，这些见解也可为其他分布式协作社区的研究提供参考。

Abstract: The usability of open-source software (OSS) is important but frequently overlooked in favor of technical and functional complexity. Argumentation can be a pivotal device for diverse stakeholders in OSS usability discussions to express opinions and persuade others. However, the characteristics of argument discourse in those discussions remain unknown, resulting in difficulties in providing effective support for discussion participants. We address this through a comprehensive analysis of argument discourse and quality in five OSS projects. Our results indicated that usability discussions are predominantly argument-driven, although their qualities vary. Issue comments exhibit lower-quality arguments than the issue posts, suggesting a shortage of collective intelligence about usability in OSS communities. Moreover, argument discourse and quality have various impacts on the subsequent behavior of participants. Overall, this research offers insights to help OSS stakeholders build more effective arguments and eventually improve OSS usability. These insights can also inform studies about other distributed collaborative communities.

</details>


### [48] [Secure or Suspect? Investigating Package Hallucinations of Shell Command in Original and Quantized LLMs](https://arxiv.org/abs/2512.08213)
*Md Nazmul Haque,Elizabeth Lin,Lawrence Arkoh,Biruk Tadesse,Bowen Xu*

Main category: cs.SE

TL;DR: 对量化如何影响LLM生成的Go包中包幻觉和漏洞风险的首个系统性实证研究


<details>
  <summary>Details</summary>
Motivation: LLM代码生成存在包幻觉和安全隐患，量化技术广泛应用但对安全影响未知

Method: 评估5个Qwen模型在FP32、8位和4位量化下在三个数据集(SO、MBPP、paraphrase)的表现

Result: 量化显著增加包幻觉率，4位模型退化最严重；精度降低时漏洞存在率上升

Conclusion: 量化LLM部署存在可靠性和安全隐患，需谨慎使用

Abstract: Large Language Models for code (LLMs4Code) are increasingly used to generate software artifacts, including library and package recommendations in languages such as Go. However, recent evidence shows that LLMs frequently hallucinate package names or generate dependencies containing known security vulnerabilities, posing significant risks to developers and downstream software supply chains. At the same time, quantization has become a widely adopted technique to reduce inference cost and enable deployment of LLMs on resource-constrained environments. Despite its popularity, little is known about how quantization affects the correctness and security of LLM-generated software dependencies while generating shell commands for package installation.
  In this work, we conduct the first systematic empirical study of the impact of quantization on package hallucination and vulnerability risks in LLM-generated Go packages. We evaluate five Qwen model sizes under full-precision, 8-bit, and 4-bit quantization across three datasets (SO, MBPP, and paraphrase). Our results show that quantization substantially increases the package hallucination rate (PHR), with 4-bit models exhibiting the most severe degradation. We further find that even among the correctly generated packages, the vulnerability presence rate (VPR) rises as precision decreases, indicating elevated security risk in lower-precision models. Finally, our analysis of hallucinated outputs reveals that most fabricated packages resemble realistic URL-based Go module paths, such as most commonly malformed or non-existent GitHub and golang.org repositories, highlighting a systematic pattern in how LLMs hallucinate dependencies. Overall, our findings provide actionable insights into the reliability and security implications of deploying quantized LLMs for code generation and dependency recommendation.

</details>


### [49] [Migrating QAOA from Qiskit 1.x to 2.x: An experience report](https://arxiv.org/abs/2512.08245)
*Julien Cardinal,Imen Benzarti,Ghizlane El boussaidi,Christophe Pere*

Main category: cs.SE

TL;DR: 量子算法在不同框架间迁移时因隐藏参数（如采样次数）差异导致结果不可重现。研究发现将QAOA从Qiskit 1.x迁移到2.x时，默认采样次数从无限变为1万次是结果差异的根源。


<details>
  <summary>Details</summary>
Motivation: 量子软件框架迭代升级时，算法迁移可能因底层参数差异导致结果不可重现，需系统性分析量子-经典交互层面对混合算法性能的影响。

Method: 通过对比Qiskit 1.x与2.x版本运行相同QAOA算法（电路、优化器、哈密顿量均一致），采用控制变量法分析性能差异的根源。

Result: 默认采样次数差异是关键：Qiskit 1.x隐式使用无限采样获得稠密概率分布，而2.x默认1万次采样仅覆盖23%状态空间。将采样次数提升至25万次后可恢复原有精度。

Conclusion: 框架迁移时需显式控制量子-经典交互参数（如采样次数），建议开发者和框架设计者制定明确参数规范以确保量子软件的可重现性。

Abstract: Migrating quantum algorithms across evolving frameworks introduces subtle behavioral changes that affect accuracy and reproducibility. This paper reports our experience converting the Quantum Approximate Optimization Algorithm (QAOA) from Qiskit Algorithms with Qiskit 1.x (v1 primitives) to a custom implementation using Qiskit 2.x (v2 primitives). Despite identical circuits, optimizers, and Hamiltonians, the new version produced drastically different results. A systematic analysis revealed the root cause: the sampling budget -- the number of circuit executions (shots) per iteration. The library's implicit use of unlimited shots yielded dense probability distributions, whereas the v2 default of 10 000 shots captured only 23% of the state space. Increasing shots to 250 000 restored library-level accuracy. This study highlights how hidden parameters at the quantum--classical interaction level can dominate hybrid algorithm performance and provides actionable recommendations for developers and framework designers to ensure reproducible results in quantum software migration.

</details>


### [50] [Token Sugar: Making Source Code Sweeter for LLMs through Token-Efficient Shorthand](https://arxiv.org/abs/2512.08266)
*Zhensu Sun,Chengran Yang,Xiaoning Du,Zhou Yang,Li Li,David Lo*

Main category: cs.SE

TL;DR: Token Sugar 通过将高频冗长代码模式替换为可逆的简写形式，有效减少 LLM 代码生成中的 token 使用量，在保持性能的同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成和理解任务中表现出色，但编程语言的冗余性（如不必要的格式元素和冗长的样板代码）导致 token 数量膨胀，增加了推理成本并减慢了生成过程。现有方法仅限于语法转换，未充分利用语义层面的 token 减少潜力。

Method: 设计了一个系统化解决方案：从代码库中挖掘高频、token 密集的模式，为每个模式创建唯一的简写，并通过代码转换将简写集成到 LLM 预训练中。最终获得了 799 对（代码模式，简写）对。

Result: 实验结果显示，使用 Token Sugar 增强数据训练的 LLM 在生成过程中实现了显著的 token 节省（最高 11.2% 的减少），同时保持与未处理代码训练的基线模型几乎相同的 Pass@1 分数。

Conclusion: Token Sugar 是一种有效的代码模式简化方法，通过语义层面的模式替换显著减少了 LLM 的 token 使用量（最高 15.1%），同时保持了代码生成质量，为降低 LLM 计算成本提供了新思路。

Abstract: Large language models (LLMs) have shown exceptional performance in code generation and understanding tasks, yet their high computational costs hinder broader adoption. One important factor is the inherent verbosity of programming languages, such as unnecessary formatting elements and lengthy boilerplate code. This leads to inflated token counts in both input and generated outputs, which increases inference costs and slows down the generation process. Prior work improves this through simplifying programming language grammar, reducing token usage across both code understanding and generation tasks. However, it is confined to syntactic transformations, leaving significant opportunities for token reduction unrealized at the semantic level.
  In this work, we propose Token Sugar, a concept that replaces frequent and verbose code patterns with reversible, token-efficient shorthand in the source code. To realize this concept in practice, we designed a systematic solution that mines high-frequency, token-heavy patterns from a code corpus, maps each to a unique shorthand, and integrates them into LLM pretraining via code transformation. With this solution, we obtain 799 (code pattern, shorthand) pairs, which can reduce up to 15.1% token count in the source code and is complementary to existing syntax-focused methods. We further trained three widely used LLMs on Token Sugar-augmented data. Experimental results show that these models not only achieve significant token savings (up to 11.2% reduction) during generation but also maintain near-identical Pass@1 scores compared to baselines trained on unprocessed code.

</details>


### [51] [FedLAD: A Modular and Adaptive Testbed for Federated Log Anomaly Detection](https://arxiv.org/abs/2512.08277)
*Yihan Liao,Jacky Keung,Zhenyu Mao,Jingyu Zhang,Jialong Li*

Main category: cs.SE

TL;DR: FedLAD is a unified federated learning platform designed specifically for log-based anomaly detection, addressing privacy and decentralization challenges while supporting plug-and-play model integration and self-adaptive features.


<details>
  <summary>Details</summary>
Motivation: Existing log-based anomaly detection approaches assume centralized training, which is impractical due to privacy constraints and the decentralized nature of system logs. There is a lack of dedicated testbeds tailored to LAD in federated settings.

Method: FedLAD is a unified platform that supports plug-and-play integration of diverse LAD models, benchmark datasets, and aggregation strategies, with runtime support for validation logging, parameter tuning, and adaptive strategy control.

Result: FedLAD enables reproducible and scalable experimentation for training and evaluating LAD models under federated learning constraints.

Conclusion: FedLAD provides a solid foundation for future research by bridging the gap between federated learning frameworks and log-based anomaly detection requirements.

Abstract: Log-based anomaly detection (LAD) is critical for ensuring the reliability of large-scale distributed systems. However, most existing LAD approaches assume centralized training, which is often impractical due to privacy constraints and the decentralized nature of system logs. While federated learning (FL) offers a promising alternative, there is a lack of dedicated testbeds tailored to the needs of LAD in federated settings. To address this, we present FedLAD, a unified platform for training and evaluating LAD models under FL constraints. FedLAD supports plug-and-play integration of diverse LAD models, benchmark datasets, and aggregation strategies, while offering runtime support for validation logging (self-monitoring), parameter tuning (self-configuration), and adaptive strategy control (self-adaptation). By enabling reproducible and scalable experimentation, FedLAD bridges the gap between FL frameworks and LAD requirements, providing a solid foundation for future research. Project code is publicly available at: https://github.com/AA-cityu/FedLAD.

</details>


### [52] [Measuring Agile Agreement: Development and Validation of the Manifesto and Principle Scales](https://arxiv.org/abs/2512.08461)
*Nicolas Matton,Anthony Simonofski,Marie-Ange Remiche,Benoît Vanderose*

Main category: cs.SE

TL;DR: 本文设计了两种不同的量表(MAS和PAS)来区分敏捷宣言价值观与具体实践间的认同差异，并通过验证证明它们测量的是敏捷认同的不同维度


<details>
  <summary>Details</summary>
Motivation: 现有研究未能区分对敏捷宣言抽象价值观的认同与对12条原则具体实践的认同，存在方法论空白

Method: 通过系统的条目创建与选择、调查设计和验证过程，开发了宣言认同量表(MAS)和原则认同量表(PAS)

Result: 两个量表均具有良好的内部一致性和结构效度，相关性分析表明它们测量的是敏捷认同的不同维度

Conclusion: 研究提供了公开可用的测量工具，为更精细地测量敏捷认同迈出了关键一步，有助于区分不同认知层面的敏捷认同

Abstract: While the importance of human factors in agile software development is widely acknowledged, the measurement of an individual's "agile agreement" remains an ill-defined and challenging area. A key limitation in existing research is the failure to distinguish between agreement with the abstract, high-level values of the Agile Manifesto and agreement with the concrete, day-to-day practices derived from the 12 Principles. This paper addresses this methodological gap by presenting the design and validation of two distinct instruments: the novel Manifesto Agreement Scale (MAS), and the Principle Agreement Scale (PAS), which is a systematic adaptation and refinement of a prior instrument.
  We detail the systematic process of item creation and selection, survey design, and validation. The results demonstrate that both scales possess important internal consistency and construct validity. A convergence and divergence analysis, including Proportional Odds Logistic Regression, a Bland-Altman plot, and an Intraclass Correlation Coefficient (ICC), reveals that while the two scales are moderately correlated, they are not interchangeable and capture distinct dimensions of agile agreement. The primary contribution of this work is a pair of publicly available instruments, validated within a specific demographic of Belgian IT professionals. These scales represent a critical initial step toward facilitating a more nuanced measurement of agile agreement, distinguishing agile agreement across various levels of perception and aiding in a more refined interpretation of person-agile fit.

</details>


### [53] [Measuring Computer Science Enthusiasm: A Questionnaire-Based Analysis of Age and Gender Effects on Students' Interest](https://arxiv.org/abs/2512.08472)
*Kai Marquardt,Robert Hanak,Anne Koziolek,Lucia Happe*

Main category: cs.SE

TL;DR: 分析年龄与性别对青少年计算机科学兴趣发展的影响，挑战早期接触是主要途径的观点，发现年龄比性别更重要，强调需要动态、年龄敏感的CS教育框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究普遍认为早期接触是维持CS兴趣的主要途径，但缺乏对年龄和性别因素交互作用的深入分析。本研究旨在通过POI理论解构年龄和性别对CS兴趣的不同影响。

Method: 基于POI理论开发问卷，对400多名参与在线CS课程的学生进行前测后测评估，分析年龄和性别相关的热情模式。

Result: 发现青春期早期热情明显下降（尤其是女孩），年龄比性别对兴趣发展的影响更大，年长学生虽然基线态度较低但干预后变化最大。

Conclusion: 需要动态、年龄敏感的CS教育框架，教学策略应与发展轨迹相匹配，精心设计的短期活动即使在较大年龄也能有效重新激活兴趣。

Abstract: This study offers new insights into students' interest in computer science (CS) education by disentangling the distinct effects of age and gender across a diverse adolescent sample. Grounded in the person-object theory of interest (POI), we conceptualize enthusiasm as a short-term, activating expression of interest that combines positive affect, perceived relevance, and intention to re-engage. Experiencing such enthusiasm can temporarily shift CS attitudes and strengthen future engagement intentions, making it a valuable lens for evaluating brief outreach activities. To capture these dynamics, we developed a theoretically grounded questionnaire for pre-post assessment of the enthusiasm potential of CS interventions. Using data from more than 400 students participating in online CS courses, we examined age- and gender-related patterns in enthusiasm. The findings challenge the prevailing belief that early exposure is the primary pathway to sustained interest in CS. Instead, we identify a marked decline in enthusiasm during early adolescence, particularly among girls, alongside substantial variability in interest trajectories across age groups. Crucially, our analyses reveal that age is a more decisive factor than gender in shaping interest development and uncover key developmental breakpoints. Despite starting with lower baseline attitudes, older students showed the largest positive changes following the intervention, suggesting that well-designed short activities can effectively re-activate interest even at later ages. Overall, the study highlights the need for a dynamic, age-sensitive framework for CS education in which instructional strategies are aligned with developmental trajectories.

</details>


### [54] [Gamification with Purpose: What Learners Prefer to Motivate Their Learning](https://arxiv.org/abs/2512.08551)
*Kai Marquardt,Mona Schulz,Anne Koziolek,Lucia Happe*

Main category: cs.SE

TL;DR: 本研究探讨了学习者对教育游戏中设计元素的偏好，提出以学习者为中心的游戏化策略，优先考虑支持学习过程和内在动机的元素。


<details>
  <summary>Details</summary>
Motivation: 开发目标驱动的游戏化策略，确保游戏化设计与教学目标一致，同时减少对内在动机的负面影响。

Method: 通过系统文献综述确定了十个广泛讨论的游戏设计元素，开发了视觉原型，并进行了125名参与者的最佳-最差尺度调查，结合定性反馈分析动机驱动因素。

Result: 学习者偏好直接支持学习过程的元素，如进度条、概念图、即时反馈和成就系统。定性分析揭示了六个主要的动机主题，包括可见进展、内容相关性和建设性反馈。

Conclusion: 学习者重视与教育内容有机整合并支持内在动机的游戏化元素。目标对齐的游戏化应优先考虑可视化学习进展和提供可操作反馈的工具，而非仅依赖外在激励。

Abstract: This study investigates learners' preferences for game design elements (GDEs) in educational contexts to inform the development of purpose-driven gamification strategies. It emphasizes a learner-centered approach that aligns gamification design with pedagogical goals, while mitigating risks such as the erosion of intrinsic motivation. A systematic literature review was conducted to identify ten widely discussed GDEs. Visual prototypes representing each element were developed, and a best-worst scaling (BWS) survey with 125 participants was administered to elicit preference rankings. Qualitative feedback was also collected to uncover motivational drivers. Learners consistently preferred GDEs that support learning processes directly-most notably progress bars, concept maps, immediate feedback, and achievements. Qualitative analysis revealed six recurring motivational themes, including visible progress, content relevance, and constructive feedback. The findings suggest that learners value gamification elements that are meaningfully integrated with educational content and support intrinsic motivation. Purpose-aligned gamification should prioritize tools that visualize learning progress and provide actionable feedback, rather than relying solely on extrinsic incentives.

</details>


### [55] [Reusability in MLOps: Leveraging Ports and Adapters to Build a Microservices Architecture for the Maritime Domain](https://arxiv.org/abs/2512.08657)
*Renato Cordeiro Ferreira,Aditya Dhinavahi,Rowanne Trapmann,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: 本文介绍了在构建海洋安全异常检测系统时应用端口适配器模式实现软件架构重用的经验教训


<details>
  <summary>Details</summary>
Motivation: ML赋能系统固有的复杂性要求多种组件协作，需要可重用的架构模式来支持构建多个微服务

Method: 应用六边形架构（端口适配器模式）从单一代码库构建多个微服务

Result: 成功展示了在海洋安全领域异常检测系统中应用该模式的经验和挑战

Conclusion: 六边形架构模式适用于构建ML赋能系统，能为软件工程师、机器学习工程师和数据科学家提供参考

Abstract: ML-Enabled Systems (MLES) are inherently complex since they require multiple components to achieve their business goal. This experience report showcases the software architecture reusability techniques applied while building Ocean Guard, an MLES for anomaly detection in the maritime domain. In particular, it highlights the challenges and lessons learned to reuse the Ports and Adapters pattern to support building multiple microservices from a single codebase. This experience report hopes to inspire software engineers, machine learning engineers, and data scientists to apply the Hexagonal Architecture pattern to build their MLES.

</details>


### [56] [RESTifAI: LLM-Based Workflow for Reusable REST API Testing](https://arxiv.org/abs/2512.08706)
*Leon Kogler,Maximilian Ehrhart,Benedikt Dornauer,Eduard Paul Enoiu*

Main category: cs.SE

TL;DR: RESTifAI是一个基于LLM的框架，用于生成可复用、CI/CD就绪的REST API测试，专注于有效测试场景（快乐路径）和负面案例验证。


<details>
  <summary>Details</summary>
Motivation: 现有工具主要关注内部服务器错误，缺乏对有效功能和业务规则验证的系统性测试方法。

Method: 采用LLM驱动的方法，系统构建有效测试场景，并推导负面案例来验证功能正确性和鲁棒性。

Result: RESTifAI性能与最新LLM工具相当，解决了可复用性、测试预言复杂性和集成等限制问题。

Conclusion: 该工具在工业服务中具有适用性，公开可用，为REST API测试提供了一种系统化的解决方案。

Abstract: With this paper, we introduce RESTifAI, an LLM-driven approach for generating reusable, CI/CD ready REST API tests, following the happy-path approach. Unlike existing tools that often focus primarily on internal server errors, RESTifAI systematically constructs valid test scenarios (happy paths) and derives negative cases to verify both intended functionality (2xx responses) and robustness against invalid inputs or business-rule violations (4xx responses). The results indicate that RESTifAI performs on par with the latest LLM tools, i.e., AutoRestTest and LogiAgent, while addressing limitations related to reusability, oracle complexity, and integration. To support this, we provide common comparative results and demonstrate the tool's applicability in industrial services. For tool demonstration, please refer to https://www.youtube.com/watch?v=2vtQo0T0Lo4. RESTifAI is publicly available at https://github.com/casablancahotelsoftware/RESTifAI.

</details>


### [57] [Multicalibration for LLM-based Code Generation](https://arxiv.org/abs/2512.08810)
*Viola Campos,Robin Kuschnereit,Adrian Ulges*

Main category: cs.SE

TL;DR: 该研究验证了多标定方法在代码LLM校准中的有效性，通过考虑代码复杂性等因素显著提升了置信度校准效果，并发布了包含代码生成、似然度和正确性标签的数据集。


<details>
  <summary>Details</summary>
Motivation: 随着基于AI的代码生成技术普及，需要确保代码LLM的置信度分数能够准确反映代码正确性的真实可能性，因此研究多标定方法以捕获编程问题的额外因素。

Method: 研究了四种多标定方法，在三个函数合成基准测试上使用最新一代代码LLM（Qwen3 Coder、GPT-OSS、DeepSeek-R1-Distill），通过消融实验分析代码复杂性、代码长度和编程语言等因素的影响。

Result: 多标定方法相比未校准的token似然度提升了1.03技能分数，相比基线校准方法提升了0.37技能分数，显示出显著改进。

Conclusion: 多标定方法在代码LLM校准方面显示出显著优势，能够有效提高置信度分数的可靠性，为未来代码生成模型的校准研究提供了有价值的数据集和方法论基础。

Abstract: As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.

</details>


### [58] [SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA](https://arxiv.org/abs/2512.08867)
*Jing Zhang,Lianghong Guo,Yanlin Wang,Mingwei Liu,Jiachi Chen,Yuchi Ma,Ensheng Shi,Terry Yue Zhuo,Hongyu Zhang,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文分析了软件开发中的知识问答任务重要性，并构建了SimpleDevQA基准。研究发现开发知识问答占用户对话的39.6%，现有基准存在局限性，提出改进方案并通过实验验证了代码LLM和RAG策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有开发知识问答基准主要关注代码理解，忽略了更广泛的开发知识需求，且部分基准未基于真实用户查询构建。

Method: 设计三阶段流程将真实对话转化为简单开发知识问答对，构建多语言基准SimpleDevQA，并通过实验比较不同LLM表现。

Result: 开发知识问答占比最高（39.6%）；代码LLM优于同规模通用LLM；RAG策略平均提升准确率11.3%；LLM存在系统性过度自信现象。

Conclusion: 开发知识问答是重要但未被充分探索的任务，SimpleDevQA基准能更好评估LLM能力，代码生成能力与知识问答性能正相关。

Abstract: The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.

</details>


### [59] [Exploring the Garden of Forking Paths in Empirical Software Engineering Research: A Multiverse Analysis](https://arxiv.org/abs/2512.08910)
*Nathan Cassee,Robert Feldt*

Main category: cs.SE

TL;DR: 该论文通过多元宇宙分析揭示，软件工程实证研究中分析方法的选择自由可能导致结论差异巨大，呼吁研究者进行稳健性检验并明确方法论选择理由。


<details>
  <summary>Details</summary>
Motivation: 软件工程实证研究中，研究者在数据处理、操作化和统计模型选择上有很大自由度，这种“分叉路径花园”现象可能威胁研究的稳健性和可重复性。

Method: 对一篇已发表的软件工程实证论文进行多元宇宙分析，识别出9个关键分析决策点，每个点至少有1个合理替代方案，系统性运行所有3072种分析组合。

Result: 仅有6种分析组合（<0.2%）复现了原始论文结果，绝大多数组合产生了质的不同甚至相反的发现。

Conclusion: 方法选择对研究结果的影响比通常承认的更深远，建议软件工程研究者通过稳健性检验或明确论证每个分析决策来提高研究的可靠性和可重复性。

Abstract: In empirical software engineering (SE) research, researchers have considerable freedom to decide how to process data, what operationalizations to use, and which statistical model to fit. Gelman and Loken refer to this freedom as leading to a "garden of forking paths". Although this freedom is often seen as an advantage, it also poses a threat to robustness and replicability: variations in analytical decisions, even when justifiable, can lead to divergent conclusions.
  To better understand this risk, we conducted a so-called multiverse analysis on a published empirical SE paper. The paper we picked is a Mining Software Repositories study, as MSR studies commonly use non-trivial statistical models to analyze post-hoc, observational data. In the study, we identified nine pivotal analytical decisions-each with at least one equally defensible alternative and systematically reran all the 3,072 resulting analysis pipelines on the original dataset. Interestingly, only 6 of these universes (<0.2%) reproduced the published results; the overwhelming majority produced qualitatively different, and sometimes even opposite, findings.
  This case study of a data analytical method commonly applied to empirical software engineering data reveals how methodological choices can exert a more profound influence on outcomes than is often acknowledged. We therefore advocate that SE researchers complement standard reporting with robustness checks across plausible analysis variants or, at least, explicitly justify each analytical decision. We propose a structured classification model to help classify and improve justification for methodological choices. Secondly, we show how the multiverse analysis is a practical tool in the methodological arsenal of SE researchers, one that can help produce more reliable, reproducible science.

</details>
