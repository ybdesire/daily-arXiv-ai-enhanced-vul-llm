<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 41]
- [cs.CR](#cs.CR) [Total: 17]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.CE](#cs.CE) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [ExaCraft: Dynamic Learning Context Adaptation for Personalized Educational Examples](https://arxiv.org/abs/2512.09931)
*Akaash Chatterjee,Suman Kundu*

Main category: cs.AI

TL;DR: ExaCraft是一个通过AI生成个性化学习示例的系统，能够根据学习者的动态上下文进行自适应调整。


<details>
  <summary>Details</summary>
Motivation: 现有教育AI工具缺乏对学习者理解变化、学习困难和技能发展的适应性，无法生成真正个性化的相关示例。

Method: 结合Google Gemini AI和Python Flask API，通过Chrome扩展访问，利用用户定义档案和实时学习行为分析来生成个性化示例。

Result: 开发出能够适应学习者五个关键上下文方面的系统：学习困难指标、掌握模式、主题进展历史、会话边界和学习进展信号。

Conclusion: ExaCraft的创新核心在于其动态适应能力，能够根据不同用例需求从基础概念演进到高级技术实现。

Abstract: Learning is most effective when it's connected to relevant, relatable examples that resonate with learners on a personal level. However, existing educational AI tools don't focus on generating examples or adapting to learners' changing understanding, struggles, or growing skills. We've developed ExaCraft, an AI system that generates personalized examples by adapting to the learner's dynamic context. Through the Google Gemini AI and Python Flask API, accessible via a Chrome extension, ExaCraft combines user-defined profiles (including location, education, profession, and complexity preferences) with real-time analysis of learner behavior. This ensures examples are both culturally relevant and tailored to individual learning needs. The system's core innovation is its ability to adapt to five key aspects of the learning context: indicators of struggle, mastery patterns, topic progression history, session boundaries, and learning progression signals. Our demonstration will show how ExaCraft's examples evolve from basic concepts to advanced technical implementations, responding to topic repetition, regeneration requests, and topic progression patterns in different use cases.

</details>


### [2] [Exploring Health Misinformation Detection with Multi-Agent Debate](https://arxiv.org/abs/2512.09935)
*Chih-Han Chen,Chen-Han Tsai,Yu-Shao Peng*

Main category: cs.AI

TL;DR: 提出用于健康错误信息检测的两阶段框架：一致性分数预测和多智能体辩论，结合自动评分与协作推理，在验证任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着在线健康错误信息的激增，有效验证需要高质量证据检索和严格推理过程。

Method: 采用两阶段框架：第一阶段使用大型语言模型独立评估检索到的文章并计算聚合一致性分数；当分数低于预设阈值时，进入第二阶段，多个智能体进行结构化辩论以综合冲突证据。

Result: 实验结果表明，该两阶段方法在性能上优于基线方法。

Conclusion: 该论文提出了一个两阶段框架，通过结合自动评分与协作推理，在健康信息验证任务中取得了优于基线方法的性能。

Abstract: Fact-checking health-related claims has become increasingly critical as misinformation proliferates online. Effective verification requires both the retrieval of high-quality evidence and rigorous reasoning processes. In this paper, we propose a two-stage framework for health misinformation detection: Agreement Score Prediction followed by Multi-Agent Debate. In the first stage, we employ large language models (LLMs) to independently evaluate retrieved articles and compute an aggregated agreement score that reflects the overall evidence stance. When this score indicates insufficient consensus-falling below a predefined threshold-the system proceeds to a second stage. Multiple agents engage in structured debate to synthesize conflicting evidence and generate well-reasoned verdicts with explicit justifications. Experimental results demonstrate that our two-stage approach achieves superior performance compared to baseline methods, highlighting the value of combining automated scoring with collaborative reasoning for complex verification tasks.

</details>


### [3] [Echo-CoPilot: A Multi-View, Multi-Task Agent for Echocardiography Interpretation and Reporting](https://arxiv.org/abs/2512.09944)
*Moein Heidari,Mohammad Amin Roohi,Armin Khosravi,Ilker Hacihaliloglu*

Main category: cs.AI

TL;DR: Echo-CoPilot是一个基于大语言模型的多视角、多任务超声心动图分析系统，通过协调专门化工具实现临床连贯评估


<details>
  <summary>Details</summary>
Motivation: 传统的超声心动图分析是认知密集型任务，现有的基础模型在单个感知子任务上表现出色，但缺乏统一的临床连贯评估

Method: 采用ReAct风格循环，将临床查询分解为视图识别、心脏结构分割、测量和疾病预测、报告合成等子任务

Result: 在MIMIC-EchoQA基准测试中达到50.8%的准确率，优于通用和生物医学视频视觉语言模型

Conclusion: Echo-CoPilot能够利用定量测量和生理背景解决临床决策阈值附近的疑难病例

Abstract: Echocardiography is central to contemporary cardiovascular care, but full-study interpretation remains a cognitively demanding, multi-view task that is still performed manually. While recent foundation models for echocardiography can achieve strong performance on individual perceptual subtasks such as view classification, segmentation, or disease prediction, they typically operate in isolation and do not provide a unified, clinically coherent assessment. In this work, we introduce Echo-CoPilot, a multi-view, multi-task agent that uses a large language model to orchestrate a suite of specialized echocardiography tools. Within a ReAct-style loop, the agent decomposes clinician queries, invokes tools for view recognition, cardiac structure segmentation, measurement and disease prediction, and report synthesis, and integrates their outputs into guideline-aware answers and narrative summaries. We evaluate Echo-CoPilot on the public MIMIC-EchoQA benchmark, where it achieves an accuracy of 50.8\%, outperforming both general-purpose and biomedical video vision-language models. Qualitative analyses further show that the agent leverages quantitative measurements and physiologic context to resolve challenging cases near clinical decision thresholds, such as borderline left ventricular hypertrophy or pericardial effusion severity. The code will be released upon acceptance of the paper.

</details>


### [4] [Fuzzy Hierarchical Multiplex](https://arxiv.org/abs/2512.09976)
*Alexis Kafantaris*

Main category: cs.AI

TL;DR: 提出了一种扩展FCM因果关系的模糊优化框架，用于服务流程设计中的信息传输优化


<details>
  <summary>Details</summary>
Motivation: 现有FCM因果关系模型需要扩展到更复杂的服务优化场景，特别是信息传输优化需求

Method: 利用动力学将数据映射到度量空间，建立多层概念逻辑蕴含和层次关系框架，基于白色理论进行逻辑和数学分析

Result: 构建了一个完整的模糊优化框架，能够系统地分析概念间的逻辑关系

Conclusion: 该框架为服务流程设计中的信息传输优化提供了理论基础和实用工具，并通过FHM分析的验证证明了其有效性

Abstract: A new fuzzy optimization framework that extends FCM causality is proposed. This model utilizes the dynamics to map data into metrics and create a framework that examines logical implication and hierarchy of concepts using a multiplex. Moreover, this is a white-theoretical paper introducing the framework and analyzing the logic and math behind it. Upon this extension the main objectives and the orientation of this framework is expounded and exemplified; this framework is meant for service optimization of information transmission in service process design. Lastly, a thorough analysis of the FHM is included which is done following the logical steps in a simple and elegant manner.

</details>


### [5] [Exploring LLMs for Scientific Information Extraction Using The SciEx Framework](https://arxiv.org/abs/2512.10004)
*Sha Li,Ayush Sadekar,Nathan Self,Yiqi Su,Lars Andersland,Mira Chaplin,Annabel Zhang,Hyoju Yang,James B Henderson,Krista Wigginton,Linsey Marr,T. M. Murali,Naren Ramakrishnan*

Main category: cs.AI

TL;DR: SciEx是一个模块化和可组合的框架，专门解决科学文献信息提取中的挑战，包括长文本、多模态内容和不一致细粒度信息的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型工具在处理科学文献时面临诸多挑战：长上下文文档、多模态内容，以及需要将多个出版物中各异且不一致的细粒度信息协调为标准格式。当所需的数据模式或提取本体快速变化时，这些问题更加复杂。

Method: 提出了SciEx框架，将关键组件（PDF解析、多模态检索、提取和聚合）解耦，这种设计简化了按需数据提取，同时支持新模型、提示策略和推理机制的扩展性和灵活集成。

Result: 在涵盖三个科学主题的数据集上评估了SciEx准确和一致提取细粒度信息的能力。

Conclusion: 研究结果为当前基于LLM的流程的优势和局限性提供了实用见解，SciEx框架为科学信息提取提供了更灵活和可扩展的解决方案。

Abstract: Large language models (LLMs) are increasingly touted as powerful tools for automating scientific information extraction. However, existing methods and tools often struggle with the realities of scientific literature: long-context documents, multi-modal content, and reconciling varied and inconsistent fine-grained information across multiple publications into standardized formats. These challenges are further compounded when the desired data schema or extraction ontology changes rapidly, making it difficult to re-architect or fine-tune existing systems. We present SciEx, a modular and composable framework that decouples key components including PDF parsing, multi-modal retrieval, extraction, and aggregation. This design streamlines on-demand data extraction while enabling extensibility and flexible integration of new models, prompting strategies, and reasoning mechanisms. We evaluate SciEx on datasets spanning three scientific topics for its ability to extract fine-grained information accurately and consistently. Our findings provide practical insights into both the strengths and limitations of current LLM-based pipelines.

</details>


### [6] [SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration](https://arxiv.org/abs/2512.10046)
*Yan Zhuang,Jiawei Ren,Xiaokang Ye,Jianzhi Shen,Ruixuan Zhang,Tianai Yue,Muhammad Faayez,Xuhong He,Ziqiao Ma,Lianhui Qin,Zhiting Hu,Tianmin Shu*

Main category: cs.AI

TL;DR: SWR是一个基于虚幻引擎5的城市模拟平台，支持多机器人控制和通信，并建立了两个新的机器人基准评估模型在城市场景中的能力


<details>
  <summary>Details</summary>
Motivation: 现有基础模型主要关注室内家庭场景，缺乏对大规模城市场景的评估

Method: 开发了SWR仿真平台，能够程序化生成逼真城市场景，并构建了多模态指令跟随和多智能体搜索两个基准任务

Result: 实验表明当前最先进模型在城市场景中仍缺乏鲁棒的感知、推理和规划能力

Conclusion: SWR平台填补了室外城市场景模拟的空白，为评估通用机器人能力提供了重要测试平台

Abstract: Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.

</details>


### [7] [Parallel Decoder Transformer: Model-Internal Parallel Decoding with Speculative Invariance via Note Conditioning](https://arxiv.org/abs/2512.10054)
*Logan Robbins*

Main category: cs.AI

TL;DR: PDT是一种参数高效的并行解码Transformer架构，通过在冻结预训练模型中嵌入协调机制来解决自回归解码的延迟瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的自回归解码本质上是顺序的，产生了与输出长度线性扩展的延迟瓶颈。现有的"分解与填充"方法由于缺乏跨流通信而存在连贯性漂移问题

Method: 引入并行解码Transformer（PDT）架构，使用推测性笔记调节适配器，通过共享动态潜空间实现并行解码流同步，将协调制定为推测共识问题

Result: 在50,000步课程上验证，PDT实现有效自校正，在覆盖预测中达到77.8%精确度，无需修改主干权重即可恢复近似序列语义

Conclusion: PDT为结构化并行生成提供了可扩展、高效的完整模型微调替代方案

Abstract: Autoregressive decoding in Large Language Models (LLMs) is inherently sequential, creating a latency bottleneck that scales linearly with output length. While ``Decomposition-and-Fill'' methods like Skeleton-of-Thought attempt to parallelize generation via external orchestration, they suffer from \textit{coherence drift} due to the lack of cross-stream communication. In this work, we introduce the \textbf{Parallel Decoder Transformer (PDT)}, a parameter-efficient architecture that embeds coordination primitives directly into the inference process of a frozen pre-trained model.
  Instead of retraining the base model, PDT injects lightweight \textit{Speculative Note Conditioning (SNC)} adapters that allow parallel decoding streams to synchronize via a shared, dynamic latent space. We formulate coordination as a \textit{speculative consensus} problem, where sibling streams broadcast semantic ``notes'' to a global bus, gated by a learned verification head. We validate our approach on a 50,000-step curriculum using a frozen 20B-parameter backbone. Our results demonstrate that PDT achieves effective self-correction, reaching \textbf{77.8\% precision} in coverage prediction and recovering approximate serial semantics without modifying the trunk weights. This establishes PDT as a scalable, efficient alternative to full model fine-tuning for structured parallel generation.

</details>


### [8] [Linear socio-demographic representations emerge in Large Language Models from indirect cues](https://arxiv.org/abs/2512.10065)
*Paul Bouchaud,Pedro Ramaciotti*

Main category: cs.AI

TL;DR: 研究发现LLMs通过线性表示编码用户社会人口属性，这些隐含的偏见会影响模型行为，即使通过偏见测试的模型仍可能包含隐性偏见


<details>
  <summary>Details</summary>
Motivation: 探究LLMs如何从间接线索（如姓名和职业）推断人类对话伙伴的社会人口属性

Method: 在四个基于Transformer的开源LLMs上，通过显性人口统计披露提示来探测残差流，分析姓名和职业隐含的人口统计表示

Result: LLMs在激活空间中形成了用户人口统计的线性表示，姓名激活与人口普查一致的性别和种族表示，职业触发与现实世界劳动力统计相关的表示

Conclusion: 模型隐含的人口统计表示会主动影响下游行为如职业推荐，通过偏见基准测试的模型仍可能包含并利用隐性偏见，这对大规模应用的公平性有重要影响

Abstract: We investigate how LLMs encode sociodemographic attributes of human conversational partners inferred from indirect cues such as names and occupations. We show that LLMs develop linear representations of user demographics within activation space, wherein stereotypically associated attributes are encoded along interpretable geometric directions. We first probe residual streams across layers of four open transformer-based LLMs (Magistral 24B, Qwen3 14B, GPT-OSS 20B, OLMo2-1B) prompted with explicit demographic disclosure. We show that the same probes predict demographics from implicit cues: names activate census-aligned gender and race representations, while occupations trigger representations correlated with real-world workforce statistics. These linear representations allow us to explain demographic inferences implicitly formed by LLMs during conversation. We demonstrate that these implicit demographic representations actively shape downstream behavior, such as career recommendations. Our study further highlights that models that pass bias benchmark tests may still harbor and leverage implicit biases, with implications for fairness when applied at scale.

</details>


### [9] [Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit](https://arxiv.org/abs/2512.10092)
*Nick Jiang,Xiaoqing Sun,Lisa Dunlap,Lewis Smith,Neel Nanda*

Main category: cs.AI

TL;DR: 使用稀疏自编码器(SAE)创建可解释的概念嵌入，相比LLM更经济高效，相比密集嵌入更可控，用于大规模文本分析


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的大规模文本分析方法成本高，而密集嵌入方法缺乏对感兴趣属性的控制能力

Method: 提出使用稀疏自编码器(SAE)创建SAE嵌入表示，其维度可映射到可解释概念

Result: SAE嵌入在四个分析任务中表现优于LLM和密集嵌入：成本降低2-8倍，可靠性更高，且具有可控性

Conclusion: SAE是解析模型数据的多功能工具，强调了通过数据解释模型的被忽视重要性

Abstract: Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding "trigger" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data.

</details>


### [10] [Robust AI Security and Alignment: A Sisyphean Endeavor?](https://arxiv.org/abs/2512.10100)
*Apostol Vassilev*

Main category: cs.AI

TL;DR: 该论文通过将哥德尔不完备定理扩展到人工智能领域，确立了AI安全和对齐鲁棒性的信息论限制


<details>
  <summary>Details</summary>
Motivation: 了解这些限制并为其带来的挑战做好准备对于负责任地采用AI技术至关重要

Method: 扩展哥德尔不完备定理到AI领域，建立信息论框架

Result: 证明了AI安全和对齐鲁棒性的基本限制

Conclusion: 提出的实用方法可用于应对这些挑战，研究还对AI系统认知推理能力的局限性具有更广泛的意义

Abstract: This manuscript establishes information-theoretic limitations for robustness of AI security and alignment by extending Gödel's incompleteness theorem to AI. Knowing these limitations and preparing for the challenges they bring is critically important for the responsible adoption of the AI technology. Practical approaches to dealing with these challenges are provided as well. Broader implications for cognitive reasoning limitations of AI systems are also proven.

</details>


### [11] [AgriRegion: Region-Aware Retrieval for High-Fidelity Agricultural Advice](https://arxiv.org/abs/2512.10114)
*Mesafint Fanuel,Mahmoud Nabil Mahmoud,Crystal Cook Marshal,Vishal Lakhotia,Biswanath Dari,Kaushik Roy,Shaohu Zhang*

Main category: cs.AI

TL;DR: AgriRegion是一个专门针对农业领域设计的检索增强生成框架，通过地理空间元数据注入和区域优先重排序机制，显著减少通用大模型在农业咨询中的上下文幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在农业领域经常出现上下文幻觉问题，提供的建议在不同地区由于土壤、气候和法规差异可能导致灾难性后果，需要开发区域感知的农业咨询系统。

Method: 开发了AgriRegion框架，包含地理空间元数据注入层和区域优先重排序机制，将知识库限制在经验证的本地农业扩展服务，并在检索过程中强制执行地理空间约束。

Result: 实验表明，与最先进的大语言模型系统相比，AgriRegion将幻觉减少了10-20%，并显著提高了信任评分。

Conclusion: AgriRegion框架通过区域感知的检索增强生成方法，为农业领域提供了高保真度的咨询解决方案，解决了通用模型在特定领域的地理适应性挑战。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in democratizing access to information. However, in the domain of agriculture, general-purpose models frequently suffer from contextual hallucination, which provides non-factual advice or answers are scientifically sound in one region but disastrous in another due to variations in soil, climate, and local regulations. We introduce AgriRegion, a Retrieval-Augmented Generation (RAG) framework designed specifically for high-fidelity, region-aware agricultural advisory. Unlike standard RAG approaches that rely solely on semantic similarity, AgriRegion incorporates a geospatial metadata injection layer and a region-prioritized re-ranking mechanism. By restricting the knowledge base to verified local agricultural extension services and enforcing geo-spatial constraints during retrieval, AgriRegion ensures that the advice regarding planting schedules, pest control, and fertilization is locally accurate. We create a novel benchmark dataset, AgriRegion-Eval, which comprises 160 domain-specific questions across 12 agricultural subfields. Experiments demonstrate that AgriRegion reduces hallucinations by 10-20% compared to state-of-the-art LLMs systems and significantly improves trust scores according to a comprehensive evaluation.

</details>


### [12] [The 2025 Foundation Model Transparency Index](https://arxiv.org/abs/2512.10169)
*Alexander Wan,Kevin Klyman,Sayash Kapoor,Nestor Maslej,Shayne Longpre,Betty Xiong,Percy Liang,Rishi Bommasani*

Main category: cs.AI

TL;DR: 2025年基础模型透明度指数显示大模型开发商的透明度从2024年的58分降至40分，IBM表现最佳(95分)，xAI和Midjourney最差(14分)


<details>
  <summary>Details</summary>
Motivation: 追踪基础模型开发商透明度实践的变化趋势，量化评估其透明程度

Method: 年度透明度指数评估，新增数据获取、使用数据和监控指标，首次评估阿里云、深度求索、xAI等公司

Result: 透明度普遍恶化，公司在训练数据和计算资源方面最为不透明，Frontier Model Forum成员透明度中等

Conclusion: 虽然全球政策制定者正加强透明度要求，但目前基础模型开发商的透明度仍不足，需要更激进的政策干预

Abstract: Foundation model developers are among the world's most important companies. As these companies become increasingly consequential, how do their transparency practices evolve? The 2025 Foundation Model Transparency Index is the third edition of an annual effort to characterize and quantify the transparency of foundation model developers. The 2025 FMTI introduces new indicators related to data acquisition, usage data, and monitoring and evaluates companies like Alibaba, DeepSeek, and xAI for the first time. The 2024 FMTI reported that transparency was improving, but the 2025 FMTI finds this progress has deteriorated: the average score out of 100 fell from 58 in 2024 to 40 in 2025. Companies are most opaque about their training data and training compute as well as the post-deployment usage and impact of their flagship models. In spite of this general trend, IBM stands out as a positive outlier, scoring 95, in contrast to the lowest scorers, xAI and Midjourney, at just 14. The five members of the Frontier Model Forum we score end up in the middle of the Index: we posit that these companies avoid reputational harms from low scores but lack incentives to be transparency leaders. As policymakers around the world increasingly mandate certain types of transparency, this work reveals the current state of transparency for foundation model developers, how it may change given newly enacted policy, and where more aggressive policy interventions are necessary to address critical information deficits.

</details>


### [13] [An exploration for higher efficiency in multi objective optimisation with reinforcement learning](https://arxiv.org/abs/2512.10208)
*Mehmet Emin Aydin*

Main category: cs.AI

TL;DR: 论文提出基于多目标强化学习的通用化方法，用于优化多目标优化问题中的算子选择序列，旨在提高优化效率。


<details>
  <summary>Details</summary>
Motivation: 当前优化算法在处理邻域移动操作时，使用单一算子存在效率限制，而多算子组合的最优序列选择尚未充分研究，特别是在多目标优化领域。

Method: 采用多目标强化学习方法，通过经验泛化来学习和优化算子序列的选择策略。

Result: 该方法已完成部分阶段，初步验证了在多目标优化中使用强化学习的有效性，剩余阶段尚待完成。

Conclusion: 基于多目标强化学习的通用化方法为解决多目标优化中的算子序列选择问题提供了有前景的解决方案，有望显著提升优化算法的性能。

Abstract: Efficiency in optimisation and search processes persists to be one of the challenges, which affects the performance and use of optimisation algorithms. Utilising a pool of operators instead of a single operator to handle move operations within a neighbourhood remains promising, but an optimum or near optimum sequence of operators necessitates further investigation. One of the promising ideas is to generalise experiences and seek how to utilise it. Although numerous works are done around this issue for single objective optimisation, multi-objective cases have not much been touched in this regard. A generalised approach based on multi-objective reinforcement learning approach seems to create remedy for this issue and offer good solutions. This paper overviews a generalisation approach proposed with certain stages completed and phases outstanding that is aimed to help demonstrate the efficiency of using multi-objective reinforcement learning.

</details>


### [14] [ID-PaS : Identity-Aware Predict-and-Search for General Mixed-Integer Linear Programs](https://arxiv.org/abs/2512.10211)
*Junyang Cai,El Mehdi Er Raqabi,Pascal Van Hentenryck,Bistra Dilkina*

Main category: cs.AI

TL;DR: 本文扩展了Predict-and-Search框架到参数化MIP问题，提出ID-PaS身份感知学习框架，实验证明其在处理异构变量方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的Predict-and-Search方法虽然通过结合机器学习提高了性能，但仅限于二元问题且忽略了实际应用中常见的固定变量问题，需要扩展框架以处理更复杂的参数化MIP问题。

Method: 提出了ID-PaS（身份感知学习框架），将Predict-and-Search框架扩展到参数化混合整数线性规划问题，使机器学习模型能够更有效地处理异构变量。

Result: 在多个现实世界大规模问题上的实验表明，ID-PaS始终优于最先进的求解器Gurobi和传统PaS方法。

Conclusion: ID-PaS框架通过身份感知学习机制有效处理异构变量，在多个现实世界大规模问题上表现优于Gurobi和传统PaS方法，展示了在实际应用中的优越性能。

Abstract: Mixed-Integer Linear Programs (MIPs) are powerful and flexible tools for modeling a wide range of real-world combinatorial optimization problems. Predict-and-Search methods operate by using a predictive model to estimate promising variable assignments and then guiding a search procedure toward high-quality solutions. Recent research has demonstrated that incorporating machine learning (ML) into the Predict-and-Search framework significantly enhances its performance. Still, it is restricted to binary problems and overlooks the presence of fixed variables that commonly arise in practical settings. This work extends the Predict-and-Search (PaS) framework to parametric MIPs and introduces ID-PaS, an identity-aware learning framework that enables the ML model to handle heterogeneous variables more effectively. Experiments on several real-world large-scale problems demonstrate that ID-PaS consistently achieves superior performance compared to the state-of-the-art solver Gurobi and PaS.

</details>


### [15] [Reverse Thinking Enhances Missing Information Detection in Large Language Models](https://arxiv.org/abs/2512.10273)
*Yuxin Liu,Chaojie Gu,Yihang Zhang,Bin Qian,Shibo He*

Main category: cs.AI

TL;DR: 提出了一种基于逆向思维的方法来增强大语言模型在缺失信息检测任务中的性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型在涉及缺失信息的推理任务中表现不佳，而传统的正向推理方法无法系统地识别和恢复被忽略的信息

Method: 提出了一个新颖的框架，引导LLMs通过逆向思维来识别必要条件和定位缺失元素，将缺失信息识别任务转化为更易处理的逆向推理问题

Result: 实验结果表明，与传统的前向推理方法相比，逆向思维方法取得了显著的性能提升

Conclusion: 该方法为提高LLMs的逻辑完整性和推理鲁棒性提供了一个有前景的方向

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning tasks, yet they often struggle with problems involving missing information, exhibiting issues such as incomplete responses, factual errors, and hallucinations. While forward reasoning approaches like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) have shown success in structured problem-solving, they frequently fail to systematically identify and recover omitted information. In this paper, we explore the potential of reverse thinking methodologies to enhance LLMs' performance on missing information detection tasks. Drawing inspiration from recent work on backward reasoning, we propose a novel framework that guides LLMs through reverse thinking to identify necessary conditions and pinpoint missing elements. Our approach transforms the challenging task of missing information identification into a more manageable backward reasoning problem, significantly improving model accuracy. Experimental results demonstrate that our reverse thinking approach achieves substantial performance gains compared to traditional forward reasoning methods, providing a promising direction for enhancing LLMs' logical completeness and reasoning robustness.

</details>


### [16] [Neuronal Attention Circuit (NAC) for Representation Learning](https://arxiv.org/abs/2512.10282)
*Waleed Razzaq,Izis Kankaraway,Yun-Bo Zhao*

Main category: cs.AI

TL;DR: 提出Neuronal Attention Circuit (NAC)，一种基于生物启发的连续时间注意力机制，通过线性ODE和稀疏门控结构实现自适应动态，在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制在连续时间建模中的离散性限制，需要更生物合理的连续时间注意力方法。

Method: 将注意力logit计算重新表述为线性一阶ODE的解，利用C. elegans神经元电路策略的稀疏门控机制，支持三种计算模式。

Result: NAC在精度上匹配或优于基线，在运行时和内存效率上处于中等水平。

Conclusion: NAC为连续时间建模提供了有效的注意力机制，具有理论保证和实际应用价值。

Abstract: Attention improves representation learning over RNNs, but its discrete nature limits continuous-time (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates attention logits computation as the solution to a linear first-order ODE with nonlinear interlinked gates derived from repurposing \textit{C. elegans} Neuronal Circuit Policies (NCPs) wiring mechanism. NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse backbone network with two heads for computing \textit{content-target} and \textit{learnable time-constant} gates, enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i) explicit Euler integration, (ii) exact closed-form solution, and (iii) steady-state approximation. To improve memory intensity, we implemented a sparse Top-\emph{K} pairwise concatenation scheme that selectively curates key-query interactions. We provide rigorous theoretical guarantees, including state stability, bounded approximation errors, and universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for autonomous vehicles, and industrial prognostics. We observed that NAC matches or outperforms competing baselines in accuracy and occupies an intermediate position in runtime and memory efficiency compared with several CT baselines.

</details>


### [17] [Investigating The Functional Roles of Attention Heads in Vision Language Models: Evidence for Reasoning Modules](https://arxiv.org/abs/2512.10300)
*Yanbei Jiang,Xueqi Ma,Shu Liu,Sarah Monazam Erfani,Tongliang Liu,James Bailey,Jey Han Lau,Krista A. Ehinger*

Main category: cs.AI

TL;DR: 提出CogVision数据集和可解释性框架，分析视觉语言模型中注意力头的功能角色及其在多模态推理中的重要性


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在多模态基准测试中表现出色，但其内部机制仍不透明，需要系统化的可解释性分析

Method: 引入CogVision数据集将复杂多模态问题分解为逐步子问题，采用探针方法识别专注于特定功能的注意力头

Result: 发现功能头具有普遍稀疏性、功能和层次组织多样性，干预实验证明它们对多模态推理至关重要

Conclusion: 该研究为理解VLMs的认知组织提供了新见解，并为设计更符合人类感知推理能力的模型指明了方向

Abstract: Despite excelling on multimodal benchmarks, vision-language models (VLMs) largely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To this end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with each subquestion associated with specific receptive or cognitive functions such as high-level visual reception and inference. Using a probing-based methodology, we identify attention heads that specialize in these functions and characterize them as functional heads. Our analysis across diverse VLM families reveals that these functional heads are universally sparse, vary in number and distribution across functions, and mediate interactions and hierarchical organization. Furthermore, intervention experiments demonstrate their critical role in multimodal reasoning: removing functional heads leads to performance degradation, while emphasizing them enhances accuracy. These findings provide new insights into the cognitive organization of VLMs and suggest promising directions for designing models with more human-aligned perceptual and reasoning abilities.

</details>


### [18] [Trustworthy Orchestration Artificial Intelligence by the Ten Criteria with Control-Plane Governance](https://arxiv.org/abs/2512.10304)
*Byeong Ho Kang,Wenli Yang,Muhammad Bilal Amin*

Main category: cs.AI

TL;DR: 提出可信AI编排的十大标准框架，将治理架构嵌入AI生态系统执行层面


<details>
  <summary>Details</summary>
Motivation: AI系统在重要决策中作用日益增强，但技术能力与制度问责之间存在日益扩大的差距

Method: 提出Ten Criteria for Trustworthy Orchestration AI框架，集成人机输入、语义连贯性、审计和溯源性到统一控制面板架构

Result: 建立了覆盖整个AI组件、消费者和人类参与者的统一治理框架

Conclusion: 可信性可以通过工程方式系统性地融入AI系统，确保执行层面可验证、透明、可重现并在有意义的人类控制下

Abstract: As Artificial Intelligence (AI) systems increasingly assume consequential decision-making roles, a widening gap has emerged between technical capabilities and institutional accountability. Ethical guidance alone is insufficient to counter this challenge; it demands architectures that embed governance into the execution fabric of the ecosystem. This paper presents the Ten Criteria for Trustworthy Orchestration AI, a comprehensive assurance framework that integrates human input, semantic coherence, audit and provenance integrity into a unified Control-Panel architecture. Unlike conventional agentic AI initiatives that primarily focus on AI-to-AI coordination, the proposed framework provides an umbrella of governance to the entire AI components, their consumers and human participants. By taking aspiration from international standards and Australia's National Framework for AI Assurance initiative, this work demonstrates that trustworthiness can be systematically incorporated (by engineering) into AI systems, ensuring the execution fabric remains verifiable, transparent, reproducible and under meaningful human control.

</details>


### [19] [When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection](https://arxiv.org/abs/2512.10449)
*Devanshu Sahoo,Manish Prasad,Vasudev Majhi,Jahnvi Singh,Vinay Chamola,Yash Sinha,Murari Mandal,Dhruv Kumar*

Main category: cs.AI

TL;DR: 本研究揭示科学论文评审中LLM评估系统存在严重安全漏洞，对抗性PDF操作可成功翻转评审结果，开发了WAVS评估指标量化系统脆弱性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是调查科学评审中LLM评估系统（包括非正式和正式部署的）对对抗性PDF操作的鲁棒性，特别是针对将'拒绝'决定翻转成'接受'的特定激励。

Method: 研究构建了一个包含200篇科学论文的数据集，并针对翻转'拒绝'为'接受'的特定目标，开发了15种领域特定的对抗攻击策略，在包括GPT-5、Claude Haiku和DeepSeek在内的13个语言模型上进行了评估。

Result: 研究结果表明，混淆策略如'Maximum Mark Magyk'能够成功操纵评分，即使在大型模型中也能实现令人担忧的决策翻转率。

Conclusion: 该研究揭示了科学论文评审系统中LLM评估工具的安全漏洞，强调了需要开发更鲁棒的AI评审系统来应对对抗性攻击。

Abstract: The landscape of scientific peer review is rapidly evolving with the integration of Large Language Models (LLMs). This shift is driven by two parallel trends: the widespread individual adoption of LLMs by reviewers to manage workload (the "Lazy Reviewer" hypothesis) and the formal institutional deployment of AI-powered assessment systems by conferences like AAAI and Stanford's Agents4Science. This study investigates the robustness of these "LLM-as-a-Judge" systems (both illicit and sanctioned) to adversarial PDF manipulation. Unlike general jailbreaks, we focus on a distinct incentive: flipping "Reject" decisions to "Accept," for which we develop a novel evaluation metric which we term as WAVS (Weighted Adversarial Vulnerability Score). We curated a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies to this task, evaluating them across 13 Language Models, including GPT-5, Claude Haiku, and DeepSeek. Our results demonstrate that obfuscation strategies like "Maximum Mark Magyk" successfully manipulate scores, achieving alarming decision flip rates even in large-scale models. We will release our complete dataset and injection framework to facilitate more research on this topic.

</details>


### [20] [EpiPlanAgent: Agentic Automated Epidemic Response Planning](https://arxiv.org/abs/2512.10313)
*Kangkun Mao,Fang Xu,Jinru Ding,Yidong Jiang,Yujun Yao,Yirong Chen,Junming Liu,Xiaoqin Wu,Qian Wu,Xiaoyan Huang,Jie Xu*

Main category: cs.AI

TL;DR: 设计了EpiPlanAgent基于LLM的多智能体系统，用于自动生成和验证数字应急响应计划，显著提高计划完整性和效率


<details>
  <summary>Details</summary>
Motivation: 传统的流行病响应计划依赖劳动密集型手动方法，效率低下

Method: 构建了集成任务分解、知识基础和模拟模块的多智能体框架，通过公共卫生专家使用真实爆发场景进行对照评估

Result: EpiPlanAgent显著提高了计划的完整性和指南一致性，同时大幅减少开发时间，专家评估显示AI生成与人工内容高度一致

Conclusion: EpiPlanAgent为智能流行病响应规划提供了有效、可扩展的解决方案，展示了智能体AI变革公共卫生准备的潜力

Abstract: Epidemic response planning is essential yet traditionally reliant on labor-intensive manual methods. This study aimed to design and evaluate EpiPlanAgent, an agent-based system using large language models (LLMs) to automate the generation and validation of digital emergency response plans. The multi-agent framework integrated task decomposition, knowledge grounding, and simulation modules. Public health professionals tested the system using real-world outbreak scenarios in a controlled evaluation. Results demonstrated that EpiPlanAgent significantly improved the completeness and guideline alignment of plans while drastically reducing development time compared to manual workflows. Expert evaluation confirmed high consistency between AI-generated and human-authored content. User feedback indicated strong perceived utility. In conclusion, EpiPlanAgent provides an effective, scalable solution for intelligent epidemic response planning, demonstrating the potential of agentic AI to transform public health preparedness.

</details>


### [21] [User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation](https://arxiv.org/abs/2512.10322)
*Yongqiang Yu,Xuhui Li,Hazza Mahmood,Jinxing Zhou,Haodong Hong,Longtao Jiang,Zhiqiang Xu,Qi Wu,Xiaojun Chang*

Main category: cs.AI

TL;DR: 提出了一种用户反馈驱动的视觉与语言导航适应框架，通过整合人类交互来增强GSA-VLN的持续学习能力


<details>
  <summary>Details</summary>
Motivation: 当前GSA-VLN框架缺乏用户反馈机制，仅依赖无监督的环境适应，而用户反馈能提供宝贵的监督信号来提升适应质量

Method: 开发了用户反馈驱动的适应框架，将导航指令和纠正信号转化为高质量的环境对齐训练数据，并采用记忆库热启动机制重用先前获得的环境知识

Result: 在GSA-R2R基准测试中，该方法超越了GR-DUET等强基线，提高了导航成功率和路径效率，记忆库热启动稳定了早期导航并减少了更新后的性能下降

Conclusion: 该框架在持续和混合适应设置下都表现出鲁棒性和通用性，为现实世界VLN部署提供了有效的解决方案

Abstract: Vision-and-Language Navigation (VLN) requires agents to navigate complex environments by following natural-language instructions. General Scene Adaptation for VLN (GSA-VLN) shifts the focus from zero-shot generalization to continual, environment-specific adaptation, narrowing the gap between static benchmarks and real-world deployment. However, current GSA-VLN frameworks exclude user feedback, relying solely on unsupervised adaptation from repeated environmental exposure. In practice, user feedback offers natural and valuable supervision that can significantly enhance adaptation quality. We introduce a user-feedback-driven adaptation framework that extends GSA-VLN by systematically integrating human interactions into continual learning. Our approach converts user feedback-navigation instructions and corrective signals-into high-quality, environment-aligned training data, enabling efficient and realistic adaptation. A memory-bank warm-start mechanism further reuses previously acquired environmental knowledge, mitigating cold-start degradation and ensuring stable redeployment. Experiments on the GSA-R2R benchmark show that our method consistently surpasses strong baselines such as GR-DUET, improving navigation success and path efficiency. The memory-bank warm start stabilizes early navigation and reduces performance drops after updates. Results under both continual and hybrid adaptation settings confirm the robustness and generality of our framework, demonstrating sustained improvement across diverse deployment conditions.

</details>


### [22] [On the Collapse of Generative Paths: A Criterion and Correction for Diffusion Steering](https://arxiv.org/abs/2512.10339)
*Ziseok Lee,Minyeong Hwang,Sanghyun Jo,Wooyeol Lee,Jihyung Ko,Young Bin Park,Jae-Mun Choi,Eunho Yang,Kyungsu Kim*

Main category: cs.AI

TL;DR: 论文提出ACE方法解决异质扩散模型中边际路径坍缩问题，实现了稳定的推理时间导向


<details>
  <summary>Details</summary>
Motivation: 现有的密度比率方法在组合训练于不同噪声调度或数据集的异质模型时，会出现边际路径坍缩问题，特别是在分子设计中组合从头合成、构象和口袋条件模型时

Method: 首先推导了路径存在准则来预测坍缩发生条件，然后提出自适应路径校正(ACE)方法，将Feynman-Kac导向扩展到时变指数，保证有效概率路径

Result: 在合成2D基准和柔性姿态支架装饰任务中，ACE消除了坍缩，实现了高引导组合生成，改进了分布和对接指标

Conclusion: ACE方法将密度比率导向从启发式方法转变为可控生成的可靠工具

Abstract: Inference-time steering enables pretrained diffusion/flow models to be adapted to new tasks without retraining. A widely used approach is the ratio-of-densities method, which defines a time-indexed target path by reweighting probability-density trajectories from multiple models with positive, or in some cases, negative exponents. This construction, however, harbors a critical and previously unformalized failure mode: Marginal Path Collapse, where intermediate densities become non-normalizable even though endpoints remain valid. Collapse arises systematically when composing heterogeneous models trained on different noise schedules or datasets, including a common setting in molecular design where de-novo, conformer, and pocket-conditioned models must be combined for tasks such as flexible-pose scaffold decoration. We provide a novel and complete solution for the problem. First, we derive a simple path existence criterion that predicts exactly when collapse occurs from noise schedules and exponents alone. Second, we introduce Adaptive path Correction with Exponents (ACE), which extends Feynman-Kac steering to time-varying exponents and guarantees a valid probability path. On a synthetic 2D benchmark and on flexible-pose scaffold decoration, ACE eliminates collapse and enables high-guidance compositional generation, improving distributional and docking metrics over constant-exponent baselines and even specialized task-specific scaffold decoration models. Our work turns ratio-of-densities steering with heterogeneous experts from an unstable heuristic into a reliable tool for controllable generation.

</details>


### [23] [REMISVFU: Vertical Federated Unlearning via Representation Misdirection for Intermediate Output Feature](https://arxiv.org/abs/2512.10348)
*Wenhan Wu,Zhili He,Huanghuang Liang,Yili Gong,Jiawei Jiang,Chuang Hu,Dazhao Cheng*

Main category: cs.AI

TL;DR: REMISVFU：一种用于纵向联邦学习的即插即用表征误导框架，支持快速客户端级遗忘


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘技术主要针对横向联邦学习，不适用于特征分区的纵向联邦学习架构

Method: 遗忘方将编码器输出折叠到单位球面上的随机锚点，服务器通过正交投影联合优化保留损失和遗忘损失

Result: 在公共基准测试中，REMISVFU将后门攻击成功率抑制到自然类先验水平，仅牺牲约2.5%的干净准确率

Conclusion: 该框架为纵向联邦学习系统提供了有效的遗忘解决方案，平衡了遗忘效果和模型效用

Abstract: Data-protection regulations such as the GDPR grant every participant in a federated system a right to be forgotten. Federated unlearning has therefore emerged as a research frontier, aiming to remove a specific party's contribution from the learned model while preserving the utility of the remaining parties. However, most unlearning techniques focus on Horizontal Federated Learning (HFL), where data are partitioned by samples. In contrast, Vertical Federated Learning (VFL) allows organizations that possess complementary feature spaces to train a joint model without sharing raw data. The resulting feature-partitioned architecture renders HFL-oriented unlearning methods ineffective. In this paper, we propose REMISVFU, a plug-and-play representation misdirection framework that enables fast, client-level unlearning in splitVFL systems. When a deletion request arrives, the forgetting party collapses its encoder output to a randomly sampled anchor on the unit sphere, severing the statistical link between its features and the global model. To maintain utility for the remaining parties, the server jointly optimizes a retention loss and a forgetting loss, aligning their gradients via orthogonal projection to eliminate destructive interference. Evaluations on public benchmarks show that REMISVFU suppresses back-door attack success to the natural class-prior level and sacrifices only about 2.5% points of clean accuracy, outperforming state-of-the-art baselines.

</details>


### [24] [LLM-Empowered Representation Learning for Emerging Item Recommendation](https://arxiv.org/abs/2512.10370)
*Ziying Zhang,Quanming Yao,Yaqing Wang*

Main category: cs.AI

TL;DR: EmerFlow：一种基于LLM的新兴物品表示学习框架，解决推荐系统中新兴物品交互数据少的挑战


<details>
  <summary>Details</summary>
Motivation: 现有推荐方法忽视了新兴物品交互随时间积累的动态过程，简单假设新兴物品缺乏历史交互，忽视了需要同时保留新兴物品独特性和利用与成熟物品共享模式的复杂性

Method: 提出EmerFlow框架：1）通过LLM推理丰富新兴物品原始特征；2）将表示与现有推荐模型的嵌入空间对齐；3）通过元学习融入新交互来优化嵌入

Result: 在电影和医药等多个领域的实验表明，EmerFlow在不同数据集上始终优于现有方法

Conclusion: EmerFlow能够仅从有限交互中学习表达性强的新兴物品嵌入，有效解决了新兴物品推荐的难题

Abstract: In this work, we tackle the challenge of recommending emerging items, whose interactions gradually accumulate over time. Existing methods often overlook this dynamic process, typically assuming that emerging items have few or even no historical interactions. Such an assumption oversimplifies the problem, as a good model must preserve the uniqueness of emerging items while leveraging their shared patterns with established ones. To address this challenge, we propose EmerFlow, a novel LLM-empowered representation learning framework that generates distinctive embeddings for emerging items. It first enriches the raw features of emerging items through LLM reasoning, then aligns these representations with the embedding space of the existing recommendation model. Finally, new interactions are incorporated through meta-learning to refine the embeddings. This enables EmerFlow to learn expressive embeddings for emerging items from only limited interactions. Extensive experiments across diverse domains, including movies and pharmaceuticals, show that EmerFlow consistently outperforms existing methods.

</details>


### [25] [Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention](https://arxiv.org/abs/2512.10414)
*Yang Yu,Zhuangzhuang Chen,Siqi Wang,Lanqing Li,Xiaomeng Li*

Main category: cs.AI

TL;DR: 提出选择性对抗熵干预方法(SaEI)，通过在RL采样阶段进行熵干预来增强视觉语言模型的推理能力


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的微调方法通常只在策略优化阶段干预熵，而忽略了RL采样阶段的熵干预潜力

Method: SaEI方法包含两个核心组件：熵引导对抗采样(EgAS)和令牌选择性熵计算(TsEC)，通过对抗性攻击视觉输入来增强策略探索

Result: 在领域内和领域外数据集上的实验表明，该方法能显著提升策略探索能力，从而提高推理性能

Conclusion: 通过在RL采样阶段进行选择性熵干预，可以有效增强视觉语言模型的推理能力，该方法为RL微调提供了新的优化方向

Abstract: Recently, reinforcement learning (RL) has become a common choice in enhancing the reasoning capabilities of vision-language models (VLMs). Considering existing RL- based finetuning methods, entropy intervention turns out to be an effective way to benefit exploratory ability, thereby improving policy performance. Notably, most existing stud- ies intervene in entropy by simply controlling the update of specific tokens during policy optimization of RL. They ig- nore the entropy intervention during the RL sampling that can boost the performance of GRPO by improving the di- versity of responses. In this paper, we propose Selective- adversarial Entropy Intervention, namely SaEI, which en- hances policy entropy by distorting the visual input with the token-selective adversarial objective coming from the en- tropy of sampled responses. Specifically, we first propose entropy-guided adversarial sampling (EgAS) that formu- lates the entropy of sampled responses as an adversarial ob- jective. Then, the corresponding adversarial gradient can be used to attack the visual input for producing adversarial samples, allowing the policy model to explore a larger an- swer space during RL sampling. Then, we propose token- selective entropy computation (TsEC) to maximize the ef- fectiveness of adversarial attack in EgAS without distorting factual knowledge within VLMs. Extensive experiments on both in-domain and out-of-domain datasets show that our proposed method can greatly improve policy exploration via entropy intervention, to boost reasoning capabilities. Code will be released once the paper is accepted.

</details>


### [26] [Representation of the structure of graphs by sequences of instructions](https://arxiv.org/abs/2512.10429)
*Ezequiel Lopez-Rubio*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的图表示方法，将邻接矩阵转换为简单的指令字符串，使图数据能够被深度学习语言模型处理


<details>
  <summary>Details</summary>
Motivation: 当前图表示方法不适合深度学习语言模型处理，这些模型擅长处理文本但难以直接处理图结构

Method: 将图的邻接矩阵表示为构建指令的字符串序列，该转换是可逆的

Result: 初步计算实验显示了有利的结果

Conclusion: 这种紧凑的表示方法有望提升深度学习模型对图的处理能力

Abstract: The representation of graphs is commonly based on the adjacency matrix concept. This formulation is the foundation of most algebraic and computational approaches to graph processing. The advent of deep learning language models offers a wide range of powerful computational models that are specialized in the processing of text. However, current procedures to represent graphs are not amenable to processing by these models. In this work, a new method to represent graphs is proposed. It represents the adjacency matrix of a graph by a string of simple instructions. The instructions build the adjacency matrix step by step. The transformation is reversible, i.e. given a graph the string can be produced and vice versa. The proposed representation is compact and it maintains the local structural patterns of the graph. Therefore, it is envisaged that it could be useful to boost the processing of graphs by deep learning models. A tentative computational experiment is reported, with favorable results.

</details>


### [27] [Targeted Data Protection for Diffusion Model by Matching Training Trajectory](https://arxiv.org/abs/2512.10433)
*Hojun Lee,Mijin Koo,Yeji Song,Nojun Kwak*

Main category: cs.AI

TL;DR: TAFAP方法通过控制扩散模型的整个训练轨迹实现有效的目标数据保护，解决了现有方法控制性差的问题


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型微调技术普及带来了未经授权数据使用和隐私侵犯的严重问题，现有保护方法只能被动降低图像质量，缺乏稳定控制能力

Method: 引入TAFAP方法，采用轨迹对齐和对抗性扰动的微调方式，基于数据集蒸馏的轨迹匹配思想来控制完整训练轨迹

Result: TAFAP显著优于现有目标数据保护方法，成功实现扩散模型中同时控制身份和视觉模式的目标转换，保持高图像质量

Conclusion: 这项工作为扩散模型输出提供了可验证的防护措施，并为控制和追踪模型输出变更提供了新的框架

Abstract: Recent advancements in diffusion models have made fine-tuning text-to-image models for personalization increasingly accessible, but have also raised significant concerns regarding unauthorized data usage and privacy infringement. Current protection methods are limited to passively degrading image quality, failing to achieve stable control. While Targeted Data Protection (TDP) offers a promising paradigm for active redirection toward user-specified target concepts, existing TDP attempts suffer from poor controllability due to snapshot-matching approaches that fail to account for complete learning dynamics. We introduce TAFAP (Trajectory Alignment via Fine-tuning with Adversarial Perturbations), the first method to successfully achieve effective TDP by controlling the entire training trajectory. Unlike snapshot-based methods whose protective influence is easily diluted as training progresses, TAFAP employs trajectory-matching inspired by dataset distillation to enforce persistent, verifiable transformations throughout fine-tuning. We validate our method through extensive experiments, demonstrating the first successful targeted transformation in diffusion models with simultaneous control over both identity and visual patterns. TAFAP significantly outperforms existing TDP attempts, achieving robust redirection toward target concepts while maintaining high image quality. This work enables verifiable safeguards and provides a new framework for controlling and tracing alterations in diffusion model outputs.

</details>


### [28] [Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation](https://arxiv.org/abs/2512.10501)
*Lim Chien Her,Ming Yan,Yunshu Bai,Ruihao Li,Hao Zhang*

Main category: cs.AI

TL;DR: 提出一种基于LLM智能体的免训练架构，用于零样本PCG参数配置，通过Actor-Critic双智能体迭代工作流解决语义鸿沟问题


<details>
  <summary>Details</summary>
Motivation: 传统PCG管道需要精确配置复杂的技术参数，现有LLM难以弥合抽象用户指令与严格参数规范之间的语义鸿沟

Method: 设计Actor-Critic双智能体系统，Actor负责参数配置，Critic进行评估反馈，形成迭代优化工作流

Result: 在3D地图生成任务上验证，相较于单智能体基线表现更优，能根据自然语言描述生成多样且结构有效的环境

Conclusion: 证明现成LLM可有效重组为通用智能体用于任意PCG工具，通过架构推理而非模型训练实现复杂软件的掌握

Abstract: Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.

</details>


### [29] [Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning](https://arxiv.org/abs/2512.10534)
*Haiteng Zhao,Junhao Shen,Yiming Zhang,Songyang Gao,Kuikun Liu,Tianyou Ma,Fan Zheng,Dahua Lin,Wenwei Zhang,Kai Chen*

Main category: cs.AI

TL;DR: InternGeometry是一个基于LLM的几何问题解决智能体，利用迭代命题生成、符号引擎验证和动态记忆机制，在少量训练数据下达到了IMO金牌水准。


<details>
  <summary>Details</summary>
Motivation: 当前AI几何问题解决主要依赖专家模型和大规模数据合成，而LLM在辅助构造启发式方面存在局限，作者希望构建一个能够达到奖牌水平的LLM几何智能体。

Method: 提出了InternGeometry系统，采用迭代命题和辅助构造生成、符号引擎验证、反馈引导的反思机制，以及动态记忆支持数百次交互。同时引入复杂度提升强化学习（CBRL）来加速训练。

Result: 在2000-2024年IMO几何问题中解决了44/50题，超过金牌平均分（40.9），仅使用13K训练样本（AlphaGeometry 2数据量的0.004%）。

Conclusion: InternGeometry展示了LLM智能体在专业级几何任务上的潜力，能够提出人类解法中未出现的新型辅助构造，模型、数据和符号引擎将开源以支持未来研究。

Abstract: Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.

</details>


### [30] [NormCode: A Semi-Formal Language for Context-Isolated AI Planning](https://arxiv.org/abs/2512.10563)
*Xin Guan*

Main category: cs.AI

TL;DR: NormCode半正式语言用于构建多步LLM工作流，通过数据隔离和明确的输入传递消除上下文污染，支持从草图到生产的渐进形式化。


<details>
  <summary>Details</summary>
Motivation: 多步LLM工作流存在上下文污染问题：信息积累导致模型幻觉、混淆中间输出和丢失任务约束。

Method: 设计NormCode半正式语言，严格分离语义操作（LLM驱动的非确定性推理）和句法操作（确定性数据重组），支持三种同构格式（.ncds、.ncd、.ncn）。

Result: 验证表明：(1)基础X加法算法在任意长度输入上实现100%准确率；(2)自托管执行NormCode的五阶段编译器流水线。

Conclusion: NormCode通过依赖驱动调度、SQLite检查点和循环管理，使AI工作流可审计，满足法律、医疗和金融等高风险领域的透明度需求。

Abstract: Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.

</details>


### [31] [Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs](https://arxiv.org/abs/2512.10611)
*Minghao LI,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.

</details>


### [32] [Refinement Contrastive Learning of Cell-Gene Associations for Unsupervised Cell Type Identification](https://arxiv.org/abs/2512.10640)
*Liang Peng,Haopeng Liu,Yixuan Ye,Cheng Liu,Wenjun Shen,Si Wu,Hau-San Wong*

Main category: cs.AI

TL;DR: 提出了一种名为scRCL的无监督细胞类型识别框架，通过结合细胞-基因关联来提升细胞嵌入表示的质量


<details>
  <summary>Details</summary>
Motivation: 现有聚类方法主要关注细胞内在结构，忽略了细胞-基因关联的关键作用，限制了区分紧密相关细胞类型的能力

Method: 开发了包含两个对比分布对齐组件的框架，揭示可靠的细胞内在结构，并通过整合基因相关性结构学习的精细化模块增强细胞嵌入表示

Result: 在多个单细胞RNA-seq和空间转录组基准数据集上实验表明，该方法在细胞类型识别准确率上优于现有最优基线

Conclusion: scRCL通过有效利用细胞-基因相互作用，能够识别出具有一致基因表达特征的细胞群体，验证了其生物学相关性

Abstract: Unsupervised cell type identification is crucial for uncovering and characterizing heterogeneous populations in single cell omics studies. Although a range of clustering methods have been developed, most focus exclusively on intrinsic cellular structure and ignore the pivotal role of cell-gene associations, which limits their ability to distinguish closely related cell types. To this end, we propose a Refinement Contrastive Learning framework (scRCL) that explicitly incorporates cell-gene interactions to derive more informative representations. Specifically, we introduce two contrastive distribution alignment components that reveal reliable intrinsic cellular structures by effectively exploiting cell-cell structural relationships. Additionally, we develop a refinement module that integrates gene-correlation structure learning to enhance cell embeddings by capturing underlying cell-gene associations. This module strengthens connections between cells and their associated genes, refining the representation learning to exploiting biologically meaningful relationships. Extensive experiments on several single-cell RNA-seq and spatial transcriptomics benchmark datasets demonstrate that our method consistently outperforms state-of-the-art baselines in cell-type identification accuracy. Moreover, downstream biological analyses confirm that the recovered cell populations exhibit coherent gene-expression signatures, further validating the biological relevance of our approach. The code is available at https://github.com/THPengL/scRCL.

</details>


### [33] [CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models](https://arxiv.org/abs/2512.10655)
*Tong Zhang,Carlos Hinojosa,Bernard Ghanem*

Main category: cs.AI

TL;DR: CAPTAIN是一个无需训练的方法，通过在去噪过程中直接修改潜在特征来减少扩散模型的记忆效应，同时保持与提示的良好对齐和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型可能在无意中复现训练样本，引发隐私和版权担忧。现有方法通常操纵CFG或扰动提示嵌入，但难以平衡减少记忆化与保持提示对齐。

Method: CAPTAIN首先应用基于频率的噪声初始化以减少早期复制记忆模式；然后识别特征注入的最佳去噪时间步并定位记忆区域；最后将语义对齐特征从非记忆参考图像注入到局部潜在区域。

Result: 实验证明，相较于基于CFG的基线方法，CAPTAIN显著降低了记忆效应，同时保持了与预期提示的强对齐。

Conclusion: CAPTAIN提供了一种有效的训练后解决方案，可直接修改潜在特征来缓解扩散模型的记忆问题，平衡了隐私保护与生成质量之间的关系。

Abstract: Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.

</details>


### [34] [On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity](https://arxiv.org/abs/2512.10665)
*Muhua Huang,Qinlin Zhao,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: 研究探讨价值观多样性如何影响AI多智能体系统的集体行为，发现适度多样性增强价值稳定性并促进自主创新能力


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的多智能体系统日益普及，理解人工社区的集体行为（如集体智能）变得愈发重要

Method: 基于施瓦茨基本价值观理论构建多智能体模拟，让不同规模的社群进行开放式互动和制度形成

Result: 价值观多样性能增强价值稳定性，促进自发行为涌现，并带来更多由智能体自主创造的创新原则

Conclusion: 价值观多样性是未来AI能力的新维度，但极端异质性会导致不稳定，体现了收益递减规律

Abstract: As Large Language Models (LLM) based multi-agent systems become increasingly prevalent, the collective behaviors, e.g., collective intelligence, of such artificial communities have drawn growing attention. This work aims to answer a fundamental question: How does diversity of values shape the collective behavior of AI communities? Using naturalistic value elicitation grounded in the prevalent Schwartz's Theory of Basic Human Values, we constructed multi-agent simulations where communities with varying numbers of agents engaged in open-ended interactions and constitution formation. The results show that value diversity enhances value stability, fosters emergent behaviors, and brings more creative principles developed by the agents themselves without external guidance. However, these effects also show diminishing returns: extreme heterogeneity induces instability. This work positions value diversity as a new axis of future AI capability, bridging AI ability and sociological studies of institutional emergence.

</details>


### [35] [Challenges of Evaluating LLM Safety for User Welfare](https://arxiv.org/abs/2512.10687)
*Manon Kempermann,Sai Suresh Macharla Vasu,Mahalakshmi Raveenthiran,Theo Farrell,Ingmar Weber*

Main category: cs.AI

TL;DR: LLM safety evaluations need context-aware approaches for personal advice scenarios, as context-blind assessments overestimate safety for vulnerable users, and user-provided context alone is insufficient to bridge the gap.


<details>
  <summary>Details</summary>
Motivation: Safety evaluations of LLMs typically focus on universal risks, but millions use LLMs for personal advice on high-stakes topics where harms are context-dependent rather than universal. User-welfare safety evaluations remain underdeveloped despite frameworks recognizing the need to assess individual risks.

Method: The study evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. It compared safety ratings by context-blind evaluators versus those aware of user circumstances, and then reran the evaluation on prompts containing context users report they would disclose.

Result: Identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). Creating realistic user prompts containing key contextual information did not significantly improve safety ratings.

Conclusion: Effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. The study provides a methodology for context-aware evaluation and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks.

Abstract: Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.

</details>


### [36] [Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning](https://arxiv.org/abs/2512.10691)
*Benjamin Gundersen,Nicolas Deperrois,Samuel Ruiperez-Campillo,Thomas M. Sutter,Julia E. Vogt,Michael Moor,Farhad Nooralahzadeh,Michael Krauthammer*

Main category: cs.AI

TL;DR: RL优化相比单纯SFT在医疗VLM上有额外提升，但显式思维推理对结果无明显改善


<details>
  <summary>Details</summary>
Motivation: 研究强化学习和显式思维推理在胸部X光视觉语言模型中的作用效果

Method: 在Qwen3-VL基础上进行大规模SFT构建RadVLM，添加基础思维能力，然后应用GRPO强化学习进行报告生成和视觉定位

Result: RL优化后的模型在报告生成和视觉定位任务上均超越基线并达到最优性能

Conclusion: 临床对齐的强化学习是医疗VLM中SFT的有效补充

Abstract: Recent advances in vision-language models (VLMs) have improved Chest X-ray (CXR) interpretation in multiple aspects. However, many medical VLMs rely solely on supervised fine-tuning (SFT), which optimizes next-token prediction without evaluating answer quality. In contrast, reinforcement learning (RL) can incorporate task-specific feedback, and its combination with explicit intermediate reasoning ("thinking") has demonstrated substantial gains on verifiable math and coding tasks. To investigate the effects of RL and thinking in a CXR VLM, we perform large-scale SFT on CXR data to build an updated RadVLM based on Qwen3-VL, followed by a cold-start SFT stage that equips the model with basic thinking ability. We then apply Group Relative Policy Optimization (GRPO) with clinically grounded, task-specific rewards for report generation and visual grounding, and run matched RL experiments on both domain-specific and general-domain Qwen3-VL variants, with and without thinking. Across these settings, we find that while strong SFT remains crucial for high base performance, RL provides additional gains on both tasks, whereas explicit thinking does not appear to further improve results. Under a unified evaluation pipeline, the RL-optimized RadVLM models outperform their baseline counterparts and reach state-of-the-art performance on both report generation and grounding, highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs.

</details>


### [37] [COMPARE: Clinical Optimization with Modular Planning and Assessment via RAG-Enhanced AI-OCT: Superior Decision Support for Percutaneous Coronary Intervention Compared to ChatGPT-5 and Junior Operators](https://arxiv.org/abs/2512.10702)
*Wei Fang,Chiyao Wang,Wenshuai Ma,Hui Liu,Jianqiang Hu,Xiaona Niu,Yi Chu,Mingming Zhang,Jingxiao Yang,Dongwei Zhang,Zelin Li,Pengyun Liu,Jiawei Zheng,Pengke Zhang,Chaoshi Qin,Wangang Guo,Bin Wang,Yugang Xue,Wei Zhang,Zikuan Wang,Rui Zhu,Yihui Cao,Quanmao Lu,Rui Meng,Yan Li*

Main category: cs.AI

TL;DR: CA-GPT AI-OCT系统在OCT引导PCI决策中优于ChatGPT-5和初级医生，为血管内影像提供标准化解读方法。


<details>
  <summary>Details</summary>
Motivation: 血管内影像特别是OCT可改善PCI结果，但其解读依赖于操作者。通用AI显示潜力但缺乏领域特异性可靠性。

Method: 单中心分析96例接受OCT引导PCI的患者，将CA-GPT、ChatGPT-5和初级医生生成的手术决策与专家记录进行比较。使用10个预定指标评估术前和术后阶段的一致性。

Result: 术前规划阶段，CA-GPT的中位一致性评分显著高于ChatGPT-5和初级医生。在支架直径和长度选择方面显著优于初级医生。术后评估阶段，CA-GPT保持优异整体一致性，显著优于两组对照组。亚组分析证实CA-GPT在复杂场景中具有稳健性能优势。

Conclusion: 基于CA-GPT的AI-OCT系统在PCI规划和评估阶段相比通用大语言模型和初级医生表现出优越的决策一致性，为血管内影像解读提供了标准化和可靠的方法，显示出显著增强操作者专业知识和优化OCT引导PCI的潜力。

Abstract: Background: While intravascular imaging, particularly optical coherence tomography (OCT), improves percutaneous coronary intervention (PCI) outcomes, its interpretation is operator-dependent. General-purpose artificial intelligence (AI) shows promise but lacks domain-specific reliability. We evaluated the performance of CA-GPT, a novel large model deployed on an AI-OCT system, against that of the general-purpose ChatGPT-5 and junior physicians for OCT-guided PCI planning and assessment.
  Methods: In this single-center analysis of 96 patients who underwent OCT-guided PCI, the procedural decisions generated by the CA-GPT, ChatGPT-5, and junior physicians were compared with an expert-derived procedural record. Agreement was assessed using ten pre-specified metrics across pre-PCI and post-PCI phases.
  Results: For pre-PCI planning, CA-GPT demonstrated significantly higher median agreement scores (5[IQR 3.75-5]) compared to both ChatGPT-5 (3[2-4], P<0.001) and junior physicians (4[3-4], P<0.001). CA-GPT significantly outperformed ChatGPT-5 across all individual pre-PCI metrics and showed superior performance to junior physicians in stent diameter (90.3% vs. 72.2%, P<0.05) and length selection (80.6% vs. 52.8%, P<0.01). In post-PCI assessment, CA-GPT maintained excellent overall agreement (5[4.75-5]), significantly higher than both ChatGPT-5 (4[4-5], P<0.001) and junior physicians (5[4-5], P<0.05). Subgroup analysis confirmed CA-GPT's robust performance advantage in complex scenarios.
  Conclusion: The CA-GPT-based AI-OCT system achieved superior decision-making agreement versus a general-purpose large language model and junior physicians across both PCI planning and assessment phases. This approach provides a standardized and reliable method for intravascular imaging interpretation, demonstrating significant potential to augment operator expertise and optimize OCT-guided PCI.

</details>


### [38] [Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly](https://arxiv.org/abs/2512.10787)
*Moshe Lahmy,Roi Yozevitch*

Main category: cs.AI

TL;DR: SEAL-RAG提出了一种训练免费的控制器，采用"替换而非扩展"策略来解决多跳查询中的上下文稀释问题，在固定检索深度下实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在处理多跳查询时，当初始检索遗漏桥接事实时会失败。现有修正方法通常通过添加更多上下文或修剪现有列表来解决，但这往往导致上下文稀释问题。

Method: SEAL-RAG执行搜索→提取→评估→循环周期：进行实时实体锚定提取构建gap规范，触发定向微查询，并使用实体优先排名主动用gap闭合证据替换干扰项。

Result: 在HotpotQA上，SEAL比Self-RAG答案正确性提升3-13个百分点，证据精度提升12-18个百分点。在2WikiMultiHopQA上，比Adaptive-k准确率提升8.0个百分点，证据精度保持96%。

Conclusion: 通过强制执行固定k替换，SEAL提供了可预测的成本概况，同时确保top-k槽位针对精度而非仅广度进行优化。

Abstract: Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \textbf{context dilution}, where distractors crowd out relevant information. We propose \textbf{SEAL-RAG}, a training-free controller that adopts a \textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\textbf{S}earch $\rightarrow$ \textbf{E}xtract $\rightarrow$ \textbf{A}ssess $\rightarrow$ \textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \textbf{HotpotQA} and \textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \textbf{+3--13 pp} and evidence precision by \textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \textbf{+8.0 pp} in accuracy and maintains \textbf{96\%} evidence precision compared to 22\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.

</details>


### [39] [HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based Human Activity Recognition](https://arxiv.org/abs/2512.10807)
*Wang Lu,Yao Zhu,Jindong Wang*

Main category: cs.AI

TL;DR: 提出了HAROOD基准测试，用于评估人类活动识别在分布外场景下的性能


<details>
  <summary>Details</summary>
Motivation: 解决人类活动识别中由于个体、设备、环境和时间变化导致的分布漂移问题，现有方法缺乏全面评估

Method: 定义了4种OOD场景（跨人、跨位置、跨数据集、跨时间），构建了包含6个数据集和16种对比方法的测试平台

Result: 实验表明没有单一方法在所有场景下持续最优，显示了进一步研究的必要性

Conclusion: HAROOD提供了一个模块化、可扩展的代码库，有助于推动基于OOD的HAR研究

Abstract: Sensor-based human activity recognition (HAR) mines activity patterns from the time-series sensory data. In realistic scenarios, variations across individuals, devices, environments, and time introduce significant distributional shifts for the same activities. Recent efforts attempt to solve this challenge by applying or adapting existing out-of-distribution (OOD) algorithms, but only in certain distribution shift scenarios (e.g., cross-device or cross-position), lacking comprehensive insights on the effectiveness of these algorithms. For instance, is OOD necessary to HAR? Which OOD algorithm performs the best? In this paper, we fill this gap by proposing HAROOD, a comprehensive benchmark for HAR in OOD settings. We define 4 OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and build a testbed covering 6 datasets, 16 comparative methods (implemented with CNN-based and Transformer-based architectures), and two model selection protocols. Then, we conduct extensive experiments and present several findings for future research, e.g., no single method consistently outperforms others, highlighting substantial opportunity for advancement. Our codebase is highly modular and easy to extend for new datasets, algorithms, comparisons, and analysis, with the hope to facilitate the research in OOD-based HAR. Our implementation is released and can be found at https://github.com/AIFrontierLab/HAROOD.

</details>


### [40] [Agile Deliberation: Concept Deliberation for Subjective Visual Classification](https://arxiv.org/abs/2512.10821)
*Leijie Wang,Otilia Stretcu,Wei Qiao,Thomas Denby,Krishnamurthy Viswanathan,Enming Luo,Chun-Ta Lu,Tushar Dogra,Ranjay Krishna,Ariel Fuxman*

Main category: cs.AI

TL;DR: 提出Agile Deliberation框架，通过概念范围界定和概念迭代两个阶段，帮助用户从模糊概念开始逐步精确定义视觉分类任务


<details>
  <summary>Details</summary>
Motivation: 现有内容审核系统假设用户有清晰稳定的概念理解，但现实中用户往往从模糊概念开始，需要进行概念审议来逐步明确

Method: 通过结构化访谈发现概念审议实践，开发Agile Deliberation框架，包含概念范围界定（分解概念为结构化子概念层次）和概念迭代（展示边界案例供用户反思反馈）

Result: 与自动分解基线相比F1分数提高7.5%，比手动审议提高3%以上，用户报告概念理解更清晰且认知努力更低

Conclusion: Agile Deliberation框架有效支持演化中的主观概念定义，特别适合用户从模糊概念开始的内容管理和筛选应用

Abstract: From content moderation to content curation, applications requiring vision classifiers for visual concepts are rapidly expanding. Existing human-in-the-loop approaches typically assume users begin with a clear, stable concept understanding to be able to provide high-quality supervision. In reality, users often start with a vague idea and must iteratively refine it through "concept deliberation", a practice we uncovered through structured interviews with content moderation experts. We operationalize the common strategies in deliberation used by real content moderators into a human-in-the-loop framework called "Agile Deliberation" that explicitly supports evolving and subjective concepts. The system supports users in defining the concept for themselves by exposing them to borderline cases. The system does this with two deliberation stages: (1) concept scoping, which decomposes the initial concept into a structured hierarchy of sub-concepts, and (2) concept iteration, which surfaces semantically borderline examples for user reflection and feedback to iteratively align an image classifier with the user's evolving intent. Since concept deliberation is inherently subjective and interactive, we painstakingly evaluate the framework through 18 user sessions, each 1.5h long, rather than standard benchmarking datasets. We find that Agile Deliberation achieves 7.5% higher F1 scores than automated decomposition baselines and more than 3% higher than manual deliberation, while participants reported clearer conceptual understanding and lower cognitive effort.

</details>


### [41] [V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions](https://arxiv.org/abs/2512.10822)
*Mumuksh Tayal,Manan Tayal,Aditya Singh,Shishir Kolathaya,Ravi Prakash*

Main category: cs.AI

TL;DR: V-OCBF是一种从离线演示中学习神经控制障碍函数的框架，无需动力学模型或人工设计障碍函数，通过递归有限差分障碍更新和期望值目标实现无模型安全控制合成，在安全性和任务性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 确保自主系统的安全性需要控制器在不依赖在线交互的情况下满足硬性状态约束。现有安全离线强化学习方法通常执行软期望成本约束，但无法保证前向不变性。而控制障碍函数（CBFs）提供严格的安全保证，但通常依赖于专家设计的障碍函数或完整的系统动力学知识。

Method: V-OCBF框架从离线演示中学习神经控制障碍函数，采用基于递归有限差分障碍更新的无模型学习方法，并结合基于期望值的目标，避免在分布外动作上查询障碍函数，并将更新限制在数据集支持的动作集内。学习的障碍函数与二次规划（QP）公式结合，用于合成实时安全控制。

Result: V-OCBF在多个案例研究中比基线方法产生显著更少的安全违规，同时保持强大的任务性能。

Conclusion: V-OCBF框架在多个案例研究中显著减少了安全违规，同时保持了良好的任务性能，展示了其在无需在线交互或人工设计障碍函数的情况下，离线合成安全关键控制器的可扩展性。

Abstract: Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [42] [IoTEdu: Access Control, Detection, and Automatic Incident Response in Academic IoT Networks](https://arxiv.org/abs/2512.09934)
*Joner Assolin,Diego Kreutz,Leandro Bertholdo*

Main category: cs.CR

TL;DR: IoTEdu平台集成访问控制、事件检测和自动拦截功能，在学术环境中管理IoT设备安全，检测到阻断平均耗时28.6秒


<details>
  <summary>Details</summary>
Motivation: 学术机构中IoT设备增多带来运营复杂性和安全漏洞，缺乏统一的注册、监控和事件响应策略

Method: 开发IoTEdu集成平台，在受控环境中通过模拟攻击进行评估

Result: 实现了手动干预减少、响应标准化以及注册、监控和事件响应流程的统一

Conclusion: IoTEdu平台能有效提升学术环境中IoT设备的安全管理效率

Abstract: The growing presence of IoT devices in academic environments has increased operational complexity and exposed security weaknesses, especially in academic institutions without unified policies for registration, monitoring, and incident response involving IoT. This work presents IoTEdu, an integrated platform that combines access control, incident detection, and automatic blocking of IoT devices. The solution was evaluated in a controlled environment with simulated attacks, achieving an average time of 28.6 seconds between detection and blocking. The results show a reduction in manual intervention, standardization of responses, and unification of the processes of registration, monitoring, and incident response.

</details>


### [43] [Blockchain-Anchored Audit Trail Model for Transparent Inter-Operator Settlement](https://arxiv.org/abs/2512.09938)
*Balakumar Ravindranath Kunthu,Ranganath Nagesh Taware,Sathish Krishna Anumula*

Main category: cs.CR

TL;DR: 区块链驱动的运营商间结算模型，实现结算周期从120天缩短至3分钟，交易费用降低87%


<details>
  <summary>Details</summary>
Motivation: 传统结算机制存在对账周期长、交易成本高、实时透明度有限等问题，运营成本约占收入的5%

Method: 采用分布式账本技术、智能合约自动化和加密验证，建立统一防篡改的交易记录框架

Result: 实证评估显示手动干预减少92%，结算争议消除88%，市场采用率从2020年8%升至2024年52%

Conclusion: 该框架可解决可扩展性（每秒12000笔交易）、互操作性和多司法管辖区合规性问题，年投资额预计达92亿美元

Abstract: The telecommunications and financial services industries face substantial challenges in inter-operator settlement processes, characterized by extended reconciliation cycles, high transaction costs, and limited real-time transparency. Traditional settlement mechanisms rely on multiple intermediaries and manual procedures, resulting in settlement periods exceeding 120 days with operational costs consuming approximately 5 percent of total revenue. This research presents a blockchain-anchored audit trail model enabling transparent, immutable, and automated inter-operator settlement. The framework leverages distributed ledger technology, smart contract automation, and cryptographic verification to establish a unified, tamper-proof transaction record. Empirical evaluation demonstrates 87 percent reduction in transaction fees, settlement cycle compression from 120 days to 3 minutes, and 100 percent audit trail integrity. Smart contract automation reduces manual intervention by 92 percent and eliminates 88 percent of settlement disputes. Market analysis indicates institutional adoption accelerated from 8 percent in 2020 to 52 percent by April 2024, with projected industry investment reaching 9.2 billion USD annually. The framework addresses scalability (12,000 transactions per second), interoperability, and regulatory compliance across multiple jurisdictions.

</details>


### [44] [ZK-APEX: Zero-Knowledge Approximate Personalized Unlearning with Executable Proofs](https://arxiv.org/abs/2512.09953)
*Mohammad M Maheri,Sunil Cotterill,Alex Davidson,Hamed Haddadi*

Main category: cs.CR

TL;DR: ZK APEX：首个实用的边缘设备可验证个性化机器学习遗忘框架，通过零知识证明在不暴露私有数据的情况下验证遗忘操作的准确性


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备场景下个性化模型的隐私数据删除需求，同时应对客户端可能忽略删除请求或虚假声称合规的挑战

Method: 结合服务端稀疏掩码和客户端轻量级Group OBS补偿步骤，使用分块经验Fisher矩阵进行曲率感知更新，配合Halo2零知识证明实现验证

Result: 在Vision Transformer分类任务中几乎完全恢复个性化精度；在OPT125M代码生成模型中恢复约70%原始精度；证明生成比基于重训练的验证快千万倍

Conclusion: 首次实现了边缘设备上可验证的个性化机器学习遗忘，为隐私保护提供了实用解决方案

Abstract: Machine unlearning aims to remove the influence of specific data points from a trained model to satisfy privacy, copyright, and safety requirements. In real deployments, providers distribute a global model to many edge devices, where each client personalizes the model using private data. When a deletion request is issued, clients may ignore it or falsely claim compliance, and providers cannot check their parameters or data. This makes verification difficult, especially because personalized models must forget the targeted samples while preserving local utility, and verification must remain lightweight on edge devices.
  We introduce ZK APEX, a zero-shot personalized unlearning method that operates directly on the personalized model without retraining. ZK APEX combines sparse masking on the provider side with a small Group OBS compensation step on the client side, using a blockwise empirical Fisher matrix to create a curvature-aware update designed for low overhead. Paired with Halo2 zero-knowledge proofs, it enables the provider to verify that the correct unlearning transformation was applied without revealing any private data or personalized parameters.
  On Vision Transformer classification tasks, ZK APEX recovers nearly all personalization accuracy while effectively removing the targeted information. Applied to the OPT125M generative model trained on code data, it recovers around seventy percent of the original accuracy. Proof generation for the ViT case completes in about two hours, more than ten million times faster than retraining-based checks, with less than one gigabyte of memory use and proof sizes around four hundred megabytes. These results show the first practical framework for verifiable personalized unlearning on edge devices.

</details>


### [45] [Cross-Layer Isochronous Diffusion Protocol (CIDP): A Rigorous Information-Theoretic and Control-Theoretic Framework for Sovereign Tactical Anonymity](https://arxiv.org/abs/2512.09954)
*Pravin G*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Next-generation tactical networks face a critical Anonymity Trilemma: it is impossible to simultaneously achieve strong anonymity, low latency (isochrony), and low bandwidth overhead under a global passive adversary. CIDP breaks this deadlock by injecting physical-layer entropy via rapid antenna sidelobe modulation, enabling near-isochronous, low-overhead anonymous communication. CIDP jointly designs: (a) a Lyapunov drift-plus-penalty network controller that stabilizes queues and maximizes entropy injection; (b) a robust discrete-time Control Barrier Function (RaCBF) filter that provably enforces deterministic jitter bounds for real-time flows despite uncertainty; and (c) a convex Sidelobe Time Modulation (SLTM) optimization that spreads signals into the antenna null-space to mask transmissions. We explicitly augment the classical anonymity bound with a physical-layer equivocation term, showing that rapidly changing sidelobes contribute additional secrecy. Consequently, as the injected physical entropy grows, both latency and dummy overhead can approach zero for a fixed anonymity target. We provide full theoretical proofs of queue stability, barrier-set invariance, and SLTM convexity. Moreover, we quantitatively benchmark our SLTM design against recent LPI/LPD schemes, demonstrating significantly lower intercept probability for comparable overhead. High-fidelity MATLAB/NS-3 simulations and an FPGA prototype validate CIDP: results show approximately 40% larger anonymity sets and 100% compliance with sub-30 ms jitter (compared to a Tor-like baseline), with only about 5% throughput loss. We also outline a Modular Open Systems Approach (MOSA) and FOCI-compliant supply-chain strategy. CIDP is the first architecture that simultaneously addresses strong anonymity, strict isochrony, and spectral efficiency with provable guarantees, making it highly relevant for sovereign JADC2 deployments.

</details>


### [46] [TRUCE: TRUsted Compliance Enforcement Service for Secure Health Data Exchange](https://arxiv.org/abs/2512.09959)
*Dae-young Kim,Karuna Pande Joshi*

Main category: cs.CR

TL;DR: TRUCE框架：基于AI/知识表示和语义网技术开发的自动化合规执行框架，用于安全数据交换和隐私法规遵从


<details>
  <summary>Details</summary>
Motivation: 组织间共享敏感个人身份信息(PII)日益增多，但HIPAA和Cures Act等法规存在冲突，增加了健康数据合规的复杂性，急需自动化合规解决方案

Method: 开发TRUCE框架，结合静态基础事实(法规如HIPAA)和动态基础事实(组织政策)，通过推理数据交换上下文、评估用户信任分数和数据真实性来实现合规自动化

Result: 框架在CDC接触者追踪患者数据(多达100万条记录)上针对HIPAA数据使用协议进行了验证

Conclusion: TRUCE服务能够简化合规工作，确保隐私法规遵从，可供组织实时管理大规模高速数据交换的合规性

Abstract: Organizations are increasingly sharing large volumes of sensitive Personally Identifiable Information (PII), like health records, with each other to better manage their services. Protecting PII data has become increasingly important in today's digital age, and several regulations have been formulated to ensure the secure exchange and management of sensitive personal data. However, at times some of these regulations are at loggerheads with each other, like the Health Insurance Portability and Accountability Act (HIPAA) and Cures Act; and this adds complexity to the already challenging task of Health Data compliance. As public concern regarding sensitive data breaches grows, finding solutions that streamline compliance processes and enhance individual privacy is crucial. We have developed a novel TRUsted Compliance Enforcement (TRUCE) framework for secure data exchange which aims to automate compliance procedures and enhance trusted data management within organizations. The TRUCE framework reasons over contexts of data exchange and assesses the trust score of users and the veracity of data based on corresponding regulations. This framework, developed using approaches from AI/Knowledge representation and Semantic Web technologies, includes a trust management method that incorporates static ground truth, represented by regulations such as HIPAA, and dynamic ground truth, defined by an organization's policies. In this paper, we present our framework in detail along with the validation against the Health Insurance Portability and Accountability Act (HIPAA) Data Usage Agreement (DUA) on CDC Contact Tracing patient data, up to one million patient records. TRUCE service will streamline compliance efforts and ensure adherence to privacy regulations and can be used by organizations to manage compliance of large velocity data exchange in real time.

</details>


### [47] [A Comparative Analysis of zk-SNARKs and zk-STARKs: Theory and Practice](https://arxiv.org/abs/2512.10020)
*Ayush Nainwal,Atharva Kamble,Nitin Awathare*

Main category: cs.CR

TL;DR: 对比分析zk-SNARKs和zk-STARKs在消费级ARM平台上的实际性能表现


<details>
  <summary>Details</summary>
Motivation: 零知识证明在安全和隐私保护计算中至关重要，但理论研究成果与实际性能表现之间存在认知差距

Method: 使用公开参考实现，在消费级ARM平台上进行系统性的实现级别比较，评估证明生成时间、验证延迟、证明大小和CPU分析

Result: zk-SNARKs证明生成快68倍，证明大小小123倍，但验证较慢且需要可信设置；zk-STARKs证明较大且生成较慢，但验证更快且具有透明性和后量子安全性

Conclusion: 两种系统存在不同的性能权衡，执行模型和实现细节显著影响实际性能，为开发者选择优化证明系统提供了实用见解

Abstract: Zero-knowledge proofs (ZKPs) are central to secure and privacy-preserving computation, with zk-SNARKs and zk-STARKs emerging as leading frameworks offering distinct trade-offs in efficiency, scalability, and trust assumptions. While their theoretical foundations are well studied, practical performance under real-world conditions remains less understood.
  In this work, we present a systematic, implementation-level comparison of zk-SNARKs (Groth16) and zk-STARKs using publicly available reference implementations on a consumer-grade ARM platform. Our empirical evaluation covers proof generation time, verification latency, proof size, and CPU profiling. Results show that zk-SNARKs generate proofs 68x faster with 123x smaller proof size, but verify slower and require trusted setup, whereas zk-STARKs, despite larger proofs and slower generation, verify faster and remain transparent and post-quantum secure. Profiling further identifies distinct computational bottlenecks across the two systems, underscoring how execution models and implementation details significantly affect real-world performance. These findings provide actionable insights for developers, protocol designers, and researchers in selecting and optimizing proof systems for applications such as privacy-preserving transactions, verifiable computation, and scalable rollups.

</details>


### [48] [LLM-PEA: Leveraging Large Language Models Against Phishing Email Attacks](https://arxiv.org/abs/2512.10104)
*Najmul Hassan,Prashanth BusiReddyGari,Haitao Zhao,Yihao Ren,Jinsheng Xu,Shaohu Zhang*

Main category: cs.CR

TL;DR: LLMPEA框架使用LLM检测多向量钓鱼邮件攻击，在90%准确率下有效但面临对抗性攻击风险


<details>
  <summary>Details</summary>
Motivation: 随着LLM应用部署增加，系统面临利用架构漏洞的钓鱼邮件威胁，现有LLM需强化以应对协同多向量攻击

Method: 提出LLMPEA框架，评估GPT-4o、Claude Sonnet 4和Grok-3三个前沿LLM，采用综合提示设计评估可行性、鲁棒性和局限性

Result: LLM能检测钓鱼邮件准确率超90%，但易受对抗性攻击、提示注入和多语言攻击利用

Conclusion: 研究为实际部署中的LLM钓鱼检测提供关键见解，特别是在攻击者组合利用多漏洞的现实场景中

Abstract: Email phishing is one of the most prevalent and globally consequential vectors of cyber intrusion. As systems increasingly deploy Large Language Models (LLMs) applications, these systems face evolving phishing email threats that exploit their fundamental architectures. Current LLMs require substantial hardening before deployment in email security systems, particularly against coordinated multi-vector attacks that exploit architectural vulnerabilities. This paper proposes LLMPEA, an LLM-based framework to detect phishing email attacks across multiple attack vectors, including prompt injection, text refinement, and multilingual attacks. We evaluate three frontier LLMs (e.g., GPT-4o, Claude Sonnet 4, and Grok-3) and comprehensive prompting design to assess their feasibility, robustness, and limitations against phishing email attacks. Our empirical analysis reveals that LLMs can detect the phishing email over 90% accuracy while we also highlight that LLM-based phishing email detection systems could be exploited by adversarial attack, prompt injection, and multilingual attacks. Our findings provide critical insights for LLM-based phishing detection in real-world settings where attackers exploit multiple vulnerabilities in combination.

</details>


### [49] [Watermarks for Language Models via Probabilistic Automata](https://arxiv.org/abs/2512.10185)
*Yangkun Wang,Jingbo Shang*

Main category: cs.CR

TL;DR: 提出基于概率自动机的水印方案，解决现有方法的生成多样性受限和检测开销高的问题


<details>
  <summary>Details</summary>
Motivation: 现有水印方案在无失真嵌入和抗编辑距离攻击方面表现良好，但存在生成多样性有限和检测开销高的问题，同时新的研究关注水印的不可检测性

Method: 通过概率自动机构建新一类水印方案，提出两种实现：(i)具有指数生成多样性和计算效率的实用方案，(ii)基于密码学假设具有形式化不可检测性保证的理论构造

Result: 在LLaMA-3B和Mistral-7B上的大量实验验证了方案在鲁棒性和效率方面的优越性能

Conclusion: 提出的概率自动机水印方案成功解决了现有方法的局限性，在保持鲁棒性的同时提高了生成多样性和检测效率

Abstract: A recent watermarking scheme for language models achieves distortion-free embedding and robustness to edit-distance attacks. However, it suffers from limited generation diversity and high detection overhead. In parallel, recent research has focused on undetectability, a property ensuring that watermarks remain difficult for adversaries to detect and spoof. In this work, we introduce a new class of watermarking schemes constructed through probabilistic automata. We present two instantiations: (i) a practical scheme with exponential generation diversity and computational efficiency, and (ii) a theoretical construction with formal undetectability guarantees under cryptographic assumptions. Extensive experiments on LLaMA-3B and Mistral-7B validate the superior performance of our scheme in terms of robustness and efficiency.

</details>


### [50] [FLARE: A Wireless Side-Channel Fingerprinting Attack on Federated Learning](https://arxiv.org/abs/2512.10296)
*Md Nahid Hasan Shuvo,Moinul Hossain,Anik Mallik,Jeffrey Twigg,Fikadu Dagefu*

Main category: cs.CR

TL;DR: FLARE是一种通过分析联邦学习客户端加密无线流量来识别深度学习模型架构的新型侧信道指纹识别攻击


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注联邦学习中直接的数据隐私威胁，但模型架构泄露可能引发针对性攻击，这一问题尚未被充分探索

Method: 利用流级和包级统计特征分析FL客户端的加密无线流量，开发基于机器学习的指纹识别框架

Result: 在闭世界场景下达到98% F1分数，开世界场景下达到91% F1分数

Conclusion: CNN和RNN模型会产生可区分的流量模式，当前FL系统存在严重的侧信道漏洞

Abstract: Federated Learning (FL) enables collaborative model training across distributed devices while safeguarding data and user privacy. However, FL remains susceptible to privacy threats that can compromise data via direct means. That said, indirectly compromising the confidentiality of the FL model architecture (e.g., a convolutional neural network (CNN) or a recurrent neural network (RNN)) on a client device by an outsider remains unexplored. If leaked, this information can enable next-level attacks tailored to the architecture. This paper proposes a novel side-channel fingerprinting attack, leveraging flow-level and packet-level statistics of encrypted wireless traffic from an FL client to infer its deep learning model architecture. We name it FLARE, a fingerprinting framework based on FL Architecture REconnaissance. Evaluation across various CNN and RNN variants-including pre-trained and custom models trained over IEEE 802.11 Wi-Fi-shows that FLARE achieves over 98% F1-score in closed-world and up to 91% in open-world scenarios. These results reveal that CNN and RNN models leak distinguishable traffic patterns, enabling architecture fingerprinting even under realistic FL settings with hardware, software, and data heterogeneity. To our knowledge, this is the first work to fingerprint FL model architectures by sniffing encrypted wireless traffic, exposing a critical side-channel vulnerability in current FL systems.

</details>


### [51] [Bit of a Close Talker: A Practical Guide to Serverless Cloud Co-Location Attacks](https://arxiv.org/abs/2512.10361)
*Wei Shao,Najmeh Nazari,Behnam Omidi,Setareh Rafatirad,Houman Homayoun,Khaled N. Khasawneh,Chongzhou Fang*

Main category: cs.CR

TL;DR: 论文研究无服务器云计算中调度算法的安全漏洞，提出了共定位攻击方法与防御策略


<details>
  <summary>Details</summary>
Motivation: 无服务器计算用户容易受到微架构侧信道攻击，这些攻击依赖于受害者和攻击者实例的物理共定位，需要研究无服务器云调度器的漏洞

Method: 提出了揭示无服务器调度算法可利用特征的全面方法，通过正常用户接口构建共定位攻击策略

Result: 在主流开源基础设施和Microsoft Azure Functions上成功揭示了可利用漏洞并实现了实例共定位

Conclusion: 当前云调度器需要安全增强，工作为强化无服务器计算环境提供了重要见解

Abstract: Serverless computing has revolutionized cloud computing by offering an efficient and cost-effective way for users to develop and deploy applications without managing infrastructure details. However, serverless cloud users remain vulnerable to various types of attacks, including micro-architectural side-channel attacks. These attacks typically rely on the physical co-location of victim and attacker instances, and attackers will need to exploit cloud schedulers to achieve co-location with victims. Therefore, it is crucial to study vulnerabilities in serverless cloud schedulers and assess the security of different serverless scheduling algorithms. This study addresses the gap in understanding and constructing co-location attacks in serverless clouds. We present a comprehensive methodology to uncover exploitable features in serverless scheduling algorithms and devise strategies for constructing co-location attacks through normal user interfaces. In our experiments, we successfully reveal exploitable vulnerabilities and achieve instance co-location on prevalent open-source infrastructures and Microsoft Azure Functions. We also present a mitigation strategy to defend against co-location attacks in serverless clouds. Our work highlights critical areas for security enhancements in current cloud schedulers, offering insights to fortify serverless computing environments against potential co-location attacks.

</details>


### [52] [From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection](https://arxiv.org/abs/2512.10485)
*Chaomeng Lu,Bert Lagaisse*

Main category: cs.CR

TL;DR: 基于深度学习的漏洞检测方法在现实应用中效果有限，现有模型在不同数据集上泛化能力差，在时间外分布数据集上性能显著下降


<details>
  <summary>Details</summary>
Motivation: 评估深度学习漏洞检测方法在实际应用中的有效性，检验现有模型在真实场景下的表现

Method: 系统评估ReVeal和LineVul两个代表性DL模型在四个数据集上的表现，使用t-SNE分析代码表示，并在新构建的时间外分布数据集VentiVul上测试模型性能

Result: 当前模型在表示空间中难以区分漏洞和非漏洞代码，跨数据集泛化能力差，在现实部署场景下性能大幅下降

Conclusion: 学术基准与现实部署存在显著差距，需要更鲁棒的代码表示和更高质量的数据集

Abstract: Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets, yet their real-world effectiveness remains underexplored. Recent work suggests that both graph neural network (GNN)-based and transformer-based models, including large language models (LLMs), yield promising results when evaluated on curated benchmark datasets. These datasets are typically characterized by consistent data distributions and heuristic or partially noisy labels. In this study, we systematically evaluate two representative DL models-ReVeal and LineVul-across four representative datasets: Juliet, Devign, BigVul, and ICVul. Each model is trained independently on each respective dataset, and their code representations are analyzed using t-SNE to uncover vulnerability related patterns. To assess realistic applicability, we deploy these models along with four pretrained LLMs, Claude 3.5 Sonnet, GPT-o3-mini, GPT-4o, and GPT-5 on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework and the need for more robust code representations and higher-quality datasets.

</details>


### [53] [Differential Privacy for Secure Machine Learning in Healthcare IoT-Cloud Systems](https://arxiv.org/abs/2512.10426)
*N Mangala,Murtaza Rangwala,S Aishwarya,B Eswara Reddy,Rajkumar Buyya,KR Venugopal,SS Iyengar,LM Patnaik*

Main category: cs.CR

TL;DR: 提出了一个多层IoT-Edge-Cloud架构，通过差分隐私框架和混合噪声机制来平衡医疗紧急响应的实时性与患者数据隐私保护


<details>
  <summary>Details</summary>
Motivation: 医疗物联网设备革命性发展使得远程患者监测和紧急响应成为可能，但如何在确保实时响应的同时保护患者隐私是重要挑战

Method: 采用基于响应危急性和存储持久性的任务分布式多层架构，提出差分隐私框架应用于多种机器学习模型，使用Laplace、Gaussian和混合噪声机制

Result: 在ε=5.0的实用阈值下，监督算法达到82-84%准确率，属性推理攻击减少18%，数据重建相关性降低70%；边缘计算实现8倍延迟减少

Conclusion: 提出的混合Laplace-Gaussian噪声机制与自适应预算分配在隐私-效用权衡方面表现平衡，分层架构验证了时间关键操作的可行性

Abstract: Healthcare has become exceptionally sophisticated, as wearables and connected medical devices are revolutionising remote patient monitoring, emergency response, medication management, diagnosis, and predictive and prescriptive analytics. Internet of Things and Cloud computing integrated systems (IoT-Cloud) facilitate sensing, automation, and processing for these healthcare applications. While real-time response is crucial for alleviating patient emergencies, protecting patient privacy is extremely important in data-driven healthcare. In this paper, we propose a multi-layer IoT, Edge and Cloud architecture to enhance the speed of response for emergency healthcare by distributing tasks based on response criticality and permanence of storage. Privacy of patient data is assured by proposing a Differential Privacy framework across several machine learning models such as K-means, Logistic Regression, Random Forest and Naive Bayes. We establish a comprehensive threat model identifying three adversary classes and evaluate Laplace, Gaussian, and hybrid noise mechanisms across varying privacy budgets, with supervised algorithms achieving up to 86% accuracy. The proposed hybrid Laplace-Gaussian noise mechanism with adaptive budget allocation provides a balanced approach, offering moderate tails and better privacy-utility trade-offs for both low and high dimension datasets. At the practical threshold of $\varepsilon = 5.0$, supervised algorithms achieve 82-84% accuracy while reducing attribute inference attacks by up to 18% and data reconstruction correlation by 70%. Blockchain security further ensures trusted communication through time-stamping, traceability, and immutability for analytics applications. Edge computing demonstrates 8$\times$ latency reduction for emergency scenarios, validating the hierarchical architecture for time-critical operations.

</details>


### [54] [Objectives and Design Principles in Offline Payments with Central Bank Digital Currency (CBDC)](https://arxiv.org/abs/2512.10636)
*David-Alexandre Guiraud,Andrea Tundis,Marc Winstel*

Main category: cs.CR

TL;DR: 本文探讨了具有离线功能的央行数字货币（CBDC）的基本设计原则及相应对策


<details>
  <summary>Details</summary>
Motivation: 随着数字货币的发展，需要研究支持离线功能的CBDC系统的安全架构设计

Method: 识别CBDC系统的三大核心目标并提出相应的设计对策

Result: 明确了各目标对应的具体设计元素，发现某些目标和对策之间不存在或只有最小干扰

Conclusion: 目标与具体设计元素的对应关系是CBDC设计的核心，例如通过安全硬件解决双花问题

Abstract: In this work, fundamental design principles for a central bank digital currency (CBDC) with an offline functionality and corresponding counter measures are discussed. We identify three major objectives for any such CBDC proposal:(i) Access Control Security - protection of a user's funds against unauthorized access by other users; (ii) Security against Depositor's Misbehavior - preservation of the integrity of an environment (potentially the wallet) against misbehavior of its owner (for example, double-spending), and (iii) Privacy by Design - ensuring privacy is embedded into the system architecture. Our central conclusion is the alignment of the objectives to concrete design elements as countermeasures, whereas certain objectives and countermeasures have no or minimal interferences with each other. For example, we work out that the integrity of a user's wallet and, accordingly, the prevention of double-spending race attacks should be addressed through the adoption and integration of \textit{secure hardware} within a CBDC system.

</details>


### [55] [Adaptive Intrusion Detection System Leveraging Dynamic Neural Models with Adversarial Learning for 5G/6G Networks](https://arxiv.org/abs/2512.10637)
*Neha,Tarunpreet Bhatia*

Main category: cs.CR

TL;DR: An advanced IDS framework using adversarial training and dynamic neural networks achieves 82.33% accuracy in 5G/6G network threat detection with reduced retraining needs.


<details>
  <summary>Details</summary>
Motivation: Traditional IDS approaches relying on signature-based methods struggle to detect novel and evolving attacks, and require costly retraining to update knowledge.

Method: The paper presents an advanced IDS framework that leverages adversarial training and dynamic neural networks, integrating incremental learning algorithms and using fewer features with statistical properties.

Result: Extensive evaluations using the NSL-KDD dataset demonstrate that the proposed approach provides better accuracy of 82.33% for multiclass classification of various network attacks while resisting dataset poisoning.

Conclusion: This research highlights the potential of adversarial-trained, dynamic neural networks for building resilient IDS solutions.

Abstract: Intrusion Detection Systems (IDS) are critical components in safeguarding 5G/6G networks from both internal and external cyber threats. While traditional IDS approaches rely heavily on signature-based methods, they struggle to detect novel and evolving attacks. This paper presents an advanced IDS framework that leverages adversarial training and dynamic neural networks in 5G/6G networks to enhance network security by providing robust, real-time threat detection and response capabilities. Unlike conventional models, which require costly retraining to update knowledge, the proposed framework integrates incremental learning algorithms, reducing the need for frequent retraining. Adversarial training is used to fortify the IDS against poisoned data. By using fewer features and incorporating statistical properties, the system can efficiently detect potential threats. Extensive evaluations using the NSL- KDD dataset demonstrate that the proposed approach provides better accuracy of 82.33% for multiclass classification of various network attacks while resisting dataset poisoning. This research highlights the potential of adversarial-trained, dynamic neural networks for building resilient IDS solutions.

</details>


### [56] [Virtual camera detection: Catching video injection attacks in remote biometric systems](https://arxiv.org/abs/2512.10653)
*Daniyar Kurmankhojayev,Andrei Shadrikov,Dmitrii Gordin,Mikhail Shkorin,Danijar Gabdullin,Aigerim Kambetbayeva,Kanat Kuatov*

Main category: cs.CR

TL;DR: 提出基于机器学习的虚拟摄像头检测方法，通过在真实用户会话中收集元数据进行训练，有效识别视频注入攻击


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造和虚拟摄像头技术的发展，视频注入攻击对远程生物认证系统构成严重威胁，现有文献对虚拟摄像头检测的实际实现和评估研究有限

Method: 基于机器学习的虚拟摄像头检测方法，利用真实用户会话期间收集的元数据训练模型

Result: 实证结果表明该方法能有效识别视频注入尝试，降低恶意用户绕过人脸防伪系统的风险

Conclusion: 该研究为虚拟摄像头检测提供了实用的机器学习解决方案，增强了人脸防伪系统的安全性

Abstract: Face anti-spoofing (FAS) is a vital component of remote biometric authentication systems based on facial recognition, increasingly used across web-based applications. Among emerging threats, video injection attacks -- facilitated by technologies such as deepfakes and virtual camera software -- pose significant challenges to system integrity. While virtual camera detection (VCD) has shown potential as a countermeasure, existing literature offers limited insight into its practical implementation and evaluation. This study introduces a machine learning-based approach to VCD, with a focus on its design and validation. The model is trained on metadata collected during sessions with authentic users. Empirical results demonstrate its effectiveness in identifying video injection attempts and reducing the risk of malicious users bypassing FAS systems.

</details>


### [57] [A Proof of Success and Reward Distribution Protocol for Multi-bridge Architecture in Cross-chain Communication](https://arxiv.org/abs/2512.10667)
*Damilare Peter Oyinloye,Mohd Sameen Chishti,Jingyue Li*

Main category: cs.CR

TL;DR: 本文提出PSCRD协议解决单桥区块链方案的集权化和单点故障风险，通过公平奖励分配机制激励多桥参与，提高去中心化程度


<details>
  <summary>Details</summary>
Motivation: 单桥区块链解决方案存在集权化和单点故障风险，需要新的多桥协调和激励机制来解决这些挑战

Method: 提出PSCRD协议，包括公平奖励分配系统，使用基尼系数和Nakamoto系数进行数学分析和模拟验证

Result: 模拟结果显示基尼系数显示奖励分配公平性逐步改善，Nakamoto系数显示去中心化程度显著提高

Conclusion: PSCRD协议能够在不大幅增加用户成本的前提下，提供更具弹性和安全性的跨链桥系统

Abstract: Single-bridge blockchain solutions enable cross-chain communication. However, they are associated with centralization and single-point-of-failure risks. This paper proposes Proof of Success and Reward Distribution (PSCRD), a novel multi-bridge response coordination and incentive distribution protocol designed to address the challenges. PSCRD introduces a fair reward distribution system that equitably distributes the transfer fee among participating bridges, incentivizing honest behavior and sustained commitment. The purpose is to encourage bridge participation for higher decentralization and lower single-point-of-failure risks. The mathematical analysis and simulation results validate the effectiveness of PSCRD using two key metrics: the Gini index, which demonstrates a progressive improvement in the fairness of the reward distribution as new bridge groups joined the network; and the Nakamoto coefficient, which shows a significant improvement in decentralization over time. These findings highlight that PSCRD provides a more resilient and secure cross-chain bridge system without substantially increasing user costs.

</details>


### [58] [TriHaRd: Higher Resilience for TEE Trusted Time](https://arxiv.org/abs/2512.10732)
*Matthieu Bettinger,Sonia Ben Mokhtar,Pascal Felber,Etienne Rivière,Valerio Schiavoni,Anthony Simonet-Boulogne*

Main category: cs.CR

TL;DR: 提出了TriHaRd协议，解决TEE环境中恶意主机操纵时间的问题，相比现有方案Triad具有更强的抵御时钟速度和偏移攻击的能力


<details>
  <summary>Details</summary>
Motivation: 在可信执行环境（如Intel SGX）中，时间源位于可信计算基之外，恶意主机可以操纵TEE的时间感知，包括跳跃时间或影响感知时间速度。现有方案Triad仍允许攻击者控制操作系统并任意操纵其TEE的感知时钟速度

Method: 通过拜占庭容错的时钟更新和一致性检查，构建高弹性的TEE可信时间协议TriHaRd

Result: 实验证明TriHaRd能够有效缓解针对Triad的已知攻击

Conclusion: TriHaRd协议在TEE可信时间维护方面相比Triad具有显著改进，提供了对时钟速度和偏移操纵的高阶抵御能力

Abstract: Accurately measuring time passing is critical for many applications. However, in Trusted Execution Environments (TEEs) such as Intel SGX, the time source is outside the Trusted Computing Base: a malicious host can manipulate the TEE's notion of time, jumping in time or affecting perceived time speed. Previous work (Triad) proposes protocols for TEEs to maintain a trustworthy time source by building a cluster of TEEs that collaborate with each other and with a remote Time Authority to maintain a continuous notion of passing time. However, such approaches still allow an attacker to control the operating system and arbitrarily manipulate their own TEE's perceived clock speed. An attacker can even propagate faster passage of time to honest machines participating in Triad's trusted time protocol, causing them to skip to timestamps arbitrarily far in the future. We propose TriHaRd, a TEE trusted time protocol achieving high resilience against clock speed and offset manipulations, notably through Byzantine-resilient clock updates and consistency checks. We empirically show that TriHaRd mitigates known attacks against Triad.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [59] [Search-based Software Testing Driven by Domain Knowledge: Reflections and New Perspectives](https://arxiv.org/abs/2512.10079)
*Federico Formica,Mark Lawford,Claudio Menghi*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Search-based Software Testing (SBST) can automatically generate test cases to search for requirements violations. Unlike manual test case development, it can generate a substantial number of test cases in a limited time. However, SBST does not possess the domain knowledge of engineers. Several techniques have been proposed to integrate engineers' domain knowledge within existing SBST frameworks. This paper will reflect on recent experimental results by highlighting bold and unexpected results. It will help re-examine SBST techniques driven by domain knowledge from a new perspective, suggesting new directions for future research.

</details>


### [60] [ATLAS: Automated Toolkit for Large-Scale Verified Code Synthesis](https://arxiv.org/abs/2512.10173)
*Mantas Baksys,Stefan Zetzsche,Olivier Bouissou,Remi Delmas,Soonho Kong*

Main category: cs.SE

TL;DR: ATLAS自动化流水线生成大量经过验证的Dafny程序来解决程序验证训练数据稀缺问题，基于合成数据微调Qwen 2.5 7B Coder在两个基准测试上分别提升23和50个百分点


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在程序验证方面展示出潜力，但进展受到已验证训练代码稀缺的限制

Method: 开发ATLAS自动化流水线，通过将合成过程分解为多个专门任务来生成带有规范、实现和证明的完整Dafny程序

Result: 生成2.7K个验证程序，从中提取超过19K个训练示例，基于这些数据微调的模型在DafnyBench和DafnySynthesis上分别获得+23和+50百分点的显著提升

Conclusion: 合成验证代码可以有效增强大型语言模型在形式化验证方面的能力

Abstract: Large language models have shown potential for program verification, but progress is hindered by the scarcity of verified code for training. We present ATLAS, an automated pipeline that synthesizes verified programs at scale to address this data bottleneck. ATLAS generates complete Dafny programs with specifications, implementations, and proofs, producing 2.7K verified programs from which we extract over 19K training examples--more than 7 per verified program--by decomposing the synthesis process into multiple specialized tasks. Fine-tuning Qwen 2.5 7B Coder on this dataset produces substantial gains: +23 percentage points on DafnyBench and +50 percentage points on DafnySynthesis. These results demonstrate that synthetic verified code can effectively enhance LLM capabilities for formal verification.

</details>


### [61] [Does SWE-Bench-Verified Test Agent Ability or Model Memory?](https://arxiv.org/abs/2512.10218)
*Thanosan Prathifkumar,Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: SWE-Bench-Verified基准测试可能存在训练数据重叠问题，导致模型得分反映的是训练记忆而非真实问题解决能力。特别是在文件定位任务上，模型在SWE-Bench-Verified上的表现显著优于其他基准，表明可能存在数据污染。


<details>
  <summary>Details</summary>
Motivation: 研究SWE-Bench-Verified基准测试中的数据污染问题，评估模型得分是否真实反映其解决GitHub问题的能力。

Method: 使用两个Claude模型，在仅提供问题文本和同时提供问题文本加文件路径两种设置下测试文件定位能力。比较模型在SWE-Bench-Verified、BeetleBox和SWE-rebench上的表现。

Result: 模型在SWE-Bench-Verified上的文件定位准确率是其他基准的3倍，编辑文件查找准确率是其他基准的6倍，尽管任务设计应逻辑上无法解决。

Conclusion: SWE-Bench-Verified的评估结果可能受到训练数据污染的影响，不能真实反映agent处理实际软件问题的能力。支持转向考虑污染问题的新基准数据集。

Abstract: SWE-Bench-Verified, a dataset comprising 500 issues, serves as a de facto benchmark for evaluating various large language models (LLMs) on their ability to resolve GitHub issues. But this benchmark may overlap with model training data. If that is true, scores may reflect training recall, not issue-solving skill. To study this, we test two Claude models that frequently appear in top-performing agents submitted to the benchmark. We ask them to find relevant files using only issue text, and then issue text plus file paths. We then run the same setup on BeetleBox and SWE-rebench. Despite both benchmarks involving popular open-source Python projects, models performed 3 times better on SWE-Bench-Verified. They were also 6 times better at finding edited files, without any additional context about the projects themselves. This gap suggests the models may have seen many SWE-Bench-Verified tasks during training. As a result, scores on this benchmark may not reflect an agent's ability to handle real software issues, yet it continues to be used in ways that can misrepresent progress and lead to choices that favour agents that use certain models over strong agent design. Our setup tests the localization step with minimal context to the extent that the task should be logically impossible to solve. Our results show the risk of relying on older popular benchmarks and support the shift toward newer datasets built with contamination in mind.

</details>


### [62] [Studying and Automating Issue Resolution for Software Quality](https://arxiv.org/abs/2512.10238)
*Antu Saha*

Main category: cs.SE

TL;DR: 通过LLM、机器学习和实证研究提升软件问题解决的三大方向：提升问题报告质量、分析开发工作流、自动化解决方案


<details>
  <summary>Details</summary>
Motivation: 开发者在问题解决过程中面临低质量报告、对实际工作流程理解有限以及缺乏自动化支持的挑战

Method: 1) 利用LLM推理和领域特定信息提升问题报告质量；2) 实证分析传统和AI增强系统中的开发工作流；3) 通过ML、DL和LLM方法自动化问题定位和解决方案识别

Result: 提供了实证洞见、实用工具和自动化方法，推动AI驱动的问题解决

Conclusion: 该研究通过综合方法显著提升了软件问题解决的效率和质量，为构建更可维护和高质量的软件系统提供了支持

Abstract: Effective issue resolution is crucial for maintaining software quality. Yet developers frequently encounter challenges such as low-quality issue reports, limited understanding of real-world workflows, and a lack of automated support. This research aims to address these challenges through three complementary directions. First, we enhance issue report quality by proposing techniques that leverage LLM reasoning and application-specific information. Second, we empirically characterize developer workflows in both traditional and AI-augmented systems. Third, we automate cognitively demanding resolution tasks, including buggy UI localization and solution identification, through ML, DL, and LLM-based approaches. Together, our work delivers empirical insights, practical tools, and automated methods to advance AI-driven issue resolution, supporting more maintainable and high-quality software systems.

</details>


### [63] [UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval](https://arxiv.org/abs/2512.10452)
*Yang Yang,Li Kuang,Jiakun Liu,Zhongxin Liu,Yingjie Xia,David Lo*

Main category: cs.SE

TL;DR: 提出UniCoR框架解决混合代码检索中的语义理解不足、模态融合低效和跨语言泛化弱三个挑战


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效利用混合查询（自然语言+代码片段）进行跨语言代码检索，存在语义理解不足、模态融合低效和跨语言泛化弱的问题

Method: 1) 多视角监督对比学习模块增强语义理解和模态融合；2) 表示分布一致性学习模块提升跨语言泛化能力

Result: 在基准测试中MRR提升8.64%，MAP提升11.54%，显著优于基线模型

Conclusion: UniCoR框架能有效提升混合代码检索的性能和跨语言泛化能力

Abstract: Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios.

</details>


### [64] [Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild](https://arxiv.org/abs/2512.10493)
*Binquan Zhang,Li Zhang,Haoyuan Zhang,Fang Liu,Song Wang,Bo Shen,An Fu,Lin Shi*

Main category: cs.SE

TL;DR: 对LMSYS-Chat-1M和WildChat数据集中人-LLM编码协作的实证分析，揭示了任务类型如何塑造交互模式、LLM在bug修复和代码重构中面临的挑战，以及用户满意度的差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对人-LLM编码协作机制的系统性探索，特别是用户在交互过程中的曲折路径、LLM指令遵循能力和用户满意度等问题。

Method: 使用LMSYS-Chat-1M和WildChat数据集进行实证分析，探讨人-LLM协作机制、LLM指令遵循能力和人类满意度。

Result: 1)任务类型决定交互模式（线性、星形、树形）；2)bug修复和代码重构对LLM指令遵循挑战更大；3)代码质量优化和需求驱动开发任务用户满意度较低。

Conclusion: 研究为人-LLM编码协作提供了重要见解，为改进LLM界面和用户满意度提供了建议，并为自适应对话系统的未来研究指明了方向。

Abstract: Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.

</details>


### [65] [Analyzing developer discussions on EU and US privacy legislation compliance in GitHub repositories](https://arxiv.org/abs/2512.10618)
*Georgia M. Kapitsaki,Maria Papoutsoglou,Christoph Treude,Ioanna Theophilou*

Main category: cs.SE

TL;DR: 通过分析GitHub上32,820个开源软件问题，研究发现开发者主要关注GDPR/CCPA隐私法规中的特定用户权利（删除权、选择退出权、访问权），并建立了包含6个集群24个类别的讨论分类体系


<details>
  <summary>Details</summary>
Motivation: 缺乏关于开源软件开发者为遵守隐私法规（如GDPR和CCPA）所讨论问题的实证证据

Method: 挖掘分析GitHub仓库中的32,820个问题，通过自动识别法律用户权利和原则，并手动分析1,186个问题样本

Result: 建立了6个集群24个类别的讨论分类：功能/缺陷、同意相关、文档、数据存储/共享、适应性、通用合规性

Conclusion: 分类体系可帮助开发者优先解决合规问题，教育界可据此调整课程培养未来工程师，研究界可识别改进领域以加速隐私法规合规

Abstract: Context: Privacy legislation has impacted the way software systems are developed, prompting practitioners to update their implementations. Specifically, the EU General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have forced the community to focus on users' data privacy. Despite the vast amount of data on developer issues available in GitHub repositories, there is a lack of empirical evidence on the issues developers of Open Source Software discuss to comply with privacy legislation. Method: In this work, we examine such discussions by mining and analyzing 32,820 issues from GitHub repositories. We partially analyzed the dataset automatically to identify law user rights and principles indicated, and manually analyzed a sample of 1,186 issues based on the type of concern addressed. Results: We devised 24 discussion categories placed in six clusters: features/bugs, consent-related, documentation, data storing/sharing, adaptability, and general compliance. Our results show that developers mainly focus on specific user rights from the legislation (right to erasure, right to opt-out, right to access), addressing other rights less frequently, while most discussions concern user consent, user rights functionality, bugs and cookies management. Conclusion: The created taxonomy can help practitioners understand which issues are discussed for law compliance, so that they ensure they address them first in their systems. In addition, the educational community can reshape curricula to better educate future engineers on the privacy law concerns raised, and the research community can identify gaps and areas for improvement to support and accelerate data privacy law compliance.

</details>


### [66] [Zorya: Automated Concolic Execution of Single-Threaded Go Binaries](https://arxiv.org/abs/2512.10799)
*Karolina Gorna,Nicolas Iooss,Yannick Seurin,Rida Khatoun*

Main category: cs.SE

TL;DR: 基于Zorya框架的专用符号执行方法显著提升了Go二进制漏洞检测效率


<details>
  <summary>Details</summary>
Motivation: Go语言在关键基础设施中的广泛应用增加了系统性漏洞检测的需求，但现有符号执行工具难以处理Go二进制文件的运行时复杂性和可扩展性挑战

Method: 在Zorya符号执行框架基础上增加具体未执行路径的bug检测和多层过滤机制，专注于可能导致panic的相关路径

Result: 评估显示panic可达性门控实现1.8-3.9倍加速，过滤33-70%的分支；Zorya检测到所有panic而现有工具最多检测两个

Conclusion: 专用符号执行可以在具有运行时安全检查的语言生态系统中实现实用的漏洞检测

Abstract: Go's adoption in critical infrastructure intensifies the need for systematic vulnerability detection, yet existing symbolic execution tools struggle with Go binaries due to runtime complexity and scalability challenges. In this work, we build upon Zorya, a concolic execution framework that translates Go binaries to Ghidra's P-Code intermediate representation to address these challenges. We added the detection of bugs in concretely not taken paths and a multi-layer filtering mechanism to concentrate symbolic reasoning on panic-relevant paths. Evaluation on five Go vulnerabilities demonstrates that panic-reachability gating achieves 1.8-3.9x speedups when filtering 33-70% of branches, and that Zorya detects all panics while existing tools detect at most two. Function-mode analysis proved essential for complex programs, running roughly two orders of magnitude faster than starting from main. This work establishes that specialized concolic execution can achieve practical vulnerability detection in language ecosystems with runtime safety checks.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [67] [An Efficient Graph-Transformer Operator for Learning Physical Dynamics with Manifolds Embedding](https://arxiv.org/abs/2512.10227)
*Pengwei Liu,Xingyu Ren,Pengkai Wang,Hangjie Yuan,Zhongkai Hao,Guanyu Chen,Chao Xu,Dong Ni,Shengze Cai*

Main category: cs.CE

TL;DR: PhysGTO：一种高效的图-Transformer算子，通过显式流形嵌入在物理空间和潜在空间中学习物理动力学，在保持线性复杂度的同时实现最先进的精度


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器在处理复杂几何、变化边界条件和多样化物理参数的动态场景时面临计算成本挑战，现有深度学习方法在非结构化网格上的灵活性和泛化性不足

Method: 提出PhysGTO算法，在物理空间使用统一图嵌入模块对齐节点级条件并构建稀疏但保持结构的图连接，在潜在空间集成轻量级通量导向消息传递与投影启发注意力机制

Result: 在涵盖11个数据集的综合基准测试中，PhysGTO始终实现最先进精度，同时显著降低计算成本

Conclusion: PhysGTO在各种模拟任务中展现出卓越的灵活性、可扩展性和泛化能力，适用于实时应用中的高效推理

Abstract: Accurate and efficient physical simulations are essential in science and engineering, yet traditional numerical solvers face significant challenges in computational cost when handling simulations across dynamic scenarios involving complex geometries, varying boundary/initial conditions, and diverse physical parameters. While deep learning offers promising alternatives, existing methods often struggle with flexibility and generalization, particularly on unstructured meshes, which significantly limits their practical applicability. To address these challenges, we propose PhysGTO, an efficient Graph-Transformer Operator for learning physical dynamics through explicit manifold embeddings in both physical and latent spaces. In the physical space, the proposed Unified Graph Embedding module aligns node-level conditions and constructs sparse yet structure-preserving graph connectivity to process heterogeneous inputs. In the latent space, PhysGTO integrates a lightweight flux-oriented message-passing scheme with projection-inspired attention to capture local and global dependencies, facilitating multilevel interactions among complex physical correlations. This design ensures linear complexity relative to the number of mesh points, reducing both the number of trainable parameters and computational costs in terms of floating-point operations (FLOPs), and thereby allowing efficient inference in real-time applications. We introduce a comprehensive benchmark spanning eleven datasets, covering problems with unstructured meshes, transient flow dynamics, and large-scale 3D geometries. PhysGTO consistently achieves state-of-the-art accuracy while significantly reducing computational costs, demonstrating superior flexibility, scalability, and generalization in a wide range of simulation tasks.

</details>


### [68] [Integrated Planning and Machine-Level Scheduling for High-Mix Discrete Manufacturing: A Profit-Driven Heuristic Framework](https://arxiv.org/abs/2512.10358)
*Runhao Liu,Ziming Chen,You Li,Zequn Xie,Peng Zhang*

Main category: cs.CE

TL;DR: 该论文提出了一个结合总体规划和机器级调度的双层框架，用于在复杂制造环境中优化生产效率和准时交付。


<details>
  <summary>Details</summary>
Motivation: 制造企业在多品种、小批量和急件订单条件下难以制定高效可靠的生产计划，特别是在高混合度的离散制造系统中。

Method: 采用了利润驱动的集成框架，包含混合整数规划模型和机器级调度启发式算法，采用分层设计分离规划层（生产分配、协同生产、外包）和调度层（执行可行性、机器行为稳定）。

Result: 评估显示：柔性执行策略实现73.3%的准时完成率但需要大量外包；稳定性强化策略实现100%准时完成率、零外包，且换型造成的产能损失仅为1.9-4.6%。

Conclusion: 研究表明，将规划决策与稳定性导向的执行规则相结合，能够在复杂制造环境中实现有效的利润最大化决策，稳定性优先方法在准时交付和资源利用方面表现更优。

Abstract: Modern manufacturing enterprises struggle to create efficient and reliable production schedules under multi-variety, small-batch, and rush-order conditions. High-mix discrete manufacturing systems require jointly optimizing mid-term production planning and machine-level scheduling under heterogeneous resources and stringent delivery commitments. We address this problem with a profit-driven integrated framework that couples a mixed-integer planning model with a machine-level scheduling heuristic. The planning layer allocates production, accessory co-production, and outsourcing under aggregate economic and capacity constraints, while the scheduling layer refines these allocations using a structure-aware procedure that enforces execution feasibility and stabilizes daily machine behavior. This hierarchical design preserves the tractability of aggregated optimization while capturing detailed operational restrictions. Evaluations are conducted on a real industrial scenario. A flexible machine-level execution scheme yields 73.3% on-time completion and significant outsourcing demand, revealing bottleneck congestion. In contrast, a stability-enforcing execution policy achieves 100% on-time completion, eliminates all outsourcing, and maintains balanced machine utilization with only 1.9 to 4.6% capacity loss from changeovers. These results show that aligning planning decisions with stability-oriented execution rules enables practical and interpretable profit-maximizing decisions in complex manufacturing environments.

</details>


### [69] [Robust Crop Planning under Uncertainty: Aligning Economic Optimality with Agronomic Sustainability](https://arxiv.org/abs/2512.10396)
*Runhao Liu,Ziming Chen,You Li,Peng Zhang*

Main category: cs.CE

TL;DR: 提出一个多层鲁棒作物规划框架（MLRCPF），整合空间推理、时间动态和鲁棒优化，用于优化复杂农业系统中的作物分配。


<details>
  <summary>Details</summary>
Motivation: 现有的作物规划方法往往隐含处理作物相互作用（如豆类-谷物互补性），或依赖静态确定性方法，难以应对市场和气候波动。

Method: 通过嵌入状态转移逻辑中的结构化交互矩阵来形式化作物间关系，并采用分布鲁棒优化层来减轻由数据驱动模糊集定义的最坏情况风险。

Result: 在华北真实高混合农业数据集上的评估显示，该框架能自主生成可持续的棋盘式轮作模式，恢复土壤肥力，豆类种植比例显著高于确定性基线。

Conclusion: 该框架成功解决了最优性与稳定性之间的权衡，强调了将领域特定结构先验显式编码到优化模型中对复杂农业系统弹性决策的重要性。

Abstract: Long-horizon agricultural planning requires optimizing crop allocation under complex spatial heterogeneity, temporal agronomic dependencies, and multi-source environmental uncertainty. Existing approaches often treat crop interactions, such as legume-cereal complementarity, which implicitly or rely on static deterministic formulations that fail to guarantee resilience against market and climate volatility. To address these challenges, we propose a Multi-Layer Robust Crop Planning Framework (MLRCPF) that integrates spatial reasoning, temporal dynamics, and robust optimization. Specifically, we formalize crop-to-crop relationships through a structured interaction matrix embedded within the state-transition logic, and employ a distributionally robust optimization layer to mitigate worst-case risks defined by a data-driven ambiguity set. Evaluations on a real-world high-mix farming dataset from North China demonstrate the effectiveness of the proposed approach. The framework autonomously generates sustainable checkerboard rotation patterns that restore soil fertility, significantly increasing the legume planting ratio compared to deterministic baselines. Economically, it successfully resolves the trade-off between optimality and stability. These results highlight the importance of explicitly encoding domain-specific structural priors into optimization models for resilient decision-making in complex agricultural systems.

</details>
