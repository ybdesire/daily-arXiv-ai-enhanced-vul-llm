<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 10]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [EMMap: A Systematic Framework for Spatial EMFI Mapping and Fault Classification on Microcontrollers](https://arxiv.org/abs/2512.09049)
*Gandham Sai Santhosh,Siddhartha Sanjay Naik,Ritwik Badola,Chester Rebeiro*

Main category: cs.CR

TL;DR: 提出了一个与平台无关的电磁故障注入空间映射与故障分类框架，用于系统性地分析微控制器对EMFI的敏感度


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏系统性的空间敏感性映射和故障行为分类方法，难以统一评估不同微控制器的EMFI脆弱性

Method: 基于O'Flynn和Kuhnapfel等人的研究，建立了空间EMFI映射和故障分类框架，并在三种代表性微控制器上进行了初步实验演示

Result: 开发了一个可重复的工作流程，研究人员可以用它来分析不同嵌入式架构的EMFI敏感性

Conclusion: 该框架为系统研究EMFI空间敏感性提供了方法论基础，有助于跨平台比较和脆弱性评估

Abstract: Electromagnetic Fault Injection (EMFI) is a powerful technique for inducing bit flips and instruction-level perturbations on microcontrollers, yet existing literature lacks a unified methodology for systematically mapping spatial sensitivity and classifying resulting fault behaviors. Building on insights from O'Flynn and Kuhnapfel et al., we introduce a platform-agnostic framework for Spatial EMFI Mapping and Fault Classification, aimed at understanding how spatial probe position influences fault outcomes. We present pilot experiments on three representative microcontroller targets including the Xtensa LX6 (ESP32) and two ChipWhisper boards not as definitive evaluations, but as illustrative demonstrations of how the proposed methodology can be applied in practice. These preliminary observations motivate a generalized and reproducible workflow that researchers can adopt when analyzing EMFI susceptibility across diverse embedded architectures.

</details>


### [2] [Exposing Vulnerabilities in Counterfeit Prevention Systems Utilizing Physically Unclonable Surface Features](https://arxiv.org/abs/2512.09150)
*Anirudh Nakra,Nayeeb Rashid,Chau-Wai Wong,Min Wu*

Main category: cs.CR

TL;DR: 纸质表面物理不可克隆特征（PUFs）认证系统存在安全隐患，面临物理拒绝服务和数字伪造攻击威胁，需加强安全措施。


<details>
  <summary>Details</summary>
Motivation: 当前基于纸质表面微观不规则性的防伪技术虽能实现低成本认证，但现有认证方法存在安全漏洞，导致技术可行性与实际安全部署之间存在差距。

Method: 通过形式化纸质PUF认证操作框架，设计物理拒绝服务攻击和数字伪造攻击，揭示系统在物理和数字领域的脆弱性。

Result: 设计的攻击有效凸显了纸质PUF认证系统的安全风险，证明现有方法易受对手破坏。

Conclusion: 需制定阶段性安全分析框架和针对性防护措施，以提升基于纸质PUF的防伪系统的可靠性和韧性，并为未来防伪系统设计提供指导。

Abstract: Counterfeit products pose significant risks to public health and safety through infiltrating untrusted supply chains. Among numerous anti-counterfeiting techniques, leveraging inherent, unclonable microscopic irregularities of paper surfaces is an accurate and cost-effective solution. Prior work of this approach has focused on enabling ubiquitous acquisition of these physically unclonable features (PUFs). However, we will show that existing authentication methods relying on paper surface PUFs may be vulnerable to adversaries, resulting in a gap between technological feasibility and secure real-world deployment. This gap is investigated through formalizing an operational framework for paper-PUF-based authentication. Informed by this framework, we reveal system-level vulnerabilities across both physical and digital domains, designing physical denial-of-service and digital forgery attacks to disrupt proper authentication. The effectiveness of the designed attacks underscores the strong need for security countermeasures for reliable and resilient authentication based on paper PUFs. The proposed framework further facilitates a comprehensive, stage-by-stage security analysis, guiding the design of future counterfeit prevention systems. This analysis delves into potential attack strategies, offering a foundational understanding of how various system components, such as physical features and verification processes, might be exploited by adversaries.

</details>


### [3] [Analysis of the Security Design, Engineering, and Implementation of the SecureDNA System](https://arxiv.org/abs/2512.09233)
*Alan T. Sherman,Jeremy J. Romanik Romano,Edward Zieglar,Enis Golaszewski,Jonathan D. Fuchs,William E. Byrd*

Main category: cs.CR

TL;DR: 对SecureDNA系统安全性的分析，发现其自定义认证协议存在结构性弱点，允许绕过速率限制并可能遭受响应篡改攻击


<details>
  <summary>Details</summary>
Motivation: 分析SecureDNA DNA合成筛选系统的安全性，特别是其系统设计、工程实现和密码学应用

Method: 通过源代码分析（版本1.0.8）检查密钥管理、证书基础设施、认证和速率限制机制，并进行形式化方法分析

Result: 发现SCEP协议仅实现单向认证，存在绕过速率限制和响应篡改的漏洞

Conclusion: 识别了结构性安全弱点并提出了缓解措施，软件版本1.1.0已通过SCEP+协议修复相关问题

Abstract: We analyze security aspects of the SecureDNA system regarding its system design, engineering, and implementation. This system enables DNA synthesizers to screen order requests against a database of hazards. By applying novel cryptography, the system aims to keep order requests and the database of hazards secret. Discerning the detailed operation of the system in part from source code (Version 1.0.8), our analysis examines key management, certificate infrastructure, authentication, and rate-limiting mechanisms. We also perform the first formal-methods analysis of the mutual authentication, basic request, and exemption-handling protocols.
  Without breaking the cryptography, our main finding is that SecureDNA's custom mutual authentication protocol SCEP achieves only one-way authentication: the hazards database and keyservers never learn with whom they communicate. This structural weakness violates the principle of defense in depth and enables an adversary to circumvent rate limits that protect the secrecy of the hazards database, if the synthesizer connects with a malicious or corrupted keyserver or hashed database. We point out an additional structural weakness that also violates the principle of defense in depth: inadequate cryptographic bindings prevent the system from detecting if responses, within a TLS channel, from the hazards database were modified. Consequently, if a synthesizer were to reconnect with the database over the same TLS session, an adversary could replay and swap responses from the database without breaking TLS. Although the SecureDNA implementation does not allow such reconnections, it would be stronger security engineering to avoid the underlying structural weakness. We identify these vulnerabilities and suggest and verify mitigations, including adding strong bindings. Software Version 1.1.0 fixes SCEP with our proposed SCEP+ protocol.

</details>


### [4] [ObliInjection: Order-Oblivious Prompt Injection Attack to LLM Agents with Multi-source Data](https://arxiv.org/abs/2512.09321)
*Ruiqi Wang,Yuqi Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 首个针对多源输入数据的提示注入攻击方法ObliInjection，通过顺序无关损失和orderGCG算法提高攻击成功率


<details>
  <summary>Details</summary>
Motivation: 现有提示注入攻击假设攻击者控制全部输入源或忽略多源输入中的顺序不确定性，在多源数据场景下效果有限

Method: 提出顺序无关损失函数来量化LLM完成攻击者指定任务的可能性，开发orderGCG算法优化污染段

Result: 在3个数据集和12个LLM上的实验证明，即使在6-100个输入段中只有1个被污染，ObliInjection仍高度有效

Conclusion: ObliInjection是首个专门针对多源输入数据的提示注入攻击方法，解决了现有方法的局限性

Abstract: Prompt injection attacks aim to contaminate the input data of an LLM to mislead it into completing an attacker-chosen task instead of the intended task. In many applications and agents, the input data originates from multiple sources, with each source contributing a segment of the overall input. In these multi-source scenarios, an attacker may control only a subset of the sources and contaminate the corresponding segments, but typically does not know the order in which the segments are arranged within the input. Existing prompt injection attacks either assume that the entire input data comes from a single source under the attacker's control or ignore the uncertainty in the ordering of segments from different sources. As a result, their success is limited in domains involving multi-source data.
  In this work, we propose ObliInjection, the first prompt injection attack targeting LLM applications and agents with multi-source input data. ObliInjection introduces two key technical innovations: the order-oblivious loss, which quantifies the likelihood that the LLM will complete the attacker-chosen task regardless of how the clean and contaminated segments are ordered; and the orderGCG algorithm, which is tailored to minimize the order-oblivious loss and optimize the contaminated segments. Comprehensive experiments across three datasets spanning diverse application domains and twelve LLMs demonstrate that ObliInjection is highly effective, even when only one out of 6-100 segments in the input data is contaminated.

</details>


### [5] [Proof of Trusted Execution: A Consensus Paradigm for Deterministic Blockchain Finality](https://arxiv.org/abs/2512.09409)
*Kyle Habib,Vladislav Kapitsyn,Giovanni Mazzeo,Faisal Mehrban*

Main category: cs.CR

TL;DR: 提出PoTE共识协议，利用TEEs实现高效的单轮验证共识，解决传统PoW和PoS的结构性问题。


<details>
  <summary>Details</summary>
Motivation: 解决PoW的高能耗、高延迟问题，以及PoS的权益集中化、长程攻击和无利害关系漏洞等结构约束。

Method: 验证器在异构VM-based TEEs中运行相同的规范程序，每个程序生成供应商支持的证明，将代码哈希与区块内容绑定，通过公开随机性唯一确定提议者。

Result: PoTE能够避免分叉，消除时隙时间瓶颈，并在单轮验证中提交区块。

Conclusion: PoTE通过利用可信执行环境实现单轮验证的共识机制，避免了传统共识协议的分叉问题，为高吞吐量应用提供了可行的解决方案。

Abstract: Current blockchain consensus protocols -- notably, Proof of Work (PoW) and Proof of Stake (PoS) -- deliver global agreement but exhibit structural constraints. PoW anchors security in heavy computation, inflating energy use and imposing high confirmation latency. PoS improves efficiency but introduces stake concentration, long-range and "nothing-at-stake" vulnerabilities, and a hard performance ceiling shaped by slot times and multi-round committee voting. In this paper, we propose Proof of Trusted Execution (PoTE), a consensus paradigm where agreement emerges from verifiable execution rather than replicated re-execution. Validators operate inside heterogeneous VM-based TEEs, each running the same canonical program whose measurement is publicly recorded, and each producing vendor-backed attestations that bind the enclave code hash to the block contents. Because the execution is deterministic and the proposer is uniquely derived from public randomness, PoTE avoids forks, eliminates slot.time bottlenecks, and commits blocks in a single round of verification. We present the design of a PoTE consensus client, describe our reference implementation, and evaluate its performance against the stringent throughput requirements of the Trillion decentralized exchange.

</details>


### [6] [Reference Recommendation based Membership Inference Attack against Hybrid-based Recommender Systems](https://arxiv.org/abs/2512.09442)
*Xiaoxiao Chi,Xuyun Zhang,Yan Wang,Hongsheng Hu,Wanchun Dou*

Main category: cs.CR

TL;DR: 本文提出了一种针对混合推荐系统的成员推理攻击方法，利用个性化推荐特性来提高攻击效果


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击方法未充分利用推荐系统的个性化特性，且仅适用于混合两种推荐算法的系统，无法有效攻击基于单一算法的混合推荐系统

Method: 提出基于度量的成员推理攻击：利用个性化特性获取目标用户的参考推荐，然后提出相对成员度量来结合用户历史交互、目标推荐和参考推荐进行成员推断

Result: 理论上和经验上都证明了所提出的基于度量的成员推理攻击方法在混合推荐系统中的有效性

Conclusion: 论文填补了混合推荐系统成员推理攻击的研究空白，证明了个性化特性对成员推理攻击的重要影响

Abstract: Recommender systems have been widely deployed across various domains such as e-commerce and social media, and intelligently suggest items like products and potential friends to users based on their preferences and interaction history, which are often privacy-sensitive. Recent studies have revealed that recommender systems are prone to membership inference attacks (MIAs), where an attacker aims to infer whether or not a user's data has been used for training a target recommender system. However, existing MIAs fail to exploit the unique characteristic of recommender systems, and therefore are only applicable to mixed recommender systems consisting of two recommendation algorithms. This leaves a gap in investigating MIAs against hybrid-based recommender systems where the same algorithm utilizing user-item historical interactions and attributes of users and items serves and produces personalised recommendations. To investigate how the personalisation in hybrid-based recommender systems influences MIA, we propose a novel metric-based MIA. Specifically, we leverage the characteristic of personalisation to obtain reference recommendation for any target users. Then, a relative membership metric is proposed to exploit a target user's historical interactions, target recommendation, and reference recommendation to infer the membership of the target user's data. Finally, we theoretically and empirically demonstrate the efficacy of the proposed metric-based MIA on hybrid-based recommender systems.

</details>


### [7] [Chasing Shadows: Pitfalls in LLM Security Research](https://arxiv.org/abs/2512.09549)
*Jonathan Evertz,Niklas Risse,Nicolai Neuer,Andreas Müller,Philipp Normann,Gaetano Sapia,Srishti Gupta,David Pape,Soumya Shaw,Devansh Srivastav,Christian Wressnegger,Erwin Quiring,Thorsten Eisenhofer,Daniel Arp,Lea Schönherr*

Main category: cs.CR

TL;DR: 这篇论文识别了LLM安全研究中常见的九个陷阱，分析了72篇论文发现每个都存在至少一个陷阱，并通过案例研究展示了这些陷阱对研究有效性的影响，最后提出了改进指南。


<details>
  <summary>Details</summary>
Motivation: LLMs在安全研究中的普及带来了可重复性、严谨性和评估方面的挑战，现有研究对LLM特有的陷阱缺乏系统分析。

Method: 通过审查2023-2024年间72篇顶级安全与软件工程会议的论文，识别九类常见陷阱，并进行四项实证案例研究。

Result: 所有被分析的论文都存在至少一个陷阱，平均每个陷阱出现在多篇论文中，但仅有15.7%的陷阱被明确讨论。

Conclusion: LLM安全研究普遍存在未被充分认识的陷阱，需要社区采纳论文提出的行动指南来提升研究质量。

Abstract: Large language models (LLMs) are increasingly prevalent in security research. Their unique characteristics, however, introduce challenges that undermine established paradigms of reproducibility, rigor, and evaluation. Prior work has identified common pitfalls in traditional machine learning research, but these studies predate the advent of LLMs. In this paper, we identify \emph{nine} common pitfalls that have become (more) relevant with the emergence of LLMs and that can compromise the validity of research involving them. These pitfalls span the entire computation process, from data collection, pre-training, and fine-tuning to prompting and evaluation.
  We assess the prevalence of these pitfalls across all 72 peer-reviewed papers published at leading Security and Software Engineering venues between 2023 and 2024. We find that every paper contains at least one pitfall, and each pitfall appears in multiple papers. Yet only 15.7\% of the present pitfalls were explicitly discussed, suggesting that the majority remain unrecognized. To understand their practical impact, we conduct four empirical case studies showing how individual pitfalls can mislead evaluation, inflate performance, or impair reproducibility. Based on our findings, we offer actionable guidelines to support the community in future work.

</details>


### [8] [Defining Cost Function of Steganography with Large Language Models](https://arxiv.org/abs/2512.09769)
*Hanzhou Wu,Yige Wang*

Main category: cs.CR

TL;DR: 首篇利用大语言模型定义隐写术成本函数的研究，通过两阶段策略显著提升抗隐写分析能力


<details>
  <summary>Details</summary>
Motivation: 传统方法过度依赖专家知识或需要大规模数据集进行成本学习，LLMs为此提供了新的可能性

Method: 结合LLM引导的程序合成与进化搜索的两阶段策略：首阶段从结构化提示生成程序形式的成本函数，次阶段通过重新训练隐写分析模型评估优化

Result: 实验表明该方法设计的成本函数在抵抗隐写分析工具方面显著优于现有工作

Conclusion: 这是首次将LLMs应用于高级隐写术成本函数设计，为隐写术设计提供了新视角

Abstract: In this paper, we make the first attempt towards defining cost function of steganography with large language models (LLMs), which is totally different from previous works that rely heavily on expert knowledge or require large-scale datasets for cost learning. To achieve this goal, a two-stage strategy combining LLM-guided program synthesis with evolutionary search is applied in the proposed method. In the first stage, a certain number of cost functions in the form of computer program are synthesized from LLM responses to structured prompts. These cost functions are then evaluated with pretrained steganalysis models so that candidate cost functions suited to steganography can be collected. In the second stage, by retraining a steganalysis model for each candidate cost function, the optimal cost function(s) can be determined according to the detection accuracy. This two-stage strategy is performed by an iterative fashion so that the best cost function can be collected at the last iteration. Experiments show that the proposed method enables LLMs to design new cost functions of steganography that significantly outperform existing works in terms of resisting steganalysis tools, which verifies the superiority of the proposed method. To the best knowledge of the authors, this is the first work applying LLMs to the design of advanced cost function of steganography, which presents a novel perspective for steganography design and may shed light on further research.

</details>


### [9] [FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning](https://arxiv.org/abs/2512.09872)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.CR

TL;DR: FlipLLM是一个基于强化学习的框架，用于高效发现大型AI模型中的位翻转攻击漏洞，比现有方法快2.5倍，并能指导硬件防护措施


<details>
  <summary>Details</summary>
Motivation: 现有的位翻转攻击发现方法缺乏通用性和可扩展性，难以在合理时间内分析现代基础模型的庞大参数空间和复杂依赖关系

Method: FlipLLM将位翻转攻击发现建模为顺序决策问题，结合灵敏度引导的层剪枝和Q学习来识别最小但影响最大的位集合

Result: FlipLLM在多个模型上表现出色：LLaMA 3.1 8B准确率从69.9%降至0.2%（仅翻转5位），LLaVA的VQA分数从78%降至接近0%（仅翻转7位）

Conclusion: FlipLLM为语言和多模态基础模型提供了首个可扩展的自适应方法，用于探索位翻转攻击漏洞，为标准硬件防护机制提供了实用指导

Abstract: Generative Artificial Intelligence models, such as Large Language Models (LLMs) and Large Vision Models (VLMs), exhibit state-of-the-art performance but remain vulnerable to hardware-based threats, specifically bit-flip attacks (BFAs). Existing BFA discovery methods lack generalizability and struggle to scale, often failing to analyze the vast parameter space and complex interdependencies of modern foundation models in a reasonable time. This paper proposes FlipLLM, a reinforcement learning (RL) architecture-agnostic framework that formulates BFA discovery as a sequential decision-making problem. FlipLLM combines sensitivity-guided layer pruning with Q-learning to efficiently identify minimal, high-impact bit sets that can induce catastrophic failure. We demonstrate the effectiveness and generalizability of FlipLLM by applying it to a diverse set of models, including prominent text-only LLMs (GPT-2 Large, LLaMA 3.1 8B, and DeepSeek-V2 7B), VLMs such as LLaVA 1.6, and datasets, such as MMLU, MMLU-Pro, VQAv2, and TextVQA. Our results show that FlipLLM can identify critical bits that are vulnerable to BFAs up to 2.5x faster than SOTA methods. We demonstrate that flipping the FlipLLM-identified bits plummets the accuracy of LLaMA 3.1 8B from 69.9% to ~0.2%, and for LLaVA's VQA score from 78% to almost 0%, by flipping as few as 5 and 7 bits, respectively. Further analysis reveals that applying standard hardware protection mechanisms, such as ECC SECDED, to the FlipLLM-identified bit locations completely mitigates the BFA impact, demonstrating the practical value of our framework in guiding hardware-level defenses. FlipLLM offers the first scalable and adaptive methodology for exploring the BFA vulnerability of both language and multimodal foundation models, paving the way for comprehensive hardware-security evaluation.

</details>


### [10] [ByteShield: Adversarially Robust End-to-End Malware Detection through Byte Masking](https://arxiv.org/abs/2512.09883)
*Daniel Gibert,Felip Manyà*

Main category: cs.CR

TL;DR: 提出一种新颖的防御机制，通过字节级掩码和阈值投票来增强端到端恶意软件检测器对抗对抗性攻击的能力。该机制优于现有的随机化和去随机化平滑防御。


<details>
  <summary>Details</summary>
Motivation: 当前基于随机化和去随机化平滑的端到端恶意软件检测器防御机制在面对大规模对抗性载荷插入攻击时仍然脆弱，需要更有效的解决方案。

Method: 采用确定性掩码策略，系统性地在输入文件上滑动掩码生成多个掩码版本，独立分类每个版本，并通过基于阈值的投票机制产生最终分类。

Result: 在EMBER和BODMAS数据集上的实验表明，该防御机制在对抗各种功能保持操作生成的对抗样本时优于随机化和去随机化平滑防御，同时在干净样本上保持高准确率。

Conclusion: 所提出的字节级掩码防御机制能够有效抵消对抗性载荷的影响，通过在最佳情况下完全遮挡或在最坏情况下部分遮挡对抗性载荷，确保投票机制能够抑制其影响，提供更强大的恶意软件检测保护。

Abstract: Research has proven that end-to-end malware detectors are vulnerable to adversarial attacks. In response, the research community has proposed defenses based on randomized and (de)randomized smoothing. However, these techniques remain susceptible to attacks that insert large adversarial payloads. To address these limitations, we propose a novel defense mechanism designed to harden end-to-end malware detectors by leveraging masking at the byte level. This mechanism operates by generating multiple masked versions of the input file, independently classifying each version, and then applying a threshold-based voting mechanism to produce the final classification. Key to this defense is a deterministic masking strategy that systematically strides a mask across the entire input file. Unlike randomized smoothing defenses, which randomly mask or delete bytes, this structured approach ensures coverage of the file over successive versions. In the best-case scenario, this strategy fully occludes the adversarial payload, effectively neutralizing its influence on the model's decision. In the worst-case scenario, it partially occludes the adversarial payload, reducing its impact on the model's predictions. By occluding the adversarial payload in one or more masked versions, this defense ensures that some input versions remain representative of the file's original intent, allowing the voting mechanism to suppress the influence of the adversarial payload. Results achieved on the EMBER and BODMAS datasets demonstrate the suitability of our defense, outperforming randomized and (de)randomized smoothing defenses against adversarial examples generated with a wide range of functionality-preserving manipulations while maintaining high accuracy on clean examples.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study](https://arxiv.org/abs/2512.09088)
*Adrian Ryser,Florian Allwein,Tim Schlippe*

Main category: cs.AI

TL;DR: 论文研究大型语言模型(LLM)的幻觉现象如何影响用户对LLM的信任及交互行为，通过定性研究发现幻觉不会导致全面不信任，而是引发情境敏感的信任校准


<details>
  <summary>Details</summary>
Motivation: 探索LLM产生的看似合理但事实错误的幻觉输出如何影响用户的信任和交互模式

Method: 对192名参与者进行定性研究，基于Lee & See的校准信任模型和Afroogh等人的信任相关因素框架

Result: 确认期望值、先前经验、用户专业知识为信任相关因素，并识别直觉作为幻觉检测的额外因素；信任动态还受情境因素影响

Conclusion: 验证了递归信任校准过程，提出负责任和反思性使用LLM的实践建议

Abstract: Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Blöbaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.

</details>


### [12] [AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance](https://arxiv.org/abs/2512.09114)
*Pamela Gupta*

Main category: cs.AI

TL;DR: AI TIPS 2.0框架解决了当前AI治理的三大挑战：用例风险评估不足、框架缺乏可操作性、规模化实施机制缺失，提供了具体的控制措施和实施方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI治理框架存在三个关键挑战：1) 缺乏针对具体用例的风险评估；2) 现有框架停留在概念层面，缺乏可操作控制；3) 组织缺乏规模化实施治理的机制。这些问题在Humana集体诉讼等案例中已显现严重后果。

Method: 提出了AI TIPS（人工智能信任集成支柱可持续性2.0）框架，这是对2019年开发的全面操作框架的更新。该框架提供具体的控制措施、风险评估方法和规模化实施机制。

Result: AI TIPS框架能够直接解决现有治理框架的不足，为组织提供将治理要求转化为具体技术实现的能力，并在整个开发生命周期中嵌入可信AI实践。

Conclusion: AI TIPS 2.0框架通过提供可操作的治理控制措施、针对具体用例的风险评估方法以及系统化的规模化实施机制，有效解决了当前AI治理框架的三大挑战，为组织实现可信赖AI提供了实用的解决方案。

Abstract: The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.

</details>


### [13] [A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem](https://arxiv.org/abs/2512.09117)
*Luciano Floridi,Yiyang Jia,Fernando Tohmé*

Main category: cs.AI

TL;DR: 本文提出一个形式化范畴框架分析人类和大型语言模型如何将内容转化为关于可能世界状态空间的真值命题，论证LLMs并非解决而是绕过了符号接地问题。


<details>
  <summary>Details</summary>
Motivation: 传统符号接地问题关注如何将符号与真实世界体验连接，本研究旨在分析LLMs在缺乏真实世界体验的情况下如何处理语义内容。

Method: 建立范畴理论框架，通过可能世界状态空间W的形式化模型，对比分析人类与LLMs生成真值命题的认知机制差异。

Result: 发现LLMs通过统计模式匹配在可能世界状态空间中生成连贯命题，但缺乏人类式的感知-动作循环基础。

Conclusion: LLMs实际上规避而非解决了符号接地问题，其语义理解建立在表面统计规律而非具身认知基础上。

Abstract: This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.

</details>


### [14] [Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration](https://arxiv.org/abs/2512.09340)
*Chethana Prasad Kabgere*

Main category: cs.AI

TL;DR: 比较人类和AI系统在模糊视觉刺激解释中的差异，探索生物与人工系统在感知、推理和决策方面的异同


<details>
  <summary>Details</summary>
Motivation: 理解人类和AI系统如何解释模糊视觉刺激对于揭示感知、推理和决策的本质至关重要

Method: 结合计算认知科学、认知架构和连接主义-符号混合模型，对比人类策略与AI的特征处理，使用Grad-CAM可视化模型注意力，并通过ACT-R和Soar模型分析人类行为

Result: 揭示了生物和人工系统在表征、推理和置信度校准方面的关键相似点和差异

Conclusion: 研究促进了未来神经符号架构的发展，这些架构将结构化符号推理与连接主义表征相结合，为实现可解释且具有认知基础的AI系统提供了路径

Abstract: Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.

</details>


### [15] [Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search](https://arxiv.org/abs/2512.09566)
*Junkai Ji,Zhangfan Yang,Dong Xu,Ruibin Bai,Jianqiang Li,Tingjun Hou,Zexuan Zhu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.

</details>


### [16] [Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions](https://arxiv.org/abs/2512.09727)
*Junlin Xiao,Victor-Alexandru Darvariu,Bruno Lacerda,Nick Hawes*

Main category: cs.AI

TL;DR: 提出一种使用高斯过程回归聚合并行MCTS统计信息的方法，在连续动作空间中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 在连续动作空间环境中，如何最优地聚合来自不同线程的并行MCTS统计信息是一个重要但研究不足的问题

Method: 使用高斯过程回归获取未在环境中试验的有希望动作的价值估计

Result: 在6个不同领域的系统评估中，该方法优于现有的聚合策略，推理时间增加适中

Conclusion: 该方法有效解决了并行MCTS在连续动作空间中的统计聚合问题

Abstract: Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.

</details>


### [17] [RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning](https://arxiv.org/abs/2512.09829)
*Khurram Khalil,Muhammad Mahad Khaliq,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: RIFT是一种基于强化学习的智能故障定位框架，用于AI加速器的设计阶段故障评估，相比传统方法可加速2.2倍并减少99%的测试向量，同时提供更好的故障覆盖率和成本效益。


<details>
  <summary>Details</summary>
Motivation: 现代AI加速器规模巨大，传统故障评估方法计算成本过高且关键故障模式覆盖不足，需要一种可扩展的自动化解决方案。

Method: 将最坏情况故障搜索转化为顺序决策问题，结合混合敏感性分析进行搜索空间剪枝，使用强化学习智能生成最小化高影响力测试套件。

Result: 在亿参数大语言模型工作负载和NVIDIA A100 GPU上评估，相比进化方法加速2.2倍，相比随机故障注入减少99%测试向量，同时获得更优故障覆盖率。选择性错误校正代码的成本效益比统一三重模块冗余保护提高12.8倍。

Conclusion: RIFT框架能自动化发现最小化高影响力故障场景，生成可直接集成到商业RTL验证流程的UVM兼容验证工件，为智能硬件保护策略提供可操作数据。

Abstract: The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \textbf{2.2$\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \textbf{99\%} compared to random fault injection, all while achieving \textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \textbf{12.8$\times$} improvement in \textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.

</details>


### [18] [Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science](https://arxiv.org/abs/2512.09895)
*Jane Greenberg,Scott McClellan,Addy Ireland,Robert Sammarco,Colton Gerber,Christopher B. Rauch,Mat Kelly,John Kunze,Yuan An,Eric Toberer*

Main category: cs.AI

TL;DR: 本文介绍了MatSci-YAMZ平台，该平台融合人工智能和众包等人在环方法，用于支持材料科学领域的元数据词汇开发。通过概念验证案例，展示了AI-人在环模型在生成术语定义方面的可行性，并强调了其与FAIR原则的契合及跨领域扩展潜力。


<details>
  <summary>Details</summary>
Motivation: 元数据词汇对推进FAIR和FARR数据原则至关重要，但其发展受限于人力资源不足和标准化实践不一致的问题。

Method: 平台采用人工智能与人在环（包括众包）相结合的方法。在材料科学领域进行概念验证，6名参与者通过平台贡献术语定义并提供示例，以促进AI生成定义的优化。

Result: 成功生成了19个AI生成的术语定义，并通过迭代反馈循环验证了AI-人在环优化的可行性。研究证实了该模型的可行性，突出表现在概念验证成功、与FAIR和开放科学原则一致、提供了未来研究指南以及具备跨领域扩展潜力。

Conclusion: MatSci-YAMZ底层模型能够增强语义透明度，并减少达成共识和开发元数据词汇所需的时间。

Abstract: Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.

</details>


### [19] [SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments](https://arxiv.org/abs/2512.09897)
*Haoye Lu,Pavan Seshadri,Kaheer Suleman*

Main category: cs.AI

TL;DR: SCOPE是一种一次性分层规划器，利用LLM生成的子目标进行轻量级学生模型预训练，在保证性能的同时显著提升效率


<details>
  <summary>Details</summary>
Motivation: 解决基于文本的环境中长期规划的挑战：开放动作空间、模糊观察和稀疏反馈，同时降低对LLM的依赖以提高效率

Method: 采用LLM生成子目标进行一次性初始化预训练，而非在训练过程中反复查询LLM，通过示例轨迹直接派生子目标

Result: 在TextCraft环境中成功率从0.52提升至0.56，推理时间从164.4秒大幅减少到3.0秒

Conclusion: LLM生成的子目标即使不是最优的，仍可作为文本规划任务中分层目标分解的良好起点，在效率和性能间取得平衡

Abstract: Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [20] [CNFinBench: A Benchmark for Safety and Compliance of Large Language Models in Finance](https://arxiv.org/abs/2512.09506)
*Jinru Ding,Chao Ding,Wenrao Pang,Boyi Xiao,Zhiqiang Liu,Pengcheng Chen,Jiayuan Chen,Tiantian Yuan,Junming Guan,Yidong Jiang,Dawei Cheng,Jie Xu*

Main category: cs.CE

TL;DR: CNFinBench是一个专门针对金融领域的安全评估基准，通过多轮对抗对话和Capability-Compliance-Safety框架来评估大型语言模型的安全性，发现模型在合规性和风险控制方面存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有金融基准主要关注教科书式问答和数值问题，但缺乏对模型实际安全行为的评估，无法有效测试监管合规、对抗攻击、工具滥用等风险。

Method: 设计了金融定制的红队对话基准，包括基于长报告的证据推理和司法感知的规则/税务合规任务，引入Harmful Instruction Compliance Score（HICS）量化模型在多轮对抗对话中的抗有害提示能力，并采用可审计的混合评估协议。

Result: 在21个模型和15个子任务上的实验显示，模型在能力任务上平均得分为61.0，但在合规和风险控制评估中降至34.18；多轮对抗对话测试中，多数系统仅达到部分抵抗（HICS 60-79）。

Conclusion: 仅靠拒绝不足以确保安全性，模型需要提供可引用和可验证的推理；CNFinBench为金融领域的安全评估提供了更全面的框架。

Abstract: Large language models are increasingly deployed across the financial sector for tasks such as research, compliance, risk analysis, and customer service, which makes rigorous safety evaluation essential. However, existing financial benchmarks primarily focus on textbook-style question answering and numerical problem solving, but fail to evaluate models' real-world safety behaviors. They weakly assess regulatory compliance and investor-protection norms, rarely stress-test multi-turn adversarial tactics such as jailbreaks or prompt injection, inconsistently ground answers in long filings, ignore tool- or RAG-induced over-reach risks, and rely on opaque or non-auditable evaluation protocols. To close these gaps, we introduce CNFinBench, a benchmark that employs finance-tailored red-team dialogues and is structured around a Capability-Compliance-Safety triad, including evidence-grounded reasoning over long reports and jurisdiction-aware rule/tax compliance tasks. For systematic safety quantification, we introduce the Harmful Instruction Compliance Score (HICS) to measure how consistently models resist harmful prompts across multi-turn adversarial dialogues. To ensure auditability, CNFinBench enforces strict output formats with dynamic option perturbation for objective tasks and employs a hybrid LLM-ensemble plus human-calibrated judge for open-ended evaluations. Experiments on 21 models across 15 subtasks confirm a persistent capability-compliance gap: models achieve an average score of 61.0 on capability tasks but fall to 34.18 on compliance and risk-control evaluations. Under multi-turn adversarial dialogue tests, most systems reach only partial resistance (HICS 60-79), demonstrating that refusal alone is not a reliable proxy for safety without cited and verifiable reasoning.

</details>


### [21] [A roadmap of geospatial soil quality analysis systems](https://arxiv.org/abs/2512.09817)
*Habiba BEN ABDERRAHMANE,Slimane Oulad-Naoui,Benameur ZIANI*

Main category: cs.CE

TL;DR: 本文提出了一种整合GIS、遥感和机器学习的统一模块化管道，用于高效、透明和可扩展的土壤质量评估，克服传统方法的局限性，并探讨未来发展趋势。


<details>
  <summary>Details</summary>
Motivation: 传统土壤质量评估方法依赖昂贵、劳动密集型的采样和实验室分析，限制了其空间和时间覆盖范围。GIS、遥感和机器学习的发展为高效土壤质量评估提供了可能。

Method: 通过整合多源土壤数据、GIS和遥感工具以及机器学习技术，构建了一个统一的模块化管道，用于土壤质量评估。

Result: 提出了一个综合路线图，区别于以往的研究，通过整合多源土壤数据、GIS和遥感工具以及机器学习技术，支持透明和可扩展的土壤质量评估，并包括实际应用。

Conclusion: 本文提出了一个统一且模块化的框架，整合了多源土壤数据、GIS和遥感工具以及机器学习技术，以支持透明和可扩展的土壤质量评估。该方法旨在克服现有研究的局限性，推动土壤质量评估系统向更透明、自适应和可持续土地管理的方向发展。

Abstract: Soil quality (SQ) plays a crucial role in sustainable agriculture, environmental conservation, and land-use planning. Traditional SQ assessment techniques rely on costly, labor-intensive sampling and laboratory analysis, limiting their spatial and temporal coverage. Advances in Geographic Information Systems (GIS), remote sensing, and machine learning (ML) enabled efficient SQ evaluation. This paper presents a comprehensive roadmap distinguishing it from previous reviews by proposing a unified and modular pipeline that integrates multi-source soil data, GIS and remote sensing tools, and machine learning techniques to support transparent and scalable soil quality assessment. It also includes practical applications. Contrary to existing studies that predominantly target isolated soil parameters or specific modeling methodologies, this approach consolidates recent advancements in Geographic Information Systems (GIS), remote sensing technologies, and machine learning algorithms within the entire soil quality assessment pipeline. It also addresses existing challenges and limitations while exploring future developments and emerging trends in the field that can deliver the next generation of soil quality systems making them more transparent, adaptive, and aligned with sustainable land management.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [22] [Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning](https://arxiv.org/abs/2512.09006)
*Dyna Soumhane Ouchebara,Stéphane Dupont*

Main category: cs.SE

TL;DR: Llama-3.1 8B模型在代码漏洞检测任务中的性能研究，探索了微调和提示工程技术，提出了双微调方法。


<details>
  <summary>Details</summary>
Motivation: 软件开发周期加速导致漏洞增加，需要自动化漏洞检测方法。

Method: 使用Llama-3.1 8B模型，结合BigVul和PrimeVul数据集，测试微调、双微调和测试时微调等方法。

Result: 微调对任务解决很重要，双微调表现良好，RAG作为示例选择技术表现相对较好。

Conclusion: Llama模型在漏洞检测方面具有潜力，但提示工程效果不佳，未来工作仍需探索。

Abstract: The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.

</details>


### [23] [TritonForge: Profiling-Guided Framework for Automated Triton Kernel Optimization](https://arxiv.org/abs/2512.09196)
*Haonan Li,Keyu Man,Partha Kanuparthy,Hanning Chen,Wei Sun,Sreen Tallam,Chenguang Zhu,Kevin Zhu,Zhiyun Qian*

Main category: cs.SE

TL;DR: TritonForge是一个基于性能分析的自动Triton GPU内核优化框架，通过分析、性能分析和迭代代码转换实现自动化优化，性能提升最高达5倍


<details>
  <summary>Details</summary>
Motivation: GPU内核优化在机器学习中至关重要但劳动密集，Triton语言虽简化编程但达到专家级性能仍需深厚GPU架构知识

Method: 集成内核分析、运行时性能分析和迭代代码转换，利用性能分析数据驱动反馈识别瓶颈并提出针对性代码修改，使用LLM辅助代码推理和转换但保持框架模块化和模型无关

Result: 在多样化内核类型和GPU架构上，相比基线实现最高5倍性能提升，平均1.76倍案例优化成功

Conclusion: TritonForge为自动化GPU性能优化研究奠定了基础，展示了基于分析的自动化优化方法潜力

Abstract: High-performance GPU kernel optimization remains a critical yet labor-intensive task in modern machine learning workloads. Although Triton, a domain-specific language for GPU programming, enables developers to write efficient kernels with concise code, achieving expert-level performance still requires deep understanding of GPU architectures and low-level performance trade-offs. We present TritonForge, a profiling-guided framework for automated Triton kernel optimization. TritonForge integrates kernel analysis, runtime profiling, and iterative code transformation to streamline the optimization process. By incorporating data-driven feedback from profiling results, the system identifies performance bottlenecks, proposes targeted code modifications, and evaluates their impact automatically. While our prototype leverages large language models (LLMs) to assist in code reasoning and transformation, the framework remains modular and model-agnostic. Across diverse kernel types and GPU architectures, TritonForge achieves up to 5x performance improvement over baseline implementations and on average 1.76x of the cases are successful, providing a foundation for future research in automated GPU performance optimization.

</details>


### [24] [Bug Priority Change Prediction: An Exploratory Study on Apache Software](https://arxiv.org/abs/2512.09216)
*Guangzong Cai,Zengyang Li,Peng Liang,Ran Mo,Hui Liu,Yutao Ma*

Main category: cs.SE

TL;DR: 提出一种基于Bug修复演进特征和类别不平衡处理策略的两阶段Bug报告优先级变化预测方法，在32个Apache项目数据集上取得良好效果


<details>
  <summary>Details</summary>
Motivation: Bug优先级在修复过程中可能变化，手工评估依赖主观判断易出错且耗时，缺乏自动预测方法

Method: 将Bug生命周期分为报告和修复两阶段，分别构建预测模型，采用Bug修复演进特征和类别不平衡处理策略

Result: 报告阶段模型F1-score达0.798，修复阶段模型F1加权和宏平均分别为0.712和0.613，不同优先级预测性能稳定

Conclusion: 提出的特征和策略能有效提升预测性能，模型具有跨项目适用性，但不同项目间性能差异较大

Abstract: Bug fixing is a critical activity in the software development process. In issue tracking systems such as JIRA, each bug report is assigned a priority level to indicate the urgency and importance level of the bug. The priority may change during the bug fixing process, indicating that the urgency and importance level of the bug will change with the bug fixing. However, manually evaluating priority changes for bugs is a tedious process that heavily relies on the subjective judgment of developers and project managers, leading to incorrect priority changes and thus hindering timely bug fixes. Given the lack of research on bug priority change prediction, we propose a novel two-phase bug report priority change prediction method based on bug fixing evolution features and class imbalance handling strategy. Specifically, we divided the bug lifecycle into two phases: bug reporting and bug fixing, and constructed bug priority change prediction models for each phase. To evaluate the performance of our method, we conducted experiments on a bug dataset constructed from 32 non-trivial Apache projects. The experimental results show that our proposed bug fixing evolution features and the adopted class imbalance handling strategy can effectively improve the performance of prediction models. The F1-score of the prediction model constructed for the bug reporting phase reached 0.798, while the F1-weighted and F1-macro of the prediction model constructed for the bug fixing phase were 0.712 and 0.613, respectively. Furthermore, we explored the cross-project applicability of our prediction models and their performance at different priority levels. The findings indicate large variations in model performance across different projects, although the overall scores remain decent. Meanwhile, the predictive performance across various priority levels remained relatively consistently high.

</details>


### [25] [Explainable Verification of Hierarchical Workflows Mined from Event Logs with Shapley Values](https://arxiv.org/abs/2512.09562)
*Radoslaw Klimek,Jakub Blazowski*

Main category: cs.SE

TL;DR: 该论文提出将挖掘的工作流转化为逻辑规范，利用自动定理证明器分析属性，并引入Shapley值量化工作流元素的贡献，以提升工作流分析的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前工作流挖掘方法虽然能从事件日志中发现层次化过程树，但无法明确解释模型为何满足或违反逻辑属性，以及各元素如何影响整体行为。

Method: 将挖掘的工作流翻译成逻辑规范，使用自动定理证明器分析可满足性、活性和安全性等属性，并采用合作博弈论中的Shapley值对工作流元素进行归因分析。

Result: 在基准数据集上的实验表明，该方法能识别关键节点、揭示冗余结构并暴露有害模式。

Conclusion: 该方法为可解释的工作流分析开辟了新方向，可直接应用于软件工程实践，支持合规性检查、流程优化、冗余减少及下一代过程挖掘工具的设计。

Abstract: Workflow mining discovers hierarchical process trees from event logs, but it remains unclear why such models satisfy or violate logical properties, or how individual elements contribute to overall behavior. We propose to translate mined workflows into logical specifications and analyze properties such as satisfiability, liveness, and safety with automated theorem provers. On this basis, we adapt Shapley values from cooperative game theory to attribute outcomes to workflow elements and quantify their contributions. Experiments on benchmark datasets show that this combination identifies critical nodes, reveals redundancies, and exposes harmful structures. This outlines a novel direction for explainable workflow analysis with direct relevance to software engineering practice, supporting compliance checks, process optimization, redundancy reduction, and the design of next-generation process mining tools.

</details>


### [26] [Model management to support systems engineering workflows using ontology-based knowledge graphs](https://arxiv.org/abs/2512.09596)
*Arkadiusz Ryś,Lucas Lima,Joeri Exelmans,Dennis Janssens,Hans Vangheluwe*

Main category: cs.SE

TL;DR: 本文提出一个基于本体的框架，用于管理系统工程工作流执行产生的建模工件，通过知识图谱支持数据存储、版本控制、查询和推理，并在实际案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着系统工程从文档中心转向基于模型的方法，数字化带来了存储和访问等挑战。多领域专家使用不同形式化方法执行复杂工作流，需要有效管理由此产生的建模工件以实现可重复性、可复制性和数据推理。

Method: 开发了一个用OML定义的工作流概念、形式化方法和工件的本体，构建包含系统工程数据的知识图谱，并开发工具支持工作流设计、执行、工件存储及版本管理。

Result: 在实际的传动系统智能传感器开发场景中应用该框架，结果表明它有效解决了存储和版本控制等基本难题，减少了访问相关信息的时间，并能从知识图谱推断新知识。

Conclusion: 提出的框架不仅帮助系统工程师克服存储和版本控制等困难，还通过知识图谱提升了信息访问效率和推理能力，证明了其在复杂系统工程中的实用价值。

Abstract: System engineering has been shifting from document-centric to model-based approaches, where assets are becoming more and more digital. Although digitisation conveys several benefits, it also brings several concerns (e.g., storage and access) and opportunities. In the context of Cyber- Physical Systems (CPS), we have experts from various domains executing complex workflows and manipulating models in a plethora of different formalisms, each with their own methods, techniques and tools. Storing knowledge on these workflows can reduce considerable effort during system development not only to allow their repeatability and replicability but also to access and reason on data generated by their execution. In this work, we propose a framework to manage modelling artefacts generated from workflow executions. The basic workflow concepts, related formalisms and artefacts are formally defined in an ontology specified in OML (Ontology Modelling Language). This ontology enables the construction of a knowledge graph that contains system engineering data to which we can apply reasoning. We also developed several tools to support system engineering during the design of workflows, their enactment, and artefact storage, considering versioning, querying and reasoning on the stored data. These tools also hide the complexity of manipulating the knowledge graph directly. Finally, we have applied our proposed framework in a real-world system development scenario of a drivetrain smart sensor system. Results show that our proposal not only helped the system engineer with fundamental difficulties like storage and versioning but also reduced the time needed to access relevant information and new knowledge that can be inferred from the knowledge graph.

</details>


### [27] [LogICL: Distilling LLM Reasoning to Bridge the Semantic Gap in Cross-Domain Log Anomaly Detection](https://arxiv.org/abs/2512.09627)
*Jingwei Ye,Zhi Wang,Chenbin Su,Jieshuai Yang,Jiayi Ding,Chunbo Liu,Ge Chu*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Effective log anomaly detection is critical to sustaining reliability in large-scale IT infrastructures. Transformer-based models require substantial resources and labeled data, exacerbating the cold-start problem in target domains where logs are scarce. Existing cross-domain methods leverage source logs but struggle with generalization due to reliance on surface lexical similarity, failing to capture latent semantic equivalence amid structural divergences. To address this, we propose LogICL, a framework distilling Large Language Model (LLM) reasoning into a lightweight encoder for cross-domain anomaly detection. During training, LogICL constructs a delta matrix measuring the utility of demonstrations selected via Maximal Marginal Relevance relative to zero-shot inference. The encoder is optimized via a multi-objective loss comprising an ICL-Guided term that aligns representations based on reasoning assistance utility, maximum mean discrepancy for domain alignment, and supervised contrastive loss. At inference, the optimized encoder retrieves reasoning-aware demonstrations using semantic similarity and delta scores, enabling frozen-LLM in-context learning with Chain-of-Thought for accurate and interpretable detection. Experiments on few-shot and zero-shot cross-domain benchmarks confirm LogICL achieves state-of-the-art performance across heterogeneous systems. Further analysis via visualizations and case studies confirms LogICL bridges the semantic gap beyond surface lexical similarity, effectively capturing latent semantic equivalence for rapid deployment.

</details>


### [28] [Understanding Chain-of-Thought Effectiveness in Code Generation: An Empirical and Information-Theoretic Analysis](https://arxiv.org/abs/2512.09679)
*Naizhu Jin,Zhong Li,Guang Yang,Tian Zhang,Qingkai Zeng*

Main category: cs.SE

TL;DR: 论文系统研究了思维链提示在不同编程语言和模型规模下的代码生成效果，发现结构化CoT方法能显著提升性能且更高效，CoT效果取决于模型能力、语言类型系统和推理质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成中表现优异，但思维链提示的有效机制尚不明确，需要系统研究CoT在不同场景下的实际效果。

Method: 采用信息论框架，评估5种CoT范式在6个Python基准、12种编程语言和6个不同规模模型上的表现，使用条件互信息作为分析工具。

Result: 外部引导的CoT持续优于直接生成，结构化方法平均提升5-12%的Pass@1且比反思推理更节省token；CoT效果受语言类型系统和模型容量影响；推理质量至关重要。

Conclusion: 研究为根据模型能力、语言特性和任务复杂度选择CoT策略提供了实用指导，强调高质量结构化推理的重要性。

Abstract: Large language models (LLMs) achieve strong performance on code generation, but the mechanisms by which Chain-of-Thought (CoT) prompting helps remain unclear. We present a systematic empirical and information-theoretic study of CoT effectiveness in neural code generation, evaluating five paradigms (Zero-Shot, Zero-Shot CoT, Self-Planning, Structured CoT, Reasoning-CoT) across six Python benchmarks, a multilingual benchmark with 12 programming languages, and six models from 7B to 480B parameters, using conditional mutual information $I(Y;C|X)$ as a conceptual lens. Our results show that externally guided CoT consistently outperforms direct generation, with structured methods improving Pass@1 by 5--12\% on average while using substantially fewer tokens than reflective reasoning, and that CoT benefits depend on language type systems and model capacity. We further find that reasoning \emph{quality} is critical: high-quality structured CoT from strong generators yields significantly higher accuracy than lightweight alternatives with the same template, whereas naive Zero-Shot CoT can even degrade performance. These findings provide practical guidance for choosing CoT strategies based on model capacity, language characteristics, and task complexity.

</details>


### [29] [Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition](https://arxiv.org/abs/2512.09775)
*Vladimir Balditsyn,Philippe Lalanda,German Vega,Stéphanie Chollet*

Main category: cs.SE

TL;DR: 提出用于量化基于机器学习系统不确定性的方法，并在人类活动识别领域进行验证


<details>
  <summary>Details</summary>
Motivation: 传统软件开发依赖于严格测试和规范，而ML模型基于数据训练，其运行边界不确定且无法保证完全无错误性能

Method: 调整并联合使用一组选定技术，在运行时评估模型预测的相关性

Result: 方法在人类活动识别领域得到验证，结果展示了方法的实用性

Conclusion: 提出的不确定性量化方法能为领域专家提供有效帮助

Abstract: The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.

</details>
