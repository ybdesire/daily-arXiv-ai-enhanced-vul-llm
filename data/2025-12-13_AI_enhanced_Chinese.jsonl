{"id": "2512.10895", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10895", "abs": "https://arxiv.org/abs/2512.10895", "authors": ["Lijie Ding", "Janell Thomson", "Jon Taylor", "Changwoo Do"], "title": "LLMs Can Assist with Proposal Selection at Large User Facilities", "comment": "9 pages, 8figures", "summary": "We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $\u03c1\\simeq 0.2-0.8$, improving to $\\geq 0.5$ after 10\\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u6765\u6539\u8fdb\u63d0\u6848\u9009\u62e9\u8fc7\u7a0b\uff0c\u76f8\u6bd4\u4f20\u7edf\u4eba\u5de5\u8bc4\u5ba1\u66f4\u9ad8\u6548\u3001\u4e00\u81f4\u4e14\u6210\u672c\u66f4\u4f4e", "motivation": "\u4f20\u7edf\u63d0\u6848\u9009\u62e9\u8fc7\u7a0b\u4e2d\u7684\u4eba\u5de5\u8bc4\u5206\u5b58\u5728\u63d0\u6848\u95f4\u76f8\u5173\u6027\u5f31\u3001\u8bc4\u5ba1\u8005\u504f\u89c1\u548c\u4e0d\u4e00\u81f4\u6027\u7684\u95ee\u9898\uff0c\u800c\u57fa\u4e8e\u6210\u5bf9\u504f\u597d\u7684\u65b9\u6cd5\u867d\u7136\u903b\u8f91\u4e0a\u66f4\u4f18\uff0c\u4f46\u5bf9\u4eba\u5de5\u8bc4\u5ba1\u8005\u5de5\u4f5c\u91cf\u8fc7\u5927\u4e0d\u5b9e\u7528", "method": "\u5229\u7528\u6a61\u6811\u5cad\u56fd\u5bb6\u5b9e\u9a8c\u5ba4\u4e09\u4e2a\u675f\u7ebf\u7684\u7cbe\u5fc3\u6574\u7406\u7684\u63d0\u6848\u548c\u53d1\u8868\u8bb0\u5f55\uff0c\u4f7f\u7528LLMs\u8fdb\u884c\u63d0\u6848\u6392\u540d\uff0c\u5e76\u4e0e\u4eba\u5de5\u6392\u540d\u8fdb\u884c\u76f8\u5173\u6027\u5206\u6790", "result": "LLM\u6392\u540d\u4e0e\u4eba\u5de5\u6392\u540d\u6709\u5f3a\u76f8\u5173\u6027(Spearman \u03c1\u7ea60.2-0.8\uff0c\u53bb\u966410%\u5f02\u5e38\u503c\u540e\u63d0\u5347\u81f3\u22650.5)\uff0c\u5728\u8bc6\u522b\u9ad8\u53d1\u8868\u6f5c\u529b\u63d0\u6848\u65b9\u9762\u8868\u73b0\u4e0d\u900a\u4e8e\u4eba\u5de5\u8bc4\u5ba1\uff0c\u6210\u672c\u964d\u4f4e\u4e24\u4e2a\u6570\u91cf\u7ea7", "conclusion": "LLMs\u4e3a\u63d0\u6848\u9009\u62e9\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u4e00\u81f4\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e0d\u4ec5\u80fd\u8fdb\u884c\u6392\u540d\uff0c\u8fd8\u80fd\u8fdb\u884c\u5982\u901a\u8fc7\u5d4c\u5165\u6a21\u578b\u91cf\u5316\u8bc4\u4f30\u63d0\u6848\u76f8\u4f3c\u6027\u7b49\u9ad8\u7ea7\u5206\u6790"}}
{"id": "2512.10903", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10903", "abs": "https://arxiv.org/abs/2512.10903", "authors": ["Muhammad Umair Haider", "Hammad Rizwan", "Hassan Sajjad", "A. B. Siddique"], "title": "Multi-Granular Node Pruning for Circuit Discovery", "comment": null, "summary": "Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.10937", "categories": ["cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.10937", "abs": "https://arxiv.org/abs/2512.10937", "authors": ["Matt Wilson"], "title": "On Decision-Making Agents and Higher-Order Causal Processes", "comment": null, "summary": "We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDPs\uff09\u4e2d\u7684\u667a\u80fd\u4f53\u4e0e\u5355\u8f93\u5165\u8fc7\u7a0b\u51fd\u6570\uff08\u9ad8\u9636\u91cf\u5b50\u64cd\u4f5c\u7684\u7ecf\u5178\u6781\u9650\uff09\u4e4b\u95f4\u5efa\u7acb\u4e86\u7cbe\u786e\u5bf9\u5e94\u5173\u7cfb", "motivation": "\u63a2\u7d22\u4eba\u5de5\u667a\u80fd\u51b3\u7b56\u667a\u80fd\u4f53\u4e0e\u91cf\u5b50\u7269\u7406\u8fc7\u7a0b\u51fd\u6570\u4e4b\u95f4\u7684\u6df1\u5c42\u6570\u5b66\u8054\u7cfb\uff0c\u4e3a\u8de8\u5b66\u79d1\u7814\u7a76\u63d0\u4f9b\u65b0\u89c6\u89d2", "method": "\u901a\u8fc7\u5c06\u667a\u80fd\u4f53\u7684\u7b56\u7565\u548c\u8bb0\u5fc6\u66f4\u65b0\u7ec4\u5408\u6210\u8fc7\u7a0b\u51fd\u6570w\uff0c\u5e76\u4f7f\u7528link product\u4e0ePOMDP\u73af\u5883\u76f8\u4e92\u4f5c\u7528", "result": "\u5efa\u7acb\u4e86POMDP\u667a\u80fd\u4f53\u4e0e\u8fc7\u7a0b\u51fd\u6570\u4e4b\u95f4\u7684\u5bf9\u5076\u89e3\u91ca\uff1a\u7269\u7406\u5b66\u89c6\u89d2\u4e0b\u8fc7\u7a0b\u51fd\u6570\u4f5c\u4e3a\u73af\u5883\uff0cAI\u89c6\u89d2\u4e0b\u8fc7\u7a0b\u51fd\u6570\u7f16\u7801\u667a\u80fd\u4f53", "conclusion": "\u8be5\u5bf9\u5e94\u5173\u7cfb\u53ef\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5c06\u89c2\u6d4b\u65e0\u5173\u7684\u5206\u5e03\u5f0fPOMDPs\u8bc6\u522b\u4e3a\u591a\u8f93\u5165\u8fc7\u7a0b\u51fd\u6570\u7684\u81ea\u7136\u9886\u57df"}}
