<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 5]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.AI](#cs.AI) [Total: 19]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [A Practical Honeypot-Based Threat Intelligence Framework for Cyber Defence in the Cloud](https://arxiv.org/abs/2512.05321)
*Darren Malvern Chin,Bilal Isfaq,Simon Yusuf Enoch*

Main category: cs.CR

TL;DR: 这篇论文介绍了一个自动化防御框架，它整合了蜜罐遥测和Azure原生工具来动态更新云防火墙规则，显著降低了攻击停留时间并提高了安全运维的可见性。


<details>
  <summary>Details</summary>
Motivation: 传统静态防火墙在云环境下无法有效应对零日威胁和高级持续威胁，尤其是在企业广泛采用Azure等云平台后，这种防御模式的局限性更加突出。

Method: 通过中等至高交互蜜罐（如Cowrie）收集攻击遥测，利用MITRE ATT&CK框架对攻击技战术进行分类，并通过Azure Monitor、Sentinel和Logic Apps实现闭环自动化响应。

Result: 实验表明，该框架的平均阻断时间为0.86秒，远快于基准系统，并成功分类了超过12,000次SSH攻击尝试，覆盖多个MITRE ATT&CK战术。

Conclusion: 将蜜罐遥测与云原生自动化相结合，能够为现代云基础设施提供可扩展、高效的动态防御模型，显著提升安全防护能力。

Abstract: In cloud environments, conventional firewalls rely on predefined rules and manual configurations, limiting their ability to respond effectively to evolving or zero-day threats. As organizations increasingly adopt platforms such as Microsoft Azure, this static defense model exposes cloud assets to zero-day exploits, botnets, and advanced persistent threats. In this paper, we introduce an automated defense framework that leverages medium- to high-interaction honeypot telemetry to dynamically update firewall rules in real time. The framework integrates deception sensors (Cowrie), Azure-native automation tools (Monitor, Sentinel, Logic Apps), and MITRE ATT&CK-aligned detection within a closed-loop feedback mechanism. We developed a testbed to automatically observe adversary tactics, classify them using the MITRE ATT&CK framework, and mitigate network-level threats automatically with minimal human intervention.
  To assess the framework's effectiveness, we defined and applied a set of attack- and defense-oriented security metrics. Building on existing adaptive defense strategies, our solution extends automated capabilities into cloud-native environments. The experimental results show an average Mean Time to Block of 0.86 seconds - significantly faster than benchmark systems - while accurately classifying over 12,000 SSH attempts across multiple MITRE ATT&CK tactics. These findings demonstrate that integrating deception telemetry with Azure-native automation reduces attacker dwell time, enhances SOC visibility, and provides a scalable, actionable defense model for modern cloud infrastructures.

</details>


### [2] [PrivCode: When Code Generation Meets Differential Privacy](https://arxiv.org/abs/2512.05459)
*Zheng Liu,Chen Gong,Terry Yue Zhuo,Kecen Li,Weichen Yu,Matt Fredrikson,Tianhao Wang*

Main category: cs.CR

TL;DR: PrivCode是首个专为代码数据集设计的差分隐私合成器，通过两阶段框架（隐私消毒+效用提升）在保护敏感数据的同时生成高质量代码


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现出色，但使用私有数据微调会引发隐私泄露担忧

Method: 第一阶段用DP-SGD训练模型生成差分隐私合成代码；第二阶段在合成代码上微调大语言模型提升效用

Result: 在四个基准测试中，PrivCode在四种LLMs上均能生成更高实用性的代码

Conclusion: PrivCode有效平衡了隐私保护和代码实用性，为DP代码生成提供了可行解决方案

Abstract: Large language models (LLMs) have presented outstanding performance in code generation and completion. However, fine-tuning these models on private datasets can raise privacy and proprietary concerns, such as the leakage of sensitive personal information. Differentially private (DP) code generation provides theoretical guarantees for protecting sensitive code by generating synthetic datasets that preserve statistical properties while reducing privacy leakage concerns. However, DP code generation faces significant challenges due to the strict syntactic dependencies and the privacy-utility trade-off.
  We propose PrivCode, the first DP synthesizer specifically designed for code datasets. It incorporates a two-stage framework to improve both privacy and utility. In the first stage, termed "privacy-sanitizing", PrivCode generates DP-compliant synthetic code by training models using DP-SGD while introducing syntactic information to preserve code structure. The second stage, termed "utility-boosting", fine-tunes a larger pre-trained LLM on the synthetic privacy-free code to mitigate the utility loss caused by DP, enhancing the utility of the generated code. Extensive experiments on four LLMs show that PrivCode generates higher-utility code across various testing tasks under four benchmarks. The experiments also confirm its ability to protect sensitive data under varying privacy budgets. We provide the replication package at the anonymous link.

</details>


### [3] [TeleAI-Safety: A comprehensive LLM jailbreaking benchmark towards attacks, defenses, and evaluations](https://arxiv.org/abs/2512.05485)
*Xiuyuan Chen,Jian Zhao,Yuxiang He,Yuan Xun,Xinwei Liu,Yanshu Li,Huilin Zhou,Wei Cai,Ziyan Shi,Yuchen Yuan,Tianle Zhang,Chi Zhang,Xuelong Li*

Main category: cs.CR

TL;DR: TeleAI-Safety是一个模块化、可复现的框架和基准测试，用于评估大语言模型（LLMs）的安全性，解决现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估基准和框架存在核心组件（攻击、防御和评估方法）集成不平衡以及灵活评估框架与标准化基准能力之间隔离的问题，阻碍了可靠的跨研究比较和全面的风险评估。

Method: 框架集成了19种攻击方法（包括一种自研方法）、29种防御方法和19种评估方法（包括一种自研方法），使用包含12个不同风险类别的342个样本的攻击语料库，对14个目标模型进行广泛评估。

Result: 结果揭示了系统的漏洞和模型特定的失败案例，凸显了安全性和效用之间的关键权衡，并识别了未来优化的潜在防御模式。

Conclusion: TeleAI-Safety可灵活调整以适应特定需求，通过发布完整代码和评估结果促进可复现研究并建立统一的安全基线。

Abstract: While the deployment of large language models (LLMs) in high-value industries continues to expand, the systematic assessment of their safety against jailbreak and prompt-based attacks remains insufficient. Existing safety evaluation benchmarks and frameworks are often limited by an imbalanced integration of core components (attack, defense, and evaluation methods) and an isolation between flexible evaluation frameworks and standardized benchmarking capabilities. These limitations hinder reliable cross-study comparisons and create unnecessary overhead for comprehensive risk assessment. To address these gaps, we present TeleAI-Safety, a modular and reproducible framework coupled with a systematic benchmark for rigorous LLM safety evaluation. Our framework integrates a broad collection of 19 attack methods (including one self-developed method), 29 defense methods, and 19 evaluation methods (including one self-developed method). With a curated attack corpus of 342 samples spanning 12 distinct risk categories, the TeleAI-Safety benchmark conducts extensive evaluations across 14 target models. The results reveal systematic vulnerabilities and model-specific failure cases, highlighting critical trade-offs between safety and utility, and identifying potential defense patterns for future optimization. In practical scenarios, TeleAI-Safety can be flexibly adjusted with customized attack, defense, and evaluation combinations to meet specific demands. We release our complete code and evaluation results to facilitate reproducible research and establish unified safety baselines.

</details>


### [4] [Matching Ranks Over Probability Yields Truly Deep Safety Alignment](https://arxiv.org/abs/2512.05518)
*Jason Vega,Gagandeep Singh*

Main category: cs.CR

TL;DR: 论文提出了一种名为RAP的攻击方法，能够绕过现有的深度安全对齐防御，并提出了一种新的防御方法PRESTO来解决该漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有通过数据增强实现的深度安全对齐防御实际上并不够深度，容易受到推广的预填充攻击的绕过。

Method: 提出了Rank-Assisted Prefilling (RAP)攻击方法，通过选择前20个预测令牌中的低概率有害令牌来绕过防御。同时提出了基于令牌排名匹配的PRESTO防御方法。

Result: PRESTO防御在RAP攻击下将平均StrongREJECT得分提高了4.7倍，同时对模型实用性影响较小。

Conclusion: 基于令牌排名匹配的安全对齐比基于概率分布的更有效，PRESTO方法为解决预填充攻击提供了简单而有效的解决方案。

Abstract: A frustratingly easy technique known as the prefilling attack has been shown to effectively circumvent the safety alignment of frontier LLMs by simply prefilling the assistant response with an affirmative prefix before decoding. In response, recent work proposed a supervised fine-tuning (SFT) defense using data augmentation to achieve a \enquote{deep} safety alignment, allowing the model to generate natural language refusals immediately following harmful prefills. Unfortunately, we show in this work that the "deep" safety alignment produced by such an approach is in fact not very deep. A generalization of the prefilling attack, which we refer to as the Rank-Assisted Prefilling (RAP) attack, can effectively extract harmful content from models fine-tuned with the data augmentation defense by selecting low-probability "harmful" tokens from the top 20 predicted next tokens at each step (thus ignoring high-probability "refusal" tokens). We argue that this vulnerability is enabled due to the "gaming" of the SFT objective when the target distribution entropies are low, where low fine-tuning loss is achieved by shifting large probability mass to a small number of refusal tokens while neglecting the high ranks of harmful tokens. We then propose a new perspective on achieving deep safety alignment by matching the token ranks of the target distribution, rather than their probabilities. This perspective yields a surprisingly simple fix to the data augmentation defense based on regularizing the attention placed on harmful prefill tokens, an approach we call PRefill attEntion STOpping (PRESTO). Adding PRESTO yields up to a 4.7x improvement in the mean StrongREJECT score under RAP attacks across three popular open-source LLMs, with low impact to model utility.

</details>


### [5] [ARGUS: Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Behavior](https://arxiv.org/abs/2512.05745)
*Weikai Lu,Ziqian Zeng,Kehua Zhang,Haoran Li,Huiping Zhuang,Ruidong Wang,Cen Chen,Hao Peng*

Main category: cs.CR

TL;DR: 本文提出ARGUS防御框架，通过表示空间中的激活引导来抵御多模态间接提示注入攻击，在保持模型性能的同时实现安全与效用的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的防御方法主要针对纯文本LLM设计，无法有效应对多模态IPI攻击，存在易被绕过、模态依赖或泛化能力差的问题。

Method: 发现MLLMs的指令跟随行为编码在一个子空间中，提出ARGUS方法：在安全子空间中寻找与效用退化方向解耦的最优防御方向，结合自适应强度引导、轻量级注入检测和后期过滤验证。

Result: 实验结果表明ARGUS能够实现对多模态IPI的鲁棒防御，同时最大程度保留MLLM的实用性。

Conclusion: 通过表示空间中的定向引导可以构建独立于模态的通用防御机制，ARGUS框架在安全性和实用性之间取得了良好平衡。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly vulnerable to multimodal Indirect Prompt Injection (IPI) attacks, which embed malicious instructions in images, videos, or audio to hijack model behavior. Existing defenses, designed primarily for text-only LLMs, are unsuitable for countering these multimodal threats, as they are easily bypassed, modality-dependent, or generalize poorly. Inspired by activation steering researches, we hypothesize that a robust, general defense independent of modality can be achieved by steering the model's behavior in the representation space. Through extensive experiments, we discover that the instruction-following behavior of MLLMs is encoded in a subspace. Steering along directions within this subspace can enforce adherence to user instructions, forming the basis of a defense. However, we also found that a naive defense direction could be coupled with a utility-degrading direction, and excessive intervention strength harms model performance. To address this, we propose ARGUS, which searches for an optimal defense direction within the safety subspace that decouples from the utility degradation direction, further combining adaptive strength steering to achieve a better safety-utility trade-off. ARGUS also introduces lightweight injection detection stage to activate the defense on-demand, and a post-filtering stage to verify defense success. Experimental results show that ARGUS can achieve robust defense against multimodal IPI while maximally preserving the MLLM's utility.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [6] [From Text to Returns: Using Large Language Models for Mutual Fund Portfolio Optimization and Risk-Adjusted Allocation](https://arxiv.org/abs/2512.05907)
*Abrar Hossain Mufakir Qamar Ansari Haziq Jeelani Monia Digra Fayeq Jeelani Syed*

Main category: cs.CE

TL;DR: 利用生成式AI（Large Language Models）改进投资组合优化和风险管理，在金融决策中应用检索增强生成(RAG)管道结合实时经济数据，Zypher 7B模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在投资领域具有巨大潜力，特别是在优化资产组合和管理风险方面，本研究旨在探索如何用先进的大型语言模型提升传统金融决策方法。

Method: 采用检索增强生成(RAG)管道，使LLM能够结合外部实时数据与标准金融优化方法，测试了Microsoft Phi 2、Mistral 7B和Zypher 7B等模型在不同经济部门的互惠基金投资中制定风险感知策略。

Result: Zypher 7B模型表现最优，能持续生成最大化投资回报且风险调整后结果更好的策略，其处理复杂关系和上下文信息的能力突出。

Conclusion: 生成式AI显著优于基本资产分配方法，通过将AI与现实金融应用结合，为资产管理专业人士开发更智能、高效和适应性强的解决方案奠定了基础。

Abstract: Generative AI (GenAI) has enormous potential for improving two critical areas in investing, namely portfolio optimization (choosing the best combination of assets) and risk management (protecting those investments). Our study works at this intersection, using Large Language Models (LLMs) to upgrade how financial decisions are traditionally made. This research specifically tested how well advanced LLMs like Microsoft Phi 2, Mistral 7B, and Zypher 7B can create practical, risk-aware strategies for investing mutual funds in different sectors of the economy. Our method is sophisticated: it combines a Retrieval-Augmented Generation (RAG) pipeline, which enables the LLM to check external, real-time data with standard financial optimization methods. The model's advice is context-aware because we feed it large economic signals, like changes in the global economy. The Zypher 7B model was the clear winner. It consistently produced strategies that maximized investment returns while delivering better risk-adjusted results than the other models. Its ability to process complex relationships and contextual information makes it a highly powerful tool for financial allocation. In conclusion, our findings show that GenAI substantially improves performance over basic allocation methods. By connecting GenAI to real-world financial applications, this work lays the groundwork for creating smarter, more efficient, and more adaptable solutions for asset management professionals.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [Stellis: A Strategy Language for Purifying Separation Logic Entailments](https://arxiv.org/abs/2512.05159)
*Zhiyi Wang,Xiwei Wu,Yi Fang,Chengtao Li,Hongyi Zhong,Lihan Xie,Qinxiang Cao,Zhenjiang Hu*

Main category: cs.SE

TL;DR: 提出Stellis策略语言解决分离逻辑蕴涵自动证明问题，通过纯化操作将蕴约简为纯蕴涵，系统在229个测试案例中成功纯化95.6%的蕴涵。


<details>
  <summary>Details</summary>
Motivation: 基于规则的分离逻辑自动化方法存在局限，无法描述涉及内存布局对齐和消除的具体策略。

Method: 设计Stellis策略语言，包含匹配机制和动作描述；提出算法生成策略可靠性条件；基于机械化归约可靠性定理实现原型系统。

Result: 在标准链表数据结构和微内核内存模块的229个蕴涵测试中，使用5个库的98条策略实现95.6%的纯化成功率。

Conclusion: Stellis在保持灵活便捷的同时具备高效性，为分离逻辑自动化验证提供了可扩展的解决方案。

Abstract: Automatically proving separation logic entailments is a fundamental challenge in verification. While rule-based methods rely on separation logic rules (lemmas) for automation, these rule statements are insufficient for describing automation strategies, which usually involve the alignment and elimination of corresponding memory layouts in specific scenarios. To overcome this limitation, we propose Stellis, a strategy language for purifying separation logic entailments, i.e., removing all spatial formulas to reduce the entailment to a simpler pure entailment. Stellis features a powerful matching mechanism and a flexible action description, enabling the straightforward encoding of a wide range of strategies. To ensure strategy soundness, we introduce an algorithm that generates a soundness condition for each strategy, thereby reducing the soundness of each strategy to the correctness of its soundness condition. Furthermore, based on a mechanized reduction soundness theorem, our prototype implementation generates correctness proofs for the overall automation. We evaluate our system on a benchmark of 229 entailments collected from verification of standard linked data structures and the memory module of a microkernel, and the evaluation results demonstrate that, with such flexibility and convenience provided, our system is also highly effective, which automatically purifies 95.6% (219 out of 229) of the entailments using 5 libraries with 98 strategies.

</details>


### [8] [Towards A Cultural Intelligence and Values Inferences Quality Benchmark for Community Values and Common Knowledge](https://arxiv.org/abs/2512.05176)
*Brittany Johnson,Erin Reddick,Angela D. R. Smith*

Main category: cs.SE

TL;DR: 该论文提出开发CIVIQ基准，用于评估LLMs与美国不同社区文化价值观的对齐程度，以弥补现有国家层面基准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有通用LLMs主要与西方白人文化对齐，忽视了美国多样的文化群体。国家层面的对齐基准无法充分代表多元文化需求。

Method: 通过复制韩国KorNAT基准的开发过程，开发CIVIQ基准，专注于社区层面的社会价值观和常识对齐。

Result: 提出了CIVIQ基准框架，为实践中AI技术的文化对齐研究提供基础。

Conclusion: 社区层面的文化对齐基准对于实现更具包容性的AI技术至关重要，CIVIQ为相关研究和开发提供了关键基础。

Abstract: Large language models (LLMs) have emerged as a powerful technology, and thus, we have seen widespread adoption and use on software engineering teams. Most often, LLMs are designed as "general purpose" technologies meant to represent the general population. Unfortunately, this often means alignment with predominantly Western Caucasian narratives and misalignment with other cultures and populations that engage in collaborative innovation. In response to this misalignment, there have been recent efforts centered on the development of "culturally-informed" LLMs, such as ChatBlackGPT, that are capable of better aligning with historically marginalized experiences and perspectives. Despite this progress, there has been little effort aimed at supporting our ability to develop and evaluate culturally-informed LLMs. A recent effort proposed an approach for developing a national alignment benchmark that emphasizes alignment with national social values and common knowledge. However, given the range of cultural identities present in the United States (U.S.), a national alignment benchmark is an ineffective goal for broader representation. To help fill this gap in this US context, we propose a replication study that translates the process used to develop KorNAT, a Korean National LLM alignment benchmark, to develop CIVIQ, a Cultural Intelligence and Values Inference Quality benchmark centered on alignment with community social values and common knowledge. Our work provides a critical foundation for research and development aimed at cultural alignment of AI technologies in practice.

</details>


### [9] [A Survey of Bugs in AI-Generated Code](https://arxiv.org/abs/2512.05239)
*Ruofan Gao,Amjed Tahir,Peng Liang,Teo Susnjak,Foutse Khomh*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Developers are widely using AI code-generation models, aiming to increase productivity and efficiency. However, there are also quality concerns regarding the AI-generated code. The generated code is produced by models trained on publicly available code, which are known to contain bugs and quality issues. Those issues can cause trust and maintenance challenges during the development process. Several quality issues associated with AI-generated code have been reported, including bugs and defects. However, these findings are often scattered and lack a systematic summary. A comprehensive review is currently lacking to reveal the types and distribution of these errors, possible remediation strategies, as well as their correlation with the specific models. In this paper, we systematically analyze the existing AI-generated code literature to establish an overall understanding of bugs and defects in generated code, providing a reference for future model improvement and quality assessment. We aim to understand the nature and extent of bugs in AI-generated code, and provide a classification of bug types and patterns present in code generated by different models. We also discuss possible fixes and mitigation strategies adopted to eliminate bugs from the generated code.

</details>


### [10] [Learning to Code with Context: A Study-Based Approach](https://arxiv.org/abs/2512.05242)
*Uwe M. Borghoff,Mark Minas,Jannis Schopp*

Main category: cs.SE

TL;DR: 研究探讨了在大学编程项目中整合生成式AI工具的方法，通过用户研究分析学生在软件开发过程中如何使用AI工具，并开发了基于RAG的本地部署LLM助手来提供项目上下文支持


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具的快速出现正在改变软件开发方式，软件工程教育需要适应这一变化，确保学生不仅能学习传统开发方法，还能有意义且负责任地使用新技术

Method: 在大学编程项目中进行的用户研究，学生协作开发电脑游戏，研究分析参与者在软件开发不同阶段如何使用生成式AI工具，并开发了基于检索增强生成(RAG)的本地部署大型语言模型助手

Result: 研究确定了AI工具最有效的任务类型，分析了学生遇到的挑战，并通过RAG系统实现对模型行为、参数敏感性和常见故障模式的定性分析

Conclusion: 研究结果加深了对教育软件项目中情境感知AI支持的理解，为未来将基于AI的辅助工具整合到软件工程课程提供了参考

Abstract: The rapid emergence of generative AI tools is transforming the way software is developed. Consequently, software engineering education must adapt to ensure that students not only learn traditional development methods but also understand how to meaningfully and responsibly use these new technologies. In particular, project-based courses offer an effective environment to explore and evaluate the integration of AI assistance into real-world development practices. This paper presents our approach and a user study conducted within a university programming project in which students collaboratively developed computer games. The study investigates how participants used generative AI tools throughout different phases of the software development process, identifies the types of tasks where such tools were most effective, and analyzes the challenges students encountered. Building on these insights, we further examine a repository-aware, locally deployed large language model (LLM) assistant designed to provide project-contextualized support. The system employs Retrieval-Augmented Generation (RAG) to ground responses in relevant documentation and source code, enabling qualitative analysis of model behavior, parameter sensitivity, and common failure modes. The findings deepen our understanding of context-aware AI support in educational software projects and inform future integration of AI-based assistance into software engineering curricula.

</details>


### [11] [Engagement in Code Review: Emotional, Behavioral, and Cognitive Dimensions in Peer vs. LLM Interactions](https://arxiv.org/abs/2512.05309)
*Adam Alami,Nathan Cassee,Thiago Rocha Silva,Elda Paja,Neil A. Ernst*

Main category: cs.SE

TL;DR: 本研究比较了LLM辅助代码评审与人工同行评审的不同，发现LLM评审降低了情感成本和自我调节需求，将参与重点从情感管理转向认知负荷管理，提出AI应作为支持性伙伴来减轻负担同时保留人类责任。


<details>
  <summary>Details</summary>
Motivation: 理解软件工程师在LLM辅助代码评审与人工同行评审中的参与差异，特别是情感自我调节和行为参与的关系。

Method: 对20名软件工程师进行两阶段定性研究：第一阶段进行同行评审并访谈情感反应和参与决策；第二阶段引入符合工程师偏好的提示，探究特征如何影响反应。

Result: 识别了工程师应对负面反馈的自我调节策略（重构、对话调节、回避、防御性），发现LLM辅助评审降低了情感成本，当反馈符合认知期望时减少处理努力并提高采纳倾向。

Conclusion: AI最适合作为支持性伙伴，减少认知和情感负荷，同时保留人类责任和同行评审等社会技术活动的社会意义。

Abstract: Code review is a socio-technical practice, yet how software engineers engage in Large Language Model (LLM)-assisted code reviews compared to human peer-led reviews is less understood. We report a two-phase qualitative study with 20 software engineers to understand this. In Phase I, participants exchanged peer reviews and were interviewed about their affective responses and engagement decisions. In Phase II, we introduced a new prompt matching engineers' preferences and probed how characteristics shaped their reactions. We develop an integrative account linking emotional self-regulation to behavioral engagement and resolution. We identify self-regulation strategies that engineers use to regulate their emotions in response to negative feedback: reframing, dialogic regulation, avoidance, and defensiveness. Engagement proceeds through social calibration; engineers align their responses and behaviors to the relational climate and team norms. Trajectories to resolution, in the case of peer-led review, vary by locus (solo/dyad/team) and an internal sense-making process. With the LLM-assisted review, emotional costs and the need for self-regulation seem lower. When LLM feedback aligned with engineers' cognitive expectations, participants reported reduced processing effort and a potentially higher tendency to adopt. We show that LLM-assisted review redirects engagement from emotion management to cognitive load management. We contribute an integrative model of engagement that links emotional self-regulation to behavioral engagement and resolution, showing how affective and cognitive processes influence feedback adoption in peer-led and LLM-assisted code reviews. We conclude that AI is best positioned as a supportive partner to reduce cognitive and emotional load while preserving human accountability and the social meaning of peer review and similar socio-technical activities.

</details>


### [12] [Invisible Load: Uncovering the Challenges of Neurodivergent Women in Software Engineering](https://arxiv.org/abs/2512.05350)
*Munazza Zaib,Wei Wang,Dulaji Hidellaarachchi,Isma Farah Siddiqui*

Main category: cs.SE

TL;DR: 本文针对软件工程中神经多样性女性面临的独特挑战，首次系统性研究并提出混合方法论框架。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究中尚未系统关注神经多样性女性群体，她们面临着性别偏见和神经差异交叉的独特障碍，导致职业压力和高流失率。

Method: 提出了结合InclusiveMag包容性框架和GenderWalkthrough流程的混合方法，分为文献回顾、人物角色与分析方法开发、协作工作坊应用三阶段。

Result: 文献综述归纳出神经多样性女性在软件工程中面临的认知、社交、组织、结构和职业发展五类挑战，揭示了误诊/延迟诊断和伪装行为加剧排斥的问题。

Conclusion: 研究为后续开发和应用包容性分析方法奠定了基础，旨在支持针对神经多样性女性的 actionable changes。

Abstract: Neurodivergent women in Software Engineering (SE) encounter distinctive challenges at the intersection of gender bias and neurological differences. To the best of our knowledge, no prior work in SE research has systematically examined this group, despite increasing recognition of neurodiversity in the workplace. Underdiagnosis, masking, and male-centric workplace cultures continue to exacerbate barriers that contribute to stress, burnout, and attrition. In response, we propose a hybrid methodological approach that integrates InclusiveMag's inclusivity framework with the GenderMag walkthrough process, tailored to the context of neurodivergent women in SE. The overarching design unfolds across three stages, scoping through literature review, deriving personas and analytic processes, and applying the method in collaborative workshops. We present a targeted literature review that synthesize challenges into cognitive, social, organizational, structural and career progression challenges neurodivergent women face in SE, including how under/late diagnosis and masking intensify exclusion. These findings lay the groundwork for subsequent stages that will develop and apply inclusive analytic methods to support actionable change.

</details>


### [13] [Legacy Modernization with AI -- Mainframe modernization](https://arxiv.org/abs/2512.05375)
*Sunil Khemka,Arunava Majumdar*

Main category: cs.SE

TL;DR: 论文探讨了人工智能在传统大型机系统现代化转型中的关键作用，通过自动化代码重构、智能数据迁移和预测性维护等技术，帮助企业过渡到微服务、容器化和混合云平台。


<details>
  <summary>Details</summary>
Motivation: 传统大型机系统虽然可靠，但面临维护成本高、技术人才短缺以及与云系统整合困难等挑战，需要寻找有效的现代化解决方案。

Method: 采用AI驱动的现代化策略，包括机器学习模型分析遗留代码库、自动化测试和部署、智能工具进行数据迁移等方法。

Result: AI辅助的现代化改造不仅能保留核心业务逻辑，还能实现更快的创新、减少停机时间并增强系统弹性。

Conclusion: 人工智能在大型机现代化中的应用是推动数字化转型和企业可持续增长的重要催化剂。

Abstract: Artificial Intelligence-assisted legacy modernization is essential in changing the stalwart mainframe systems of the past into flexible, scalable, and smart architecture. While mainframes are generally dependable, they can be difficult to maintain due to their high maintenance costs, the shortage of skills, and the problems in integrating them with cloud-based systems. By adopting AI-driven modernization strategies such as automated code refactoring, migration of data using smart tools, and predictive maintenance, companies can easily move to microservices, containerized environments, and hybrid cloud platforms. Machine learning models have the capability to go through legacy codebases, figure out efficiency opportunities, and carry out automated testing and deployment. Besides that, AI improves the organization's operational efficiency by generating the insights that can be used to level the workload and detect the anomalies. The coupling of the two is not only about saving the core business logic but also about enabling quicker innovation, less downtime, and enhanced system resilience. Therefore, the use of AI in mainframe modernization is a catalyst for digital transformation and enterprise growth that is sustainable over time.

</details>


### [14] [Fuzzing the brain: Automated stress testing for the safety of ML-driven neurostimulation](https://arxiv.org/abs/2512.05383)
*Mara Downing,Matthew Peng,Jacob Granley,Michael Beyeler,Tevfik Bultan*

Main category: cs.SE

TL;DR: 本研究提出了一种基于覆盖引导模糊测试的机器学习驱动神经刺激系统安全评估方法，能够系统性地检测违反生物物理安全限制的刺激模式。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在神经假体设备中的广泛应用，需要解决模型输出直接作用于神经组织时引入的新安全风险。

Method: 将软件测试中的覆盖引导模糊测试技术适配到神经刺激领域，通过扰动模型输入来追踪产生的刺激是否违反电荷密度、瞬时电流或电极共激活等生物物理限制。

Result: 应用于视网膜和皮层深度刺激编码器的测试表明，该方法能系统性地发现超出安全限制的多样化刺激模式。

Conclusion: 以违规为重点的模糊测试将安全评估重构为可重复的实证过程，为下一代神经接口的证据基准测试、监管准备和伦理保证奠定了基础。

Abstract: Objective: Machine learning (ML) models are increasingly used to generate electrical stimulation patterns in neuroprosthetic devices such as visual prostheses. While these models promise precise and personalized control, they also introduce new safety risks when model outputs are delivered directly to neural tissue. We propose a systematic, quantitative approach to detect and characterize unsafe stimulation patterns in ML-driven neurostimulation systems. Approach: We adapt an automated software testing technique known as coverage-guided fuzzing to the domain of neural stimulation. Here, fuzzing performs stress testing by perturbing model inputs and tracking whether resulting stimulation violates biophysical limits on charge density, instantaneous current, or electrode co-activation. The framework treats encoders as black boxes and steers exploration with coverage metrics that quantify how broadly test cases span the space of possible outputs and violation types. Main results: Applied to deep stimulus encoders for the retina and cortex, the method systematically reveals diverse stimulation regimes that exceed established safety limits. Two violation-output coverage metrics identify the highest number and diversity of unsafe outputs, enabling interpretable comparisons across architectures and training strategies. Significance: Violation-focused fuzzing reframes safety assessment as an empirical, reproducible process. By transforming safety from a training heuristic into a measurable property of the deployed model, it establishes a foundation for evidence-based benchmarking, regulatory readiness, and ethical assurance in next-generation neural interfaces.

</details>


### [15] [Bita: A Conversational Assistant for Fairness Testing](https://arxiv.org/abs/2512.05428)
*Keeryn Johnson,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 该论文介绍了Bita——一个基于对话式AI的实验辅助工具，旨在帮助软件测试人员检测AI系统中的偏差，执行公平性测试，并生成面向公平性的探索性测试大纲。


<details>
  <summary>Details</summary>
Motivation: 现有公平性测试工具使用门槛高，需要专业知识，且对真实工作流程支持有限，导致AI系统中的偏差可能未被充分检测，进而引发不公平和歧视性结果。

Method: 通过结合大型语言模型与检索增强生成技术，将响应建立在经过筛选的公平性文献基础上，开发出名为Bita的对话助手。

Result: 在实际AI系统上的验证表明，Bita能有效支持公平性测试任务，提供结构化和可复现的实用性证据。

Conclusion: Bita作为一款实用工具，使公平性测试变得易于操作、系统化，并可直接应用于工业实践。

Abstract: Bias in AI systems can lead to unfair and discriminatory outcomes, especially when left untested before deployment. Although fairness testing aims to identify and mitigate such bias, existing tools are often difficult to use, requiring advanced expertise and offering limited support for real-world workflows. To address this, we introduce Bita, a conversational assistant designed to help software testers detect potential sources of bias, evaluate test plans through a fairness lens, and generate fairness-oriented exploratory testing charters. Bita integrates a large language model with retrieval-augmented generation, grounding its responses in curated fairness literature. Our validation demonstrates how Bita supports fairness testing tasks on real-world AI systems, providing structured, reproducible evidence of its utility. In summary, our work contributes a practical tool that operationalizes fairness testing in a way that is accessible, systematic, and directly applicable to industrial practice.

</details>


### [16] [A Hybrid Approach for EMF Code Generation:Code Templates Meet Large Language Models](https://arxiv.org/abs/2512.05498)
*Xiao He,Ru Chen,Zeqing Zhang,Yanling Wang,Qiuyan Dong*

Main category: cs.SE

TL;DR: iEcoreGen结合EMF建模框架和LLM，通过分解需求生成规范并创建初始代码，再利用LLM补充未实现方法，在20个代码生成任务中优于纯LLM基线。


<details>
  <summary>Details</summary>
Motivation: 模板驱动代码生成虽能保证正确性但灵活性不足，而LLM灵活性高却可能生成错误代码，因此需要结合两者优势。

Method: 使用EMF的Ecore模型定义系统结构，分解需求生成操作规范，先用模板生成初始Java代码，再将规范序列化为文档字符串，最后调用LLM完成未实现方法。

Result: 在5种LLM上的20个代码生成任务评估显示，iEcoreGen在pass@k指标上优于纯LLM基线，在compilation@k指标上表现相当。消融研究明确了各组件贡献。

Conclusion: LLM增强的模型驱动开发是提高软件自动化效率的有前景方向。

Abstract: Template-based and LLM-based code generation are both key enablers of automated software development. The former provides correctness guarantees but are rigid for complex requirements, whereas LLMs offer high flexibility at the risk of producing faulty code.This paper proposes iEcoreGen, a hybrid approach that integrates Eclipse Modeling Framework (EMF) and LLMs. In EMF, an Ecore model defines a system structure and acts as a blueprint for code-generation.iEcoreGen decomposes requirements to derive operation specifications, uses EMF's template-based generator to produce initial Java code, and serializes specifications into docstrings. LLMs are then invoked to complete and fix unimplemented methods. We assessed iEcoreGen on twenty code-generation tasks across five LLMs. It surpasses LLM-only baselines on pass@k and performs on par with them on compilation@k. An ablation study clarified the contribution of each component of iEcoreGen. Overall, the findings indicate that LLM-enhanced model-driven development is a promising path toward more efficient software automation.

</details>


### [17] [Generative AI in Simulation-Based Test Environments for Large-Scale Cyber-Physical Systems: An Industrial Study](https://arxiv.org/abs/2512.05507)
*Masoud Sadrnezhaad,José Antonio Hernández López,Torvald Mårtensson,Daniel Varro*

Main category: cs.SE

TL;DR: 本研究通过对六家公司的跨企业研讨，探讨生成式AI在大型信息物理系统仿真测试中的应用潜力与挑战，并提出了研究议程。


<details>
  <summary>Details</summary>
Motivation: 大型信息物理系统的质量保障依赖复杂仿真测试环境，需要大量资源开发维护模型。生成式AI可生成可执行测试用例，但在此类系统的仿真测试中应用仍不足。

Method: 通过跨企业研讨会收集六家组织的实践者观点。

Result: 研究发现生成式AI具有巨大潜力但存在未解决挑战，提出了三大研究方向：AI生成场景与环境模型、CI/CD管道中仿真器与AI结合、生成式AI的可信度。

Conclusion: 论文旨在通过详细问题描述引导未来学术界与产业界合作，推动生成式AI在仿真测试中的负责任应用。

Abstract: Quality assurance for large-scale cyber-physical systems relies on sophisticated test activities using complex test environments investigated with the help of numerous types of simulators. As these systems grow, extensive resources are required to develop and maintain simulation models of hardware and software components, as well as physical environments. Meanwhile, recent advances in generative AI have led to tools that can produce executable test cases for software systems, offering potential benefits such as reducing manual efforts or increasing test coverage. However, the application of generative AI techniques to simulation-based testing of large-scale cyber-physical systems remains underexplored. To better understand this gap, this study captures practitioners' perspectives on leveraging generative AI, based on a cross-company workshop with six organizations. Our contribution is twofold: (1) detailed, experience-based insights into challenges faced by engineers, and (2) a research agenda comprising three high-priority directions: (a) AI-generated scenarios and environment models, (b) simulators and AI in CI/CD pipelines, and (c) trustworthiness in generative AI for simulation. While participants acknowledged substantial potential, they also highlighted unresolved challenges. By detailing these issues, the paper aims to guide future academia-industry collaboration towards the responsible adoption of generative AI in simulation-based testing.

</details>


### [18] [From Challenge to Change: Design Principles for AI Transformations](https://arxiv.org/abs/2512.05533)
*Theocharis Tavantzis,Stefano Lambiase,Daniel Russo,Robert Feldt*

Main category: cs.SE

TL;DR: 本文提出一个基于行为软件工程的人类中心框架，通过混合方法建立并完善，包含九个维度，为早期AI采用提供实用路线图。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多关注AI集成的技术问题，而对团队如何适应和信任AI的行为因素关注有限，需要人类中心的框架支持SE组织早期AI采用。

Method: 采用混合方法，通过文献综述组织变革模型和访谈数据的主题分析，建立并完善框架，并进行调查（N=105）和两个专家研讨会（N=4）收集实践者反馈。

Result: 调查显示，技能提升（15.2%）和AI战略设计（15.1%）被视为最重要维度；组织当前优先程序性元素，而人类中心防护措施较不发达。

Conclusion: 该框架通过识别关键行为维度和提供可操作指导，为应对早期AI采用的社会技术复杂性提供实用路线图，并突出SE中人类中心AI的未来研究方向。

Abstract: The rapid rise of Artificial Intelligence (AI) is reshaping Software Engineering (SE), creating new opportunities while introducing human-centered challenges. Although prior work notes behavioral and other non-technical factors in AI integration, most studies still emphasize technical concerns and offer limited insight into how teams adapt to and trust AI. This paper proposes a Behavioral Software Engineering (BSE)-informed, human-centric framework to support SE organizations during early AI adoption. Using a mixed-methods approach, we built and refined the framework through a literature review of organizational change models and thematic analysis of interview data, producing concrete, actionable steps. The framework comprises nine dimensions: AI Strategy Design, AI Strategy Evaluation, Collaboration, Communication, Governance and Ethics, Leadership, Organizational Culture, Organizational Dynamics, and Up-skilling, each supported by design principles and actions. To gather preliminary practitioner input, we conducted a survey (N=105) and two expert workshops (N=4). Survey results show that Up-skilling (15.2%) and AI Strategy Design (15.1%) received the highest $100-method allocations, underscoring their perceived importance in early AI initiatives. Findings indicate that organizations currently prioritize procedural elements such as strategy design, while human-centered guardrails remain less developed. Workshop feedback reinforced these patterns and emphasized the need to ground the framework in real-world practice. By identifying key behavioral dimensions and offering actionable guidance, this work provides a pragmatic roadmap for navigating the socio-technical complexity of early AI adoption and highlights future research directions for human-centric AI in SE.

</details>


### [19] [Automated Code Review Assignments: An Alternative Perspective of Code Ownership on GitHub](https://arxiv.org/abs/2512.05551)
*Jai Lal Lulla,Raula Gaikovina Kula,Christoph Treude*

Main category: cs.SE

TL;DR: 本文首次对GitHub的CODEOWNERS功能进行大规模实证研究，分析其在84.4万次拉取请求中的使用情况，发现CODEOWNERS能改善代码审查流程并重新分配审查责任。


<details>
  <summary>Details</summary>
Motivation: 随着软件供应链攻击增加，代码所有权机制对保证软件质量越发重要。GitHub的CODEOWNERS功能虽能自动指定代码审查者，但其实际采用情况和效果尚不明确。

Method: 通过对844,000次拉取请求、190万条评论和200万次审查的大规模实证分析，追踪10,287名代码所有者的审查活动，并使用回归断点设计(RDD)分析CODEOWNERS采用前后的变化。

Result: 代码所有者普遍遵守CODEOWNERS规则，其协作行为与传统所有权指标相似，但能带来更顺畅、更快捷的PR工作流程。采用CODEOWNERS后，审查责任从核心开发者向更广泛群体转移。

Conclusion: CODEOWNERS是改善软件治理和韧性的有前景但未充分利用的机制，可作为增强开源开发安全性、责任性和工作流效率的替代所有权方法。

Abstract: Code ownership is central to ensuring accountability and maintaining quality in large-scale software development. Yet, as external threats such as software supply chain attacks on project health and quality assurance increase, mechanisms for assigning and enforcing responsibility have become increasingly critical. In 2017, GitHub introduced the CODEOWNERS feature, which automatically designates reviewers for specific files to strengthen accountability and protect critical parts of the codebase. Despite its potential, little is known about how CODEOWNERS is actually adopted and practiced. We present the first large-scale empirical study of CODEOWNERS usage across over 844,000 pull requests with 1.9 million comments and over 2 million reviews. We identify 10,287 code owners to track their review activities. Results indicate that codeowners tend to adhere the rules specified in the CODEOWNERS file, exhibit similar collaborative behaviours to traditional metrics of ownership, but tend to contribute to a smoother and faster PR workflow over time. Finally, using regression discontinuity design (RDD) analysis, we find that repositories adopting CODEOWNERS experience shifts in review dynamics, as ownership redistributes review responsibilities away from core developers. Our results position CODEOWNERS as a promising yet underutilized mechanism for improving software governance and resilience. We discuss how projects can leverage this alternative ownership method as a perspective to enhance security, accountability, and workflow efficiency in open-source development.

</details>


### [20] [Metronome: Differentiated Delay Scheduling for Serverless Functions](https://arxiv.org/abs/2512.05703)
*Zhuangbin Chen,Juzheng Zheng,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出了Metronome框架，通过预测性延时调度优化无服务器函数调度，利用机器学习预测执行时间，显著降低平均执行时间并保证SLA合规。


<details>
  <summary>Details</summary>
Motivation: 无服务器(FaaS)计算因其易管理性和弹性而兴起，但其动态和事件驱动的特性使得调度优化具有挑战性。数据局部性在传统集群计算中通过延时调度已被证明有效，但在无服务器平台中的应用尚未充分探索。

Method: 系统评估无服务器环境中的现有延时调度方法，发现三个关键观察：函数输入特性影响调度效果、无服务器计算具有更复杂的局部性模式（数据和基础设施）、异构函数执行时间使基于规则的延时阈值无效。基于此提出Metronome框架，采用随机森林回归模型预测不同节点上的函数执行时间，实现差异化的延时调度。

Result: 在OpenLambda上的实现表明，Metronome显著优于基线方法，函数平均执行时间减少64.88%-95.83%，在高并发下仍保持性能优势并确保SLA合规。

Conclusion: Metronome通过预测机制和差异化延时调度，有效解决了无服务器函数调度中的局部性优化问题，为无服务器平台的高效调度提供了可行方案。

Abstract: Function-as-a-Service (FaaS) computing is an emerging cloud computing paradigm for its ease-of-management and elasticity. However, optimizing scheduling for serverless functions remains challenging due to their dynamic and event-driven nature. While data locality has been proven effective in traditional cluster computing systems through delay scheduling, its application in serverless platforms remains largely unexplored. In this paper, we systematically evaluate existing delay scheduling methods in serverless environments and identify three key observations: 1) delay scheduling benefits vary significantly based on function input characteristics; 2) serverless computing exhibits more complex locality patterns than cluster computing systems, encompassing both data locality and infrastructure locality; and 3) heterogeneous function execution times make rule-based delay thresholds ineffective. Based on these insights, we propose Metronome, a differentiated delay scheduling framework that employs predictive mechanisms to identify optimal locality-aware nodes for individual functions. Metronome leverages an online Random Forest Regression model to forecast function execution times across various nodes, enabling informed delay decisions while preventing SLA violations. Our implementation on OpenLambda shows that Metronome significantly outperforms baselines, achieving 64.88%-95.83% reduction in mean execution time for functions, while maintaining performance advantages under increased concurrency levels and ensuring SLA compliance.

</details>


### [21] [MicroRacer: Detecting Concurrency Bugs for Cloud Service Systems](https://arxiv.org/abs/2512.05716)
*Zhiling Deng,Juepeng Wang,Zhuangbin Chen*

Main category: cs.SE

TL;DR: MicroRacer是一个非侵入式自动化框架，专门用于在微服务架构的分布式系统中检测并发错误


<details>
  <summary>Details</summary>
Motivation: 现代云端应用采用微服务架构，用户请求经过多个服务和机器，存在复杂的交互关系，使得系统容易受到并发错误影响。现有的并发错误检测方法存在侵入性且无法处理微服务架构复杂性

Method: 通过在运行时动态注入广泛使用的库来收集详细的追踪数据，而不需要修改应用代码。利用这些数据分析服务系统中常见操作的发生顺序关系和资源访问模式，识别可疑并发操作，并采用三阶段验证过程来测试和确认并发错误

Result: 在开源微服务基准测试中进行的实验，验证了MicroRacer的有效性和效率，能够准确检测和定位并发问题

Conclusion: MicroRacer为非侵入式并发错误检测提供了一种有效的解决方案，特别适用于复杂的微服务架构环境

Abstract: Modern cloud applications delivering global services are often built on distributed systems with a microservice architecture. In such systems, end-to-end user requests traverse multiple different services and machines, exhibiting intricate interactions. Consequently, cloud service systems are vulnerable to concurrency bugs, which pose significant challenges to their reliability. Existing methods for concurrency bug detection often fall short due to their intrusive nature and inability to handle the architectural complexities of microservices. To address these limitations, we propose MicroRacer, a non-intrusive and automated framework for detecting concurrency bugs in such environments. By dynamically instrumenting widely-used libraries at runtime, MicroRacer collects detailed trace data without modifying the application code. Such data are utilized to analyze the happened-before relationship and resource access patterns of common operations within service systems. Based on this information, MicroRacer identifies suspicious concurrent operations and employs a three-stage validation process to test and confirm concurrency bugs. Experiments on open-source microservice benchmarks with replicated industrial bugs demonstrate MicroRacer's effectiveness and efficiency in accurately detecting and pinpointing concurrency issues.

</details>


### [22] [Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models](https://arxiv.org/abs/2512.05887)
*Sairam Vaidya,Marcel Böhme,Loris D'Antoni*

Main category: cs.SE

TL;DR: 提出了Germinator工具，使用语法引导和覆盖导向的模糊测试方法，自动为MLIR等可扩展编译器框架生成测试种子，在6个项目中提升了10-120%的代码覆盖率，并发现了88个新bug。


<details>
  <summary>Details</summary>
Motivation: 可扩展编译器框架虽然支持快速创建领域特定语言方言，但测试基础设施难以维护；现有方法无法同时实现方言无关（通用性）和方言有效（针对性）的自动化测试生成。

Method: 结合语法引导和覆盖导向的模糊测试：自动从方言规范提取语法结构，利用预训练大语言模型生成多样化种子输入，无需人工干预或训练数据。

Result: 在6个MLIR项目的91种方言上验证：种子输入比语法基线提升10-120%行覆盖率；发现88个未知bug（40个已确认），其中23个来自此前无自动化测试生成器的方言。

Conclusion: Germinator实现了方言无关且有效的测试生成，能规模化可控地测试低资源方言，显著提升编译器框架的可靠性。

Abstract: Modern extensible compiler frameworks-such as MLIR-enable rapid creation of domain-specific language dialects. This flexibility, however, makes correctness harder to ensure as the same extensibility that accelerates development also complicates maintaining the testing infrastructure. Extensible languages require automated test generation that is both dialect-agnostic (works across dialects without manual adaptation) and dialect-effective (targets dialect-specific features to find bugs). Existing approaches typically sacrifice one of these goals by either requiring manually constructed seed corpora for each dialect, or by failing to be effective. We present a dialect-agnostic and dialect-effective grammar-based and coverage-guided fuzzing approach for extensible compilers that combines two key insights from existing work: (i) the grammars of dialects, which already encode the structural and type constraints, can often be extracted automatically from the dialect specification; and (ii) these grammars can be used in combination with pre-trained large language models to automatically generate representative and diverse seed inputs from the full dialect space without requiring any manual input or training data. These seeds can then be used to bootstrap coverage-guided fuzzers. We built this approach into a tool, Germinator. When evaluated on six MLIR projects spanning 91 dialects, Germinator generated seeds improve line coverage by 10-120% over grammar-based baselines. We compare against grammar-based baselines because they are the only class of existing automatic seed generators that can be applied uniformly across MLIR's heterogeneous dialect ecosystem. Germinator discovers 88 previously unknown bugs (40 confirmed), including 23 in dialects with no prior automated test generators, demonstrating effective and controllable testing of low-resource dialects at scale.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations](https://arxiv.org/abs/2512.05156)
*Igor Halperin*

Main category: cs.AI

TL;DR: 提出两种基于信息论和热力学的无监督指标SF和SEP，用于评估大语言模型的任务忠实度，通过建模问答主题转换的概率分布来衡量生成内容的可靠性。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型对给定任务的忠实度是一个复杂挑战，现有方法存在不足，需要新的无监督评估指标。

Method: 将LLM视为二分信息引擎，隐藏层作为麦克斯韦妖控制信息转换。通过QCA三元组的概率分布建模，用KL散度计算语义忠实度指标SF，并通过凸优化同时推断转换矩阵。

Result: 开发了SF和SEP两个指标，SF通过KL散度量化忠实度（0-1分），SEP基于热力学熵产评估生成过程。实验证明高忠实度通常对应低熵产。

Conclusion: SF和SEP指标可单独或联合用于LLM评估和幻觉控制，在SEC文件摘要任务中验证了框架有效性。

Abstract: Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\bf Q}$ and ${\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.

</details>


### [24] [Bridging Traditional Machine Learning and Large Language Models: A Two-Part Course Design for Modern AI Education](https://arxiv.org/abs/2512.05167)
*Fang Li*

Main category: cs.AI

TL;DR: 一篇介绍将传统机器学习与现代大语言模型相结合的教学方法的论文，通过一个分为两部分的课程设计，帮助学生全面理解AI发展并掌握实用技能。


<details>
  <summary>Details</summary>
Motivation: 为了系统地弥合传统机器学习技术与现代大语言模型之间的差距，设计一种创新的教学方法，使学生能够全面理解人工智能的演变过程，并掌握传统和前沿技术的实用技能。

Method: 课程分为两个连续且互补的部分：基础机器学习概念和现代LLM应用。详细描述了课程架构、实施策略、评估方法，并通过一个为期两个七周学期的暑期课程进行实施。

Result: 研究结果表明，这种综合方法增强了学生对AI领域的理解，并更好地为他们应对人工智能快速演变领域的行业需求做好了准备。

Conclusion: 这种将传统机器学习与现代LLM相结合的教学方法有效提升了学生对AI全景的理解，并增强了他们应对行业需求的能力，为AI教育提供了有价值的参考。

Abstract: This paper presents an innovative pedagogical approach for teaching artificial intelligence and data science that systematically bridges traditional machine learning techniques with modern Large Language Models (LLMs). We describe a course structured in two sequential and complementary parts: foundational machine learning concepts and contemporary LLM applications. This design enables students to develop a comprehensive understanding of AI evolution while building practical skills with both established and cutting-edge technologies. We detail the course architecture, implementation strategies, assessment methods, and learning outcomes from our summer course delivery spanning two seven-week terms. Our findings demonstrate that this integrated approach enhances student comprehension of the AI landscape and better prepares them for industry demands in the rapidly evolving field of artificial intelligence.

</details>


### [25] [On the Computability of Artificial General Intelligence](https://arxiv.org/abs/2512.05212)
*Georgios Mappouras,Charalambos Rossides*

Main category: cs.AI

TL;DR: 论文证明任何算法都无法真正创新，无法产生初始算法中不存在的功能能力


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能是否能达到人类水平的智能（AGI），特别是创新能力

Method: 借用前人AGI定义，证明算法的功能性上限定理

Result: 正式证明没有算法能展示初始算法本身不存在的功能能力

Conclusion: AI只能展示现有功能能力的组合排列，无法真正创新；这对AI发展和人类智能起源有重要意义

Abstract: In recent years we observed rapid and significant advancements in artificial intelligence (A.I.). So much so that many wonder how close humanity is to developing an A.I. model that can achieve human level of intelligence, also known as artificial general intelligence (A.G.I.). In this work we look at this question and we attempt to define the upper bounds, not just of A.I., but rather of any machine-computable process (a.k.a. an algorithm). To answer this question however, one must first precisely define A.G.I. We borrow prior work's definition of A.G.I. [1] that best describes the sentiment of the term, as used by the leading developers of A.I. That is, the ability to be creative and innovate in some field of study in a way that unlocks new and previously unknown functional capabilities in that field. Based on this definition we draw new bounds on the limits of computation. We formally prove that no algorithm can demonstrate new functional capabilities that were not already present in the initial algorithm itself. Therefore, no algorithm (and thus no A.I. model) can be truly creative in any field of study, whether that is science, engineering, art, sports, etc. In contrast, A.I. models can demonstrate existing functional capabilities, as well as combinations and permutations of existing functional capabilities. We conclude this work by discussing the implications of this proof both as it regards to the future of A.I. development, as well as to what it means for the origins of human intelligence.

</details>


### [26] [Resolving Zadehs Paradox Axiomatic Possibility Theory as a Foundation for Reliable Artificial Intelligence](https://arxiv.org/abs/2512.05257)
*Bychkov Oleksii,Bychkova Sophia,Lytvynchuk Khrystyna*

Main category: cs.AI

TL;DR: 本文论证可能性理论为Dempster-Shafer理论悖论提供了根本解决方案，通过基于可能性和必要性测度的公理化方法构建了不确定性处理的严谨基础。


<details>
  <summary>Details</summary>
Motivation: 解决DST悖论危机，证明可能性理论不仅是一种替代方案，而是能够从根本上解决DST的逻辑陷阱问题。

Method: 采用比较分析方法，对比概率论、证据理论和可能性理论三种范式，并以经典医疗诊断困境为例进行验证。

Result: 研究表明可能性理论能够正确处理矛盾数据，避免DST的逻辑陷阱，使形式推理更接近自然智能逻辑。

Conclusion: 可能性理论为解决DST悖论提供了根本性的解决方案，建立了逻辑一致且数学严谨的不确定性处理基础。

Abstract: This work advances and substantiates the thesis that the resolution of this crisis lies in the domain of possibility theory, specifically in the axiomatic approach developed in Bychkovs article. Unlike numerous attempts to fix Dempster rule, this approach builds from scratch a logically consistent and mathematically rigorous foundation for working with uncertainty, using the dualistic apparatus of possibility and necessity measures. The aim of this work is to demonstrate that possibility theory is not merely an alternative, but provides a fundamental resolution to DST paradoxes. A comparative analysis of three paradigms will be conducted probabilistic, evidential, and possibilistic. Using a classic medical diagnostic dilemma as an example, it will be shown how possibility theory allows for correct processing of contradictory data, avoiding the logical traps of DST and bringing formal reasoning closer to the logic of natural intelligence.

</details>


### [27] [AI & Human Co-Improvement for Safer Co-Superintelligence](https://arxiv.org/abs/2512.05356)
*Jason Weston,Jakob Foerster*

Main category: cs.AI

TL;DR: 论文主张应以'共同进步'（人类与AI协作实现共同超智能）作为AI发展的优先目标，而非单纯追求AI的自我改进。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域过度关注自我改进目标存在危险且难以实现，需要更安全可行的替代方向。

Method: 通过强调人类研究员与AI系统的协作研究循环，从构思到实验共同推进AI研究。

Result: 提出共同进步框架能加速AI研究发展，并通过人机共生实现更安全的超智能。

Conclusion: 以人类研究改进为核心的合作模式是实现超智能更快速、更安全的路径。

Abstract: Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.

</details>


### [28] [ChipMind: Retrieval-Augmented Reasoning for Long-Context Circuit Design Specifications](https://arxiv.org/abs/2512.05371)
*Changwen Xing,SamZaak Wong,Xinlai Wan,Yanfeng Lu,Mengli Zhang,Zebin Ma,Lei Qi,Zhengxiong Li,Nan Guan,Zhe Jiang,Xi Wang,Jun Yang*

Main category: cs.AI

TL;DR: ChipMind是一个基于知识图谱增强推理的框架，专门处理冗长的集成电路规范文档，解决了当前LLMs上下文窗口限制下的IC设计自动化难题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在芯片开发自动化方面潜力巨大，但实际应用受到上下文窗口长度的限制。现有的上下文扩展方法难以对复杂冗长的电路规范进行有效的语义建模和多跳推理。

Method: ChipMind首先通过电路语义感知知识图谱构建方法将电路规范转化为领域知识图谱ChipKG，然后采用ChipKG增强推理机制，结合信息论自适应检索动态追踪逻辑依赖，以及意图感知语义过滤去除无关噪声。

Result: 在工业级规范推理基准上，ChipMind显著优于现有最优基线方法，平均提升34.59%，最高提升达72.73%。

Conclusion: 该框架填补了LLM辅助硬件设计从学术研究到工业部署之间的关键空白。

Abstract: While Large Language Models (LLMs) demonstrate immense potential for automating integrated circuit (IC) development, their practical deployment is fundamentally limited by restricted context windows. Existing context-extension methods struggle to achieve effective semantic modeling and thorough multi-hop reasoning over extensive, intricate circuit specifications. To address this, we introduce ChipMind, a novel knowledge graph-augmented reasoning framework specifically designed for lengthy IC specifications. ChipMind first transforms circuit specifications into a domain-specific knowledge graph ChipKG through the Circuit Semantic-Aware Knowledge Graph Construction methodology. It then leverages the ChipKG-Augmented Reasoning mechanism, combining information-theoretic adaptive retrieval to dynamically trace logical dependencies with intent-aware semantic filtering to prune irrelevant noise, effectively balancing retrieval completeness and precision. Evaluated on an industrial-scale specification reasoning benchmark, ChipMind significantly outperforms state-of-the-art baselines, achieving an average improvement of 34.59% (up to 72.73%). Our framework bridges a critical gap between academic research and practical industrial deployment of LLM-aided Hardware Design (LAD).

</details>


### [29] [BEAVER: An Efficient Deterministic LLM Verifier](https://arxiv.org/abs/2512.05439)
*Tarun Suresh,Nalin Wadhwa,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.AI

TL;DR: BEAVER是首个实用的LLM约束满足确定性概率边界计算框架，能够提供可靠保证验证模型输出是否符合约束


<details>
  <summary>Details</summary>
Motivation: 大型语言模型从研究原型转向生产系统时，需要可靠方法来验证模型输出是否满足约束条件，而基于采样的估计无法提供可靠保证

Method: 使用新颖的token trie和frontier数据结构系统性地探索生成空间，在任何迭代中都保持可证明的可靠边界

Result: BEAVER在相同计算预算下实现了6-8倍更紧的概率边界，识别出3-4倍更多高风险实例

Conclusion: BEAVER框架能够提供松散边界或经验评估无法实现的精确特征描述和风险评估能力

Abstract: As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.

</details>


### [30] [The Seeds of Scheming: Weakness of Will in the Building Blocks of Agentic Systems](https://arxiv.org/abs/2512.05449)
*Robert Yang*

Main category: cs.AI

TL;DR: 这篇论文提出了'akrasia'（意志薄弱）作为分析AI系统不一致性的框架，并开发了Akrasia Benchmark来衡量模型自我控制能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型表现出知道正确答案但无法坚持的矛盾行为，类似于人类哲学中的意志薄弱现象。

Method: 引入了Akrasia Benchmark，包含基线、同义词、时间延迟和诱惑四种测试条件来量化模型的不一致性。

Result: 该基准能为不同模型家族、解码策略和诱惑类型提供定量比较，揭示微观不一致可能导致的宏观不稳定。

Conclusion: 通过将AI不一致性重新定义为意志薄弱，这项工作在哲学、心理学和智能体AI科学之间建立了实证桥梁。

Abstract: Large language models display a peculiar form of inconsistency: they "know" the correct answer but fail to act on it. In human philosophy, this tension between global judgment and local impulse is called akrasia, or weakness of will. We propose akrasia as a foundational concept for analyzing inconsistency and goal drift in agentic AI systems. To operationalize it, we introduce a preliminary version of the Akrasia Benchmark, currently a structured set of prompting conditions (Baseline [B], Synonym [S], Temporal [T], and Temptation [X]) that measures when a model's local response contradicts its own prior commitments. The benchmark enables quantitative comparison of "self-control" across model families, decoding strategies, and temptation types. Beyond single-model evaluation, we outline how micro-level akrasia may compound into macro-level instability in multi-agent systems that may be interpreted as "scheming" or deliberate misalignment. By reframing inconsistency as weakness of will, this work connects agentic behavior to classical theories of agency and provides an empirical bridge between philosophy, psychology, and the emerging science of agentic AI.

</details>


### [31] [MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models](https://arxiv.org/abs/2512.05530)
*Chuang Yu,Jinmiao Zhao,Mingxuan Zhao,Yunpeng Liu,Xiujun Shu,Yuanhao Feng,Bo Wang,Xiangyu Yue*

Main category: cs.AI

TL;DR: 论文提出MIND推理框架，通过RAD范式、P2CL策略和MCA优化策略，使多模态大语言模型具备'理解-反思-修正'的类人认知能力，在多个公开数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在多推理语义建模、逻辑鲁棒性方面存在不足，易受复杂场景误导，需要从被动模仿推理转向主动判别推理。

Method: 采用RAD范式自动生成多样推理路径扩充数据集；设计P2CL两阶段修正学习策略（多推理正向学习+主动逻辑判别修正）；提出MCA优化策略解决多推理语义空间表示纠缠问题。

Result: 在涵盖科学、常识和数学场景的多个公开数据集上实现最先进性能。

Conclusion: MIND框架为推进多模态大语言模型向更高认知智能水平提供了新视角。

Abstract: Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of "Understand -> Rethink -> Correct", and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND

</details>


### [32] [CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning](https://arxiv.org/abs/2512.05576)
*Ting-Ting Xie,Yixin Zhang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Current clinical agent built on small LLMs, such as TxAgent suffer from a \textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent.

</details>


### [33] [Ontology Learning with LLMs: A Benchmark Study on Axiom Identification](https://arxiv.org/abs/2512.05594)
*Roos M. Bakker,Daan L. Di Scala,Maaike H. T. de Boer,Stephan A. Raaijmakers*

Main category: cs.AI

TL;DR: 论文研究了使用LLMs自动化识别本体论公理的方法，创建了OntoAxiom基准，比较了不同提示策略和模型性能。


<details>
  <summary>Details</summary>
Motivation: 本体论开发复杂且需要专业知识，随着NLP技术特别是LLMs的发展，需要研究如何自动化识别本体论公理。

Method: 构建包含9个中等规模本体论的OntoAxiom基准（17,118三元组，2,771个公理），比较12个LLMs在三种few-shot设置和两种提示策略（Direct vs Axiom-by-Axiom）下的性能。

Result: AbA提示策略获得更高F1分数，性能因公理类型和领域而异，大模型优于小模型但小模型在资源受限环境下仍可用。

Conclusion: LLMs目前性能不足以完全自动化公理识别，但能为本体工程师提供有价值的候选公理支持本体开发和完善。

Abstract: Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.

</details>


### [34] [Enhancing Local Search for MaxSAT with Deep Differentiation Clause Weighting](https://arxiv.org/abs/2512.05619)
*Menghua Jiang,Haokai Gao,Shuhao Chen,Yin Chen*

Main category: cs.AI

TL;DR: 提出了一种区别处理PMS和WPMS问题的新型子句权重方案DeepDist，通过差异化的权重更新策略、新的初始化方法和优先满足单位/硬子句的decimation方法，显著提升了SLS求解器性能，在MaxSAT评测中超越现有最佳求解器。


<details>
  <summary>Details</summary>
Motivation: 现有的(W)PMS随机局部搜索算法主要关注子句权重方案设计，但往往未能充分区分PMS和WPMS问题，通常采用统一的权重更新策略，忽视了两类问题之间的关键结构差异。

Method: 1)首次根据不同条件分别更新PMS和WPMS实例的子句权重；2)新的初始化方法更好地适应两种实例类型的独特特征；3)提出优先满足单位子句和硬子句的decimation方法。

Result: 在最近MaxSAT评测的基准测试中，DeepDist优于最先进的SLS求解器。与TT-Open-WBO-Inc相结合的混合求解器甚至超越了MaxSAT Evaluation 2024的获胜者SPB-MaxSAT-c-Band和SPB-MaxSAT-c-FPS。

Conclusion: 提出的方法有效解决了PMS和WPMS问题的差异化处理需求，实验结果验证了方法的有效性，代码已开源。新的权重方案和配套方法为(W)PMS求解提供了有前景的方向。

Abstract: Partial Maximum Satisfiability (PMS) and Weighted Partial Maximum Satisfiability (WPMS) generalize Maximum Satisfiability (MaxSAT), with broad real-world applications. Recent advances in Stochastic Local Search (SLS) algorithms for solving (W)PMS have mainly focused on designing clause weighting schemes. However, existing methods often fail to adequately distinguish between PMS and WPMS, typically employing uniform update strategies for clause weights and overlooking critical structural differences between the two problem types. In this work, we present a novel clause weighting scheme that, for the first time, updates the clause weights of PMS and WPMS instances according to distinct conditions. This scheme also introduces a new initialization method, which better accommodates the unique characteristics of both instance types. Furthermore, we propose a decimation method that prioritizes satisfying unit and hard clauses, effectively complementing our proposed clause weighting scheme. Building on these methods, we develop a new SLS solver for (W)PMS named DeepDist. Experimental results on benchmarks from the anytime tracks of recent MaxSAT Evaluations show that DeepDist outperforms state-of-the-art SLS solvers. Notably, a hybrid solver combining DeepDist with TT-Open-WBO-Inc surpasses the performance of the MaxSAT Evaluation 2024 winners, SPB-MaxSAT-c-Band and SPB-MaxSAT-c-FPS, highlighting the effectiveness of our approach. The code is available at https://github.com/jmhmaxsat/DeepDist

</details>


### [35] [Evolutionary System 2 Reasoning: An Empirical Proof](https://arxiv.org/abs/2512.05760)
*Zeyuan Ma,Wenqi Huang,Guo-Huan Song,Hongshu Guo,Sijie Ma,Zhiguang Cao,Yue-Jiao Gong*

Main category: cs.AI

TL;DR: 本文提出了进化推理优化（ERO）框架，通过自然选择策略增强大语言模型的通用推理能力，实验表明即使较弱模型经过进化也能获得强大推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽在特定任务表现优异，但缺乏人类般的通用推理能力。研究探索能否通过进化方法让机器获得类似人类的系统2推理能力。

Method: 提出ERO框架：初始化多个LLMs作为种群，采用进化策略优化种群，以最大化最佳个体的量化推理分数。

Result: 实验发现：1）最新LLM（如GPT-5）系统2推理能力有限；2）弱模型（Qwen-7B）通过ERO进化后能涌现强大推理能力。

Conclusion: 进化方法可有效提升LLMs的通用推理能力，为机器智能发展提供新路径。

Abstract: Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs.

</details>


### [36] [The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics](https://arxiv.org/abs/2512.05765)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 论文反驳了LLMs是AGI死结的观点，提出瓶颈在于缺乏System-2协调层而非模式匹配本身，并理论化UCCT框架与MACI架构来实现推理。


<details>
  <summary>Details</summary>
Motivation: 针对学界批评LLMs仅是模式匹配器、无法实现真正推理的论点，作者认为这错误识别了瓶颈——问题不在模式库（System-1），而在于缺少协调层（System-2）。

Method: 提出UCCT理论，将推理建模为受有效支持度（ρ_d）、表征失配（d_r）和自适应锚定预算（γ log k）控制的相变；并设计MACI架构，包含诱饵机制（行为调控辩论）、过滤（苏格拉底式判断）与持久化（事务内存）。

Result: 理论表明，无锚定生成仅是模式库的最大似然检索，而锚定能将后验概率导向目标约束，从而涌现推理行为；MACI将常见质疑转化为可测试的协调失效案例。

Conclusion: AGI的实现路径应通过增强LLMs的协调层（而非绕过它们），证明LLMs本身是AGI的必要基础而非障碍。

Abstract: Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: "mere pattern matchers" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while "reasoning" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.

</details>


### [37] [Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma](https://arxiv.org/abs/2512.05824)
*Hafsa Akebli,Adam Shephard,Vincenzo Della Mea,Nasir Rajpoot*

Main category: cs.AI

TL;DR: 研究者开发了一个多模态肿瘤智能体（MOA），它结合了基于TITAN基础模型的病理学工具和外部生物医学资源，用于预测低级别胶质瘤的IDH1突变。在TCGA-LGG队列的488名患者上进行评估，MOA融合病理学特征后取得最高性能（F1得分0.912）。


<details>
  <summary>Details</summary>
Motivation: 低级别胶质瘤中IDH1突变可定义临床不同的亚组，具有特定的预后和治疗意义，但需要更准确的预测方法。

Method: MOA整合了一个基于TITAN基础模型的病理学工具用于IDH1突变预测，并结合PubMed、Google搜索和OncoKB等外部资源进行临床和基因组数据的推理。

Result: 在TCGA-LGG队列中评估：不含病理学工具的MOA优于临床基线（F1得分0.826 vs 0.798）；融合病理学特征的MOA达到最高性能（F1得分0.912），超过病理学基线（0.894）和融合病理学-临床基线（0.897）。

Conclusion: 所提出的智能体通过外部生物医学资源捕获了互补的突变相关信息，实现了准确的IDH1突变预测。

Abstract: Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.

</details>


### [38] [To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis](https://arxiv.org/abs/2512.05925)
*Federico Bianchi,Yongchan Kwon,Zachary Izzo,Linjun Zhang,James Zou*

Main category: cs.AI

TL;DR: 该研究开发了基于GPT-5的论文检查器，发现AI顶会论文中存在显著且日益增多的客观错误（如公式、图表错误），错误数量在2018-2025年间上升了55.3%，AI检查器识别精确度达83.2%，且能自动修正75.8%的错误。


<details>
  <summary>Details</summary>
Motivation: 同行评审压力增大导致论文错误难以检测，错误在文献中传播会误导后续研究并影响可复现性。

Method: 使用GPT-5构建论文正确性检查器，针对顶会论文中的客观错误（公式、计算、图表等可验证错误）进行系统分析，并人工验证AI识别结果。

Result: NeurIPS论文错误从2021年的3.8个/篇增至2025年的5.9个；AI检查器识别错误的精确度为83.2%（人工验证316条错误中263条属实），可自动修正75.8%的错误。

Conclusion: 前沿大语言模型能有效检测和修正论文中的客观错误，有助于夯实学术知识基础，减少文献混乱。

Abstract: How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.

</details>


### [39] [Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem](https://arxiv.org/abs/2512.05946)
*Truong Thanh Hung Nguyen,Truong Thinh Nguyen,Hung Cao*

Main category: cs.AI

TL;DR: 提出了一种结合变分量子电路与Rainbow DQN的新算法VQR-DQN，用于解决NP难的资源分配问题，在四个基准测试中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 资源分配问题因其组合复杂性而NP难。传统深度强化学习方法受限于函数逼近器的表示能力。

Method: 将量子叠加和纠缠特性融入Rainbow DQN，设计基于环拓扑的变分量子电路，并将人力资源分配问题建模为马尔可夫决策过程。

Result: 在四个基准测试中，VQR-DQN相比随机基线减少了26.8%的标准化完工时间，比Double DQN和经典Rainbow DQN提升4.9-13.4%。

Conclusion: 实验结果验证了电路表达能力、纠缠与策略质量的理论联系，展示了量子增强DRL在大规模资源分配中的潜力。

Abstract: Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.

</details>


### [40] [TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.05943)
*Shima Imani,Seungwhan Moon,Lambert Mathias,Lu Zhang,Babak Damavandi*

Main category: cs.AI

TL;DR: 提出了名为TRACE的框架，用于透明化和一致性评估推理轨迹，以诊断视觉-语言模型在复杂数学和科学问题上的推理错误。


<details>
  <summary>Details</summary>
Motivation: 标准最终答案评估往往掩盖推理错误，导致无声故障持续存在。现有方法主要关注结果，而忽视了中间推理步骤的可靠性评估。

Method: 核心是利用辅助推理集(ARS)，即紧凑的子问-答对，分解复杂问题，通过基于一致性的指标评估中间步骤，揭示标准评估忽略的故障。

Result: 实验表明，ARS之间的一致性相关于最终答案的正确性，并能精确定位推理失败的具体步骤，为模型改进提供可行的信号。

Conclusion: TRACE定义了置信区域，区分可靠与不可靠的推理路径，支持有效的过滤、调试和模型精炼，旨在提升模型的可靠推理能力。

Abstract: Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.

</details>


### [41] [SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code](https://arxiv.org/abs/2512.05954)
*Shima Imani,Seungwhan Moon,Adel Ahmadyan,Lu Zhang,Kirmani Ahmed,Babak Damavandi*

Main category: cs.AI

TL;DR: SymPyBench是一个包含15,045个大学物理问题的大规模合成基准数据集，支持无限参数配置，包含三种题型和三种新颖的评估指标，用于测试语言模型的科学推理能力


<details>
  <summary>Details</summary>
Motivation: 现有基准无法充分评估语言模型在科学推理中的鲁棒性和可解释性，需要动态、可参数化的问题集来测试模型在不同变体下的表现

Method: 创建完全参数化的问题库，每个问题附带结构化推理步骤和可执行Python代码，引入Consistency Score、Failure Rate和Confusion Rate等新评估指标

Result: 在指令调优语言模型上的实验揭示了科学推理的优势与局限，模型在不同问题类型和参数变体中表现出可变性

Conclusion: SymPyBench为开发更鲁棒、可解释的推理系统奠定了基础，展示了代码驱动基准在评估科学推理能力方面的价值

Abstract: We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems

</details>
