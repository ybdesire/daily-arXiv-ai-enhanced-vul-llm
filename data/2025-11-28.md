<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 3]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [On the Limits of Innate Planning in Large Language Models](https://arxiv.org/abs/2511.21591)
*Charles Schepanowski,Charles Ling*

Main category: cs.AI

TL;DR: 研究表明现有大语言模型在执行8拼图规划任务时存在明显局限性，尤其在状态跟踪和目标导向规划方面表现不足


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在不需要代码执行或其他工具的纯提示条件下的规划和状态推理能力

Method: 通过8拼图任务测试四种模型在不同提示策略下的表现，包括零样本、思维链和算法思维等条件

Result: 即使提供分层纠正反馈，模型成功率仍有限；在外部验证器辅助下，所有模型仍无法解决任何拼图

Conclusion: 当前大语言模型在规划和状态推理方面存在实质性限制，需要开发显式状态维护和结构化搜索机制

Abstract: Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.

</details>


### [2] [Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling](https://arxiv.org/abs/2511.21636)
*Peter S. Hovmand,Kari O'Donnell,Callie Ogland-Hand,Brian Biroscak,Douglas D. Gunzler*

Main category: cs.AI

TL;DR: 本文提出了一个将系统动力学和结构方程建模结合的数学框架，用于生成系统分布、开发方法并比较结果，以促进负责任AI/ML的发展。


<details>
  <summary>Details</summary>
Motivation: 尽管AI/ML模型能解决以往无法解决的问题，但也会意外放大人类偏见。现有方法因基于不同假设（如Dana Meadow的"不可避免的先验"）而难以整合，阻碍了负责任AI/ML的进展。

Method: 将系统动力学和结构方程建模统一到一个共同的数学框架中，用于生成系统分布、开发方法论，并进行结果比较。

Result: 该框架能为数据科学和AI/ML应用提供系统动力学的认识论基础，支持负责任AI的开发。

Conclusion: 整合系统动力学与结构方程建模的数学框架有助于克服方法差异，推动更负责任、透明的AI/ML系统发展。

Abstract: AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's "the unavoidable a priori"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.

</details>


### [3] [Agentic Learner with Grow-and-Refine Multimodal Semantic Memory](https://arxiv.org/abs/2511.21678)
*Weihao Bo,Shan Zhang,Yanpeng Sun,Jingjing Wu,Qunyi Xie,Xiao Tan,Kunbin Chen,Wei He,Xiaofan Li,Na Zhao,Jingdong Wang,Zechao Li*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [4] [TAB-DRW: A DFT-based Robust Watermark for Generative Tabular Data](https://arxiv.org/abs/2511.21600)
*Yizhou Zhao,Xiang Li,Peter Song,Qi Long,Weijie Su*

Main category: cs.CR

TL;DR: 本文提出了一种高效的表格数据水印方案TAB-DRW，通过频域嵌入水印信号解决现有方法计算成本高、混合数据类型支持差和鲁棒性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI产生的高保真合成表格数据在医疗、金融等领域引发数据溯源和滥用的担忧，现有水印方法存在计算效率低、不支持混合数据、鲁棒性差等局限性。

Method: 采用Yeo-Johnson变换和标准化归一化异构特征，应用离散傅里叶变换(DFT)，根据预计算伪随机位调整自适应选择条目的虚部，并引入基于排名的伪随机位生成方法实现无存储开销的行级检索。

Result: 在五个基准表格数据集上的实验表明，TAB-DRW对常见后处理攻击具有强检测性和鲁棒性，同时保持高数据保真度并完全支持混合类型特征。

Conclusion: TAB-DRW为生成式表格数据提供了一种高效、鲁棒的水印解决方案，有效解决了数据溯源和防滥用问题。

Abstract: The rise of generative AI has enabled the production of high-fidelity synthetic tabular data across fields such as healthcare, finance, and public policy, raising growing concerns about data provenance and misuse. Watermarking offers a promising solution to address these concerns by ensuring the traceability of synthetic data, but existing methods face many limitations: they are computationally expensive due to reliance on large diffusion models, struggle with mixed discrete-continuous data, or lack robustness to post-modifications. To address them, we propose TAB-DRW, an efficient and robust post-editing watermarking scheme for generative tabular data. TAB-DRW embeds watermark signals in the frequency domain: it normalizes heterogeneous features via the Yeo-Johnson transformation and standardization, applies the discrete Fourier transform (DFT), and adjusts the imaginary parts of adaptively selected entries according to precomputed pseudorandom bits. To further enhance robustness and efficiency, we introduce a novel rank-based pseudorandom bit generation method that enables row-wise retrieval without incurring storage overhead. Experiments on five benchmark tabular datasets show that TAB-DRW achieves strong detectability and robustness against common post-processing attacks, while preserving high data fidelity and fully supporting mixed-type features.

</details>
