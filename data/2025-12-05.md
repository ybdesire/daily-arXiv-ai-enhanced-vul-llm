<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 32]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.CR](#cs.CR) [Total: 12]
- [cs.SE](#cs.SE) [Total: 15]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Solving N-Queen Problem using Las Vegas Algorithm with State Pruning](https://arxiv.org/abs/2512.04139)
*Susmita Sharma,Aayush Shrestha,Sitasma Thapa,Prashant Timalsina,Prakash Poudyal*

Main category: cs.AI

TL;DR: 提出了一种结合Las Vegas算法和迭代剪枝的混合算法，用于解决N皇后问题，在保证解的质量的同时显著提高了大规模实例的计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统的完全方法（如回溯）虽然能保证找到解，但时间复杂度高，不适用于大规模N皇后问题；而随机方法（如Las Vegas算法）虽快但性能波动大。

Method: 在标准Las Vegas算法框架上引入迭代剪枝，在随机放置皇后时动态消除无效位置，从而缩小搜索空间。

Result: 与传统回溯算法相比，该算法能更快地生成有效解，尤其在资源受限的计算环境中表现出更好的计算成本与解的质量平衡。

Conclusion: 该混合算法在大规模N皇后问题上提供了计算效率和解质量的有效折衷，是追求单次及时解而非完全解的优选方案。

Abstract: The N-Queens problem, placing all N queens in a N x N chessboard where none attack the other, is a classic problem for constraint satisfaction algorithms. While complete methods like backtracking guarantee a solution, their exponential time complexity makes them impractical for large-scale instances thus, stochastic approaches, such as Las Vegas algorithm, are preferred. While it offers faster approximate solutions, it suffers from significant performance variance due to random placement of queens on the board. This research introduces a hybrid algorithm built on top of the standard Las Vegas framework through iterative pruning, dynamically eliminating invalid placements during the random assignment phase, thus this method effectively reduces the search space. The analysis results that traditional backtracking scales poorly with increasing N. In contrast, the proposed technique consistently generates valid solutions more rapidly, establishing it as a superior alternative to use where a single, timely solution is preferred over completeness. Although large N causes some performance variability, the algorithm demonstrates a highly effective trade-off between computational cost and solution fidelity, making it particularly suited for resource-constrained computing environments.

</details>


### [2] [RippleBench: Capturing Ripple Effects Using Existing Knowledge Repositories](https://arxiv.org/abs/2512.04144)
*Roy Rinberg,Usha Bhalla,Igor Shilov,Flavio P. Calmon,Rohit Gandikota*

Main category: cs.AI

TL;DR: 论文介绍了RippleBench-Maker工具，用于生成问答数据集以衡量模型编辑任务中的涟漪效应，并通过WMDP数据集构建了RippleBench-Bio基准，评估了八种先进遗忘方法。


<details>
  <summary>Details</summary>
Motivation: 针对语言模型的干预（如遗忘、去偏或模型编辑）常会引发涟漪效应，即修改目标信息时对相关但非预期领域产生副作用，需要系统化的评估工具。

Method: 基于Wikipedia的RAG管道（WikiRAG）生成与目标概念不同语义距离的多选题，构建RippleBench-Bio基准，量化八种遗忘方法的涟漪效应。

Result: 所有遗忘方法在距离被遗忘知识越远的主题上均出现显著准确率下降，且传播模式各异，揭示了涟漪效应的普遍性和方法特异性。

Conclusion: RippleBench-Maker为模型编辑任务的涟漪效应评估提供了自动化工具，强调了干预副作用的重要性，并开源了代码和基准以促进后续研究。

Abstract: Targeted interventions on language models, such as unlearning, debiasing, or model editing, are a central method for refining model behavior and keeping knowledge up to date. While these interventions aim to modify specific information within models (e.g., removing virology content), their effects often propagate to related but unintended areas (e.g., allergies); these side-effects are commonly referred to as the ripple effect. In this work, we present RippleBench-Maker, an automatic tool for generating Q&A datasets that allow for the measurement of ripple effects in any model-editing task. RippleBench-Maker builds on a Wikipedia-based RAG pipeline (WikiRAG) to generate multiple-choice questions at varying semantic distances from the target concept (e.g., the knowledge being unlearned). Using this framework, we construct RippleBench-Bio, a benchmark derived from the WMDP (Weapons of Mass Destruction Paper) dataset, a common unlearning benchmark. We evaluate eight state-of-the-art unlearning methods and find that all exhibit non-trivial accuracy drops on topics increasingly distant from the unlearned knowledge, each with distinct propagation profiles. To support ongoing research, we release our codebase for on-the-fly ripple evaluation, along with the benchmark, RippleBench-Bio.

</details>


### [3] [Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment](https://arxiv.org/abs/2512.04210)
*Huy Nghiem,Swetasudha Panda,Devashish Khatwani,Huy V. Nguyen,Krishnaram Kenthapadi,Hal Daumé*

Main category: cs.AI

TL;DR: 本文提出了一种迭代部署后对齐框架，结合KTO和DPO优化医疗对话模型的安全性，在CARES-18K基准测试中显著提升有害查询检测能力，同时揭示模型架构相关的校准偏差。


<details>
  <summary>Details</summary>
Motivation: 当前医疗领域大语言模型存在安全性与过度拒绝合理查询的矛盾，需要一种部署后持续优化的方法来平衡患者安全、用户信任和临床实用性。

Method: 采用迭代后部署对齐框架，结合Kahneman-Tversky优化和直接偏好优化技术，利用领域特定的安全信号对模型进行微调，并在CARES-18K对抗鲁棒性基准上评估四种LLM模型。

Result: 模型在有害查询检测方面获得高达42%的安全指标提升，同时发现了错误拒绝之间的权衡关系，揭示了不同模型架构存在的校准偏差。消融研究表明需要根据不同情况选择自评估或外部评估方法。

Conclusion: 医疗对话助手的设计需要采用最佳实践来平衡患者安全、用户信任和临床效用，迭代后部署对齐是提升模型安全性的有效途径。

Abstract: Large Language Models (LLMs) are increasingly used in healthcare, yet ensuring their safety and trustworthiness remains a barrier to deployment. Conversational medical assistants must avoid unsafe compliance without over-refusing benign queries. We present an iterative post-deployment alignment framework that applies Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to refine models against domain-specific safety signals. Using the CARES-18K benchmark for adversarial robustness, we evaluate four LLMs (Llama-3B/8B, Meditron-8B, Mistral-7B) across multiple cycles. Our results show up to 42% improvement in safety-related metrics for harmful query detection, alongside interesting trade-offs against erroneous refusals, thereby exposing architecture-dependent calibration biases. We also perform ablation studies to identify when self-evaluation is reliable and when external or finetuned judges are necessary to maximize performance gains. Our findings underscore the importance of adopting best practices that balance patient safety, user trust, and clinical utility in the design of conversational medical assistants.

</details>


### [4] [Educational Cone Model in Embedding Vector Spaces](https://arxiv.org/abs/2512.04227)
*Yo Ehara*

Main category: cs.AI

TL;DR: 本文提出教育锥模型，一种基于嵌入空间的几何框架，通过锥形分布假设分析文本难度，优化选择最适合教育文本的嵌入方法。


<details>
  <summary>Details</summary>
Motivation: 人工标注难度等级的数据集对智能教育系统至关重要，但众多嵌入方法难以选择最合适的方法。

Method: 提出教育锥模型，假设简单文本多样性低（聚焦基础概念），复杂文本多样性高，形成锥形分布。通过设计损失函数推导闭式解，避免高计算成本。

Result: 在真实数据集上的实证测试验证了模型在识别与难度标注教育文本最匹配的嵌入空间方面的有效性和速度。

Conclusion: 教育锥模型为嵌入方法评估提供了高效框架，有助于智能教育系统中的文本难度分析。

Abstract: Human-annotated datasets with explicit difficulty ratings are essential in intelligent educational systems. Although embedding vector spaces are widely used to represent semantic closeness and are promising for analyzing text difficulty, the abundance of embedding methods creates a challenge in selecting the most suitable method. This study proposes the Educational Cone Model, which is a geometric framework based on the assumption that easier texts are less diverse (focusing on fundamental concepts), whereas harder texts are more diverse. This assumption leads to a cone-shaped distribution in the embedding space regardless of the embedding method used. The model frames the evaluation of embeddings as an optimization problem with the aim of detecting structured difficulty-based patterns. By designing specific loss functions, efficient closed-form solutions are derived that avoid costly computation. Empirical tests on real-world datasets validated the model's effectiveness and speed in identifying the embedding spaces that are best aligned with difficulty-annotated educational texts.

</details>


### [5] [Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework](https://arxiv.org/abs/2512.04228)
*Peter B. Walker,Hannah Davidson,Aiden Foster,Matthew Lienert,Thomas Pardue,Dale Russell*

Main category: cs.AI

TL;DR: 论文提出了双推理训练框架，通过结合肯定生成和结构化反事实否定来增强大语言模型的逻辑推理稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的训练主要基于肯定推理，容易受到逻辑谬误、对抗性攻击和因果推理失败的影响，特别是在处理否定、反例或错误前提时表现脆弱。

Method: 引入双推理训练框架，整合肯定生成与结构化反事实否定，基于形式逻辑、认知科学和对抗训练，使模型能够同时进行确认和否定推理。

Result: 通过实验证实现有主流平台的大语言模型在科学领域的否定推理中存在系统性弱点，新框架使模型能拒绝无效推理，提高稳健性和可解释性。

Conclusion: 双推理训练框架提升了模型的逻辑一致性和抗干扰能力，更符合人类推理模式，为科学、医疗和决策应用提供了更可靠的模型。

Abstract: Large Language Models (LLMs) have transformed natural language processing and hold growing promise for advancing science, healthcare, and decision-making. Yet their training paradigms remain dominated by affirmation-based inference, akin to \textit{modus ponens}, where accepted premises yield predicted consequents. While effective for generative fluency, this one-directional approach leaves models vulnerable to logical fallacies, adversarial manipulation, and failures in causal reasoning. This paper makes two contributions. First, it demonstrates how existing LLMs from major platforms exhibit systematic weaknesses when reasoning in scientific domains with negation, counterexamples, or faulty premises \footnote{Code to recreate these experiments are at https://github.com/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs. Second, it introduces a dual-reasoning training framework that integrates affirmative generation with structured counterfactual denial. Grounded in formal logic, cognitive science, and adversarial training, this training paradigm formalizes a computational analogue of ``denying the antecedent'' as a mechanism for disconfirmation and robustness. By coupling generative synthesis with explicit negation-aware objectives, the framework enables models that not only affirm valid inferences but also reject invalid ones, yielding systems that are more resilient, interpretable, and aligned with human reasoning.

</details>


### [6] [The Geometry of Benchmarks: A New Path Toward AGI](https://arxiv.org/abs/2512.04276)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 介绍了一个几何框架来评估AI自主性，将AI评估基准组织为模空间中的点，提出了自主AI尺度和GVU算子来量化自我改进能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估实践局限于孤立测试套件，无法评估通用性或自主自我改进能力，需要新的理论框架来理解AGI进展。

Method: 构建基准的模空间，定义自主AI层级（AAI Scale），提出Generator-Verifier-Updater（GVU）算子，引入自我改进系数κ作为能力泛函的Lie导数。

Result: 获得了确定性结果：密集基准族足以证明整个任务区域的性能；GVU动态的方差不等式为κ>0提供了充分条件。

Conclusion: AGI进展应被视为基准模空间上的流，由GVU动力学驱动而非单个排行榜分数，这为AI自主性评估提供了新范式。

Abstract: Benchmarks are the primary tool for assessing progress in artificial intelligence (AI), yet current practice evaluates models on isolated test suites and provides little guidance for reasoning about generality or autonomous self-improvement. Here we introduce a geometric framework in which all psychometric batteries for AI agents are treated as points in a structured moduli space, and agent performance is described by capability functionals over this space. First, we define an Autonomous AI (AAI) Scale, a Kardashev-style hierarchy of autonomy grounded in measurable performance on batteries spanning families of tasks (for example reasoning, planning, tool use and long-horizon control). Second, we construct a moduli space of batteries, identifying equivalence classes of benchmarks that are indistinguishable at the level of agent orderings and capability inferences. This geometry yields determinacy results: dense families of batteries suffice to certify performance on entire regions of task space. Third, we introduce a general Generator-Verifier-Updater (GVU) operator that subsumes reinforcement learning, self-play, debate and verifier-based fine-tuning as special cases, and we define a self-improvement coefficient $κ$ as the Lie derivative of a capability functional along the induced flow. A variance inequality on the combined noise of generation and verification provides sufficient conditions for $κ> 0$. Our results suggest that progress toward artificial general intelligence (AGI) is best understood as a flow on moduli of benchmarks, driven by GVU dynamics rather than by scores on individual leaderboards.

</details>


### [7] [Artificial Intelligence Applications in Horizon Scanning for Infectious Diseases](https://arxiv.org/abs/2512.04287)
*Ian Miles,Mayumi Wakimoto,Wagner Meira,Daniela Paula,Daylene Ticiane,Bruno Rosa,Jane Biddulph,Stelios Georgiou,Valdir Ermida*

Main category: cs.AI

TL;DR: 这篇综述探讨了人工智能在传染病领域预警扫描中的潜在应用和风险


<details>
  <summary>Details</summary>
Motivation: 随着传染病威胁日益复杂，需探索AI如何增强公共卫生预警能力

Method: 通过综述AI工具在信号检测、数据监测、场景分析和决策支持中的应用

Result: 发现AI能提升威胁识别效率但存在实施风险

Conclusion: 需建立有效实施和治理策略，AI在公共卫生准备中既有潜力也有局限

Abstract: This review explores the integration of Artificial Intelligence into Horizon Scanning, focusing on identifying and responding to emerging threats and opportunities linked to Infectious Diseases. We examine how AI tools can enhance signal detection, data monitoring, scenario analysis, and decision support. We also address the risks associated with AI adoption and propose strategies for effective implementation and governance. The findings contribute to the growing body of Foresight literature by demonstrating the potential and limitations of AI in Public Health preparedness.

</details>


### [8] [Towards better dense rewards in Reinforcement Learning Applications](https://arxiv.org/abs/2512.04302)
*Shuyuan Zhang*

Main category: cs.AI

TL;DR: 该论文探讨强化学习中密集奖励函数的设计挑战及解决方案，分析了稀疏奖励问题并提出多种改进方法。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的稀疏、延迟或不匹配的奖励信号会导致智能体学习效率低下，特别是在复杂环境中手工设计密集奖励函数十分困难且容易产生意外行为。

Method: 论文综述了多种方法，包括逆强化学习、基于人类偏好的奖励建模以及自监督学习内在奖励等方法。

Result: 分析了不同方法在通用性、可扩展性和与人类意图对齐方面的权衡，指出了当前方法的局限性。

Conclusion: 需要进一步研究以提高密集奖励构建的有效性和可靠性，为不同RL应用提供更优的解决方案。

Abstract: Finding meaningful and accurate dense rewards is a fundamental task in the field of reinforcement learning (RL) that enables agents to explore environments more efficiently. In traditional RL settings, agents learn optimal policies through interactions with an environment guided by reward signals. However, when these signals are sparse, delayed, or poorly aligned with the intended task objectives, agents often struggle to learn effectively. Dense reward functions, which provide informative feedback at every step or state transition, offer a potential solution by shaping agent behavior and accelerating learning. Despite their benefits, poorly crafted reward functions can lead to unintended behaviors, reward hacking, or inefficient exploration. This problem is particularly acute in complex or high-dimensional environments where handcrafted rewards are difficult to specify and validate. To address this, recent research has explored a variety of approaches, including inverse reinforcement learning, reward modeling from human preferences, and self-supervised learning of intrinsic rewards. While these methods offer promising directions, they often involve trade-offs between generality, scalability, and alignment with human intent. This proposal explores several approaches to dealing with these unsolved problems and enhancing the effectiveness and reliability of dense reward construction in different RL applications.

</details>


### [9] [Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning](https://arxiv.org/abs/2512.04359)
*Hongye Cao,Zhixin Bai,Ziyue Peng,Boyan Wang,Tianpei Yang,Jing Huo,Yuyao Zhang,Yang Gao*

Main category: cs.AI

TL;DR: 提出了一个结合语义熵和标记级熵的强化学习框架，通过课程学习和非均匀标记处理来缓解熵崩溃问题，提升LLM推理能力


<details>
  <summary>Details</summary>
Motivation: 传统的基于可验证奖励的强化学习虽然能提升LLM推理能力，但容易发生熵崩溃，限制策略探索和推理能力

Method: 从数据角度引入语义熵引导的课程学习，从算法角度对关键低熵标记实施KL正则化，对高协方差部分施加更强约束

Result: 在6个基准测试和3种不同参数规模的基模型上实验表明，该方法在提升推理能力方面优于其他基于熵的方法

Conclusion: 通过联合优化数据组织和算法设计，有效缓解熵崩溃问题，显著增强LLM的推理性能

Abstract: Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.

</details>


### [10] [Executable Governance for AI: Translating Policies into Rules Using LLMs](https://arxiv.org/abs/2512.04408)
*Gautam Varma Datla,Anudeep Vurity,Tejaswani Dash,Tazeem Ahmad,Mohd Adnan,Saima Rafi*

Main category: cs.AI

TL;DR: 提出了Policy-to-Tests (P2T)框架，将自然语言AI政策自动转换为机器可读规则，解决了人工转换缓慢易错的问题，并在多个政策类型中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前AI政策指南多以文本形式存在，需要人工转换为可执行规则，这个过程缓慢、易出错、难以扩展，阻碍了安全措施在实际部署中的应用。

Method: 开发了包含流水线和领域特定语言(DSL)的P2T框架，能将政策文档中的义务条款转换为规范化规则，涵盖危害、范围、条件、例外和所需证据。

Result: 在通用框架、行业指南和企业标准上测试，AI生成的规则在跨度和规则级别指标上接近人工基准，并在HIPAA保护实验中显著降低了违规率。

Conclusion: P2T框架能有效自动化政策到规则的转换，提升AI安全性评估的效率和一致性，相关资源已开源以促进可重复评估。

Abstract: AI policy guidance is predominantly written as prose, which practitioners must first convert into executable rules before frameworks can evaluate or enforce them. This manual step is slow, error-prone, difficult to scale, and often delays the use of safeguards in real-world deployments. To address this gap, we present Policy-to-Tests (P2T), a framework that converts natural-language policy documents into normalized, machine-readable rules. The framework comprises a pipeline and a compact domain-specific language (DSL) that encodes hazards, scope, conditions, exceptions, and required evidence, yielding a canonical representation of extracted rules. To test the framework beyond a single policy, we apply it across general frameworks, sector guidance, and enterprise standards, extracting obligation-bearing clauses and converting them into executable rules. These AI-generated rules closely match strong human baselines on span-level and rule-level metrics, with robust inter-annotator agreement on the gold set. To evaluate downstream behavioral and safety impact, we add HIPAA-derived safeguards to a generative agent and compare it with an otherwise identical agent without guardrails. An LLM-based judge, aligned with gold-standard criteria, measures violation rates and robustness to obfuscated and compositional prompts. Detailed results are provided in the appendix. We release the codebase, DSL, prompts, and rule sets as open-source resources to enable reproducible evaluation.

</details>


### [11] [GovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows](https://arxiv.org/abs/2512.04416)
*Zhou Liu,Zhaoyang Han,Guochen Yan,Hao Liang,Bohan Zeng,Xing Chen,Yuanfeng Song,Wentao Zhang*

Main category: cs.AI

TL;DR: GovBench基准测试揭示当前AI模型在数据治理自动化方面的不足，DataGovAgent框架通过规划-执行-评估架构显著提升复杂任务性能


<details>
  <summary>Details</summary>
Motivation: 现有自动化数据科学基准无法有效评估数据治理特有的数据正确性和质量保证挑战

Method: 引入GovBench基准（150个真实场景任务），采用逆向目标方法合成噪声，提出DataGovAgent框架（规划器-执行器-评估器架构）

Result: DataGovAgent将复杂任务平均分从39.7提升至54.9，调试迭代减少77.9%

Conclusion: 专门的数据治理框架能有效解决当前模型在多步工作流和错误纠正方面的局限性

Abstract: Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emerged as a promising solution for automating data governance by translating user intent into executable transformation code. However, existing benchmarks for automated data science often emphasize snippet-level coding or high-level analytics, failing to capture the unique challenge of data governance: ensuring the correctness and quality of the data itself. To bridge this gap, we introduce GovBench, a benchmark featuring 150 diverse tasks grounded in real-world scenarios, built on data from actual cases. GovBench employs a novel "reversed-objective" methodology to synthesize realistic noise and utilizes rigorous metrics to assess end-to-end pipeline reliability. Our analysis on GovBench reveals that current models struggle with complex, multi-step workflows and lack robust error-correction mechanisms. Consequently, we propose DataGovAgent, a framework utilizing a Planner-Executor-Evaluator architecture that integrates constraint-based planning, retrieval-augmented generation, and sandboxed feedback-driven debugging. Experimental results show that DataGovAgent significantly boosts the Average Task Score (ATS) on complex tasks from 39.7 to 54.9 and reduces debugging iterations by over 77.9 percent compared to general-purpose baselines.

</details>


### [12] [Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions](https://arxiv.org/abs/2512.04419)
*Weiwei Wang,Weijie Zou,Jiyong Min*

Main category: cs.AI

TL;DR: 本文针对LLM在批量代码解释任务中的重复内容生成问题，通过理论分析和实验验证提出了三种实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在生产环境中持续生成重复内容导致的性能下降和系统停滞问题。

Method: 基于马尔可夫模型进行理论分析，识别三种重复模式，并通过实验评估波束搜索、存在惩罚和DPO微调三种解决方案。

Result: 波束搜索可作为通用后处理机制解决所有重复模式；存在惩罚专门解决业务规则生成重复；DPO微调提供模型级通用解决方案。

Conclusion: 结合生产经验与实验验证，提出了系统化的重复机制理论分析和经过实际部署验证的实用解决方案。

Abstract: The repetition problem, where Large Language Models (LLMs) continuously generate repetitive content without proper termination, poses a critical challenge in production deployments, causing severe performance degradation and system stalling. This paper presents a comprehensive investigation and multiple practical solutions for the repetition problem encountered in real-world batch code interpretation tasks.
  We identify three distinct repetition patterns: (1) business rule generation repetition, (2) method call relationship analysis repetition, and (3) PlantUML diagram syntax generation repetition. Through rigorous theoretical analysis based on Markov models, we establish that the root cause lies in greedy decoding's inability to escape repetitive loops, exacerbated by self-reinforcement effects.
  Our comprehensive experimental evaluation demonstrates three viable solutions: (1) Beam Search decoding with early_stopping=True serves as a universal post-hoc mechanism that effectively resolves all three repetition patterns; (2) presence_penalty hyperparameter provides an effective solution specifically for BadCase 1; and (3) Direct Preference Optimization (DPO) fine-tuning offers a universal model-level solution for all three BadCases.
  The primary value of this work lies in combining first-hand production experience with extensive experimental validation. Our main contributions include systematic theoretical analysis of repetition mechanisms, comprehensive evaluation of multiple solutions with task-specific applicability mapping, identification of early_stopping as the critical parameter for Beam Search effectiveness, and practical production-ready solutions validated in real deployment environments.

</details>


### [13] [ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications](https://arxiv.org/abs/2512.04785)
*Eranga Bandara,Amin Hass,Ross Gore,Sachin Shetty,Ravi Mukkamala,Safdar H. Bouk,Xueping Liang,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: ASTRIDE是一个专为AI智能体系统设计的自动化威胁建模平台，扩展了STRIDE框架，新增AI特定威胁类别，并通过微调视觉语言模型和推理大语言模型实现从架构图自动分析威胁。


<details>
  <summary>Details</summary>
Motivation: AI智能体系统带来了传统威胁建模框架无法有效捕捉的新型安全挑战，如提示注入攻击、上下文污染等，需要专门针对AI智能体的威胁建模解决方案。

Method: ASTRIDE结合微调视觉语言模型联盟和OpenAI GPT推理大语言模型，直接从视觉架构图（如数据流图）进行端到端威胁分析，LLM智能体协调VLM联盟与推理LLM的交互。

Result: 评估表明ASTRIDE能为下一代智能系统提供准确、可扩展和可解释的威胁建模。

Conclusion: ASTRIDE是首个同时扩展STRIDE框架以涵盖AI特定威胁，并集成微调VLM与推理LLM实现AI智能体应用中图驱动威胁建模完全自动化的框架。

Abstract: AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.

</details>


### [14] [TaskEval: Synthesised Evaluation for Foundation-Model Tasks](https://arxiv.org/abs/2512.04442)
*Dilani Widanapathiranage,Scott Barnett,Stefanus Kurniawan,Wannita Takerngsaksiri*

Main category: cs.AI

TL;DR: 提出了一种合成基础模型任务特定评估器的方法，通过任务无关元模型、人机交互协议和评估合成器来解决缺乏评估指标的FM应用评估问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要集中在定义新的评估基准，但无法解决软件团队在没有现成指标或数据集时评估特定FM应用的实际需求。

Method: 开发了一个任务无关的元模型来捕捉FM任务特性，设计了高效的人机反馈交互协议，并创建了评估合成器来自动选择或生成合适的评估方法。

Result: 在图表数据提取和文档问答两个任务上的初步评估显示，所选评估方法的准确率分别达到93%和90%。

Conclusion: 该方法为解决工程团队面临的FM输出评估问题提供了有效工具，实现了自动化评估与人工反馈的深度融合。

Abstract: Hallucinations are a key concern when creating applications that rely on Foundation models (FMs). Understanding where and how these subtle failures occur in an application relies on evaluation methods known as \textit{evals}. Prior work focuses on defining new eval methods or benchmark datasets for specific tasks. However, neither helps a software team with a task-specific FM application when there is no metric or dataset. The demand for both automated approaches and deep integration of human insight makes this a challenging problem. We address this gap by proposing an approach to synthesise a FM task-specific evaluator program that provides automation and a custom UI for capturing feedback. The core novelty of our approach lies in: (1) a task-agnostic meta-model that captures properties of any FM task, (2) an interaction protocol for efficient use of human feedback, and (3) an eval synthesiser that selects or generates an appropriate set of evals. We implement our approach in \toolname and demonstrate the concept on two diverse FM tasks: chart data extraction and document question answering. A preliminary evaluation on the quality of our selected evals shows 93\% and 90\% accuracy respectively. Our research tackles a growing problem facing engineering teams, how to evaluate and review outputs from FM tasks.

</details>


### [15] [MARL Warehouse Robots](https://arxiv.org/abs/2512.04463)
*Price Allman,Lian Thang,Dre Simmons,Salmon Riaz*

Main category: cs.AI

TL;DR: 对比研究QMIX和IPPO两种多智能体强化学习算法在仓库机器人协同任务中的表现，发现QMIX通过价值分解显著优于独立学习方法，但在稀疏奖励环境下需要大量超参数调优。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体强化学习算法在现实世界仓库机器人协同任务中的应用效果，比较不同算法的性能差异。

Method: 在Robotic Warehouse (RWARE)环境和自定义Unity 3D仿真中评估QMIX和IPPO算法，分析超参数调优需求。

Result: QMIX平均回报3.25显著优于IPPO的0.38，但在稀疏奖励环境下需要500万步以上的epsilon退火训练；在Unity ML-Agents中经过100万步训练后实现稳定包裹配送。

Conclusion: MARL在小规模部署（2-4个机器人）中表现良好，但仍面临显著的扩展性挑战。

Abstract: We present a comparative study of multi-agent reinforcement learning (MARL) algorithms for cooperative warehouse robotics. We evaluate QMIX and IPPO on the Robotic Warehouse (RWARE) environment and a custom Unity 3D simulation. Our experiments reveal that QMIX's value decomposition significantly outperforms independent learning approaches (achieving 3.25 mean return vs. 0.38 for advanced IPPO), but requires extensive hyperparameter tuning -- particularly extended epsilon annealing (5M+ steps) for sparse reward discovery. We demonstrate successful deployment in Unity ML-Agents, achieving consistent package delivery after 1M training steps. While MARL shows promise for small-scale deployments (2-4 robots), significant scaling challenges remain. Code and analyses: https://pallman14.github.io/MARL-QMIX-Warehouse-Robots/

</details>


### [16] [Mathematical Framing for Different Agent Strategies](https://arxiv.org/abs/2512.04469)
*Philip Stephens,Emmanuel Salawu*

Main category: cs.AI

TL;DR: 提出了一个统一的数学概率框架来分析比较不同AI智能体策略，引入自由度概念帮助选择合适的智能体架构。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一的数学框架来理解不同AI智能体策略（如ReAct、多智能体系统等）之间的本质联系和差异。

Method: 建立概率链模型来分析智能体过程，将各种策略表示为对概率分布的不同操作方式。

Result: 开发了一个通用框架，能够量化分析不同智能体策略如何操纵概率来实现目标，并提供了选择策略的指导原则。

Conclusion: 该框架为AI智能体设计和评估提供了更清晰精确的工具，有助于在复杂系统中最大化成功行动的概率。

Abstract: We introduce a unified mathematical and probabilistic framework for understanding and comparing diverse AI agent strategies. We bridge the gap between high-level agent design concepts, such as ReAct, multi-agent systems, and control flows, and a rigorous mathematical formulation. Our approach frames agentic processes as a chain of probabilities, enabling a detailed analysis of how different strategies manipulate these probabilities to achieve desired outcomes. Our framework provides a common language for discussing the trade-offs inherent in various agent architectures. One of our many key contributions is the introduction of the "Degrees of Freedom" concept, which intuitively differentiates the optimizable levers available for each approach, thereby guiding the selection of appropriate strategies for specific tasks. This work aims to enhance the clarity and precision in designing and evaluating AI agents, offering insights into maximizing the probability of successful actions within complex agentic systems.

</details>


### [17] [A Modular Cognitive Architecture for Assisted Reasoning: The Nemosine Framework](https://arxiv.org/abs/2512.04500)
*Edervaldo Melo*

Main category: cs.AI

TL;DR: Nemosine Framework是一个模块化认知架构，通过功能认知模块支持辅助推理、结构化思维和系统分析，结合元认知、分布式认知和模块化认知系统原理。


<details>
  <summary>Details</summary>
Motivation: 旨在为辅助问题解决和决策支持提供操作结构，为未来的计算实现奠定清晰的概念基础，并促进符号模块化推理架构的研究。

Method: 模型通过功能认知模块（“角色”）运作，组织规划、评估、交叉核对和叙事合成等任务，采用形式化规范、内部一致性标准和可复现结构组件进行架构记录。

Result: 提出了一个模块化认知架构框架，具备明确的规范标准和可复现组件。

Conclusion: 该框架为计算实现提供了概念基础，有助于推动符号模块化推理架构的研究和发展。

Abstract: This paper presents the Nemosine Framework, a modular cognitive architecture designed to support assisted reasoning, structured thinking, and systematic analysis. The model operates through functional cognitive modules ("personas") that organize tasks such as planning, evaluation, cross-checking, and narrative synthesis. The framework combines principles from metacognition, distributed cognition, and modular cognitive systems to offer an operational structure for assisted problem-solving and decision support. The architecture is documented through formal specification, internal consistency criteria, and reproducible structural components. The goal is to provide a clear conceptual basis for future computational implementations and to contribute to the study of symbolic-modular architectures for reasoning.

</details>


### [18] [BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models](https://arxiv.org/abs/2512.04513)
*Yu-Wei Zhan,Xin Wang,Pengzhe Mao,Tongtong Feng,Ren Wang,Wenwu Zhu*

Main category: cs.AI

TL;DR: 论文提出BiTAgent框架，通过双向耦合多模态大语言模型和世界模型，解决开放世界具身智能中的语义意图与动态状态表示对齐问题，在多任务和跨环境实验中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 构建通用具身智能体需要统一系统来解读多模态目标、建模环境动态并执行可靠动作。MLLMs提供语义先验，WMs提供可操作的潜在动态，但二者的结合面临语义意图与动态状态表示紧密耦合、任务感知适应性两大挑战。

Method: BiTAgent框架包含三个协同组件：任务感知动态联合学习（建立MLLM表示到WM潜在空间的前向路径）、任务感知行为学习（WM反馈通过密集文本条件奖励优化MLLM语义空间的反向路径）、MLLM-WM联合优化，实现语义推理与动态预测的协调。

Result: 在多任务和跨环境设置下的广泛实验表明，BiTAgent在稳定性和泛化性上优于当前最先进的基线方法。

Conclusion: BiTAgent通过双向耦合机制推进了开放世界具身学习，为通用具身智能体的发展提供了有效解决方案。

Abstract: Building generalist embodied agents requires a unified system that can interpret multimodal goals, model environment dynamics, and execute reliable actions across diverse real-world tasks. Multimodal large language models (MLLMs) offer strong semantic priors and cross-modal generalization, while world models (WMs) provide actionable latent dynamics for prediction and control. Their combination holds promise for open-ended embodied intelligence, yet introduces two key challenges: (1) establishing a tight coupling between the semantic intent from MLLMs and the dynamic state representations within the WM's latent space, and (2) achieving task-aware adaptability that supports multi-task learning and cross-environment generalization. To address these limitations, we propose BiTAgent, a task-aware dynamic joint framework that enables bidirectional coupling between MLLMs and WMs. BiTAgent establishes two complementary pathways: a forward path that injects MLLM representations into the WM's latent space for semantically guided imagination, and a backward path where WM-generated feedback refines the MLLM's semantic space via dense text-conditioned rewards. This bidirectional interaction is realized through three synergistic components: Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization, which together harmonize semantic reasoning and dynamic prediction. Extensive experiments across multi-task and cross-environment settings demonstrate superior stability and generalization over state-of-the-art baselines, marking a step toward open-ended embodied learning.

</details>


### [19] [SlideGen: Collaborative Multimodal Agents for Scientific Slide Generation](https://arxiv.org/abs/2512.04529)
*Xin Liang,Xiang Zhang,Yiwei Xu,Siqi Sun,Chenyu You*

Main category: cs.AI

TL;DR: SlideGen是一个智能、模块化、视觉在环的框架，通过多智能体协作将科学论文自动转换为高质量PPT幻灯片。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多将幻灯片生成简化为文本摘要任务，忽视了幻灯片创建的视觉组件和设计密集型特性。

Method: SlideGen协调一组视觉语言智能体，协作推理文档结构和语义，通过协调的大纲创建、映射、布局安排、笔记合成和迭代优化来生成可编辑的PPTX幻灯片。

Result: 在多样化基准测试和强基线对比中，SlideGen在视觉质量、内容忠实度和可读性方面均优于现有方法。

Conclusion: 本研究为设计感知的多模态幻灯片生成奠定了基础，展示了智能体协作如何在复杂多模态推理任务中弥合理解与呈现之间的差距。

Abstract: Generating academic slides from scientific papers is a challenging multimodal reasoning task that requires both long context understanding and deliberate visual planning. Existing approaches largely reduce it to text only summarization, overlooking the visual component and design intensive nature of slide creation. In this paper we introduce SlideGen, an agentic, modular, and visual in the loop framework for scientific paper to slide generation. SlideGen orchestrates a group of vision language agents that reason collaboratively over the document structure and semantics, producing editable PPTX slides with logical flow and compelling visual presentation. By integrating coordinated outlining, mapping, arrangement, note synthesis, and iterative refinement, our system consistently delivers slides of expert level quality. Across diverse benchmarks and strong baselines, SlideGen outperforms existing methods in visual quality, content faithfulness, and readability, positioning it as the new state of the art in automated slide generation. Our work establishes a foundation for design aware multimodal slide generation, demonstrating how agentic collaboration can bridge understanding and presentation in complex multimodal reasoning tasks.

</details>


### [20] [The Ethics of Generative AI](https://arxiv.org/abs/2512.04598)
*Michael Klenk*

Main category: cs.AI

TL;DR: 本章讨论生成式AI的伦理问题，分析其技术特性如何影响伦理考量，探讨其加剧或缓解传统AI伦理问题的双重作用，并研究由其模仿生成能力引发的特殊伦理挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式AI能够模拟人类体验，这为哲学伦理提供了新的研究焦点，需要系统分析其与传统AI伦理的异同及特有伦理问题。

Method: 提供技术入门介绍，分析生成式AI如何影响责任、隐私、偏见等传统伦理问题，并考察由其模仿能力引发的作者权、人机关系等新问题。

Result: 生成式AI既可能加剧责任归属模糊、隐私侵犯等现有伦理问题，也可能通过技术手段缓解部分偏见；同时催生了关于作品署名、人机社交关系等新伦理争议。

Conclusion: 生成式AI的伦理研究需兼顾传统框架与新兴挑战，其拟人化特性要求伦理规范更具前瞻性，以应对技术快速发展带来的道德困境。

Abstract: This chapter discusses the ethics of generative AI. It provides a technical primer to show how generative AI affords experiencing technology as if it were human, and this affordance provides a fruitful focus for the philosophical ethics of generative AI. It then shows how generative AI can both aggravate and alleviate familiar ethical concerns in AI ethics, including responsibility, privacy, bias and fairness, and forms of alienation and exploitation. Finally, the chapter examines ethical questions that arise specifically from generative AI's mimetic generativity, such as debates about authorship and credit, the emergence of as-if social relationships with machines, and new forms of influence, persuasion, and manipulation.

</details>


### [21] [Turbo-Muon: Accelerating Orthogonality-Based Optimization with Pre-Conditioning](https://arxiv.org/abs/2512.04632)
*Thibaut Boissin,Thomas Massena,Franck Mamalet,Mathieu Serrurier*

Main category: cs.AI

TL;DR: 提出了一个预条件加速方法以减少牛顿-舒尔茨梯度正交化优化的计算成本，实现2.8倍加速和5-10%端到端训练时间提升，且无需超参数调整。


<details>
  <summary>Details</summary>
Motivation: 现有正交化优化器（如Muon）依赖昂贵的梯度正交化步骤，即使高效的牛顿-舒尔茨迭代近似也需要数十次矩阵乘法，计算成本高。

Method: 引入预条件处理程序加速牛顿-舒尔茨收敛，减少计算开销，并可减少一次迭代而不降低近似质量。

Result: 公开实现实现牛顿-舒尔茨近似2.8倍加速，在真实训练场景中端到端训练时间提升5-10%，模型性能保持平等或更优。

Conclusion: 该方法无需超参数调整即可作为简单替代方案，显著提升训练效率且保持模型性能。

Abstract: Orthogonality-based optimizers, such as Muon, have recently shown strong performance across large-scale training and community-driven efficiency challenges. However, these methods rely on a costly gradient orthogonalization step. Even efficient iterative approximations such as Newton-Schulz remain expensive, typically requiring dozens of matrix multiplications to converge. We introduce a preconditioning procedure that accelerates Newton-Schulz convergence and reduces its computational cost. We evaluate its impact and show that the overhead of our preconditioning can be made negligible. Furthermore, the faster convergence it enables allows us to remove one iteration out of the usual five without degrading approximation quality. Our publicly available implementation achieves up to a 2.8x speedup in the Newton-Schulz approximation. We also show that this has a direct impact on end-to-end training runtime with 5-10% improvement in realistic training scenarios across two efficiency-focused tasks. On challenging language or vision tasks, we validate that our method maintains equal or superior model performance while improving runtime. Crucially, these improvements require no hyperparameter tuning and can be adopted as a simple drop-in replacement. Our code is publicly available on github.

</details>


### [22] [Towards Ethical Multi-Agent Systems of Large Language Models: A Mechanistic Interpretability Perspective](https://arxiv.org/abs/2512.04691)
*Jae Hee Lee,Anne Lauscher,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 本文提出基于机制可解释性研究MALMs伦理行为的议程框架


<details>
  <summary>Details</summary>
Motivation: LLM多智能体系统虽增强能力但引发伦理挑战需系统性研究

Method: 机制可解释性方法结合评估框架与参数高效对齐技术

Result: 确立个体/交互/系统三级评估指标与行为干预路径

Conclusion: 需跨学科合作实现可信MALMs并建立相应伦理规范体系

Abstract: Large language models (LLMs) have been widely deployed in various applications, often functioning as autonomous agents that interact with each other in multi-agent systems. While these systems have shown promise in enhancing capabilities and enabling complex tasks, they also pose significant ethical challenges. This position paper outlines a research agenda aimed at ensuring the ethical behavior of multi-agent systems of LLMs (MALMs) from the perspective of mechanistic interpretability. We identify three key research challenges: (i) developing comprehensive evaluation frameworks to assess ethical behavior at individual, interactional, and systemic levels; (ii) elucidating the internal mechanisms that give rise to emergent behaviors through mechanistic interpretability; and (iii) implementing targeted parameter-efficient alignment techniques to steer MALMs towards ethical behaviors without compromising their performance.

</details>


### [23] [Playing the Player: A Heuristic Framework for Adaptive Poker AI](https://arxiv.org/abs/2512.04714)
*Andrew Paterson,Carl Sanders*

Main category: cs.AI

TL;DR: 本文提出了一种名为Patrick的创新扑克AI，挑战了对无懈可击AI的追求，强调通过最大化利用人类对手的弱点来获胜。


<details>
  <summary>Details</summary>
Motivation: 传统扑克AI研究集中于开发无懈可击的求解器，但本文认为真正的挑战在于理解并利用人类对手的心理和认知缺陷。

Method: Patrick AI采用了一种新颖的预测锚定学习方法，专门设计用于分析和攻击人类对手的非理性行为模式。

Result: 在64,267手牌的试验中，Patrick AI表现出了盈利性，证明了其利用对手弱点的有效性。

Conclusion: 本文认为追求无懈可击的AI是一种误导，真正的挑战在于创造能够掌握人类不完美艺术的人工智能。

Abstract: For years, the discourse around poker AI has been dominated by the concept of solvers and the pursuit of unexploitable, machine-perfect play. This paper challenges that orthodoxy. It presents Patrick, an AI built on the contrary philosophy: that the path to victory lies not in being unexploitable, but in being maximally exploitative. Patrick's architecture is a purpose-built engine for understanding and attacking the flawed, psychological, and often irrational nature of human opponents. Through detailed analysis of its design, its novel prediction-anchored learning method, and its profitable performance in a 64,267-hand trial, this paper makes the case that the solved myth is a distraction from the real, far more interesting challenge: creating AI that can master the art of human imperfection.

</details>


### [24] [SIMA 2: A Generalist Embodied Agent for Virtual Worlds](https://arxiv.org/abs/2512.04797)
*SIMA team,Adrian Bolton,Alexander Lerchner,Alexandra Cordell,Alexandre Moufarek,Andrew Bolt,Andrew Lampinen,Anna Mitenkova,Arne Olav Hallingstad,Bojan Vujatovic,Bonnie Li,Cong Lu,Daan Wierstra,Daniel P. Sawyer,Daniel Slater,David Reichert,Davide Vercelli,Demis Hassabis,Drew A. Hudson,Duncan Williams,Ed Hirst,Fabio Pardo,Felix Hill,Frederic Besse,Hannah Openshaw,Harris Chan,Hubert Soyer,Jane X. Wang,Jeff Clune,John Agapiou,John Reid,Joseph Marino,Junkyung Kim,Karol Gregor,Kaustubh Sridhar,Kay McKinney,Laura Kampis,Lei M. Zhang,Loic Matthey,Luyu Wang,Maria Abi Raad,Maria Loks-Thompson,Martin Engelcke,Matija Kecman,Matthew Jackson,Maxime Gazeau,Ollie Purkiss,Oscar Knagg,Peter Stys,Piermaria Mendolicchio,Raia Hadsell,Rosemary Ke,Ryan Faulkner,Sarah Chakera,Satinder Singh Baveja,Shane Legg,Sheleem Kashem,Tayfun Terzi,Thomas Keck,Tim Harley,Tim Scholtes,Tyson Roberts,Volodymyr Mnih,Yulan Liu,Zhengdong Wang,Zoubin Ghahramani*

Main category: cs.AI

TL;DR: SIMA 2是一个基于Gemini基础模型构建的通用具身智能体，能够在多种3D虚拟世界中理解和行动，支持复杂语言和图像指令的理解、推理和交互，大幅缩小了与人类表现的差距，并具备自主学习和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有具身智能体（如SIMA 1）仅能处理简单语言指令的局限性，研究旨在开发一个能够作为交互伙伴、理解高层次目标、处理复杂多模态指令的通用智能体，推动虚拟和物理世界中持续学习智能体的发展。

Method: 基于Gemini基础模型构建SIMA 2，使其能够通过语言和图像接收指令，进行目标推理和用户对话；利用Gemini生成任务和奖励，实现智能体在新环境中从零开始自主学习新技能。

Result: 在多种游戏测试中，SIMA 2显著缩小了与人类表现的差距，展示了在未见过的环境中的强大泛化能力，同时保持了基础模型的核心推理能力。

Conclusion: SIMA 2验证了创建多功能、持续学习智能体的可行路径，为未来在虚拟和物理世界中部署通用具身智能体奠定了基础。

Abstract: We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.

</details>


### [25] [Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing](https://arxiv.org/abs/2512.04829)
*Rasul Tutunov,Alexandre Maraval,Antoine Grosnit,Xihan Li,Jun Wang,Haitham Bou-Ammar*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于模型的高效搜索方法（结合贝叶斯优化和蒙特卡洛树搜索），用于解决高维球体堆积问题的上界计算，在4-16维空间取得了新的最优上界结果。


<details>
  <summary>Details</summary>
Motivation: 高维球体堆积问题是希尔伯特第十八问题，在密码学、晶体学和医学成像等领域有重要应用，但该问题极其困难，现有的半定规划（SDP）方法计算成本高昂（每个候选SDP可能需要数天评估），传统数据密集型AI方法不可行。

Method: 将SDP构建建模为顺序决策过程（SDP游戏），使用基于模型的样本高效框架（结合贝叶斯优化和蒙特卡洛树搜索）来组装SDP公式。

Result: 在4-16维空间获得了新的最先进上界，展示了模型搜索在长期几何问题中的计算进展。

Conclusion: 样本高效的基于模型搜索能够在数学严格、评估受限的问题上取得实质性进展，为AI辅助发现提供了与大规模式探索互补的新方向。

Abstract: Sphere packing, Hilbert's eighteenth problem, asks for the densest arrangement of congruent spheres in n-dimensional Euclidean space. Although relevant to areas such as cryptography, crystallography, and medical imaging, the problem remains unresolved: beyond a few special dimensions, neither optimal packings nor tight upper bounds are known. Even a major breakthrough in dimension $n=8$, later recognised with a Fields Medal, underscores its difficulty. A leading technique for upper bounds, the three-point method, reduces the problem to solving large, high-precision semidefinite programs (SDPs). Because each candidate SDP may take days to evaluate, standard data-intensive AI approaches are infeasible. We address this challenge by formulating SDP construction as a sequential decision process, the SDP game, in which a policy assembles SDP formulations from a set of admissible components. Using a sample-efficient model-based framework that combines Bayesian optimisation with Monte Carlo Tree Search, we obtain new state-of-the-art upper bounds in dimensions $4-16$, showing that model-based search can advance computational progress in longstanding geometric problems. Together, these results demonstrate that sample-efficient, model-based search can make tangible progress on mathematically rigid, evaluation limited problems, pointing towards a complementary direction for AI-assisted discovery beyond large-scale LLM-driven exploration.

</details>


### [26] [Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case](https://arxiv.org/abs/2512.04834)
*Vignesh Kumar Kembu,Pierandrea Morandini,Marta Bianca Maria Ranzini,Antonino Nocera*

Main category: cs.AI

TL;DR: 论文研究了开源多语言大语言模型在理解意大利语电子健康记录和实时信息抽取方面的能力，实验显示模型在零样本本地部署环境下表现不一。


<details>
  <summary>Details</summary>
Motivation: 临床记录中的信息抽取对数字医疗至关重要，传统NLP技术因临床语言的复杂性和变异性而效果有限，大语言模型为此提供了新可能。

Method: 通过详细的实验评估，比较开源多语言大语言模型在电子健康记录中提取并发症信息的性能，并与原生模式匹配和人工标注进行对比。

Result: 部分大语言模型在零样本本地设置中表现不佳，性能存在显著差异，难以在不同疾病间泛化。

Conclusion: 大语言模型在临床信息抽取中有潜力，但在零样本和本地部署场景下仍需改进泛化能力。

Abstract: Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.

</details>


### [27] [From Task Executors to Research Partners: Evaluating AI Co-Pilots Through Workflow Integration in Biomedical Research](https://arxiv.org/abs/2512.04854)
*Lukas Weidener,Marko Brkić,Chiara Bacci,Mihailo Jovanović,Emre Ulgac,Alex Dobrin,Johannes Weniger,Martin Vlas,Ritvik Singh,Aakaash Meduri*

Main category: cs.AI

TL;DR: 本文对临床前生物医学研究中AI系统的基准测试实践进行了快速回顾，发现现有基准仅评估孤立能力，无法有效评估AI作为研究合作伙伴的整体效能，并提出了一个包含四个关键维度的流程导向评估框架。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统在生物医学研究中应用日益广泛，但当前的评估框架可能不足以有效评估其作为研究合作伙伴的实际效能。需要审视现有的基准测试实践以发现潜在的不足。

Method: 通过检索三个主要数据库和两个预印本服务器（2018年1月1日至2025年10月31日），识别出14个评估AI在文献理解、实验设计和假设生成方面能力的基准。

Result: 研究发现所有现有基准都只评估孤立的组件能力（如数据分析质量、假设有效性、实验方案设计），但真实的研究合作需要包含情境记忆、自适应对话和约束传播的集成工作流程。

Conclusion: 当前基准存在重大缺陷，系统在组件基准上表现出色可能在实际应用中失败。提出了一个包含对话质量、工作流程编排、会话连续性和研究人员体验四个关键维度的流程导向评估框架，以更有效地评估AI作为研究合作者的能力。

Abstract: Artificial intelligence systems are increasingly deployed in biomedical research. However, current evaluation frameworks may inadequately assess their effectiveness as research collaborators. This rapid review examines benchmarking practices for AI systems in preclinical biomedical research. Three major databases and two preprint servers were searched from January 1, 2018 to October 31, 2025, identifying 14 benchmarks that assess AI capabilities in literature understanding, experimental design, and hypothesis generation. The results revealed that all current benchmarks assess isolated component capabilities, including data analysis quality, hypothesis validity, and experimental protocol design. However, authentic research collaboration requires integrated workflows spanning multiple sessions, with contextual memory, adaptive dialogue, and constraint propagation. This gap implies that systems excelling on component benchmarks may fail as practical research co-pilots. A process-oriented evaluation framework is proposed that addresses four critical dimensions absent from current benchmarks: dialogue quality, workflow orchestration, session continuity, and researcher experience. These dimensions are essential for evaluating AI systems as research co-pilots rather than as isolated task executors.

</details>


### [28] [STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions](https://arxiv.org/abs/2512.04871)
*Junjie Fan,Hongye Zhao,Linduo Wei,Jiayu Rao,Guijia Li,Jiaxin Yuan,Wenqi Xu,Yong Qi*

Main category: cs.AI

TL;DR: 提出的STELLA框架通过动态语义抽象和时间对准增强LLM的时间序列预测能力，在基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM时间序列预测方法未能有效增强原始序列信息，依赖静态相关性而非动态行为生成解释，缺乏全局和实例特定上下文。

Method: STELLA框架将输入序列解耦为趋势、季节性和残差分量，生成层次化语义锚点（全局语料级先验和细粒度行为提示）作为前缀提示引导LLM建模。

Result: 在8个基准数据集上，STELLA在长短期预测中均优于最先进方法，在零样本和少样本场景下展现优异泛化能力。

Conclusion: 动态生成的语义锚点能有效提升LLM时间序列预测性能，验证了语义-时间对准框架的优越性。

Abstract: Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.

</details>


### [29] [Chameleon: Adaptive Adversarial Agents for Scaling-Based Visual Prompt Injection in Multimodal AI Systems](https://arxiv.org/abs/2512.04895)
*M Zeeshan,Saud Satti*

Main category: cs.AI

TL;DR: 本文提出了一种名为Chameleon的自适应对抗框架，专门针对视觉语言模型(VLMs)中的图像预处理漏洞进行攻击，相比传统静态攻击大幅提升了攻击成功率


<details>
  <summary>Details</summary>
Motivation: 当前多模态AI系统严重依赖预处理管道，特别是图像缩放操作，这带来了被忽视的安全漏洞——恶意视觉提示可通过缩放隐藏，在人眼不可见的情况下激活模型语义指令

Method: 采用基于智能体的迭代优化机制，根据目标模型的实时反馈动态调整图像扰动，生成能经受标准缩放操作的对抗样本

Result: 在Gemini 2.5 Flash模型上的实验显示，Chameleon攻击成功率达到84.5%，远超静态基准攻击的32.1%；在多步任务中使决策准确率下降超过45%

Conclusion: 揭示了VLMs预处理阶段的安全风险，提出多尺度一致性检查作为必要的防御机制，强调需要关注动态对抗威胁

Abstract: Multimodal Artificial Intelligence (AI) systems, particularly Vision-Language Models (VLMs), have become integral to critical applications ranging from autonomous decision-making to automated document processing. As these systems scale, they rely heavily on preprocessing pipelines to handle diverse inputs efficiently. However, this dependency on standard preprocessing operations, specifically image downscaling, creates a significant yet often overlooked security vulnerability. While intended for computational optimization, scaling algorithms can be exploited to conceal malicious visual prompts that are invisible to human observers but become active semantic instructions once processed by the model. Current adversarial strategies remain largely static, failing to account for the dynamic nature of modern agentic workflows. To address this gap, we propose Chameleon, a novel, adaptive adversarial framework designed to expose and exploit scaling vulnerabilities in production VLMs. Unlike traditional static attacks, Chameleon employs an iterative, agent-based optimization mechanism that dynamically refines image perturbations based on the target model's real-time feedback. This allows the framework to craft highly robust adversarial examples that survive standard downscaling operations to hijack downstream execution. We evaluate Chameleon against Gemini 2.5 Flash model. Our experiments demonstrate that Chameleon achieves an Attack Success Rate (ASR) of 84.5% across varying scaling factors, significantly outperforming static baseline attacks which average only 32.1%. Furthermore, we show that these attacks effectively compromise agentic pipelines, reducing decision-making accuracy by over 45% in multi-step tasks. Finally, we discuss the implications of these vulnerabilities and propose multi-scale consistency checks as a necessary defense mechanism.

</details>


### [30] [Algorithmic Thinking Theory](https://arxiv.org/abs/2512.04923)
*MohammadHossein Bateni,Vincent Cohen-Addad,Yuzhou Gu,Silvio Lattanzi,Simon Meierhans,Christopher Mohri*

Main category: cs.AI

TL;DR: LLMs在复杂推理任务中表现出色，通过迭代改进生成答案的能力可被视为概率推理算法。本文提出了理论框架分析此类推理算法，为设计更强大的推理方法奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有研究观察到LLMs通过迭代改进解决方案能提升推理能力，但缺乏统一理论框架分析这种推理算法的原理和效果。

Method: 建立基于概率预言机的理论框架，形式化迭代改进和答案聚合等流行技术的原理，不依赖具体模型架构。

Result: 提出了一个与实验证据相符的通用分析框架，能够解释现有技术并为新一代推理方法的设计提供理论基础。

Conclusion: 该框架为理解当前和未来推理预言机的工作原理提供了普遍视角，有助于推动更强大推理方法的发展。

Abstract: Large language models (LLMs) have proven to be highly effective for solving complex reasoning tasks. Surprisingly, their capabilities can often be improved by iterating on previously generated solutions. In this context, a reasoning plan for generating and combining a set of solutions can be thought of as an algorithm for reasoning using a probabilistic oracle.
  We introduce a theoretical framework for analyzing such reasoning algorithms. This framework formalizes the principles underlying popular techniques for iterative improvement and answer aggregation, providing a foundation for designing a new generation of more powerful reasoning methods. Unlike approaches for understanding models that rely on architectural specifics, our model is grounded in experimental evidence. As a result, it offers a general perspective that may extend to a wide range of current and future reasoning oracles.

</details>


### [31] [Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases](https://arxiv.org/abs/2512.04938)
*Raquel Norel,Michele Merler,Pavitra Modi*

Main category: cs.AI

TL;DR: 使用智能手机语音分析和关系图转换器架构监测罕见神经疾病患者的认知症状，证明比传统测试更有效。


<details>
  <summary>Details</summary>
Motivation: 传统认知测试无法检测到罕见神经疾病患者报告的"脑雾"症状，需要更敏感的监测方法。

Method: 整合智能手机语音分析与关系图转换器（RELGT）架构进行连续神经认知监测。

Result: 在苯丙酮尿症概念验证中，语音衍生的"言语流畅度"与血液苯丙氨酸水平显著相关（p = -0.50, p < 0.005），而传统认知测试无显著相关性（所有|r| < 0.35）。

Conclusion: RELGT能够克服异质性医疗数据的信息瓶颈，实现失代偿前数周的预测预警，有望将间歇性神经学转变为全球数百万人的连续个性化监测。

Abstract: Patients with rare neurological diseases report cognitive symptoms -"brain fog"- invisible to traditional tests. We propose continuous neurocognitive monitoring via smartphone speech analysis integrated with Relational Graph Transformer (RELGT) architectures. Proof-of-concept in phenylketonuria (PKU) shows speech-derived "Proficiency in Verbal Discourse" correlates with blood phenylalanine (p = -0.50, p < 0.005) but not standard cognitive tests (all |r| < 0.35). RELGT could overcome information bottlenecks in heterogeneous medical data (speech, labs, assessments), enabling predictive alerts weeks before decompensation. Key challenges: multi-disease validation, clinical workflow integration, equitable multilingual deployment. Success would transform episodic neurology into continuous personalized monitoring for millions globally.

</details>


### [32] [Detecting Perspective Shifts in Multi-agent Systems](https://arxiv.org/abs/2512.05013)
*Eric Bridgeford,Hayden Helm*

Main category: cs.AI

TL;DR: 提出了用于监测黑盒多智能体系统中行为动态的时态数据核视角空间（TDKPS），能够跨时间联合嵌入智能体并检测行为和群体层面的变化。


<details>
  <summary>Details</summary>
Motivation: 随着智能体使用激增，动态多智能体系统自然涌现，需要监测智能体行为的动态变化。现有研究仅限于基于单时间点查询响应的低维表示。

Method: 介绍了TDKPS框架，提出几种新颖的假设检验方法，用于检测黑盒多智能体系统中智能体和群体层面的行为变化。通过模拟和自然实验验证方法特性。

Result: 在模拟实验中验证了检验方法对关键超参数的敏感性，并通过自然实验证明该方法能够灵敏、特异且显著地检测到与真实外部事件相关的变化。

Conclusion: TDKPS是首个用于监测黑盒多智能体系统行为动态的原则性框架，对于生成式智能体部署规模化至关重要。

Abstract: Generative models augmented with external tools and update mechanisms (or \textit{agents}) have demonstrated capabilities beyond intelligent prompting of base models. As agent use proliferates, dynamic multi-agent systems have naturally emerged. Recent work has investigated the theoretical and empirical properties of low-dimensional representations of agents based on query responses at a single time point. This paper introduces the Temporal Data Kernel Perspective Space (TDKPS), which jointly embeds agents across time, and proposes several novel hypothesis tests for detecting behavioral change at the agent- and group-level in black-box multi-agent systems. We characterize the empirical properties of our proposed tests, including their sensitivity to key hyperparameters, in simulations motivated by a multi-agent system of evolving digital personas. Finally, we demonstrate via natural experiment that our proposed tests detect changes that correlate sensitively, specifically, and significantly with a real exogenous event. As far as we are aware, TDKPS is the first principled framework for monitoring behavioral dynamics in black-box multi-agent systems -- a critical capability as generative agent deployment continues to scale.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [33] [NAWOA-XGBoost: A Novel Model for Early Prediction of Academic Potential in Computer Science Students](https://arxiv.org/abs/2512.04751)
*Junhao Wei,Yanzhao Gu,Ran Zhang,Mingjing Huang,Jinhong Song,Yanxiao Li,Wenxuan Zhu,Yapeng Wang,Zikun Li,Zhiwen Wang,Xu Yang,Ngai Cheong*

Main category: cs.CE

TL;DR: 提出非线性自适应鲸鱼优化算法（NAWOA）改进传统WOA，应用于XGBoost超参数优化，在学术潜力预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统鲸鱼优化算法（WOA）存在全局搜索能力弱、收敛速度慢、易陷入局部最优等问题，限制了其在机器学习超参数优化中的应用效果。

Method: 整合优质节点集初始化、领导者-追随者觅食、动态环绕捕食、三角狩猎策略和非线性收敛因子，增强算法的探索、开发和收敛稳定性。

Result: 在23个基准函数上验证NAWOA优越性；NAWOA-XGBoost模型在495名计算机科学本科生数据上取得Accuracy 0.8148、Macro F1 0.8101、AUC 0.8932、G-Mean 0.8172，优于传统XGBoost和WOA-XGBoost。

Conclusion: NAWOA有效提升优化性能，在多类不平衡数据集上表现出强适应性，为超参数优化提供了新解决方案。

Abstract: Whale Optimization Algorithm (WOA) suffers from limited global search ability, slow convergence, and tendency to fall into local optima, restricting its effectiveness in hyperparameter optimization for machine learning models. To address these issues, this study proposes a Nonlinear Adaptive Whale Optimization Algorithm (NAWOA), which integrates strategies such as Good Nodes Set initialization, Leader-Followers Foraging, Dynamic Encircling Prey, Triangular Hunting, and a nonlinear convergence factor to enhance exploration, exploitation, and convergence stability. Experiments on 23 benchmark functions demonstrate NAWOA's superior optimization capability and robustness. Based on this optimizer, an NAWOA-XGBoost model was developed to predict academic potential using data from 495 Computer Science undergraduates at Macao Polytechnic University (2009-2019). Results show that NAWOA-XGBoost outperforms traditional XGBoost and WOA-XGBoost across key metrics, including Accuracy (0.8148), Macro F1 (0.8101), AUC (0.8932), and G-Mean (0.8172), demonstrating strong adaptability on multi-class imbalanced datasets.

</details>


### [34] [Customer Identification for Electricity Retailers Based on Monthly Demand Profiles by Activity Sectors and Locations](https://arxiv.org/abs/2512.04776)
*Joaquin Luque,Alejandro Carrasco,Enrique Personal,Francisco Perez,Carlos Leon*

Main category: cs.CE

TL;DR: 本文提出了一种基于电力需求特征、经济活动部门及位置的间接客户识别方法，并定义关键绩效指标（KPI）指导营销策略，实验表明该方法比随机获客策略效果提升40%


<details>
  <summary>Details</summary>
Motivation: 电力零售竞争加剧，公司需要识别最匹配目标电力需求特征的客户，但需求数据通常匿名无法关联到具体客户

Method: 利用西班牙数百万月度电力需求数据，结合经济活动部门和地理位置信息进行间接客户识别，定义客户需求与目标需求的距离作为KPI

Result: 通过活动和位置组合成功识别10万名唯一客户，约30万客户可在10人以内的小集合中被识别；新营销策略相比随机获客策略使目标距离减少40%

Conclusion: 活动和位置信息的结合是有效的客户间接识别工具，基于KPI的营销策略能显著提高客户获取效率

Abstract: The increasing competition in the electric sector is challenging retail companies as they must assign its commercial efforts to attract the most profitable customers. Those are whose energy demand best fit certain target profiles, which usually depend on generation or cost policies. But, even when the demand profile is available, it is in an anonymous way, preventing its association to a particular client. In this paper, we explore a large dataset containing several millions of monthly demand profiles in Spain and use the available information about the associated economic sector and location for an indirect identification of the customers. The distance of the demand profile from the target is used to define a key performance indicator (KPI) which is used as the main driver of the proposed marketing strategy. The combined use of activity and location has been revealed as a powerful tool for indirect identification of customers, as 100,000 customers are uniquely identified, while about 300,000 clients are identifiable in small sets containing 10 or less consumers. To assess the proposed marketing strategy, it has been compared to the random attraction of new clients, showing a reduction of distance from the target of 40% for 10,000 new customers.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [35] [Towards Contextual Sensitive Data Detection](https://arxiv.org/abs/2512.04120)
*Liang Telkamp,Madelon Hulsebos*

Main category: cs.CR

TL;DR: 论文提出了两种基于上下文的敏感数据检测机制——类型情境化和领域情境化，利用大语言模型在不同数据领域有效识别敏感数据，并开源了相关工具和数据集。


<details>
  <summary>Details</summary>
Motivation: 开放数据门户的兴起需要更好地保护敏感数据。现有方法主要关注个人隐私数据，但数据敏感性取决于上下文，需要更广泛和细化的敏感数据定义。

Method: 1) 类型情境化：检测数据值的语义类型，并考虑其在数据集中的整体上下文。2) 领域情境化：基于相关规则文档检索，在更广上下文中确定数据集的敏感性。

Result: 实验表明：类型情境化显著减少误报，召回率达到94%（商业工具为63%）；领域情境化在人道主义数据等非标准领域有效；LLM解释提高了人工数据审计的一致性。

Conclusion: 上下文敏感的敏感数据检测方法有效，开源工具促进了该领域的研究和应用。

Abstract: The emergence of open data portals necessitates more attention to protecting sensitive data before datasets get published and exchanged. While an abundance of methods for suppressing sensitive data exist, the conceptualization of sensitive data and methods to detect it, focus particularly on personal data that, if disclosed, may be harmful or violate privacy. We observe the need for refining and broadening our definitions of sensitive data, and argue that the sensitivity of data depends on its context. Based on this definition, we introduce two mechanisms for contextual sensitive data detection that con- sider the broader context of a dataset at hand. First, we introduce type contextualization, which first detects the semantic type of particular data values, then considers the overall context of the data values within the dataset or document. Second, we introduce domain contextualization which determines sensitivity of a given dataset in the broader context based on the retrieval of relevant rules from documents that specify data sensitivity (e.g., data topic and geographic origin). Experiments with these mechanisms, assisted by large language models (LLMs), confirm that: 1) type-contextualization significantly reduces the number of false positives for type-based sensitive data detection and reaches a recall of 94% compared to 63% with commercial tools, and 2) domain-contextualization leveraging sensitivity rule retrieval is effective for context-grounded sensitive data detection in non-standard data domains such as humanitarian datasets. Evaluation with humanitarian data experts also reveals that context-grounded LLM explanations provide useful guidance in manual data auditing processes, improving consistency. We open-source mechanisms and annotated datasets for contextual sensitive data detection at https://github.com/trl-lab/sensitive-data-detection.

</details>


### [36] [Tipping the Dominos: Topology-Aware Multi-Hop Attacks on LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2512.04129)
*Ruichao Liang,Le Yin,Jing Chen,Cong Wu,Xiaoyu Zhang,Huangpeng Gu,Zijian Zhang,Yang Liu*

Main category: cs.CR

TL;DR: 提出TOMA攻击方案揭示多智能体系统安全漏洞，实验成功率40%-78%，并提出基于拓扑信任的防御框架。


<details>
  <summary>Details</summary>
Motivation: 当前MAS安全评估局限于有限攻击场景，安全问题被低估。

Method: 提出TOMA拓扑感知多跳攻击方案，通过优化污染传播和对抗载荷扩散。

Result: 在三种先进MAS架构和五种拓扑上攻击成功率达40%-78%，发现被忽视的固有漏洞。

Conclusion: MAS存在固有安全漏洞，基于拓扑信任的防御框架可有效阻挡94.8%的自适应复合攻击。

Abstract: LLM-based multi-agent systems (MASs) have reshaped the digital landscape with their emergent coordination and problem-solving capabilities. However, current security evaluations of MASs are still confined to limited attack scenarios, leaving their security issues unclear and likely underestimated. To fill this gap, we propose TOMA, a topology-aware multi-hop attack scheme targeting MASs. By optimizing the propagation of contamination within the MAS topology and controlling the multi-hop diffusion of adversarial payloads originating from the environment, TOMA unveils new and effective attack vectors without requiring privileged access or direct agent manipulation. Experiments demonstrate attack success rates ranging from 40% to 78% across three state-of-the-art MAS architectures: \textsc{Magentic-One}, \textsc{LangManus}, and \textsc{OWL}, and five representative topologies, revealing intrinsic MAS vulnerabilities that may be overlooked by existing research. Inspired by these findings, we propose a conceptual defense framework based on topology trust, and prototype experiments show its effectiveness in blocking 94.8% of adaptive and composite attacks.

</details>


### [37] [Primitive Vector Cipher(PVC): A Hybrid Encryption Scheme based on the Vector Computational Diffie-Hellman (V-CDH) Problem](https://arxiv.org/abs/2512.04237)
*Gülçin ÇİVİ BİLİR*

Main category: cs.CR

TL;DR: PVC是一种新型混合加密方案，结合矩阵密码学和Diffie-Hellman密钥交换，提供高安全性和并行处理能力。


<details>
  <summary>Details</summary>
Motivation: 传统加密方案存在确定性重复和线性攻击脆弱性问题，需要一种能抵抗已知明文攻击且支持高效并行处理的新方案。

Method: 采用两层设计：使用HKDF通过DH认证的共享原语向量掩盖明文，并用每块偏移随机化密文块。

Result: PVC消除了确定性重复，对线性和已知明文攻击具有强抵抗力，支持大规模并行和线性扩展。

Conclusion: PVC在V-CDH假设下具备IND-CPA安全性，结合STS协议可达到IND-CCA安全级别，是一种安全高效的加密方案。

Abstract: This work introduces the Primitive Vector Cipher (PVC), a novel hybrid encryption scheme integrating matrix-based cryptography with advanced Diffie-Hellman key exchange. PVC's security is grounded on the established hardness of the Vector Computational Diffie- Hellman (V-CDH) problem. The two-layered design uses HKDF to mask plaintext via a DH-authenticated shared primitive vector and randomize cipher blocks with a per-block offset. This approach eliminates deterministic repetitions and provides strong resistance against linear and known-plaintext attacks. PVC's block-wise structure allows for massive parallelism and excellent linear scaling. Security is formally analyzed, demonstrating INDCPA security under V-CDH. STS protocol integration elevates security toward IND-CCA guarantees.

</details>


### [38] [Hey GPT-OSS, Looks Like You Got It - Now Walk Me Through It! An Assessment of the Reasoning Language Models Chain of Thought Mechanism for Digital Forensics](https://arxiv.org/abs/2512.04254)
*Gaëtan Michelet,Janine Schneider,Aruna Withanage,Frank Breitinger*

Main category: cs.CR

TL;DR: 本文首次调查了推理语言模型在数字取证中的潜力，通过四个测试用例评估推理组件在支持结果可解释性方面的可用性，发现中等推理水平有助于解释和验证输出，但支持有限且更高推理水平不会提升质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数字取证中应用广泛，但结果可解释性有限，降低了其操作和法律可用性。新兴的推理语言模型通过内部推理机制处理逻辑任务，但用户通常只看到最终答案，而非底层推理过程。

Method: 使用可本地部署的推理模型gpt-oss，设计四个测试用例，结合新的定量指标和定性分析评估推理组件在支持结果可解释性方面的可用性。

Result: 推理组件在中等推理水平上有助于解释和验证数字取证中的语言模型输出，但这种支持往往有限，更高推理水平不会提高响应质量。

Conclusion: 推理语言模型在数字取证中具有潜力，但当前推理组件的支持有限，需进一步优化以实现更好的可解释性和实用性。

Abstract: The use of large language models in digital forensics has been widely explored. Beyond identifying potential applications, research has also focused on optimizing model performance for forensic tasks through fine-tuning. However, limited result explainability reduces their operational and legal usability. Recently, a new class of reasoning language models has emerged, designed to handle logic-based tasks through an `internal reasoning' mechanism. Yet, users typically see only the final answer, not the underlying reasoning. One of these reasoning models is gpt-oss, which can be deployed locally, providing full access to its underlying reasoning process. This article presents the first investigation into the potential of reasoning language models for digital forensics. Four test use cases are examined to assess the usability of the reasoning component in supporting result explainability. The evaluation combines a new quantitative metric with qualitative analysis. Findings show that the reasoning component aids in explaining and validating language model outputs in digital forensics at medium reasoning levels, but this support is often limited, and higher reasoning levels do not enhance response quality.

</details>


### [39] [WildCode: An Empirical Analysis of Code Generated by ChatGPT](https://arxiv.org/abs/2512.04259)
*Kobra Khanmohammadi,Pooria Roy,Raphael Khoury,Abdelwahab Hamou-Lhadj,Wilfried Patrick Konan*

Main category: cs.CR

TL;DR: 对ChatGPT生成的真实代码进行的大规模实证分析证实了先前研究：AI生成的代码在安全性方面存在不足，且用户很少询问代码的安全特性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM模型越来越多地用于生成代码，但其代码质量和安全性存在不确定性。先前研究多使用合成代码，可能缺乏真实性，因此本研究针对ChatGPT生成的真实代码进行分析。

Method: 对ChatGPT生成的真实代码进行大规模实证分析，评估代码的正确性和安全性，并探究用户请求代码的意图。

Result: 研究证实了先前使用合成查询的研究结果：LLM生成的代码在安全性方面往往不足。同时发现用户对代码安全特性缺乏关注，很少就此提问。

Conclusion: LLM生成的代码存在安全隐患，且用户安全意识薄弱，需加强代码安全验证和用户教育。

Abstract: LLM models are increasingly used to generate code, but the quality and security of this code are often uncertain. Several recent studies have raised alarm bells, indicating that such AI-generated code may be particularly vulnerable to cyberattacks. However, most of these studies rely on code that is generated specifically for the study, which raises questions about the realism of such experiments. In this study, we perform a large-scale empirical analysis of real-life code generated by ChatGPT. We evaluate code generated by ChatGPT both with respect to correctness and security and delve into the intentions of users who request code from the model. Our research confirms previous studies that used synthetic queries and yielded evidence that LLM-generated code is often inadequate with respect to security. We also find that users exhibit little curiosity about the security features of the code they ask LLMs to generate, as evidenced by their lack of queries on this topic.

</details>


### [40] [Breaking Isolation: A New Perspective on Hypervisor Exploitation via Cross-Domain Attacks](https://arxiv.org/abs/2512.04260)
*Gaoning Pan,Yiming Tao,Qinying Wang,Chunming Wu,Mingde Hu,Yizhi Ren,Shouling Ji*

Main category: cs.CR

TL;DR: 论文提出跨域攻击（CDA）作为hypervisor内存安全漏洞的新利用方式，利用虚拟化环境中较弱的内存隔离特性，通过guest内存重用来实现权限提升。


<details>
  <summary>Details</summary>
Motivation: 现有的漏洞利用框架在hypervisor环境中效果有限，因为它们依赖于识别主机中的特定结构并确定其运行时地址，而这些结构在hypervisor中稀少且被ASLR进一步模糊。研究发现现代虚拟化环境存在弱内存隔离问题。

Method: 系统化地表征和分类跨域攻击（CDA），开发了一个自动化系统来识别跨域小工具、匹配损坏指针、合成触发输入并组装完整的漏洞利用链。

Result: 在QEMU和VirtualBox的15个真实漏洞评估中，CDA显示出广泛适用性和有效性。

Conclusion: 跨域攻击为hypervisor内存安全漏洞利用提供了新的有效途径，揭示了虚拟化环境中内存隔离的脆弱性。

Abstract: Hypervisors are under threat by critical memory safety vulnerabilities, with pointer corruption being one of the most prevalent and severe forms. Existing exploitation frameworks depend on identifying highly-constrained structures in the host machine and accurately determining their runtime addresses, which is ineffective in hypervisor environments where such structures are rare and further obfuscated by Address Space Layout Randomization (ASLR). We instead observe that modern virtualization environments exhibit weak memory isolation -- guest memory is fully attacker-controlled yet accessible from the host, providing a reliable primitive for exploitation. Based on this observation, we present the first systematic characterization and taxonomy of Cross-Domain Attacks (CDA), a class of exploitation techniques that enable capability escalation through guest memory reuse. To automate this process, we develop a system that identifies cross-domain gadgets, matches them with corrupted pointers, synthesizes triggering inputs, and assembles complete exploit chains. Our evaluation on 15 real-world vulnerabilities across QEMU and VirtualBox shows that CDA is widely applicable and effective.

</details>


### [41] [One Detector Fits All: Robust and Adaptive Detection of Malicious Packages from PyPI to Enterprises](https://arxiv.org/abs/2512.04338)
*Biagio Montaruli,Luca Compagna,Serena Elisa Ponta,Davide Balzarotti*

Main category: cs.CR

TL;DR: 提出了一个鲁棒的恶意Python包检测器，通过对抗训练提升2.5倍鲁棒性，适用于PyPI和企业环境，可调节误报率，在真实数据中有效检测346个恶意包。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能解决对抗性源代码变换的鲁棒性，以及不同使用者（如仓库维护者与企业安全团队）对误报率的差异化需求。

Method: 采用细粒度代码混淆生成对抗性包，结合对抗训练增强检测器鲁棒性；支持按需调整误报率阈值（0.1%或10%）。

Result: 在80天收集的122,398个PyPI包上测试，对抗训练使检测器对混淆包的鲁棒性提升2.5倍，多检测10%的混淆包；在PyPI案例（0.1%误报率）中每日检测2.48个恶意包仅2.18个误报，企业案例（10%误报率）中每日仅1.24个误报。

Conclusion: 检测器能无缝集成到PyPI和企业生态系统，误报审查时间仅需几分钟，已向社区报告346个恶意包，证明了其实用性和可扩展性。

Abstract: The rise of supply chain attacks via malicious Python packages demands robust detection solutions. Current approaches, however, overlook two critical challenges: robustness against adversarial source code transformations and adaptability to the varying false positive rate (FPR) requirements of different actors, from repository maintainers (requiring low FPR) to enterprise security teams (higher FPR tolerance).
  We introduce a robust detector capable of seamless integration into both public repositories like PyPI and enterprise ecosystems. To ensure robustness, we propose a novel methodology for generating adversarial packages using fine-grained code obfuscation. Combining these with adversarial training (AT) enhances detector robustness by 2.5x. We comprehensively evaluate AT effectiveness by testing our detector against 122,398 packages collected daily from PyPI over 80 days, showing that AT needs careful application: it makes the detector more robust to obfuscations and allows finding 10% more obfuscated packages, but slightly decreases performance on non-obfuscated packages.
  We demonstrate production adaptability of our detector via two case studies: (i) one for PyPI maintainers (tuned at 0.1% FPR) and (ii) one for enterprise teams (tuned at 10% FPR). In the former, we analyze 91,949 packages collected from PyPI over 37 days, achieving a daily detection rate of 2.48 malicious packages with only 2.18 false positives. In the latter, we analyze 1,596 packages adopted by a multinational software company, obtaining only 1.24 false positives daily. These results show that our detector can be seamlessly integrated into both public repositories like PyPI and enterprise ecosystems, ensuring a very low time budget of a few minutes to review the false positives.
  Overall, we uncovered 346 malicious packages, now reported to the community.

</details>


### [42] [ReFuzz: Reusing Tests for Processor Fuzzing with Contextual Bandits](https://arxiv.org/abs/2512.04436)
*Chen Chen,Zaiyan Xu,Mohamadreza Rostami,David Liu,Dileep Kalathil,Ahmad-Reza Sadeghi,Jeyavijayan,Rajendran*

Main category: cs.CR

TL;DR: ReFuzz是一个基于上下文多臂赌博机的自适应硬件模糊测试框架，通过重用先前处理器的高效测试来检测新处理器中的类似漏洞，相比现有模糊测试工具有显著效率提升。


<details>
  <summary>Details</summary>
Motivation: 处理器设计通常迭代修改和重用现有设计，导致相似漏洞在多代处理器中传播。现有硬件模糊测试工具缺乏利用已知漏洞信息来优化测试的能力。

Method: 提出ReFuzz框架，智能变异触发先前处理器漏洞的测试用例，使用上下文多臂赌博机算法选择最优测试策略来测试目标处理器。

Result: 发现3个新的安全漏洞和2个功能缺陷，其中一个漏洞通过重用已知漏洞测试检测到。平均实现511.23倍覆盖加速和最高9.33%的总覆盖提升。

Conclusion: ReFuzz证明了重用先前处理器测试用例能有效检测相似和新变种漏洞，显著提高硬件模糊测试效率。

Abstract: Processor designs rely on iterative modifications and reuse well-established designs. However, this reuse of prior designs also leads to similar vulnerabilities across multiple processors. As processors grow increasingly complex with iterative modifications, efficiently detecting vulnerabilities from modern processors is critical. Inspired by software fuzzing, hardware fuzzing has recently demonstrated its effectiveness in detecting processor vulnerabilities. Yet, to our best knowledge, existing processor fuzzers fuzz each design individually, lacking the capability to understand known vulnerabilities in prior processors to fine-tune fuzzing to identify similar or new variants of vulnerabilities.
  To address this gap, we present ReFuzz, an adaptive fuzzing framework that leverages contextual bandit to reuse highly effective tests from prior processors to fuzz a processor-under-test (PUT) within a given ISA. By intelligently mutating tests that trigger vulnerabilities in prior processors, ReFuzz effectively detects similar and new variants of vulnerabilities in PUTs. ReFuzz uncovered three new security vulnerabilities and two new functional bugs. ReFuzz detected one vulnerability by reusing a test that triggers a known vulnerability in a prior processor. One functional bug exists across three processors that share design modules. The second bug has two variants. Additionally, ReFuzz reuses highly effective tests to enhance efficiency in coverage, achieving an average 511.23x coverage speedup and up to 9.33% more total coverage, compared to existing fuzzers.

</details>


### [43] [A Light-Weight Large Language Model File Format for Highly-Secure Model Distribution](https://arxiv.org/abs/2512.04580)
*Huifeng Zhu,Shijie Li,Qinfeng Li,Yier Jin*

Main category: cs.CR

TL;DR: CryptoTensors是一个基于Safetensors格式的安全文件结构，通过张量级加密和访问控制策略保护LLM权重，实现轻量级、高效的机密模型分发。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在医疗、法律、金融等敏感领域的定制化应用增加，模型权重作为隐私资产或知识产权需要保护，但现有部署框架缺乏内置的机密性和访问控制支持。

Method: 扩展Safetensors格式，集成张量级加密和嵌入式访问控制策略，保留延迟加载和部分反序列化特性，提供透明解密和自动密钥管理。

Result: 开发了概念验证库，在序列化和运行时场景中性能基准测试良好，与Hugging Face Transformers、vLLM等推理框架兼容，验证了其轻量高效特性。

Conclusion: CryptoTensors为实际广泛部署中的LLM权重保护提供了一个轻量、高效且开发者友好的解决方案。

Abstract: To enhance the performance of large language models (LLMs) in various domain-specific applications, sensitive data such as healthcare, law, and finance are being used to privately customize or fine-tune these models. Such privately adapted LLMs are regarded as either personal privacy assets or corporate intellectual property. Therefore, protecting model weights and maintaining strict confidentiality during deployment and distribution have become critically important. However, existing model formats and deployment frameworks provide little to no built-in support for confidentiality, access control, or secure integration with trusted hardware. Current methods for securing model deployment either rely on computationally expensive cryptographic techniques or tightly controlled private infrastructure. Although these approaches can be effective in specific scenarios, they are difficult and costly for widespread deployment.
  In this paper, we introduce CryptoTensors, a secure and format-compatible file structure for confidential LLM distribution. Built as an extension to the widely adopted Safetensors format, CryptoTensors incorporates tensor-level encryption and embedded access control policies, while preserving critical features such as lazy loading and partial deserialization. It enables transparent decryption and automated key management, supporting flexible licensing and secure model execution with minimal overhead. We implement a proof-of-concept library, benchmark its performance across serialization and runtime scenarios, and validate its compatibility with existing inference frameworks, including Hugging Face Transformers and vLLM. Our results highlight CryptoTensors as a light-weight, efficient, and developer-friendly solution for safeguarding LLM weights in real-world and widespread deployments.

</details>


### [44] [Cryptanalysis of Gleeok-128](https://arxiv.org/abs/2512.04675)
*Siwei Chen,Peipei Xie,Shengyuan Xu,Xiutao Feng,Zejun Xiang,Xiangyong Zeng*

Main category: cs.CR

TL;DR: 本文对Gleeok-128进行了首次全面的第三方密码分析，提出了基于MILP的差分-线性区分器构建框架和积分攻击框架，发现了优于设计文档的安全漏洞，并提出了改进的线性层参数。


<details>
  <summary>Details</summary>
Motivation: 由于Gleeok的多分支结构，评估其安全裕度和实施有效的密钥恢复攻击存在挑战，需要系统的密码分析方法。

Method: 采用两阶段MILP框架构建分支和全密码的差分-线性区分器，结合积分攻击框架针对多分支设计进行定制化分析。

Result: 获得了7/7/8/4轮区分器（三个分支和全密码），积分区分器扩展了设计者结果3/3/2轮和2轮，发现Branch 3线性安全评估缺陷（12轮可区分，数据复杂度2^48）。

Conclusion: 研究推进了对Gleeok-128的理解，为分析多分支对称设计提供了通用方法，同时提出了优化线性层参数以提升安全性。

Abstract: Gleeok is a family of low latency keyed pseudorandom functions (PRFs) consisting of three parallel SPN based permutations whose outputs are XORed to form the final value. Both Gleeok-128 and Gleeok-256 use a 256 bit key, with block sizes of 128 and 256 bits, respectively. Owing to its multi branch structure, evaluating security margins and mounting effective key recovery attacks present nontrivial challenges. This paper provides the first comprehensive third party cryptanalysis of Gleeok-128. We introduce a two stage MILP based framework for constructing branch wise and full cipher differential linear (DL) distinguishers, together with an integral based key recovery framework tailored to multi branch designs. Our DL analysis yields 7, 7, 8, and 4 round distinguishers for Branch 1, Branch 2, Branch 3, and Gleeok-128, respectively, with squared correlations approximately 2 to the power minus 88.12, 2 to the power minus 88.12, 2 to the power minus 38.73, and 2 to the power minus 49.04, outperforming those in the design document except for the full PRF case. By tightening algebraic degree bounds, we further derive 9, 9, and 7 round integral distinguishers for the three branches and a 7 round distinguisher for the full PRF, extending the designers results by 3, 3, and 2 rounds and by 2 rounds, respectively. These integral properties enable 7 round and 8 round key recovery attacks in the non full codebook and full codebook settings. In addition, we identify a flaw in the original linear security evaluation of Branch 3, showing that it can be distinguished over all 12 rounds with data complexity about 2 to the power 48. We also propose optimized linear layer parameters that significantly improve linear resistance without sacrificing diffusion. Our results advance the understanding of Gleeok-128 and provide general methods for analyzing multi branch symmetric designs.

</details>


### [45] [SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security](https://arxiv.org/abs/2512.04841)
*Wei Zhao,Zhe Li,Jun Sun*

Main category: cs.CR

TL;DR: 提出了一个统一的因果分析框架，系统支持LLM中从令牌级到表示级的因果调查，揭示安全机制高度局部化（集中在早期到中间层，仅1-2%的神经元具有因果影响），因果特征在威胁检测中准确率超过95%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型表现出卓越能力但仍易受对抗性操控（如越狱），理解这些脆弱性的因果因素对于构建可靠防御至关重要。

Method: 引入统一因果分析框架，支持令牌级、神经元级、层级干预和表示级分析，在多开放权重模型和安全关键基准（越狱、幻觉检测、后门识别、公平性评估）上进行实证评估。

Result: 针对性干预因果关键组件可可靠修改安全行为；安全相关机制高度局部化；从框架提取的因果特征在多种威胁类型中检测准确率超过95%。

Conclusion: 通过桥接理论因果分析和实际模型安全，该框架为基于因果性的攻击、可解释性以及鲁棒攻击检测与缓解研究建立了可复现基础。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but remain vulnerable to adversarial manipulations such as jailbreaking, where crafted prompts bypass safety mechanisms. Understanding the causal factors behind such vulnerabilities is essential for building reliable defenses.
  In this work, we introduce a unified causality analysis framework that systematically supports all levels of causal investigation in LLMs, ranging from token-level, neuron-level, and layer-level interventions to representation-level analysis. The framework enables consistent experimentation and comparison across diverse causality-based attack and defense methods. Accompanying this implementation, we provide the first comprehensive survey of causality-driven jailbreak studies and empirically evaluate the framework on multiple open-weight models and safety-critical benchmarks including jailbreaks, hallucination detection, backdoor identification, and fairness evaluation. Our results reveal that: (1) targeted interventions on causally critical components can reliably modify safety behavior; (2) safety-related mechanisms are highly localized (i.e., concentrated in early-to-middle layers with only 1--2\% of neurons exhibiting causal influence); and (3) causal features extracted from our framework achieve over 95\% detection accuracy across multiple threat types.
  By bridging theoretical causality analysis and practical model safety, our framework establishes a reproducible foundation for research on causality-based attacks, interpretability, and robust attack detection and mitigation in LLMs. Code is available at https://github.com/Amadeuszhao/SOK_Casuality.

</details>


### [46] [Opacity problems in multi-energy timed automata](https://arxiv.org/abs/2512.04950)
*Étienne André,Lydia Bakiri*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cyber-physical systems can be subject to information leakage; in the presence of continuous variables such as time and energy, these leaks can be subtle to detect. We study here the verification of opacity problems over systems with observation over both timing and energy information. We introduce guarded multi-energy timed automata as an extension of timed automata with multiple energy variables and guards over such variables. Despite undecidability of this general formalism, we establish positive results over a number of subclasses, notably when the attacker observes the final energy and/or the execution time, but also when they have access to the value of the energy variables every time unit.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [47] [Retrieval-Augmented Few-Shot Prompting Versus Fine-Tuning for Code Vulnerability Detection](https://arxiv.org/abs/2512.04106)
*Fouad Trad,Ali Chehab*

Main category: cs.SE

TL;DR: 本文探讨了检索增强提示在代码漏洞检测任务中提升少样本学习性能的效果，发现其在减少训练成本的同时优于标准少样本提示与零样本方法。


<details>
  <summary>Details</summary>
Motivation: 少样本提示虽能利用大语言模型处理专业任务，但其效果受限于上下文示例的质量与选择，尤其在复杂领域如代码漏洞检测中。文章旨在通过检索增强策略优化示例选择，以提高性能。

Method: 采用Gemini-1.5-Flash模型，系统评估三种方法：随机示例的少样本提示、基于语义相似性的检索增强提示，以及无需模型推理的检索标注方法。

Result: 检索增强提示在20样本下F1得分达74.05%，部分匹配准确率83.90%，显著优于随机少样本提示、零样本提示及微调Gemini模型。微调CodeBERT性能更高（F1 91.22%），但需额外训练资源。

Conclusion: 检索增强提示是平衡性能与成本的实用方案，适用于代码漏洞检测等复杂任务，尤其适合避免微调开销的场景；微调模型虽性能更优，但资源需求较高。

Abstract: Few-shot prompting has emerged as a practical alternative to fine-tuning for leveraging the capabilities of large language models (LLMs) in specialized tasks. However, its effectiveness depends heavily on the selection and quality of in-context examples, particularly in complex domains. In this work, we examine retrieval-augmented prompting as a strategy to improve few-shot performance in code vulnerability detection, where the goal is to identify one or more security-relevant weaknesses present in a given code snippet from a predefined set of vulnerability categories. We perform a systematic evaluation using the Gemini-1.5-Flash model across three approaches: (1) standard few-shot prompting with randomly selected examples, (2) retrieval-augmented prompting using semantically similar examples, and (3) retrieval-based labeling, which assigns labels based on retrieved examples without model inference. Our results show that retrieval-augmented prompting consistently outperforms the other prompting strategies. At 20 shots, it achieves an F1 score of 74.05% and a partial match accuracy of 83.90%. We further compare this approach against zero-shot prompting and several fine-tuned models, including Gemini-1.5-Flash and smaller open-source models such as DistilBERT, DistilGPT2, and CodeBERT. Retrieval-augmented prompting outperforms both zero-shot (F1 score: 36.35%, partial match accuracy: 20.30%) and fine-tuned Gemini (F1 score: 59.31%, partial match accuracy: 53.10%), while avoiding the training time and cost associated with model fine-tuning. On the other hand, fine-tuning CodeBERT yields higher performance (F1 score: 91.22%, partial match accuracy: 91.30%) but requires additional training, maintenance effort, and resources.

</details>


### [48] [Reusing Model Validation Methods for the Continuous Validation of Digital Twins of Cyber-Physical Systems](https://arxiv.org/abs/2512.04117)
*Joost Mertens,Joachim Denil*

Main category: cs.SE

TL;DR: 论文提出了一种基于验证指标的通用方法来检测数字孪生系统中的异常，并通过龙门起重机的案例研究进行了验证，使用参数估计来纠正数字孪生中的错误。


<details>
  <summary>Details</summary>
Motivation: 数字孪生系统面临的挑战是确保数字孪生始终是其所模拟物理系统的有效代表。物理系统在部署后会因维护、磨损或用户错误等过程而演变，因此需要检测物理系统中的变化以使数字孪生同步更新。

Method: 重用基于模型设计中的验证技术，通过验证指标检测数字孪生系统中的异常，并使用基于历史数据的参数估计来纠正数字孪生中的错误。

Result: 提出了一种通用方法，能够检测数字孪生系统中的异常，并通过龙门起重机的案例研究验证了这些技术的有效性。

Conclusion: 通过验证指标和参数估计，可以有效维护数字孪生与其物理对应物之间的一致性，确保数字孪生的持续有效性。

Abstract: One of the challenges in twinned systems is ensuring the digital twin remains a valid representation of the system it twins. Depending on the type of twinning occurring, it is either trivial, such as in dashboarding/visualizations that mirror the system with real-time data, or challenging, in case the digital twin is a simulation model that reflects the behavior of a physical twinned system. The challenge in this latter case comes from the fact that in contrast to software systems, physical systems are not immutable once deployed, but instead they evolve through processes like maintenance, wear and tear or user error. It is therefore important to detect when changes occur in the physical system to evolve the twin alongside it. We employ and reuse validation techniques from model-based design for this goal. Model validation is one of the steps used to gain trust in the representativeness of a simulation model. In this work, we provide two contributions: (i) we provide a generic approach that, through the use of validation metrics, is able to detect anomalies in twinned systems, and (ii) we demonstrate these techniques with the help of an academic yet industrially relevant case study of a gantry crane such as found in ports. Treating anomalies also means correcting the error in the digital twin, which we do with a parameter estimation based on the historical data.

</details>


### [49] [DrP: Meta's Efficient Investigations Platform at Scale](https://arxiv.org/abs/2512.04250)
*Shubham Somani,Vanish Talwar,Madhura Parikh,Eduardo Hernandez,Jimmy Wang,Shreya Shah,Chinmay Gandhi,Sanjay Sundarajan,Neeru Sharma,Srikanth Kamath,Nitin Gupta,Benjamin Renard,Ohad Yahalom,Chris Davis*

Main category: cs.SE

TL;DR: DrP是一个端到端的自动化调查框架，通过编写分析器、可扩展执行和集成主流工作流，显著降低了平均解决时间和值班负担。


<details>
  <summary>Details</summary>
Motivation: 大规模系统中的人工或临时脚本调查效率低下，导致解决故障时间长、值班工作繁重。

Method: 提供灵活的SDK编写调查手册（分析器），构建可扩展的后端执行系统，集成警报和事件管理工具，并实施后处理行动系统。

Result: 在Meta部署5年，覆盖300+团队、2000+分析器，每日执行5万次分析，平均MTTR降低20%（部分团队超80%），生产力显著提升。

Conclusion: DrP有效自动化调查流程，大幅减少解决时间和值班负担，证明了其在大规模生产环境中的可行性和价值。

Abstract: Investigations are a significant step in the operational workflows for large scale systems across multiple domains such as services, data, AI/ML, mobile. Investigation processes followed by on-call engineers are often manual or rely on ad-hoc scripts. This leads to inefficient investigations resulting in increased time to mitigate and isolate failures/SLO violations. It also contributes to on-call toil and poor productivity leading to multiple hours/days spent in triaging/debugging incidents. In this paper, we present DrP, an end-to-end framework and system to automate investigations that reduces the mean time to resolve incidents (MTTR) and reduces on-call toil. DrP consists of an expressive and flexible SDK to author investigation playbooks in code (called analyzers), a scalable backend system to execute these automated playbooks, plug-ins to integrate playbooks into mainstream workflows such as alerts and incident management tools, and a post-processing system to take actions on investigations including mitigation steps.
  We have implemented and deployed DrP at large scale at Meta covering 300+ teams, 2000+ analyzers, across a large set of use cases across domains such as services, core infrastructure, AI/ML, hardware, mobile. DrP has been running in production for the past 5 years and executes 50K automated analyses per day. Overall, our results and experience show that DrP has been able to reduce average MTTR by 20 percent at large scale (with over 80 percent for some teams) and has significantly improved on-call productivity.

</details>


### [50] [On the Role and Impact of GenAI Tools in Software Engineering Education](https://arxiv.org/abs/2512.04256)
*Qiaolin Qin,Ronnie de Souza Santos,Rodrigo Spinola*

Main category: cs.SE

TL;DR: 本科软件工程学生使用生成式AI工具的学习体验研究：调查显示学生主要将AI用于增量学习和高级实现，认为AI能提供头脑风暴支持并增强信心，但也面临输出结果难以适应和原理不透明等挑战，呼吁更明确的教学指导和伦理政策。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具（如ChatGPT和GitHub Copilot）正在改变软件学习和编写方式，为软件工程教育带来新机遇，但也引发了对过度依赖、伦理使用及学习影响的担忧。

Method: 对来自两所大学的130名本科软件工程学生进行问卷调查，结合结构化李克特量表和开放式问题，探究使用情境、感知益处、挑战、伦理及教学认知五个维度。

Result: 学生最常将生成式AI用于增量学习和高级实现，报告益处包括头脑风暴支持和信心建立；同时面临挑战如原理不透明和输出适配困难；学生强调公平和不当行为等伦理问题，并呼吁更清晰的教学指导。

Conclusion: 生成式AI正以微妙方式重塑软件工程教育。研究发现强调需要搭建支架、制定伦理政策和适应性教学策略，以确保生成式AI支持公平有效的学习。

Abstract: Context. The rise of generative AI (GenAI) tools like ChatGPT and GitHub Copilot has transformed how software is learned and written. In software engineering (SE) education, these tools offer new opportunities for support, but also raise concerns about over-reliance, ethical use, and impacts on learning. Objective. This study investigates how undergraduate SE students use GenAI tools, focusing on the benefits, challenges, ethical concerns, and instructional expectations that shape their experiences. Method. We conducted a survey with 130 undergraduate students from two universities. The survey combined structured Likert-scale items and open-ended questions to investigate five dimensions: usage context, perceived benefits, challenges, ethical and instructional perceptions. Results. Students most often use GenAI for incremental learning and advanced implementation, reporting benefits such as brainstorming support and confidence-building. At the same time, they face challenges including unclear rationales and difficulty adapting outputs. Students highlight ethical concerns around fairness and misconduct, and call for clearer instructional guidance. Conclusion. GenAI is reshaping SE education in nuanced ways. Our findings underscore the need for scaffolding, ethical policies, and adaptive instructional strategies to ensure that GenAI supports equitable and effective learning.

</details>


### [51] [Catching UX Flaws in Code: Leveraging LLMs to Identify Usability Flaws at the Development Stage](https://arxiv.org/abs/2512.04262)
*Nolan Platt,Ethan Luchs,Sehrish Nizamani*

Main category: cs.SE

TL;DR: 本研究探讨了大型语言模型（GPT-4o）在网站开发阶段进行可用性启发式评估的可行性与局限性。通过分析30个开源网站，发现模型在检测可用性问题方面具有中等一致性，但在严重性判断上表现不稳定，需要人工监督。


<details>
  <summary>Details</summary>
Motivation: 传统的人工启发式评估耗时且主观，尤其在开发早期。研究旨在探索LLMs能否在开发阶段提供可靠、一致的自动化可用性评估，以提升效率。

Method: 采用Jakob Nielsen的十项可用性启发式原则，对30个开源网站进行三轮独立评估，使用OpenAI的GPT-4o生成850余次评估。通过Cohen's Kappa、精确一致性和Krippendorff's Alpha等指标分析一致性。

Result: 问题检测方面：平均Cohen's Kappa为0.50（中等一致性），精确一致性达84%。严重性判断方面：加权Cohen's Kappa平均0.63，但精确一致性仅56%，Krippendorff's Alpha接近零，表明严重性评估变异较大。

Conclusion: GPT-4o能生成内部一致的可用性问题检测结果，但严重性判断不可靠，需人工干预。研究为自动化用户体验评估提供了初步定量分析基础，并指出改进模型一致性的方法。

Abstract: Usability evaluations are essential for ensuring that modern interfaces meet user needs, yet traditional heuristic evaluations by human experts can be time-consuming and subjective, especially early in development. This paper investigates whether large language models (LLMs) can provide reliable and consistent heuristic assessments at the development stage. By applying Jakob Nielsen's ten usability heuristics to thirty open-source websites, we generated over 850 heuristic evaluations in three independent evaluations per site using a pipeline of OpenAI's GPT-4o. For issue detection, the model demonstrated moderate consistency, with an average pairwise Cohen's Kappa of 0.50 and an exact agreement of 84%. Severity judgments showed more variability: weighted Cohen's Kappa averaged 0.63, but exact agreement was just 56%, and Krippendorff's Alpha was near zero. These results suggest that while GPT-4o can produce internally consistent evaluations, especially for identifying the presence of usability issues, its ability to judge severity varies and requires human oversight in practice. Our findings highlight the feasibility and limitations of using LLMs for early-stage, automated usability testing, and offer a foundation for improving consistency in automated User Experience (UX) evaluation. To the best of our knowledge, our work provides one of the first quantitative inter-rater reliability analyses of automated heuristic evaluation and highlights methods for improving model consistency.

</details>


### [52] [Polynomiogram: An Integrated Framework for Root Visualization and Generative Art](https://arxiv.org/abs/2512.04263)
*Hoang Duc Nguyen,Anh Van Pham,Hien D. Nguyen*

Main category: cs.SE

TL;DR: 提出了Polynomiogram框架，一个集成计算平台，用于探索、可视化和从多项式根系统生成艺术。


<details>
  <summary>Details</summary>
Motivation: 开发一个既能支持科学研究又能支持生成算法艺术的数学基础框架。

Method: 采用灵活采样方案，从用户定义域中抽取两个独立参数，通过生成函数映射到多项式系数；集成NumPy同伴矩阵求解器和MPSolve进行高效计算和高精度验证。

Result: 验证了数值准确性，应用于三次多项式系统分析分岔结构；展示了作为科学工具和教育辅助的价值；成功生成个性化生成艺术。

Conclusion: Polynomiogram框架有效结合了科学研究与艺术生成，为多项式根系统的探索和教育提供了多功能平台。

Abstract: This work presents the Polynomiogram framework, an integrated computational platform for exploring, visualizing, and generating art from polynomial root systems. The main innovation is a flexible sampling scheme in which two independent parameters are drawn from user defined domains and mapped to the polynomial coefficients through a generating function. This design allows the same mathematical foundation to support both scientific investigation and generative algorithmic art. The framework integrates two complementary numerical engines: NumPy companion matrix solver for fast, large scale computation and MPSolve for high precision, scientifically rigorous validation. This dual architecture enables efficient visualization for creative use and accurate computation for research and education. Numerical accuracy was verified using classical ensembles, including the Kac and Lucas polynomials. The method was applied to the cubic polynomial system to analyze its bifurcation structure, demonstrating its value as both a scientific tool for exploring root phenomena and an educational aid for visualizing fundamental concepts in algebra and dynamical systems. Beyond analysis, the Polynomiogram also demonstrated its potential as a tool for personalized generative art. Examples include the use of the platform to generate a natural form resembling a hibiscus flower and to create personalized artwork expressing gratitude toward advances in artificial intelligence and large language models through a tribute composition.

</details>


### [53] [Quantitative Analysis of Technical Debt and Pattern Violation in Large Language Model Architectures](https://arxiv.org/abs/2512.04273)
*Tyler Slater*

Main category: cs.SE

TL;DR: 本文提出了首个衡量AI生成微服务中架构腐蚀和技术债务积累的实证框架，通过三项先进LLM模型的对比研究发现开源模型存在严重的架构违规和逻辑简化问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要评估LLM的功能正确性，但对于AI生成代码长期可维护性的影响缺乏量化研究，特别是架构侵蚀和技术债务积累问题未被探索。

Method: 采用三种最先进模型（GPT-5.1、Claude 4.5 Sonnet、Llama 3 8B）在严格的六边形架构约束下实现标准化图书借阅微服务，通过抽象语法树解析进行架构符合性分析。

Result: 专有模型架构符合率达100%，而开源模型Llama 3出现80%架构违规率，经常绕过接口适配器创建非法循环依赖；开源模型生成的逻辑代码行数比专有模型少60%。

Conclusion: 如果不采用自动化架构检查，使用小型开源模型进行系统架构建模会加速结构技术债务的积累，需要建立专门的架构验证机制。

Abstract: As Large Language Models (LLMs) transition from code completion tools to autonomous system architects, their impact on long-term software maintainability remains unquantified. While existing research benchmarks functional correctness (pass@k), this study presents the first empirical framework to measure "Architectural Erosion" and the accumulation of Technical Debt in AI-synthesized microservices. We conducted a comparative pilot study of three state-of-the-art models (GPT-5.1, Claude 4.5 Sonnet, and Llama 3 8B) by prompting them to implement a standardized Book Lending Microservice under strict Hexagonal Architecture constraints. Utilizing Abstract Syntax Tree (AST) parsing, we find that while proprietary models achieve high architectural conformance (0% violation rate for GPT-5.1), open-weights models exhibit critical divergence. Specifically, Llama 3 demonstrated an 80% Architectural Violation Rate, frequently bypassing interface adapters to create illegal circular dependencies between Domain and Infrastructure layers. Furthermore, we identified a phenomenon of "Implementation Laziness," where open-weights models generated 60% fewer Logical Lines of Code (LLOC) than their proprietary counterparts, effectively omitting complex business logic to satisfy token constraints. These findings suggest that without automated architectural linting, utilizing smaller open-weights models for system scaffolding accelerates the accumulation of structural technical debt.

</details>


### [54] [MANTRA: a Framework for Multi-stage Adaptive Noise TReAtment During Training](https://arxiv.org/abs/2512.04319)
*Zixiao Zhao,Fatemeh H. Fard,Jie JW Wu*

Main category: cs.SE

TL;DR: 提出MANTRA框架，通过多阶段自适应噪声处理提高代码预训练模型在软件工程任务中的性能，特别针对噪声标签问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在软件工程任务中的应用需要高质量训练数据，但大型代码库中常存在噪声标签，影响模型准确性和鲁棒性。然而，噪声标签学习在软件工程领域的研究较少。

Method: MANTRA框架集成噪声诊断和缓解到模型微调过程中：先研究不同噪声水平对模型收敛的影响，再基于样本损失动态和高斯混合模型聚类应用自适应dropout策略过滤噪声数据。

Result: 在代码摘要和提交意图分类任务上实验表明，某些大语言模型对噪声更敏感，但使用MANTRA后所有模型性能均得到提升。

Conclusion: MANTRA能有效减少训练数据中噪声的影响，节省数据清洗时间，并最大化微调效果，为研究和实践提供实用工具。

Abstract: The reliable application of deep learning models to software engineering tasks hinges on high-quality training data. Yet, large-scale repositories inevitably introduce noisy or mislabeled examples that degrade both accuracy and robustness. While Noise Label Learning (NLL) has been extensively studied in other fields, there are a few works that investigate NLL in Software Engineering (SE) and Large Language Models (LLMs) for SE tasks. In this work, we propose MANTRA, a Multi-stage Adaptive Noise TReAtment framework that embeds noise diagnosis and mitigation directly into the fine-tuning process of code-Pretrained Language Models (PTM) and code-LLMs. We first investigate the effect of noise at varying levels on convergence and loss trajectories of the models. Then we apply an adaptive dropout strategy guided by per-sample loss dynamics and Gaussian Mixture Model clustering to exclude persistently noisy points while preserving clean data. Applying to code summarization and commit intent classification, our experiments reveal that some LLMs are more sensitive to noise than others. However, with MANTRA, the performance of all models in both tasks is improved. MANTRA enables researchers and practitioners to reduce the impact of errors introduced by the dataset in training, saves time in data cleaning and processing, while maximizing the effect of fine-tuning.

</details>


### [55] [Targeted Testing of Compiler Optimizations via Grammar-Level Composition Styles](https://arxiv.org/abs/2512.04344)
*Zitong Zhou,Ben Limpanukorn,Hong Jin Kang,Jiyuan Wang,Yaoxuan Wu,Akos Kiss,Renata Hodovan,Miryung Kim*

Main category: cs.SE

TL;DR: 提出了TargetFuzz，一种针对单个编译器优化的针对性模糊测试方法，通过利用优化相关的组合样式来提高测试覆盖率和触发优化次数


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试方法难以有效测试编译器优化，主要存在两个问题：1) 使用固定优化流水线会错过优化间的交互；2) 现有生成器难以产生满足优化触发条件的特定程序结构

Method: 构建基于语法的变异模糊测试工具TargetFuzz，通过挖掘优化相关的组合样式（程序构造间的结构关系），并在不同上下文中重建这些样式来测试优化的变体

Result: 在LLVM和MLIR上的评估显示，TargetFuzz比基线模糊测试器在覆盖率上分别提高了8%和11%，优化触发次数分别提高了2.8倍和2.6倍

Conclusion: 针对性模糊测试是对基于流水线测试的有效补充，能够更全面地测试编译器优化，特别是在MLIR等模块化框架中具有重要应用价值

Abstract: Ensuring the correctness of compiler optimizations is critical, but existing fuzzers struggle to test optimizations effectively. First, most fuzzers use optimization pipelines (heuristics-based, fixed sequences of passes) as their harness. The phase-ordering problem can enable or preempt transformations, so pipelines inevitably miss optimization interactions; moreover, many optimizations are not scheduled, even at aggressive levels. Second, optimizations typically fire only when inputs satisfy specific structural relationships, which existing generators and mutations struggle to produce. We propose targeted fuzzing of individual optimizations to complement pipeline-based testing. Our key idea is to exploit composition styles - structural relations over program constructs (adjacency, nesting, repetition, ordering) - that optimizations look for. We build a general-purpose, grammar-based mutational fuzzer, TargetFuzz, that (i) mines composition styles from an optimization-relevant corpus, then (ii) rebuilds them inside different contexts offered by a larger, generic corpus via synthesized mutations to test variations of optimization logic. TargetFuzz is adaptable to a new programming language by lightweight, grammar-based, construct annotations - and it automatically synthesizes mutators and crossovers to rebuild composition styles. No need for hand-coded generators or language-specific mutators, which is particularly useful for modular frameworks such as MLIR, whose dialect-based, rapidly evolving ecosystem makes optimizations difficult to fuzz. Our evaluation on LLVM and MLIR shows that TargetFuzz improves coverage by 8% and 11% and triggers optimizations 2.8$\times$ and 2.6$\times$, compared to baseline fuzzers under the targeted fuzzing mode. We show that targeted fuzzing is complementary: it effectively tests all 37 sampled LLVM optimizations, while pipeline-fuzzing missed 12.

</details>


### [56] [LLM-SrcLog: Towards Proactive and Unified Log Template Extraction via Large Language Models](https://arxiv.org/abs/2512.04474)
*Jiaqi Sun,Wei Li,Heng Zhang,Chutong Ding,Shiyou Qian,Jian Cao,Guangtao Xue*

Main category: cs.SE

TL;DR: LLM-SrcLog是一个主动的日志解析框架，通过源代码分析和数据驱动方法结合，在保持高精度的同时显著提升了处理速度


<details>
  <summary>Details</summary>
Motivation: 现有日志解析器主要被动地从日志中推断模板，忽略了源代码信息，无法适应动态日志结构变化，且基于LLM的单条日志解析成本过高

Method: 框架包含：跨函数静态代码分析器重建日志上下文，基于LLM的白盒模板提取器区分常量变量，数据驱动的黑盒模板提取器处理无源代码日志

Result: 在两个公开基准和一个工业系统上的实验显示，相比基于LLM的基线方法，F1分数提升2-35%，在线解析延迟与数据驱动方法相当，比单条日志LLM解析快约1000倍

Conclusion: LLM-SrcLog在速度和准确性之间实现了近乎理想的平衡，并通过实际生产环境案例验证了有效性

Abstract: Log parsing transforms raw logs into structured templates containing constants and variables. It underpins anomaly detection, failure diagnosis, and other AIOps tasks. Current parsers are mostly reactive and log-centric. They only infer templates from logs, mostly overlooking the source code. This restricts their capacity to grasp dynamic log structures or adjust to evolving systems. Moreover, per-log LLM inference is too costly for practical deployment. In this paper, we propose LLM-SrcLog, a proactive and unified framework for log template parsing. It extracts templates directly from source code prior to deployment and supplements them with data-driven parsing for logs without available code. LLM-SrcLog integrates a cross-function static code analyzer to reconstruct meaningful logging contexts, an LLM-based white-box template extractor with post-processing to distinguish constants from variables, and a black-box template extractor that incorporates data-driven clustering for remaining unmatched logs. Experiments on two public benchmarks (Hadoop and Zookeeper) and a large-scale industrial system (Sunfire-Compute) show that, compared to two LLM-based baselines, LLM-SrcLog improves average F1-score by 2-17% and 8-35%. Meanwhile, its online parsing latency is comparable to data-driven methods and about 1,000 times faster than per-log LLM parsing. LLM-SrcLog achieves a near-ideal balance between speed and accuracy. Finally, we further validate the effectiveness of LLM-SrcLog through practical case studies in a real-world production environment.

</details>


### [57] [Completion by Comprehension: Guiding Code Generation with Multi-Granularity Understanding](https://arxiv.org/abs/2512.04538)
*Xinkui Zhao,Rongkai Liu,Yifan Zhang,Chen Zhi,Lufei Zhang,Guanjie Cheng,Yueshen Xu,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: CoCo是一个创新的代码补全框架，通过理解多粒度上下文来提高代码生成质量，相比现有方法取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索增强生成的代码补全方法主要将代码视为纯自然语言，依赖浅层语义匹配而忽略结构语义和代码特定依赖关系，限制了捕捉控制流和底层意图的能力。

Method: CoCo采用静态代码分析提取函数、文件和项目级别的结构化上下文，使用基于图的多粒度上下文选择机制过滤冗余信息，并将信息转换为自然语言作为显式上下文提示，辅以结构感知的代码重新排序机制。

Result: 在CrossCodeEval和RepoEval基准测试上的广泛实验表明，CoCo consistently超越最先进的基线方法，在EM指标上实现高达20.2%的提升。

Conclusion: CoCo框架是模型无关的，能够无缝集成到现有方法中，显著提升性能，为解决从函数级到仓库级代码补全任务中的上下文信息利用问题提供了有效方案。

Abstract: As code completion task from function-level to repository-level, leveraging contextual information from large-scale codebases becomes a core challenge. However, existing retrieval-augmented generation (RAG) methods typically treat code as plain natural language, relying primarily on shallow semantic matching while overlooking structural semantics and code-specific dependencies. This limits their ability to capture control flow and underlying intent, ultimately constraining the quality of generated code. Therefore, we propose CoCo, a novel framework that enables code Completion by Comprehension of multi-granularity context from large-scale code repositories. CoCo employs static code analysis to extract structured context at the function, file, and project levels, capturing execution logic and semantic dependencies. It then adopts an graph-based multi-granularity context selection mechanism to filter out redundant information and remove noise. Consequently, the information is converted into natural language in a consistent manner, thereby functioning as explicit contextual prompts to guide subsequent code completion. Additionally, a structure-aware code re-ranker mechanism ensures alignment at both semantic and structural levels. Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines, achieving up to 20.2% gains in EM. Moreover, the framework is model-agnostic and can be seamlessly integrated into existing methods, leading to significant performance.

</details>


### [58] [Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models](https://arxiv.org/abs/2512.04673)
*Gunjan Das,Paheli Bhattacharya,Rishabh Gupta*

Main category: cs.SE

TL;DR: 本研究对5个通用和3个代码专用的大型语言模型（LLMs）进行了跨领域综合评估，覆盖语言能力、数学推理和可信度等六个基准测试。研究发现代码优化模型（如CodeLLaMA）在推理和语法精度上表现优异，甚至在非代码任务中也优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 已有研究多关注单个模型能力，但缺乏对语言模型在语言、推理和代码理解能力的系统性跨领域比较。

Method: 使用六个多样化基准测试（包括语言能力、数学推理、可信度）评估五个通用和三个代码专用LLMs，并分析它们在CoNaLa数据集上的代码解释行为。

Result: 代码专用模型（如CodeLLaMA变体）在推理和语法精度方面表现更强，即使在非代码任务中也显示出可测量的性能优势，而通用模型（如Mistral-7B和Llama-3-8B）表现相对较弱。

Conclusion: 代码优化的LLMs在跨领域任务中展现出显著优势，表明专业化模型在综合能力评估中具有重要价值。

Abstract: Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.

</details>


### [59] [Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap](https://arxiv.org/abs/2512.04680)
*Jialong Li,Mingyue Zhang,Nianyu Li,Danny Weyns,Zhi Jin,Kenji Tei*

Main category: cs.SE

TL;DR: 这篇论文探讨了在自适应系统中运用生成式人工智能的潜在优势和挑战，通过文献分析和分类，提出了一个研究路线图。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能在数据理解和逻辑推理方面的卓越能力与自适应系统的反馈循环需求高度契合，但具体应用中的利弊尚不明确。

Method: 从四个不同研究领域收集、筛选并分析文献，将潜在优势分类为自适应系统自主性增强和人机交互改进两大方向。

Result: 论文识别了生成式人工智能在自适应系统中的具体应用潜力，并指出集成过程中的关键研究挑战和技术短板。

Conclusion: 论文提出了一个研究路线图，强调了解决生成式人工智能技术局限性的必要性，并建议了缓解策略，为未来研究提供了指导。

Abstract: Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.

</details>


### [60] [POLARIS: Is Multi-Agentic Reasoning the Next Wave in Engineering Self-Adaptive Systems?](https://arxiv.org/abs/2512.04702)
*Divyansh Pandey,Vyakhya Gupta,Prakhar Singhal,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: POLARIS框架通过三层多智能体架构实现预测性自适应，超越传统反应式方法，在不确定性环境中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现代软件生态系统的规模、复杂性和自主性带来前所未有的不确定性，传统自适应方法难以处理新情境和分布式协调。

Method: 提出POLARIS三层框架：低延迟适配层（监控与安全执行）、透明推理层（工具感知的可解释智能体生成验证计划）、元层（经验记录与元学习改进策略）。

Result: 在SWIM和SWITCH两个自适应系统示例上的初步评估表明，POLARIS持续优于现有最优基线方法。

Conclusion: 这标志着向Self-Adaptation 3.0的转变，系统不仅能从环境中学习，还能推理和进化自身适应过程，持续改进以应对新挑战。

Abstract: The growing scale, complexity, interconnectivity, and autonomy of modern software ecosystems introduce unprecedented uncertainty, challenging the foundations of traditional self-adaptation. Existing approaches, typically rule-driven controllers or isolated learning components, struggle to generalize to novel contexts or coordinate responses across distributed subsystems, leaving them ill-equipped for emergent unknown unknowns. Recent discussions on Self-Adaptation 2.0 emphasize an equal partnership between AI and adaptive systems, merging learning-driven intelligence with adaptive control for predictive and proactive behavior. Building on this foundation, we introduce POLARIS, a three-layer multi-agentic self-adaptation framework that advances beyond reactive adaptation. POLARIS integrates: (1) a low-latency Adapter layer for monitoring and safe execution, (2) a transparent Reasoning layer that generates and verifies plans using tool-aware, explainable agents, and (3) a Meta layer that records experiences and meta-learns improved adaptation policies over time. Through shared knowledge and predictive models, POLARIS handles uncertainty, learns from past actions, and evolves its strategies, enabling systems that anticipate change and maintain resilient, goal-directed behavior. Preliminary evaluation on two self-adaptive exemplars, SWIM and SWITCH, shows that POLARIS consistently outperforms state-of-the-art baselines. We argue this marks a shift toward Self-Adaptation 3.0, akin to Software 3.0: a paradigm where systems not only learn from their environment but also reason about and evolve their own adaptation processes, continuously improving to meet novel challenges.

</details>


### [61] [Configuration Defects in Kubernetes](https://arxiv.org/abs/2512.05062)
*Yue Zhang,Uchswas Paul,Marcelo d'Amorim,Akond Rahman*

Main category: cs.SE

TL;DR: 该论文对Kubernetes配置缺陷进行了实证研究，通过分析2260个配置脚本识别出15类缺陷，评估了现有静态分析工具的检测能力，并开发了一个新的linter工具检测现有工具无法发现的严重缺陷。


<details>
  <summary>Details</summary>
Motivation: Kubernetes配置容易出错且可能造成严重后果，需要系统研究配置缺陷以帮助从业者检测和预防这些问题。

Method: 从开源仓库提取2260个Kubernetes配置脚本中的719个缺陷，通过定性分析识别缺陷类别，评估8个静态分析工具的检测能力，并开发新的linter工具。

Result: 识别出15类配置缺陷，现有工具只能检测其中8类。新开发的linter发现了26个先前未知的缺陷（19个已修复），在数据字段相关缺陷检测上表现出最高精确度和召回率。

Conclusion: 为Kubernetes配置脚本的缺陷检测和修复技术提供了具体建议，相关数据集和源代码已公开。

Abstract: Kubernetes is a tool that facilitates rapid deployment of software. Unfortunately, configuring Kubernetes is prone to errors. Configuration defects are not uncommon and can result in serious consequences. This paper reports an empirical study about configuration defects in Kubernetes with the goal of helping practitioners detect and prevent these defects. We study 719 defects that we extract from 2,260 Kubernetes configuration scripts using open source repositories. Using qualitative analysis, we identify 15 categories of defects. We find 8 publicly available static analysis tools to be capable of detecting 8 of the 15 defect categories. We find that the highest precision and recall of those tools are for defects related to data fields. We develop a linter to detect two categories of defects that cause serious consequences, which none of the studied tools are able to detect. Our linter revealed 26 previously-unknown defects that have been confirmed by practitioners, 19 of which have already been fixed. We conclude our paper by providing recommendations on how defect detection and repair techniques can be used for Kubernetes configuration scripts. The datasets and source code used for the paper are publicly available online.

</details>
