<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 4]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.CR](#cs.CR) [Total: 10]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Coverage Isn't Enough: SBFL-Driven Insights into Manually Created vs. Automatically Generated Tests](https://arxiv.org/abs/2512.11223)
*Sasara Shimizu,Yoshiki Higo*

Main category: cs.SE

TL;DR: 论文比较了自动生成测试与手动创建测试在SBFL分数和代码覆盖率上的差异，发现自动测试覆盖率更高但故障定位能力较弱


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注测试覆盖率，很少评估测试对故障定位的支持效果，特别是使用变异测试引入人工故障的情况

Method: 比较自动生成测试与手动创建测试的SBFL分数和代码覆盖率，使用SBFL分数作为评估指标

Result: 自动生成测试的分支覆盖率高于手动测试，但SBFL分数较低，特别是在深度嵌套结构的代码中

Conclusion: 研究结果为如何有效结合自动生成和手动创建测试方法提供了指导

Abstract: The testing phase is an essential part of software development, but manually creating test cases can be time-consuming. Consequently, there is a growing need for more efficient testing methods. To reduce the burden on developers, various automated test generation tools have been developed, and several studies have been conducted to evaluate the effectiveness of the tests they produce. However, most of these studies focus primarily on coverage metrics, and only a few examine how well the tests support fault localization-particularly using artificial faults introduced through mutation testing. In this study, we compare the SBFL (Spectrum-Based Fault Localization) score and code coverage of automatically generated tests with those of manually created tests. The SBFL score indicates how accurately faults can be localized using SBFL techniques. By employing SBFL score as an evaluation metric-an approach rarely used in prior studies on test generation-we aim to provide new insights into the respective strengths and weaknesses of manually created and automatically generated tests. Our experimental results show that automatically generated tests achieve higher branch coverage than manually created tests, but their SBFL score is lower, especially for code with deeply nested structures. These findings offer guidance on how to effectively combine automatically generated and manually created testing approaches.

</details>


### [2] [REMODEL-LLM: Transforming C code to Java using LLMs](https://arxiv.org/abs/2512.11402)
*Aryan Gupta,Y. Raghu Reddy*

Main category: cs.SE

TL;DR: 本研究检验了19个小参数量化LLM在C到Java代码翻译任务上的表现。采用AST语义分解和规则约束提示策略，结果显示模型性能分为三个层级，仅三个模型通过超过50%测试，但对复杂C概念仍存在明显局限。


<details>
  <summary>Details</summary>
Motivation: C到Java的自动翻译因编程范式、内存模型和数据类型差异而具有挑战性，研究旨在评估小参数量化LLM在此任务上的实际效能。

Method: 构建混合流水线，利用抽象语法树进行语义分解，并采用高度约束的基于规则的提示策略，对19个参数量小于200亿的量化LLM进行测试。

Result: 模型性能呈现三级分化：第三梯队模型完全失败；第二梯队生成可运行代码但存在语义错误；仅第一梯队三个模型通过超50%测试，但在处理函数指针等复杂概念时全部失败。

Conclusion: 当前小参数量化LLM在C到Java翻译任务上存在明显能力上限，仅少数模型表现可行，但对复杂C语言特性仍缺乏足够推理能力。

Abstract: The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.

</details>


### [3] [Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models](https://arxiv.org/abs/2512.11482)
*Melih Catal,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.

</details>


### [4] [A Study of Library Usage in Agent-Authored Pull Requests](https://arxiv.org/abs/2512.11589)
*Lukas Twist*

Main category: cs.SE

TL;DR: 该研究分析了26,760个AI编程智能体撰写的PRs，发现智能体在库使用上表现专业：经常导入库但很少添加新依赖，版本控制实践良好，且库选择多样化，为AI在软件开发中的实际应用提供了实证支持。


<details>
  <summary>Details</summary>
Motivation: 填补对编程智能体在实际软件开发中使用库行为的理解空白，现有研究对此了解甚少。

Method: 研究分析了来自AIDev数据集的26,760个智能体撰写的拉取请求，重点考察三个问题：智能体导入库的频率、引入新依赖的频率（及版本控制情况）以及具体选择的库类型。

Result: 智能体经常导入库（29.5%的PRs），但很少添加新依赖（1.3%的PRs）；当添加新依赖时，75.0%会指定版本，优于直接使用LLM时很少提及版本的情况；智能体使用的外部库种类非常多样化，与先前非智能体LLM研究中观察到的有限"库偏好"形成对比。

Conclusion: AI编程智能体在库使用方面表现出色，遵循良好的版本控制实践，并且能够利用多样化的外部库生态系统，这为AI在真实软件开发中的集成提供了积极的早期实证依据。

Abstract: Coding agents are becoming increasingly capable of completing end-to-end software engineering workflows that previously required a human developer, including raising pull requests (PRs) to propose their changes. However, we still know little about how these agents use libraries when generating code, a core part of real-world software development. To fill this gap, we study 26,760 agent-authored PRs from the AIDev dataset to examine three questions: how often do agents import libraries, how often do they introduce new dependencies (and with what versioning), and which specific libraries do they choose? We find that agents often import libraries (29.5% of PRs) but rarely add new dependencies (1.3% of PRs); and when they do, they follow strong versioning practices (75.0% specify a version), an improvement on direct LLM usage where versions are rarely mentioned. Generally, agents draw from a surprisingly diverse set of external libraries, contrasting with the limited "library preferences" seen in prior non-agentic LLM studies. Our results offer an early empirical view into how AI coding agents interact with today's software ecosystems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound](https://arxiv.org/abs/2512.11169)
*Akhil S Anand,Elias Aarekol,Martin Mziray Dalseg,Magnus Stalhane,Sebastien Gros*

Main category: cs.AI

TL;DR: 提出CORL框架，使用强化学习端到端微调MILP方案，以在真实世界数据上最大化操作性能，解决了传统方法难以准确建模随机问题的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统MILP方法难以准确建模随机真实世界问题，导致实际性能不佳。现有机器学习方法依赖监督学习，需要真实最优决策，并使用MILP梯度的替代方法。

Method: 将MILP建模为可微分的随机策略，使用强化学习在真实世界数据上进行端到端微调，以最大化操作性能。

Result: 在简单的组合序列决策示例中验证了CORL方法的有效性。

Conclusion: CORL框架通过将MILP建模为可微分的随机策略，成功地将强化学习应用于组合序列决策问题的端到端优化，为实际应用提供了新的解决方案。

Abstract: Combinatorial sequential decision making problems are typically modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&B) algorithms. The inherent difficulty of modeling MILPs that accurately represent stochastic real world problems leads to suboptimal performance in the real world. Recently, machine learning methods have been applied to build MILP models for decision quality rather than how accurately they model the real world problem. However, these approaches typically rely on supervised learning, assume access to true optimal decisions, and use surrogates for the MILP gradients. In this work, we introduce a proof of concept CORL framework that end to end fine tunes an MILP scheme using reinforcement learning (RL) on real world data to maximize its operational performance. We enable this by casting an MILP solved by B&B as a differentiable stochastic policy compatible with RL. We validate the CORL method in a simple illustrative combinatorial sequential decision making example.

</details>


### [6] [FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration](https://arxiv.org/abs/2512.11213)
*Dongwon Jung,Peng Shi,Yi Zhang*

Main category: cs.AI

TL;DR: FutureWeaver框架通过模块化协作和双层规划架构，在固定预算下优化多智能体系统中的测试时计算分配，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算扩展技术难以在多智能体系统中有效应用，缺乏在预算约束下促进智能体协作的计算分配机制。

Method: 提出模块化协作（可调用函数封装可重用工作流）和双层规划架构（基于当前状态推理同时推测未来步骤来优化计算分配）。

Result: 在复杂智能体基准测试中，FutureWeaver在不同预算设置下 consistently 优于基线方法。

Conclusion: FutureWeaver为多智能体协作中的推理时优化提供了有效的计算分配框架。

Abstract: Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.

</details>


### [7] [A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation](https://arxiv.org/abs/2512.11270)
*Hong Je-Gal,Chan-Bin Yi,Hyun-Suk Lee*

Main category: cs.AI

TL;DR: A-LAMP框架使用LLM自动将自然语言任务描述转化为MDP建模和策略训练，通过分阶段验证确保语义对齐，在多领域表现优于单一大型模型。


<details>
  <summary>Details</summary>
Motivation: 将RL应用于现实任务需要手动将非正式描述转化为MDP、实现可执行环境和训练策略，这一过程易受建模错误、代码脆弱和目标不对齐的影响。

Method: 提出基于LLM的A-LAMP框架，将建模、编码和训练分解为可验证阶段，自动完成从自然语言到MDP公式和训练策略的转化。

Result: 在经典控制任务和自定义RL领域中，A-LAMP的策略生成能力持续优于单一最先进LLM模型，其轻量版甚至接近更大模型的性能。

Conclusion: A-LAMP通过自动化流程和分阶段验证提高了RL应用的可访问性和可靠性，失败分析揭示了改进原因，案例研究证实了其正确性。

Abstract: Applying reinforcement learning (RL) to real-world tasks requires converting informal descriptions into a formal Markov decision process (MDP), implementing an executable environment, and training a policy agent. Automating this process is challenging due to modeling errors, fragile code, and misaligned objectives, which often impede policy training. We introduce an agentic large language model (LLM)-based framework for automated MDP modeling and policy generation (A-LAMP), that automatically translates free-form natural language task descriptions into an MDP formulation and trained policy. The framework decomposes modeling, coding, and training into verifiable stages, ensuring semantic alignment throughout the pipeline. Across both classic control and custom RL domains, A-LAMP consistently achieves higher policy generation capability than a single state-of-the-art LLM model. Notably, even its lightweight variant, which is built on smaller language models, approaches the performance of much larger models. Failure analysis reveals why these improvements occur. In addition, a case study also demonstrates that A-LAMP generates environments and policies that preserve the task's optimality, confirming its correctness and reliability.

</details>


### [8] [TriFlow: A Progressive Multi-Agent Framework for Intelligent Trip Planning](https://arxiv.org/abs/2512.11271)
*Yuxing Chen,Basem Suleiman,Qifan Chen*

Main category: cs.AI

TL;DR: TriFlow：一种渐进式多智能体框架，通过三阶段流程解决现实旅行规划中的约束满足问题，在基准测试中达到91.1%-97%的成功率，效率提升10倍


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在旅行规划中难以同时满足空间、时间和预算约束，且存在工具协调和效率问题

Method: 提出三阶段流程（检索-规划-治理），通过规则与LLM协作渐进缩小搜索空间，构建约束一致的行程，并进行有界迭代优化

Result: 在TravelPlanner和TripTailor基准测试中分别达到91.1%和97%的最终通过率，运行效率比现有SOTA提升10倍以上

Conclusion: TriFlow框架有效解决了复杂约束下的旅行规划问题，实现了约束满足与个性化需求的平衡

Abstract: Real-world trip planning requires transforming open-ended user requests into executable itineraries under strict spatial, temporal, and budgetary constraints while aligning with user preferences. Existing LLM-based agents struggle with constraint satisfaction, tool coordination, and efficiency, often producing infeasible or costly plans. To address these limitations, we present TriFlow, a progressive multi-agent framework that unifies structured reasoning and language-based flexibility through a three-stage pipeline of retrieval, planning, and governance. By this design, TriFlow progressively narrows the search space, assembles constraint-consistent itineraries via rule-LLM collaboration, and performs bounded iterative refinement to ensure global feasibility and personalisation. Evaluations on TravelPlanner and TripTailor benchmarks demonstrated state-of-the-art results, achieving 91.1% and 97% final pass rates, respectively, with over 10x runtime efficiency improvement compared to current SOTA.

</details>


### [9] [CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving](https://arxiv.org/abs/2512.11323)
*Jianyi Zhang,Ziyin Zhou,Xu Ji,Shizhao Liu,Zhangchi Zhao*

Main category: cs.AI

TL;DR: 研究者首次为LVLMs创建了一个名为CAPTURE的综合性CAPTCHA基准测试，涵盖4大类型25种子类型，发现现有模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有CAPTCHA基准测试局限于特定研究目标，无法全面覆盖所有验证码类型，缺乏专门针对LVLMs的专用基准。

Method: 引入CAPTURE基准，包含来自31个供应商的4种主要CAPTCHA类型和25种子类型，具有广泛类别多样性、大规模数据和专门针对LVLMs的标签。

Result: 当前LVLMs在解决CAPTCHAs方面表现较差，基准测试显示模型性能不足。

Conclusion: CAPTURE基准填补了现有研究在数据全面性和标签针对性方面的空白，为LVLMs提供了多维度的全面评估工具。

Abstract: Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.

</details>


### [10] [Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance](https://arxiv.org/abs/2512.11421)
*Gonca Gürsun*

Main category: cs.AI

TL;DR: 提出一个任务完成框架，使基于LLM的智能体能够在强化学习环境中按照明确的行为指导行动，提高可靠性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理和生成方面表现出色，但在多轮任务中的行为缺乏可靠性和可验证性。

Method: 框架包含三个组件：轻量级任务分析器（选择推理和生成策略）、推理模块（学习可验证的观察-行动映射）和生成模块（通过验证或确定性合成强制执行约束兼容输出）。

Result: 智能体与环境交互时，这些组件共同进化，产生可信赖的行为。

Conclusion: 该框架能够使LLM智能体在结构化环境中表现出更可靠和可验证的行为。

Abstract: Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.
  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.

</details>


### [11] [AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints](https://arxiv.org/abs/2512.11426)
*Shuowei Cai,Yansong Ning,Hao Liu*

Main category: cs.AI

TL;DR: AgentBalance框架：一种在明确令牌成本和延迟预算下构建成本效益多智能体系统的骨干优先拓扑设计方法


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统主要关注通信拓扑优化和智能体骨干选择，但缺乏在明确令牌成本和延迟预算约束下的建模和优化，导致预算约束时成本效益不佳

Method: 1) 骨干导向的智能体生成：通过LLM池构建、池选择和角色-骨干匹配构建异构骨干智能体；2) 自适应MAS拓扑生成：通过智能体表示学习、门控和延迟感知拓扑合成指导智能体间通信

Result: 在14个候选LLM骨干基准测试中，AgentBalance在匹配令牌成本和延迟预算下分别实现10%和22%的性能提升，在性能-预算曲线上表现优异

Conclusion: AgentBalance可作为现有MAS的插件，在相同约束下提升性能，并能很好地泛化到未见过的LLM，适用于实际的预算感知部署

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance

</details>


### [12] [Back to the Baseline: Examining Baseline Effects on Explainability Metrics](https://arxiv.org/abs/2512.11433)
*Agustin Martin Picard,Thibaut Boissin,Varshini Subhash,Rémi Cadène,Thomas Fel*

Main category: cs.AI

TL;DR: 该论文指出现有XAI基准线的不足，提出新的模型依赖基准线


<details>
  <summary>Details</summary>
Motivation: 现有归因方法评估中的基准线选择存在问题，不同基准线会偏好不同归因方法

Method: 通过分析基准线的两个理想特性（移除信息且不过度产生分布外图像），提出基于特征可视化的新基准线

Result: 证明现有基准线均无法同时满足两个标准，新基准线在权衡上优于现有方法

Conclusion: 基准线选择对归因方法评估有显著影响，需要更严谨的基准线设计标准

Abstract: Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline

</details>


### [13] [Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes](https://arxiv.org/abs/2512.11463)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Taehyun Kim,Eunhwan Park,Jeesoo Lee,Jeongdoo Lee,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Minsu Ha,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Minjae Kim,Taewhan Kim,Youngrok Kim,Hyukjin Kweon,Haesol Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Dongjoo Weon*

Main category: cs.AI

TL;DR: Motif-2-12.7B-Reasoning是一个12.7B参数的语言模型，旨在弥合开源模型与前沿专有模型在复杂推理和长上下文理解方面的差距。它通过系统、数据和算法优化提供了一种可复现的训练方法，在数学、编码和智能体基准测试中达到了与更大参数模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前开源权重系统与前沿专有模型在复杂推理和长上下文理解能力上存在显著差距，同时在推理适应过程中普遍面临模型崩溃和训练不稳定的挑战。

Method: 提出了一种综合性的训练方法，包括：1）使用混合并行和内核级优化的内存高效基础设施，支持64K标记的长上下文；2）两阶段监督微调课程，通过经过验证的对齐合成数据缓解分布不匹配；3）稳健的强化学习微调流程，通过难度感知数据过滤和混合策略轨迹重用来稳定训练。

Result: 实证结果表明，Motif-2-12.7B-Reasoning在数学、编码和智能体基准测试中实现了与参数数量显著更多的模型相当的性能。

Conclusion: 该研究为社区提供了一个具有竞争力的开源模型，并为在现实计算约束下扩展推理能力提供了一个实用的蓝图。

Abstract: We introduce Motif-2-12.7B-Reasoning, a 12.7B parameter language model designed to bridge the gap between open-weight systems and proprietary frontier models in complex reasoning and long-context understanding. Addressing the common challenges of model collapse and training instability in reasoning adaptation, we propose a comprehensive, reproducible training recipe spanning system, data, and algorithmic optimizations. Our approach combines memory-efficient infrastructure for 64K-token contexts using hybrid parallelism and kernel-level optimizations with a two-stage Supervised Fine-Tuning (SFT) curriculum that mitigates distribution mismatch through verified, aligned synthetic data. Furthermore, we detail a robust Reinforcement Learning Fine-Tuning (RLFT) pipeline that stabilizes training via difficulty-aware data filtering and mixed-policy trajectory reuse. Empirical results demonstrate that Motif-2-12.7B-Reasoning achieves performance comparable to models with significantly larger parameter counts across mathematics, coding, and agentic benchmarks, offering the community a competitive open model and a practical blueprint for scaling reasoning capabilities under realistic compute constraints.

</details>


### [14] [Three methods, one problem: Classical and AI approaches to no-three-in-line](https://arxiv.org/abs/2512.11469)
*Pranav Ramanathan,Thomas Prellberg,Matthew Lewis,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.AI

TL;DR: 本文首次系统比较了经典优化算法（整数线性规划ILP）与人工智能方法（PatternBoost变压器学习和PPO强化学习）在解决No-Three-In-Line组合几何问题上的性能。结果表明，ILP在19x19网格内可得到最优解，而AI方法在小规模问题上表现具有竞争力，混合方法有望扩展到更大规模问题。


<details>
  <summary>Details</summary>
Motivation: No-Three-In-Line问题是一个著名的组合几何问题，但经典优化方法如ILP在处理大规模网格时面临指数级复杂度挑战。近年来机器学习方法在模式识别和近似求解方面展现出潜力，需要进行系统比较来评估不同方法的适用性和局限性。

Method: 1. 使用整数线性规划（ILP）作为经典优化基准
2. 首次应用PatternBoost变压器学习方法
3. 首次应用PPO强化学习方法
4. 在10x10至19x19的不同规模网格上进行测试比较

Result: ILP在19x19网格内获得可证明的最优解；PatternBoost在14x14网格内达到最优性能，测试损失减少96%；PPO在10x10网格上获得完美解，但在11x11网格上因约束违规而失效。

Conclusion: 经典优化方法对于精确解仍然至关重要，而AI方法在小规模实例上具有竞争力。混合方法是将问题扩展到更大规模的最有前途的方向。

Abstract: The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.

</details>


### [15] [BAID: A Benchmark for Bias Assessment of AI Detectors](https://arxiv.org/abs/2512.11505)
*Priyam Basu,Yunfeng Zhang,Vipul Raheja*

Main category: cs.AI

TL;DR: 提出BAID框架，系统评估AI文本检测器的偏见，发现主流AI检测器对边缘群体文本识别准确率显著偏低


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测器在教育及专业场景中日益普及，但缺乏针对社会语言学因素的偏见系统性评估

Method: 构建包含7大类200k+样本的测试集，采用精心设计的提示词生成保持内容但体现亚群写作风格的合成文本，评估4个开源AI检测器

Result: AI检测器在所有偏见类别中表现出一致的性能差异，对代表性不足群体的文本召回率特别低

Conclusion: BAID为AI检测器审计提供可扩展、透明的方法，强调部署前进行偏见评估的必要性

Abstract: AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.

</details>


### [16] [AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives](https://arxiv.org/abs/2512.11544)
*Yuan Shen,Xiaojun Wu,Linghua Yu*

Main category: cs.AI

TL;DR: 这篇研究模拟真实临床场景，评估LLMs从嘈杂冗余的患者主诉中提取核心医学信息的能力，发现所有测试模型都表现出不同程度的功能缺陷，并首次提出"AI-代谢功能障碍相关性脂肪肝病"的创新概念。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs在处理临床信息时是否会表现出类似代谢功能障碍的功能下降，为AI在医疗领域的应用提供安全警示。

Method: 采用基于标准化医学探针的横断面分析设计，选择四种主流LLMs作为研究对象，使用包含五个核心维度的20个医学探针评估系统，采用双盲反向评分量表进行评估。

Result: 所有测试模型都表现出不同程度的功能缺陷，Qwen3-Max整体表现最佳，Gemini 2.5最差。极端噪音条件下大多数模型出现功能崩溃，GPT-4o在肺栓塞风险评估中出现严重误判。

Conclusion: 首次实证确认LLMs处理临床信息时表现出代谢功能障碍特征，强调当前LLMs必须在人类专家监督下作为辅助工具使用，其理论知识与实际临床应用仍存在显著差距。

Abstract: This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.

</details>


### [17] [AI Benchmark Democratization and Carpentry](https://arxiv.org/abs/2512.11588)
*Gregor von Laszewski,Wesley Brewer,Jeyan Thiyagalingam,Juri Papay,Armstrong Foundjem,Piotr Luszczek,Murali Emani,Shirley V. Moore,Vijay Janapa Reddi,Matthew D. Sinclair,Sebastian Lobentanzer,Sujata Goswami,Benjamin Hawks,Marco Colombo,Nhan Tran,Christine R. Kirkpatrick,Abdulkareem Alsudais,Gregg Barrett,Tianhao Li,Kirsten Morehouse,Shivaram Venkataraman,Rutwik Jain,Kartik Mathur,Victor Lu,Tejinder Singh,Khojasteh Z. Mirza,Kongtao Chen,Sasidhar Kunapuli,Gavin Farrell,Renato Umeton,Geoffrey C. Fox*

Main category: cs.AI

TL;DR: 论文提出AI基准测试需要从静态向动态自适应转变，以解决模型记忆基准、评估与现实差距等问题，并强调需要AI基准木工技能教育和社区努力。


<details>
  <summary>Details</summary>
Motivation: 当前AI基准测试过于静态，无法跟上模型架构、数据和部署环境的快速演变，导致基准结果与实际性能脱节。

Method: 通过MLCommons、教育项目和DOE万亿参数联盟等实践经验，识别资源需求高、硬件访问有限、设计专家缺乏等障碍，倡导动态基准框架。

Result: 动态基准测试应整合演化模型、更新数据和异构平台，保持透明性、可重复性和可解释性，支持应用相关比较。

Conclusion: 动态包容的基准测试将确保评估与AI发展同步，支持负责任、可重复和可访问的AI部署，社区努力可为AI基准木工提供基础。

Abstract: Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.
  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.
  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.

</details>


### [18] [Causal Inference in Energy Demand Prediction](https://arxiv.org/abs/2512.11653)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: 本文提出了一种基于结构因果模型的新型能源需求预测方法，结合贝叶斯建模获得优异性能。


<details>
  <summary>Details</summary>
Motivation: 能源需求预测受多种因果相关因素影响，单纯的相关性学习难以充分捕获复杂关系，需要因果建模方法。

Method: 构建结构因果模型揭示变量间因果关系，并基于因果洞察构建贝叶斯模型作为先验知识。

Result: 模型在测试集上达到3.84% MAPE，跨两年数据交叉验证平均MAPE为3.88%，显示出色鲁棒性。

Conclusion: 因果建模能有效提升能源需求预测性能，揭示了温度敏感度的季节性差异等有价值洞察。

Abstract: Energy demand prediction is critical for grid operators, industrial energy
  consumers, and service providers. Energy demand is influenced by multiple
  factors, including weather conditions (e.g. temperature, humidity, wind
  speed, solar radiation), and calendar information (e.g. hour of day and
  month of year), which further affect daily work and life schedules. These
  factors are causally interdependent, making the problem more complex than
  simple correlation-based learning techniques satisfactorily allow for. We
  propose a structural causal model that explains the causal relationship
  between these variables. A full analysis is performed to validate our causal
  beliefs, also revealing important insights consistent with prior studies.
  For example, our causal model reveals that energy demand responds to
  temperature fluctuations with season-dependent sensitivity. Additionally, we
  find that energy demand exhibits lower variance in winter due to the
  decoupling effect between temperature changes and daily activity patterns.
  We then build a Bayesian model, which takes advantage of the causal insights
  we learned as prior knowledge. The model is trained and tested on unseen
  data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on
  the test set. The model also demonstrates strong robustness, as the
  cross-validation across two years of data yields an average MAPE of 3.88 percent.

</details>


### [19] [MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition](https://arxiv.org/abs/2512.11682)
*Tim Cofala,Christian Kalfar,Jingge Xiao,Johanna Schrader,Michelle Tang,Wolfgang Nejdl*

Main category: cs.AI

TL;DR: TxAgent是一个用于临床治疗决策的AI系统，通过迭代式检索增强生成整合生物医学工具，在CURE-Bench挑战赛中因优异的工具检索策略获奖


<details>
  <summary>Details</summary>
Motivation: 临床治疗决策需要结合患者特征、疾病进程和药物信息的复杂推理，但现有AI系统在安全性和推理轨迹准确性方面存在不足

Method: 使用微调的Llama-3.1-8B模型，动态调用统一生物医学工具套件（FDA Drug API、OpenTargets、Monarch），采用检索增强生成方法

Result: 在CURE-Bench神经信息处理系统2025挑战赛中表现优异，证明了改进工具检索策略对提升模型性能的重要性，获得开放科学优秀奖

Conclusion: 医疗AI应用需要严格的安全性约束，工具调用和推理轨迹的准确性至关重要，迭代式RAG方法能有效提升治疗推理系统的性能

Abstract: Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [20] [Integrated Prediction and Multi-period Portfolio Optimization](https://arxiv.org/abs/2512.11273)
*Qi Deng,Yuxuan Linghu,Zhiyuan Liu*

Main category: cs.CE

TL;DR: 本文提出IPMO模型，将多周期收益预测与投资组合优化整合为端到端学习框架，通过可微凸优化层和MDFP微分方案解决传统两阶段方法中的预测与决策错位问题，在真实市场数据中展现出优越的风险调整后表现。


<details>
  <summary>Details</summary>
Motivation: 传统的多周期投资组合优化采用两阶段方法（先预测后优化），导致预测与决策结果不匹配，且忽略交易成本影响，需要端到端的集成学习方法来解决这些问题。

Method: 提出IPMO模型，包含多周期收益预测器和可微凸优化层，采用镜像下降固定点（MDFP）微分方案避免KKT系统分解，实现稳定的隐式梯度和近似规模不敏感的计算效率。

Result: 在真实市场数据和两种典型时间序列预测模型上的实验表明，IPMO在扣除交易成本后的风险调整表现持续优于两阶段基准方法，并产生更一致的资金分配路径。

Conclusion: 多周期设置下将机器学习预测与优化集成可改善财务结果，并保持计算可控性，IPMO为实际问题提供了有效的端到端解决方案。

Abstract: Multi-period portfolio optimization is important for real portfolio management, as it accounts for transaction costs, path-dependent risks, and the intertemporal structure of trading decisions that single-period models cannot capture. Classical methods usually follow a two-stage framework: machine learning algorithms are employed to produce forecasts that closely fit the realized returns, and the predicted values are then used in a downstream portfolio optimization problem to determine the asset weights. This separation leads to a fundamental misalignment between predictions and decision outcomes, while also ignoring the impact of transaction costs. To bridge this gap, recent studies have proposed the idea of end-to-end learning, integrating the two stages into a single pipeline. This paper introduces IPMO (Integrated Prediction and Multi-period Portfolio Optimization), a model for multi-period mean-variance portfolio optimization with turnover penalties. The predictor generates multi-period return forecasts that parameterize a differentiable convex optimization layer, which in turn drives learning via portfolio performance. For scalability, we introduce a mirror-descent fixed-point (MDFP) differentiation scheme that avoids factorizing the Karush-Kuhn-Tucker (KKT) systems, which thus yields stable implicit gradients and nearly scale-insensitive runtime as the decision horizon grows. In experiments with real market data and two representative time-series prediction models, the IPMO method consistently outperforms the two-stage benchmarks in risk-adjusted performance net of transaction costs and achieves more coherent allocation paths. Our results show that integrating machine learning prediction with optimization in the multi-period setting improves financial outcomes and remains computationally tractable.

</details>


### [21] [Task-Specific Sparse Feature Masks for Molecular Toxicity Prediction with Chemical Language Models](https://arxiv.org/abs/2512.11412)
*Kwun Sy Lee,Jiawei Chen,Fuk Sheng Ford Chung,Tianyu Zhao,Zhenyuan Chen,Debby D. Wang*

Main category: cs.CE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reliable in silico molecular toxicity prediction is a cornerstone of modern drug discovery, offering a scalable alternative to experimental screening. However, the black-box nature of state-of-the-art models remains a significant barrier to adoption, as high-stakes safety decisions demand verifiable structural insights alongside predictive performance. To address this, we propose a novel multi-task learning (MTL) framework designed to jointly enhance accuracy and interpretability. Our architecture integrates a shared chemical language model with task-specific attention modules. By imposing an L1 sparsity penalty on these modules, the framework is constrained to focus on a minimal set of salient molecular fragments for each distinct toxicity endpoint. The resulting framework is trained end-to-end and is readily adaptable to various transformer-based backbones. Evaluated on the ClinTox, SIDER, and Tox21 benchmark datasets, our approach consistently outperforms both single-task and standard MTL baselines. Crucially, the sparse attention weights provide chemically intuitive visualizations that reveal the specific fragments influencing predictions, thereby enhancing insight into the model's decision-making process.

</details>


### [22] [Contiguous Storage of Grid Data for Heterogeneous Computing](https://arxiv.org/abs/2512.11473)
*Fan Gu,Xiangyu Hu*

Main category: cs.CE

TL;DR: 针对GPU优化的稀疏结构化笛卡尔网格存储架构，采用SYCL统一编程模型，提升异构平台性能和编程便捷性。


<details>
  <summary>Details</summary>
Motivation: 传统结构化笛卡尔网格在稀疏域中内存效率低，现有CPU优化框架在GPU上存在并行性不足和内存延迟问题。

Method: 重新设计存储架构，抽象GPU底层细节，基于SYCL实现统一编程模型，支持主机-设备无缝集成。

Result: 提升了GPU编程的简易性，在稀疏网格和网格-粒子耦合数值模拟中实现了更好的可扩展性和可移植性。

Conclusion: 该架构有效解决了GPU上稀疏结构化网格的计算效率问题，为异构平台数值模拟提供了高效解决方案。

Abstract: Structured Cartesian grids are a fundamental component in numerical simulations. Although these grids facilitate straightforward discretization schemes, their naïve use in sparse domains leads to excessive memory overhead and inefficient computation. Existing frameworks address are primarily optimized for CPU execution and exhibit performance bottlenecks on GPU architectures due to limited parallelism and high memory access latency. This work presents a redesigned storage architecture optimized for GPU compatibility and efficient execution across heterogeneous platforms. By abstracting low-level GPU-specific details and adopting a unified programming model based on SYCL, the proposed data structure enables seamless integration across host and device environments. This architecture simplifies GPU programming for end-users while improving scalability and portability in sparse-grid and gird-particle coupling numerical simulations.

</details>


### [23] [Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation](https://arxiv.org/abs/2512.11748)
*Mohammed El Fallaki Idrissi,Jad Mounayer,Sebastian Rodriguez,Fodil Meraghni,Francisco Chinesta*

Main category: cs.CE

TL;DR: 本文提出了一种名为生成参数设计(GPD)的新框架，通过两个秩减少自动编码器和回归技术在潜在空间中连接设计与参数解，实现高效的设计探索和优化。


<details>
  <summary>Details</summary>
Motivation: 传统的仿真工程科学方法在设计优化和参数解决方案生成方面效率较低，需要一种能够同时生成新设计及其对应参数解决方案的框架。

Method: 使用两个秩减少自动编码器(RRAE)，一个用于编码和生成设计几何，另一个用于编码稀疏适当广义分解(sPGD)模式解，通过在潜在空间中使用回归技术连接这两个模型。

Result: 开发出的GPD框架能够有效生成新设计及其对应的参数解，并在两相微观结构上得到验证，能够处理两个关键材料参数的变化。

Conclusion: GPD框架为仿真工程科学提供了新范式，不仅促进设计探索和优化，还推动了数字和混合孪生技术的发展，增强了工程应用中的预测建模和实时决策能力。

Abstract: This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solutions. These models are linked in the latent space using regression techniques, allowing efficient transitions between design and their associated sPGD modes. By empowering design exploration and optimization, this framework also advances digital and hybrid twin development, enhancing predictive modeling and real-time decision-making in engineering applications. The developed framework is demonstrated on two-phase microstructures, in which the multiparametric solutions account for variations in two key material parameters.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [24] [SCOUT: A Defense Against Data Poisoning Attacks in Fine-Tuned Language Models](https://arxiv.org/abs/2512.10998)
*Mohamed Afane,Abhishek Satyam,Ke Chen,Tao Li,Junaid Farooq,Juntao Chen*

Main category: cs.CR

TL;DR: 该论文提出了三种新型上下文感知后门攻击场景，并开发了SCOUT防御框架来检测传统和复杂后门攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法无法有效对抗使用上下文适当触发器的复杂后门攻击，这些攻击能无缝融入自然语言中，对医疗等敏感领域的AI系统构成严重威胁。

Method: 提出了三种新型攻击场景（ViralApp、Fever、Referral），并开发了SCOUT防御框架，通过基于显著性的令牌级分析来识别后门触发器。SCOUT通过测量移除单个令牌对目标标签输出逻辑的影响来构建显著图。

Result: 在标准基准数据集（SST-2、IMDB、AG News）上评估SCOUT对抗传统攻击（BadNet、AddSent等）和新型攻击的效果，证明SCOUT能成功检测复杂威胁，同时保持对干净输入的高准确率。

Conclusion: SCOUT框架有效解决了当前后门攻击防御的局限性，能够检测使用上下文适当触发器的复杂攻击，为AI系统在敏感领域的部署提供了更强的安全保障。

Abstract: Backdoor attacks create significant security threats to language models by embedding hidden triggers that manipulate model behavior during inference, presenting critical risks for AI systems deployed in healthcare and other sensitive domains. While existing defenses effectively counter obvious threats such as out-of-context trigger words and safety alignment violations, they fail against sophisticated attacks using contextually-appropriate triggers that blend seamlessly into natural language. This paper introduces three novel contextually-aware attack scenarios that exploit domain-specific knowledge and semantic plausibility: the ViralApp attack targeting social media addiction classification, the Fever attack manipulating medical diagnosis toward hypertension, and the Referral attack steering clinical recommendations. These attacks represent realistic threats where malicious actors exploit domain-specific vocabulary while maintaining semantic coherence, demonstrating how adversaries can weaponize contextual appropriateness to evade conventional detection methods. To counter both traditional and these sophisticated attacks, we present \textbf{SCOUT (Saliency-based Classification Of Untrusted Tokens)}, a novel defense framework that identifies backdoor triggers through token-level saliency analysis rather than traditional context-based detection methods. SCOUT constructs a saliency map by measuring how the removal of individual tokens affects the model's output logits for the target label, enabling detection of both conspicuous and subtle manipulation attempts. We evaluate SCOUT on established benchmark datasets (SST-2, IMDB, AG News) against conventional attacks (BadNet, AddSent, SynBkd, StyleBkd) and our novel attacks, demonstrating that SCOUT successfully detects these sophisticated threats while preserving accuracy on clean inputs.

</details>


### [25] [An LLVM-Based Optimization Pipeline for SPDZ](https://arxiv.org/abs/2512.11112)
*Tianye Dai,Hammurabi Mendes,Heuichan Lim*

Main category: cs.CR

TL;DR: 提出了基于LLVM的SPDZ协议优化框架，通过自动批处理、非阻塞调度和GPU加速，显著提升安全多方计算的性能而不牺牲可用性


<details>
  <summary>Details</summary>
Motivation: 现有安全算术MPC框架存在编译栈依赖、需显式并行编程、通信开销高等限制性能和使用的问题

Method: 设计LLVM前端接受带隐私注释的C子集，后端进行数据流和控制流分析驱动非阻塞调度器，支持CPU/GPU并行计算

Result: CPU后端在中等和重度代数负载下实现最高5.56倍加速，GPU后端随输入规模扩大展现更好扩展性

Conclusion: 结合LLVM和协议感知调度是有效提取并行性同时保持可用性的架构方向

Abstract: Actively secure arithmetic MPC is now practical for real applications, but performance and usability are still limited by framework-specific compilation stacks, the need for programmers to explicitly express parallelism, and high communication overhead. We design and implement a proof-of-concept LLVM-based optimization pipeline for the SPDZ protocol that addresses these bottlenecks. Our front end accepts a subset of C with lightweight privacy annotations and lowers it to LLVM IR, allowing us to reuse mature analyses and transformations to automatically batch independent arithmetic operations. Our back end performs data-flow and control-flow analysis on the optimized IR to drive a non-blocking runtime scheduler that overlaps independent operations and aggressively overlaps communication with computation; when enabled, it can map batched operations to GPU kernels. This design preserves a low learning curve by using a mainstream language and hiding optimization and hardware-specific mechanics from programmers. We evaluate the system on controlled microbenchmarks against MP-SPDZ, focusing on online phase performance. Our CPU back end achieves up to 5.56 times speedup under intermediate and heavy algebraic workloads, shows strong scaling with thread count, and our GPU back end scales better as the input size increases. Overall, these results indicate that leveraging LLVM with protocol-aware scheduling is an effective architectural direction for extracting parallelism without sacrificing usability.

</details>


### [26] [Network and Compiler Optimizations for Efficient Linear Algebra Kernels in Private Transformer Inference](https://arxiv.org/abs/2512.11135)
*Karthik Garimella,Negar Neda,Austin Ebel,Nandan Kumar Jha,Brandon Reagen*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language model (LLM) based services are primarily structured as client-server interactions, with clients sending queries directly to cloud providers that host LLMs. This approach currently compromises data privacy as all queries must be processed in the cloud and in the clear. Fully Homomorphic Encryption (FHE) is a solution to this data privacy issue by enabling computations directly upon encrypted queries. However, running encrypted transformer inference is challenging as programmers must map standard kernels to the constrained instruction set provided by FHE. In this work, we explore implementations of linear algebra kernels needed for transformer inference in FHE and understand how network optimization can help mitigate FHE costs while remaining performant.
  We leverage the Orion PyTorch to FHE framework to benchmark several linear algebra kernels in order to profile two linear transformation methods, packed row and BSGS, and find that BSGS outperforms packed row methods by up to $13.7 \times$ at transformer-level scales. We also incorporate network-level pruning strategies that reduce FHE runtimes of feed forward layers by up to $11.46\times$. Furthermore, we extend Orion to include ciphertext-ciphertext matrix-matrix products, a key component in the self-attention blocks. Finally, we perform a roofline analysis of FHE primitives and encrypted linear transformations and find that (SIMD encoded) implementations are memory-bound with primitives having roughly $0.1$ integer operations per byte of DRAM traffic. These findings illustrate the need for exploring alternative encoding schemes and models of computation within CKKS to unlock scalable private transformer inference. We conduct all experiments using the Orion framework which can be found at: https://github.com/baahl-nyu/orion.

</details>


### [27] [A Scalable Multi-GPU Framework for Encrypted Large-Model Inference](https://arxiv.org/abs/2512.11269)
*Siddharth Jayashankar,Joshua Kim,Michael B. Sullivan,Wenting Zheng,Dimitrios Skarlatos*

Main category: cs.CR

TL;DR: Cerium是一个用于大型模型全同态加密推理的多GPU框架，通过自动化生成高性能GPU内核、管理TB级内存和跨GPU并行计算，首次在GPU上实现BERT-Base和Llama3-8B的加密推理。


<details>
  <summary>Details</summary>
Motivation: 全同态加密虽提供强隐私保障，但性能慢且现有ASIC方案成本高、可访问性差；GPU平台更易获取但难以达到ASIC级性能，且现有方法仅支持小模型，缺乏对大模型（如LLMs）加密推理的支持。

Method: 集成领域特定语言、优化编译器和运行时系统，引入新IR结构、编译器传递、稀疏多项式表示、内存高效数据布局和通信感知并行技术，自动生成高性能GPU内核并管理多GPU计算。

Result: 在NVIDIA GPU上实现显著性能提升：小模型比手工优化库快2.25倍；性能与先进FHE ASIC竞争，匹配CraterLake；首次在GPU上10毫秒内完成引导（7.5毫秒），首次实现BERT-Base（8秒）和Llama3-8B（134秒）加密推理。

Conclusion: Cerium突破了GPU加速FHE的性能限制，使大模型加密推理变得可行，为隐私保护AI提供了可访问的高效解决方案。

Abstract: Encrypted AI using fully homomorphic encryption (FHE) provides strong privacy guarantees; but its slow performance has limited practical deployment. Recent works proposed ASICs to accelerate FHE, but require expensive advanced manufacturing processes that constrain their accessibility. GPUs are a far more accessible platform, but achieving ASIC-level performance using GPUs has remained elusive. Furthermore, state-of-the-art approaches primarily focus on small models that fit comfortably within a single device. Supporting large models such as LLMs in FHE introduces a dramatic increase in computational complexity that requires optimized GPU kernels, along with managing terabyte-scale memory footprints that far exceed the capacity of a single GPU. This paper presents Cerium, a multi-GPU framework for FHE inference on large models. Cerium integrates a domain-specific language, an optimizing compiler, and a runtime system to automatically generate high-performance GPU kernels, manage terabyte-scale memory footprints, and parallelize computation across multiple GPUs. It introduces new IR constructs, compiler passes, sparse polynomial representations, memory-efficient data layouts, and communication-aware parallelization techniques that together enable encrypted inference for models ranging from small CNNs to Llama3-8B. We build Cerium on NVIDIA GPUs and demonstrate significant performance gains. For small models, Cerium outperforms expert-written hand-optimized GPU libraries by up to 2.25 times. Cerium achieves performance competitive with state-of-the-art FHE ASICs, outright matching prior FHE ASIC CraterLake. It is the first GPU system to execute bootstrapping in under 10 milliseconds, achieving 7.5 milliseconds, and is the first to demonstrate encrypted inference for BERT-Base and Llama3-8B in 8 seconds and 134 seconds, respectively.

</details>


### [28] [Visualisation for the CIS benchmark scanning results](https://arxiv.org/abs/2512.11316)
*Zhenshuo Zhao,Maria Spichkova,Duttkumari Champavat,Juilee N. Kulkarni,Sahil Singla,Muhammad A. Zulkefli,Pradhuman Khandelwal*

Main category: cs.CR

TL;DR: GraphSecure是一个用于分析AWS安全扫描结果的可视化WEB应用，支持CIS基准验证和账户状态预警。


<details>
  <summary>Details</summary>
Motivation: 当前云安全分析工具缺乏直观的可视化界面和CIS基准集成，用户难以快速评估AWS账户安全性。

Method: 开发WEB应用，集成AWS安全扫描、CIS基准验证、数据统计图表生成和预警功能。

Result: 实现了可执行扫描、基准验证、可视化展示和状态预警的一体化安全分析平台。

Conclusion: GraphSecure有效提升了云安全评估的效率和直观性，为AWS用户提供了实用的安全管理工具。

Abstract: In this paper, we introduce GraphSecure, a web application that provides advanced analysis and visualisation of security scanning results. GraphSecure enables users to initiate scans for their AWS account, validate them against specific Center for Internet Security (CIS) Benchmarks and return results, showcase those returned results in the form of statistical charts and warn the users about their account status.

</details>


### [29] [Proving DNSSEC Correctness: A Formal Approach to Secure Domain Name Resolution](https://arxiv.org/abs/2512.11431)
*Qifan Zhang,Zilin Shen,Imtiaz Karim,Elisa Bertino,Zhou Li*

Main category: cs.CR

TL;DR: 本文提出了DNSSECVerif——首个针对DNSSEC协议套件的自动化形式化安全分析框架，用于发现协议规范中的模糊性和漏洞。


<details>
  <summary>Details</summary>
Motivation: DNSSEC对防止DNS欺骗至关重要，但其规范存在模糊性和漏洞，传统'打破-修复'方法难以应对，急需系统性安全分析。

Method: 基于SAPIC+符号验证器构建高保真模型，捕捉协议级交互（包括加密操作和细粒度并发控制的状态缓存），实现自动化形式验证。

Result: 正式证明了DNSSEC的四个核心安全保证，发现标准中的关键模糊性（如NSEC与NSEC3的不安全共存），自动重新发现三类已知攻击。

Conclusion: 通过主流DNS软件针对性测试和220万开放解析器的大规模测量验证了发现的现实影响，为强化DNSSEC规范和实现提供了关键证据建议。

Abstract: The Domain Name System Security Extensions (DNSSEC) are critical for preventing DNS spoofing, yet its specifications contain ambiguities and vulnerabilities that elude traditional "break-and-fix" approaches. A holistic, foundational security analysis of the protocol has thus remained an open problem. This paper introduces DNSSECVerif, the first framework for comprehensive, automated formal security analysis of the DNSSEC protocol suite. Built on the SAPIC+ symbolic verifier, our high-fidelity model captures protocol-level interactions, including cryptographic operations and stateful caching with fine-grained concurrency control. Using DNSSECVerif, we formally prove four of DNSSEC's core security guarantees and uncover critical ambiguities in the standards--notably, the insecure coexistence of NSEC and NSEC3. Our model also automatically rediscovers three classes of known attacks, demonstrating fundamental weaknesses in the protocol design. To bridge the model-to-reality gap, we validate our findings through targeted testing of mainstream DNS software and a large-scale measurement study of over 2.2 million open resolvers, confirming the real-world impact of these flaws. Our work provides crucial, evidence-based recommendations for hardening DNSSEC specifications and implementations.

</details>


### [30] [Capacitive Touchscreens at Risk: Recovering Handwritten Trajectory on Smartphone via Electromagnetic Emanations](https://arxiv.org/abs/2512.11484)
*Yukun Cheng,Shiyu Zhu,Changhai Ou,Xingshuo Han,Yuan Li,Shihui Zheng*

Main category: cs.CR

TL;DR: TESLA攻击利用触摸屏电磁侧信道非接触式窃取手写轨迹，在商用手机上实现77%字符识别准确率


<details>
  <summary>Details</summary>
Motivation: 电容式触摸屏电磁侧信道泄露足够信息来恢复精细手写轨迹的安全漏洞

Method: 提出TESLA攻击框架，捕获屏幕书写时的电磁信号并实时回归为二维手写轨迹

Result: 在多种商用手机上评估显示，TESLA达到77%字符识别准确率和0.74杰卡德指数

Conclusion: 电磁侧信道攻击能有效恢复高度可识别的手写轨迹，暴露触摸屏安全风险

Abstract: This paper reveals and exploits a critical security vulnerability: the electromagnetic (EM) side channel of capacitive touchscreens leaks sufficient information to recover fine-grained, continuous handwriting trajectories. We present Touchscreen Electromagnetic Side-channel Leakage Attack (TESLA), a non-contact attack framework that captures EM signals generated during on-screen writing and regresses them into two-dimensional (2D) handwriting trajectories in real time. Extensive evaluations across a variety of commercial off-the-shelf (COTS) smartphones show that TESLA achieves 77% character recognition accuracy and a Jaccard index of 0.74, demonstrating its capability to recover highly recognizable motion trajectories that closely resemble the original handwriting under realistic attack conditions.

</details>


### [31] [Leveraging FPGAs for Homomorphic Matrix-Vector Multiplication in Oblivious Message Retrieval](https://arxiv.org/abs/2512.11690)
*Grant Bosworth,Keewoo Lee,Sunwoong Kim*

Main category: cs.CR

TL;DR: 这篇论文提出了一种硬件加速器架构，用于加速不经意消息检索中的同态矩阵-向量乘法操作，相比软件实现获得了13.86倍的加速比。


<details>
  <summary>Details</summary>
Motivation: 端到端加密虽然保护消息内容，但元数据容易通过流量分析暴露。现有解决方案使用发送者在公共公告板发布加密消息，接收者扫描相关消息的方式保护元数据隐私，但其中的同态矩阵-向量乘法计算开销限制了实用性。

Method: 采用高层次综合实现同态运算器，设计不同并行度的参数，通过设计空间探索策略部署到FPGA平台上，构建专门的硬件架构来加速同态矩阵-向量乘法。

Result: 相比软件实现，提出的硬件加速器实现了13.86倍的加速性能。

Conclusion: 该硬件加速架构有效解决了不经意消息检索中同态矩阵-向量乘法的计算瓶颈，显著提升了系统实用性。

Abstract: While end-to-end encryption protects the content of messages, it does not secure metadata, which exposes sender and receiver information through traffic analysis. A plausible approach to protecting this metadata is to have senders post encrypted messages on a public bulletin board and receivers scan it for relevant messages. Oblivious message retrieval (OMR) leverages homomorphic encryption (HE) to improve user experience in this solution by delegating the scan to a resource-rich server while preserving privacy. A key process in OMR is the homomorphic detection of pertinent messages for the receiver from the bulletin board. It relies on a specialized matrix-vector multiplication algorithm, which involves extensive multiplications between ciphertext vectors and plaintext matrices, as well as homomorphic rotations. The computationally intensive nature of this process limits the practicality of OMR. To address this challenge, this paper proposes a hardware architecture to accelerate the matrix-vector multiplication algorithm. The building homomorphic operators in this algorithm are implemented using high-level synthesis, with design parameters for different parallelism levels. These operators are then deployed on a field-programmable gate array platform using an efficient design space exploration strategy to accelerate homomorphic matrix-vector multiplication. Compared to a software implementation, the proposed hardware accelerator achieves a 13.86x speedup.

</details>


### [32] [SoK: Demystifying the multiverse of MPC protocols](https://arxiv.org/abs/2512.11699)
*Roberta De Viti,Vaastav Anand,Pierfrancesco Ingo,Deepak Garg*

Main category: cs.CR

TL;DR: 对多方计算(MPC)协议性能进行系统性分析，指出实际应用障碍并提供协议选择指导


<details>
  <summary>Details</summary>
Motivation: MPC协议虽然具有强隐私和正确性保证，但在实际应用中因高成本和缺乏具体工作负载的协议选择指导而受到限制

Method: 识别影响MPC效率的理论和实践参数，通过多种基准测试进行广泛实验研究

Result: 分析了不同协议之间的权衡，确定了最适合不同应用场景和需求的技术

Conclusion: 为开发者提供可操作的指导并为研究人员指出开放挑战，旨在缩小MPC理论与实践的差距

Abstract: This paper systematizes knowledge on the performance of Multi-Party Computation (MPC) protocols. Despite strong privacy and correctness guarantees, MPC adoption in real-world applications remains limited by high costs (especially in the malicious setting) and lack of guidance on choosing suitable protocols for concrete workloads. We identify the theoretical and practical parameters that shape MPC efficiency and conduct an extensive experimental study across diverse benchmarks. Our analysis discusses the trade-offs between protocols, and highlights which techniques align best with different application scenarios and needs. By providing actionable guidance for developers and outlining open challenges for researchers, this work seeks to narrow the gap between MPC theory and practice.

</details>


### [33] [Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously](https://arxiv.org/abs/2512.11783)
*Andrew Adiletta,Kathryn Adiletta,Kemal Derya,Berk Sunar*

Main category: cs.CR

TL;DR: 本文提出Super Suffix攻击方法可绕过Llama Prompt Guard 2防护，同时开发了基于模型内部状态分析的DeltaGuard检测技术，有效识别恶意提示，提升防护能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速部署，处理不可信文本输入和生成可执行代码的需求日益增长，但现有保护机制存在安全漏洞。研究发现Llama Prompt Guard 2等防护模型可能被联合优化技术绕过，需要更有效的检测方法。

Method: 通过分析模型内部状态在处理token序列过程中与特定概念方向的余弦相似度变化，提出了DeltaGuard检测方法。该方法基于残差流与概念方向相似度作为模型意图的特征指纹。

Result: Super Suffix攻击能够成功绕过Llama Prompt Guard 2对五个不同文本生成模型的保护机制。DeltaGuard检测方法将恶意提示的非良性分类率提升至近100%，显著增强了对抗性提示攻击的鲁棒性。

Conclusion: 本文提出了一种有效的对抗性攻击检测方法DeltaGuard，通过分析模型内部状态与特定概念方向的相似性变化，显著提高了对Super Suffix攻击的检测能力，将非良性分类率提升至近100%，为保护模型安全提供了有价值的解决方案。

Abstract: The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.
  Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.

</details>
