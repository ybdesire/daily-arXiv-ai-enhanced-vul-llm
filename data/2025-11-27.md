<div id=toc></div>

# Table of Contents

- [cs.CE](#cs.CE) [Total: 1]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.AI](#cs.AI) [Total: 18]


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [1] [Numerical Optimization of Nozzle Shapes for Fused Deposition Modeling](https://arxiv.org/abs/2511.21449)
*Steffen Tillmann,Felipe A. González,Stefanie Elgeti*

Main category: cs.CE

TL;DR: 本研究对比不同模拟模型下FDM喷嘴几何形状优化，发现最佳开口角随进料速率变化；粘性模型影响显著，粘弹性模型影响较弱；基于样条的参数化相比角度优化改进有限。


<details>
  <summary>Details</summary>
Motivation: 尽管喷嘴在FDM中至关重要，但大多数应用仍依赖标准设计。本研究旨在探讨喷嘴几何形状对内部压力损失的影响，以提升高速打印性能。

Method: 聚焦优化喷嘴形状以最小化压力损失，建立允许简单角度优化和高级样条参数化的框架。比较两种聚合物熔体流动模型：温度依赖剪切稀化粘性模型和等温粘弹性模型。

Result: 粘性模型中最佳半开口角依赖进料速率，高速率下偏好约30°，低速率下大角度更高效；粘弹性模型预测最佳角对进料速率依赖较弱。两种模型下基于样条参数化相比角度优化在压力损失减少方面改进有限。

Conclusion: 研究比较了不同模拟模型下FDM喷嘴形状优化，引入了灵活的优化框架，结果强调了模型选择对最优几何的影响，并为高速打印中喷嘴设计改进提供了支持。

Abstract: Purpose: In fused deposition modeling (FDM), the nozzle plays a critical role in enabling high printing speeds while maintaining precision. Despite its importance, most applications still rely on standard nozzle designs. This work investigates the influence of nozzle geometry on pressure loss inside the nozzle, a key factor in high-speed printing performance. Design/methodology/approach: We focus on optimizing the nozzle shape to minimize the pressure loss and establish a framework that allows both sim- ple angle-based optimization and more advanced spline-based parametrization. To model the polymer melt flow, we compare two constitutive descriptions commonly employed in the literature: a temperature-dependent, shear-thinning viscous model and an isothermal viscoelastic model. Findings: For the viscous model, the optimal half-opening angle exhibits a strong dependence on the feeding rate, with higher rates favoring half-opening angles near 30°, whereas lower rates are more efficient at larger angles. In con- trast, the viscoelastic model predicts a weaker dependence of the optimal angle on the feeding rate. For both models, spline-based parametrization yields only marginal improvements over angle optimization in terms of reducing pressure loss. Originality/value: This paper presents a comparative study of FDM nozzle shape optimization using different simulation models. We introduce a flexible optimization framework that accommodates both simple and advanced geomet- ric parametrizations. The results highlight the impact of model choice on the optimal nozzle geometry and provide support for improving nozzle design in high-speed printing applications.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [2] [Supporting Students in Navigating LLM-Generated Insecure Code](https://arxiv.org/abs/2511.20878)
*Jaehwan Park,Kyungchan Lim,Seonhye Park,Doowon Kim*

Main category: cs.CR

TL;DR: Bifröst是一个教育框架，通过在AI辅助开发环境中模拟不安全代码生成并分析漏洞来培养学生的安全意识。


<details>
  <summary>Details</summary>
Motivation: AI特别是大语言模型（LLMs）在软件开发中提升了效率，但也引入了安全隐患——LLMs可能生成不安全的代码。当前教育方法侧重效率而忽视安全问题，使学生缺乏识别和缓解AI辅助工作流程中安全问题的能力。

Method: Bifröst框架整合了三部分：(1) Visual Studio Code扩展模拟真实AI辅助开发环境，(2) 对抗性配置的LLMs以生成不安全代码，(3) 反馈系统高亮显示漏洞。通过让学生在受损LLMs环境下完成任务并提供定向安全分析，培养其批判性评估技能。

Result: 课堂部署（n=61）显示学生对不安全代码的漏洞性，而干预后调查（n=21）表明学生对LLM输出更加怀疑。

Conclusion: Bifröst有效提升了学生对AI生成代码的安全意识，强调了在AI辅助开发教育中融合安全培训的重要性。

Abstract: The advent of Artificial Intelligence (AI), particularly large language models (LLMs), has revolutionized software development by enabling developers to specify tasks in natural language and receive corresponding code, boosting productivity. However, this shift also introduces security risks, as LLMs may generate insecure code that can be exploited by adversaries. Current educational approaches emphasize efficiency while overlooking these risks, leaving students underprepared to identify and mitigate security issues in AI-assisted workflows.
  To address this gap, we present Bifröst, an educational framework that cultivates security awareness in AI-augmented development. Bifröst integrates (1) a Visual Studio Code extension simulating realistic environments, (2) adversarially configured LLMs that generate insecure code, and (3) a feedback system highlighting vulnerabilities. By immersing students in tasks with compromised LLMs and providing targeted security analysis, Bifröst cultivates critical evaluation skills; classroom deployments (n=61) show vulnerability to insecure code, while a post-intervention survey (n=21) indicates increased skepticism toward LLM outputs.

</details>


### [3] [A Taxonomy of Pix Fraud in Brazil: Attack Methodologies, AI-Driven Amplification, and Defensive Strategies](https://arxiv.org/abs/2511.20902)
*Glener Lanes Pizzolato,Brenda Medeiros Lopes,Claudio Schepke,Diego Kreutz*

Main category: cs.CR

TL;DR: 回顾针对巴西Pix即时支付系统的攻击方法，分类主要欺诈类型，分析其演变趋势。结合文献综述和银行专业人士访谈，发现诈骗手段从纯社会工程转向混合策略。


<details>
  <summary>Details</summary>
Motivation: 识别和分类影响用户及金融机构的主要欺诈类型，揭示攻击技术的演变和日益复杂化趋势。

Method: 结合结构化文献综述与对银行行业专业人士的探索性访谈。

Result: 欺诈方案已从纯粹的社会工程方法发展为整合人为操纵与技术利用的混合策略。

Conclusion: 安全措施必须与攻击方法日益增长的复杂性同步发展，特别强调适应性防御和持续的用户意识教育。

Abstract: This work presents a review of attack methodologies targeting Pix, the instant payment system launched by the Central Bank of Brazil in 2020. The study aims to identify and classify the main types of fraud affecting users and financial institutions, highlighting the evolution and increasing sophistication of these techniques. The methodology combines a structured literature review with exploratory interviews conducted with professionals from the banking sector. The results show that fraud schemes have evolved from purely social engineering approaches to hybrid strategies that integrate human manipulation with technical exploitation. The study concludes that security measures must advance at the same pace as the growing complexity of attack methodologies, with particular emphasis on adaptive defenses and continuous user awareness.

</details>


### [4] [Securing the Model Context Protocol (MCP): Risks, Controls, and Governance](https://arxiv.org/abs/2511.20920)
*Herman Errico,Jiquan Ngiam,Shanita Sojan*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Model Context Protocol (MCP) replaces static, developer-controlled API integrations with more dynamic, user-driven agent systems, which also introduces new security risks. As MCP adoption grows across community servers and major platforms, organizations encounter threats that existing AI governance frameworks (such as NIST AI RMF and ISO/IEC 42001) do not yet cover in detail. We focus on three types of adversaries that take advantage of MCP s flexibility: content-injection attackers that embed malicious instructions into otherwise legitimate data; supply-chain attackers who distribute compromised servers; and agents who become unintentional adversaries by over-stepping their role. Based on early incidents and proof-of-concept attacks, we describe how MCP can increase the attack surface through data-driven exfiltration, tool poisoning, and cross-system privilege escalation. In response, we propose a set of practical controls, including per-user authentication with scoped authorization, provenance tracking across agent workflows, containerized sandboxing with input/output checks, inline policy enforcement with DLP and anomaly detection, and centralized governance using private registries or gateway layers. The aim is to help organizations ensure that unvetted code does not run outside a sandbox, tools are not used beyond their intended scope, data exfiltration attempts are detectable, and actions can be audited end-to-end. We close by outlining open research questions around verifiable registries, formal methods for these dynamic systems, and privacy-preserving agent operations.

</details>


### [5] [Readout-Side Bypass for Residual Hybrid Quantum-Classical Models](https://arxiv.org/abs/2511.20922)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Hongyang He,Hailong Jiang*

Main category: cs.CR

TL;DR: 提出一种轻量级残差混合架构，将量子特征与原始输入在分类前拼接，绕过测量瓶颈，在中心化和联邦设置下均优于纯量子模型和现有混合模型。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习存在测量瓶颈问题 - 狭窄的量子-经典读出限制了性能并放大了隐私风险。

Method: 采用残差混合架构，在量子-经典接口处添加残差连接，将量子特征与原始输入拼接后进行分类，不增加量子复杂度。

Result: 实验显示模型在中心化和联邦设置下均优于纯量子模型和现有混合模型，相比量子基线精度提升高达+55%，同时保持低通信成本和增强的隐私鲁棒性。

Conclusion: 该方法为在隐私敏感、资源受限的环境（如联邦边缘学习）中集成量子模型提供了一个实用的近期实现路径。

Abstract: Quantum machine learning (QML) promises compact and expressive representations, but suffers from the measurement bottleneck - a narrow quantum-to-classical readout that limits performance and amplifies privacy risk. We propose a lightweight residual hybrid architecture that concatenates quantum features with raw inputs before classification, bypassing the bottleneck without increasing quantum complexity. Experiments show our model outperforms pure quantum and prior hybrid models in both centralized and federated settings. It achieves up to +55% accuracy improvement over quantum baselines, while retaining low communication cost and enhanced privacy robustness. Ablation studies confirm the effectiveness of the residual connection at the quantum-classical interface. Our method offers a practical, near-term pathway for integrating quantum models into privacy-sensitive, resource-constrained settings like federated edge learning.

</details>


### [6] [Road Network-Aware Personalized Trajectory Protection with Differential Privacy under Spatiotemporal Correlations](https://arxiv.org/abs/2511.21020)
*Minghui Min,Jiahui Liu,Mingge Cao,Shiyin Li,Hongliang Zhang,Miao Pan,Zhu Han*

Main category: cs.CR

TL;DR: 本文提出了一种个性化轨迹隐私保护机制(PTPPM)，通过结合地理不可区分性和失真隐私，让用户可配置隐私预算和推断误差界限，在保护位置隐私的同时维持服务质量。


<details>
  <summary>Details</summary>
Motivation: 基于位置的服务(LBS)为用户带来便利但存在隐私风险，攻击者可通过时空相关性推断敏感信息。由于用户对不同位置数据的敏感度存在差异，需要个性化的隐私保护方案。

Method: 1) 建模攻击者对用户轨迹时空相关性的知识 2) 结合地理不可区分性和失真隐私构建保护位置集 3) 提出个性化隐私预算分配算法 4) 使用Permute-and-Flip机制生成扰动位置

Result: 仿真结果表明该机制在隐私保护和服务质量方面均优于现有基准方法。

Conclusion: PTPPM机制能有效平衡隐私保护与服务质量，为用户提供个性化的轨迹隐私保护解决方案。

Abstract: Location-Based Services (LBSs) offer significant convenience to mobile users but pose significant privacy risks, as attackers can infer sensitive personal information through spatiotemporal correlations in user trajectories. Since users' sensitivity to location data varies based on factors such as stay duration, access frequency, and semantic sensitivity, implementing personalized privacy protection is imperative. This paper proposes a Personalized Trajectory Privacy Protection Mechanism (PTPPM) to address these challenges. Our approach begins by modeling an attacker's knowledge of a user's trajectory spatiotemporal correlations, which enables the attacker to identify possible location sets and disregard low-probability location sets. To combat this, we integrate geo-indistinguishability with distortion privacy, allowing users to customize their privacy preferences through a configurable privacy budget and expected inference error bound. This approach provides the theoretical framework for constructing a Protection Location Set (PLS) that obscures users' actual locations. Additionally, we introduce a Personalized Privacy Budget Allocation Algorithm (PPBA), which assesses the sensitivity of locations based on trajectory data and allocates privacy budgets accordingly. This algorithm considers factors such as location semantics and road network constraints. Furthermore, we propose a Permute-and-Flip mechanism that generates perturbed locations while minimizing perturbation distance, thus balancing privacy protection and Quality of Service (QoS). Simulation results demonstrate that our mechanism outperforms existing benchmarks, offering superior privacy protection while maintaining user QoS requirements.

</details>


### [7] [CAHS-Attack: CLIP-Aware Heuristic Search Attack Method for Stable Diffusion](https://arxiv.org/abs/2511.21180)
*Shuhan Xia,Jing Dai,Hui Ouyang,Yadong Shang,Dongxiao Zhao,Peipei Li*

Main category: cs.CR

TL;DR: 本文提出了CAHS-Attack方法，基于CLIP文本编码器的脆弱性，利用蒙特卡洛树搜索和遗传算法优化对抗性提示，显著提升了扩散模型的攻击效果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型对对抗性提示脆弱，但现有方法依赖于白盒梯度或手工工程，在实际部署中不可行。因此，需要一种高效的黑盒攻击方法揭示模型漏洞。

Method: CAHS-Attack结合蒙特卡洛树搜索（MCTS）进行细粒度后缀优化，使用约束遗传算法预选高潜力对抗提示作为根节点，并在每次模拟中保留语义破坏性最强的结果以进行高效局部搜索。

Result: 大量实验表明，该方法在不同语义的短提示和长提示上均达到了最先进的攻击性能。

Conclusion: SD模型的脆弱性源于其基于CLIP的文本编码器的内在漏洞，这表明当前文本到图像生成流程存在根本性安全风险。

Abstract: Diffusion models exhibit notable fragility when faced with adversarial prompts, and strengthening attack capabilities is crucial for uncovering such vulnerabilities and building more robust generative systems. Existing works often rely on white-box access to model gradients or hand-crafted prompt engineering, which is infeasible in real-world deployments due to restricted access or poor attack effect. In this paper, we propose CAHS-Attack , a CLIP-Aware Heuristic Search attack method. CAHS-Attack integrates Monte Carlo Tree Search (MCTS) to perform fine-grained suffix optimization, leveraging a constrained genetic algorithm to preselect high-potential adversarial prompts as root nodes, and retaining the most semantically disruptive outcome at each simulation rollout for efficient local search. Extensive experiments demonstrate that our method achieves state-of-the-art attack performance across both short and long prompts of varying semantics. Furthermore, we find that the fragility of SD models can be attributed to the inherent vulnerability of their CLIP-based text encoders, suggesting a fundamental security risk in current text-to-image pipelines.

</details>


### [8] [AuthenLoRA: Entangling Stylization with Imperceptible Watermarks for Copyright-Secure LoRA Adapters](https://arxiv.org/abs/2511.21216)
*Fangming Shi,Li Li,Kejiang Chen,Guorui Feng,Xinpeng Zhang*

Main category: cs.CR

TL;DR: AuthenLoRA：一种在LoRA训练过程中嵌入不可察觉水印的统一框架，可确保使用带水印LoRA生成的图像可靠携带水印，同时保持风格化质量


<details>
  <summary>Details</summary>
Motivation: 现有水印技术要么针对基础模型，要么验证LoRA模块本身，但无法将水印传播到生成的图像中；基础模型的追溯水印与风格化不紧密耦合，常导致视觉退化或高误检率

Method: 采用双目标优化策略联合学习目标风格分布和水印引起的分布偏移；设计扩展LoRA架构增强多尺度适应；引入零消息正则化机制大幅降低水印验证误检率

Result: AuthenLoRA实现高保真风格化、鲁棒水印传播，相比现有方法显著降低误检率

Conclusion: AuthenLoRA解决了LoRA模块水印传播的关键空白，为扩散模型定制提供有效追溯机制

Abstract: Low-Rank Adaptation (LoRA) offers an efficient paradigm for customizing diffusion models, but its ease of redistribution raises concerns over unauthorized use and the generation of untraceable content. Existing watermarking techniques either target base models or verify LoRA modules themselves, yet they fail to propagate watermarks to generated images, leaving a critical gap in traceability. Moreover, traceability watermarking designed for base models is not tightly coupled with stylization and often introduces visual degradation or high false-positive detection rates. To address these limitations, we propose AuthenLoRA, a unified watermarking framework that embeds imperceptible, traceable watermarks directly into the LoRA training process while preserving stylization quality. AuthenLoRA employs a dual-objective optimization strategy that jointly learns the target style distribution and the watermark-induced distribution shift, ensuring that any image generated with the watermarked LoRA reliably carries the watermark. We further design an expanded LoRA architecture for enhanced multi-scale adaptation and introduce a zero-message regularization mechanism that substantially reduces false positives during watermark verification. Extensive experiments demonstrate that AuthenLoRA achieves high-fidelity stylization, robust watermark propagation, and significantly lower false-positive rates compared with existing approaches. Open-source implementation is available at: https://github.com/ShiFangming0823/AuthenLoRA

</details>


### [9] [Illuminating the Black Box: Real-Time Monitoring of Backdoor Unlearning in CNNs via Explainable AI](https://arxiv.org/abs/2511.21291)
*Tien Dat Hoang*

Main category: cs.CR

TL;DR: 提出一种结合Grad-CAM可视化技术的后门遗忘框架，通过定量指标TAR实时监控注意力转移，实现高透明度的后门清除。


<details>
  <summary>Details</summary>
Motivation: 现有后门遗忘方法缺乏透明度和实时可解释性，难以观察和验证模型修复过程。

Method: 设计均衡遗忘策略：1）在后门样本上使用梯度上升 2）EWC防止灾难性遗忘 3）恢复阶段保持干净精度 4）集成Grad-CAM实时可视化

Result: 在CIFAR-10数据集上，攻击成功率从96.51%降至5.52%，同时保持99.48%的干净精度恢复率（82.06%基准），ASR降低94.28%

Conclusion: 可解释AI技术的集成实现了透明、可观测、可验证的后门清除，为深度学习安全提供新范式

Abstract: Backdoor attacks pose severe security threats to deep neural networks by embedding malicious triggers that force misclassification. While machine unlearning techniques can remove backdoor behaviors, current methods lack transparency and real-time interpretability. This paper introduces a novel framework that integrates Gradient-weighted Class Activation Mapping (Grad-CAM) into the unlearning process to provide real-time monitoring and explainability. We propose the Trigger Attention Ratio (TAR) metric to quantitatively measure the model's attention shift from trigger patterns to legitimate object features. Our balanced unlearning strategy combines gradient ascent on backdoor samples, Elastic Weight Consolidation (EWC) for catastrophic forgetting prevention, and a recovery phase for clean accuracy restoration. Experiments on CIFAR-10 with BadNets attacks demonstrate that our approach reduces Attack Success Rate (ASR) from 96.51% to 5.52% while retaining 99.48% of clean accuracy (82.06%), achieving a 94.28% ASR reduction. The integration of explainable AI enables transparent, observable, and verifiable backdoor removal.

</details>


### [10] [Empirical Assessment of the Code Comprehension Effort Needed to Attack Programs Protected with Obfuscation](https://arxiv.org/abs/2511.21301)
*Leonardo Regano,Daniele Canavese,Cataldo Basile,Marco Torchiano*

Main category: cs.CR

TL;DR: 这是一篇关于软件混淆技术有效性的实证研究论文，首次通过控制实验评估多层混淆技术对代码理解的影响。


<details>
  <summary>Details</summary>
Motivation: 软件保护技术（特别是混淆技术）虽然广泛使用，但其有效性缺乏系统评估。研究者希望填补这一空白，为选择最有效的软件保护方法提供实证依据。

Method: 采用控制实验方法，让硕士生在混淆后的应用程序上执行代码理解任务，评估混淆技术在延迟代码理解方面的效果，并检验复杂度指标是否能准确预测保护效果。

Result: 研究发现混淆技术确实能有效延迟攻击者的代码理解过程，同时证实了客观代码指标与攻击成功率之间的相关性。

Conclusion: 该研究为软件保护效果评估提供了新的实验方法，揭示了混淆技术的实际效果，并为后续研究开辟了新方向。该研究首次评估了在单一代码上叠加多层混淆技术的效果。

Abstract: Evaluating the effectiveness of software protection is crucial for selecting the most effective methods to safeguard assets within software applications. Obfuscation involves techniques that deliberately modify software to make it more challenging to understand and reverse-engineer, while maintaining its original functionality. Although obfuscation is widely adopted, its effectiveness remains largely unexplored and unthoroughly evaluated. This paper presents a controlled experiment involving Master's students performing code comprehension tasks on applications hardened with obfuscation. The experiment's goals are to assess the effectiveness of obfuscation in delaying code comprehension by attackers and to determine whether complexity metrics can accurately predict the impact of these protections on success rates and durations of code comprehension tasks. The study is the first to evaluate the effect of layering multiple obfuscation techniques on a single piece of protected code. It also provides experimental evidence of the correlation between objective metrics of the attacked code and the likelihood of a successful attack, bridging the gap between objective and subjective approaches to estimating potency. Finally, the paper highlights significant aspects that warrant additional analysis and opens new avenues for further experiments.

</details>


### [11] [MAD-DAG: Protecting Blockchain Consensus from MEV](https://arxiv.org/abs/2511.21552)
*Roi Bar-Zur,Aviv Tamar,Ittay Eyal*

Main category: cs.CR

TL;DR: 介绍了MAD-DAG协议，这是首个在不利条件下（如MEV、抢先交易、小贿赂矿工）实用的自私挖矿防御协议，相比现有协议（Colordag和比特币）具有更高的安全性阈值。


<details>
  <summary>Details</summary>
Motivation: 现有区块链协议（如Colordag和比特币）在不利条件下无法有效抵御自私挖矿攻击，特别是当存在抢先传输优势、MEV导致区块奖励变异以及小贿赂矿工时，安全性阈值降至0%。

Method: 提出MAD-DAG协议，采用新型账本函数，通过丢弃竞争最长链的等长链内容来抵御自私挖矿。使用马尔可夫决策过程（MDP）对理性矿工建模，并开发保守奖励规则以获得自私挖矿收益的上界。

Result: MAD-DAG在不利条件下安全性阈值介于11%至31%，而Colordag和比特币在所有条件下均为0%。协议在保持可比安全性的同时，成功抵御了Colordag和比特币失效的场景。

Conclusion: MAD-DAG是首个在现实不利条件下可实用的自私挖矿防御协议，显著提升了区块链安全性，为DAG-based区块链提供了首个可处理的自私挖矿模型。

Abstract: Blockchain security is threatened by selfish mining, where a miner (operator) deviates from the protocol to increase their revenue. Selfish mining is exacerbated by adverse conditions: rushing (network propagation advantage for the selfish miner), varying block rewards due to block contents, called miner extractable value (MEV), and petty-compliant miners who accept bribes from the selfish miner.
  The state-of-the-art selfish-mining-resistant blockchain protocol, Colordag, does not treat these adverse conditions and was proven secure only when its latency is impractically high.
  We present MAD-DAG, Mutually-Assured-Destruction Directed-Acyclic-Graph, the first practical protocol to counter selfish mining under adverse conditions. MAD-DAG achieves this thanks to its novel ledger function, which discards the contents of equal-length chains competing to be the longest.
  We analyze selfish mining in both Colordag and MAD-DAG by modeling a rational miner using a Markov Decision Process (MDP). We obtain a tractable model for both by developing conservative reward rules that favor the selfish miner to yield an upper bound on selfish mining revenue. To the best of our knowledge, this is the first tractable model of selfish mining in a practical DAG-based blockchain. This enables us to obtain a lower bound on the security threshold, the minimum fraction of computational power a miner needs in order to profit from selfish mining.
  MAD-DAG withstands adverse conditions under which Colordag and Bitcoin fail, while otherwise maintaining comparable security. For example, with petty-compliant miners and high levels of block reward variability, MAD-DAG's security threshold ranges from 11% to 31%, whereas both Colordag and Bitcoin achieve 0% for all levels.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation](https://arxiv.org/abs/2511.20709)
*Abhijeet Pathak,Suvadra Barua,Dinesh Gudimetla,Rupam Patir,Jiawei Guo,Hongxin Hu,Haipeng Cai*

Main category: cs.SE

TL;DR: DUALGAUGE是首个自动化框架，用于同时评估LLM生成代码的安全性和正确性。它包含一个基准测试套件和一个评估系统，揭示了当前LLM在安全代码生成方面的严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试要么只评估安全性而忽视功能性，要么在分离的数据集上评估两者，无法满足同时评估安全性和正确性的需求。

Method: 开发了DUALGAUGE-BENCH基准套件（包含多样化编程任务及手动验证的测试套件）和DUALGAUGE框架（包含沙盒程序执行器和基于LLM的评估器）。

Result: 对十个主流LLM的评估显示，它们在生成正确且安全的代码方面存在重大缺陷。

Conclusion: DUALGAUGE为可重复、可扩展的严格评估提供了开源系统和数据集，有助于加速安全代码生成领域的进展。

Abstract: Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental need for simultaneous joint evaluation. We present DUALGAUGE, the first fully automated benchmarking framework designed to rigorously evaluate the security and correctness of LLM-generated code in unison. Given the lack of datasets enabling joint evaluation of secure code generation, we also present DUALGAUGE-BENCH, a curated benchmark suite of diverse coding tasks, each paired with manually validated test suites for both security and functionality, designed for full coverage of specification requirements. At the core of DUALGAUGE is an agentic program executor, which runs a program against given tests in sandboxed environments, and an LLM-based evaluator, which assesses both correctness and vulnerability behavior against expected outcomes. We rigorously evaluated and ensured the quality of DUALGAUGE-BENCH and the accuracy of DUALGAUGE, and applied DUALGAUGE to benchmarking ten leading LLMs on DUALGAUGE-BENCH across thousands of test scenarios. Our results reveal critical gaps in correct and secure code generation by these LLMs, for which our open-source system and datasets help accelerate progress via reproducible, scalable, and rigorous evaluation.

</details>


### [13] [Train While You Fight -- Technical Requirements for Advanced Distributed Learning Platforms](https://arxiv.org/abs/2511.20813)
*Simon Hacks*

Main category: cs.SE

TL;DR: TWYF advocates continuous learning during operations. The paper examines technical requirements for ADL platforms to support TWYF using design science research.


<details>
  <summary>Details</summary>
Motivation: Address the gap in supporting continuous learning during military operations through advanced distributed learning platforms.

Method: Design Science Research approach involving challenge derivation from PfPC/NATO docs, solution objective definition, and systematic pattern mapping.

Result: Identified seven technical challenges: interoperability, resilience, multilingual support, data security/privacy, scalability, platform independence, and modularity.

Conclusion: Proposes pattern-based solutions validated through a German armed forces use case, demonstrating practical applicability of TWYF framework.

Abstract: "Train While You Fight" (TWYF) advocates for continuous learning that occurs during operations, not just before or after. This paper examines the technical requirements that advanced distributed learning (ADL) platforms must meet to support TWYF, and how existing software engineering patterns can fulfill these requirements. Using a Design Science Research approach, we (i) derive challenges from PfPC/NATO documentation and recent practice, (ii) define solution objectives, and (iii) conduct a systematic mapping from challenges to proven patterns. We identify seven technical challenges: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. We illustrate the patterns with a national use case from the German armed forces.

</details>


### [14] [Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code](https://arxiv.org/abs/2511.20933)
*Mootez Saad,Boqi Chen,José Antonio Hernández López,Dániel Varró,Tushar Sharma*

Main category: cs.SE

TL;DR: 评估DeepSeek-R1模型系列对软件设计概念（内聚性和耦合性）的理解。在理想条件下模型表现良好，但在实际嘈杂环境中表现脆弱且不对称。耦合性推理容易崩溃，而内聚性分析在指导任务中相对稳健。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在软件工程领域应用日益广泛，但其对核心软件设计概念的鲁棒性理解尚不明确，需要系统性评估。

Method: 通过程序生成设计不良的代码片段，测试DeepSeek-R1模型家族（14B、32B、70B）在不同指导级别（验证、指导、开放生成）和不同上下文噪声条件下的表现。

Result: 模型对耦合性推理脆弱，在嘈杂开放场景中F1分数下降超过50%；而对内聚性分析在指导任务中鲁棒，性能下降小。推理轨迹分析显示耦合性存在认知捷径，内聚性分析更详尽但仍失败。

Conclusion: LLMs可可靠协助识别设计缺陷，但在嘈杂现实环境中自主推理能力有限，凸显需要更可扩展和鲁棒的程序理解能力。

Abstract: Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \textit{Verification} to \textit{Guided} and \textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.

</details>


### [15] [SpaceX: Exploring metrics with the SPACE model for developer productivity](https://arxiv.org/abs/2511.20955)
*Sanchit Kaul,Kevin Nhu,Jason Eissayou,Ivan Eser,Victor Borup*

Main category: cs.SE

TL;DR: 本研究通过SPACE框架和开源仓库数据分析，开发了综合生产力评分(CPS)来克服传统单维生产力指标的局限性


<details>
  <summary>Details</summary>
Motivation: 传统确定性单维生产力启发式方法存在局限，需要更全面的多维度生产力评估框架

Method: 运用广义线性混合模型(GLM)和RoBERTa情感分类对开源仓库数据进行统计分析

Result: 发现负面情感状态与提交频率呈显著正相关；基于贡献者交互拓扑的分析比传统基于数量的指标更能准确映射协作动态

Conclusion: 提出复合生产力评分(CPS)来解决开发者效能异质性问题，为软件工程生产力评估提供新的多维视角

Abstract: This empirical investigation elucidates the limitations of deterministic, unidimensional productivity heuristics by operationalizing the SPACE framework through extensive repository mining. Utilizing a dataset derived from open-source repositories, the study employs rigorous statistical methodologies including Generalized Linear Mixed Models (GLMM) and RoBERTa-based sentiment classification to synthesize a holistic, multi-faceted productivity metric. Analytical results reveal a statistically significant positive correlation between negative affective states and commit frequency, implying a cycle of iterative remediation driven by frustration. Furthermore, the investigation has demonstrated that analyzing the topology of contributor interactions yields superior fidelity in mapping collaborative dynamics compared to traditional volume-based metrics. Ultimately, this research posits a Composite Productivity Score (CPS) to address the heterogeneity of developer efficacy.

</details>


### [16] [Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations](https://arxiv.org/abs/2511.21022)
*Guancheng Lin,Xiao Yu,Jacky Keung,Xing Hu,Xin Xia,Alex X. Liu*

Main category: cs.SE

TL;DR: 该研究首次系统性地应用10种先进的模型编辑技术来更新LLMs中的过时API知识，提出了AdaLoRA-L方法来解决特异性问题


<details>
  <summary>Details</summary>
Motivation: LLMs在代码补全任务中表现出色，但其嵌入知识受限于训练数据的时效性，常常生成已弃用的API。重新训练成本高昂，而现有的轻量级模型编辑方法是否能有效更新过时API知识尚不明确

Method: 在3个LLMs上应用10种最先进的模型编辑技术，引入EDAPIBench基准（包含8个流行Python库的70多个过时API，3000多个编辑实例），提出AdaLoRA-L方法定义'通用API层'和'特定API层'

Result: AdaLoRA在生成正确、最新的API方面表现最佳但在特异性方面不足；AdaLoRA-L显著提高了特异性，同时在其他评估指标上保持可比性能

Conclusion: 提出的AdaLoRA-L方法通过在特定API层进行编辑，有效解决了模型编辑的特异性问题，为LLMs的知识更新提供了有效的解决方案

Abstract: Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines "Common API Layers" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to "Specific API Layers" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.

</details>


### [17] [Exploring Hidden Geographic Disparities in Android Apps](https://arxiv.org/abs/2511.21151)
*M. Alecci,P. Jiménez,J. Samhi,T. Bissyandé,J. Klein*

Main category: cs.SE

TL;DR: 本文通过大规模研究揭示了Android应用在地区间的差异化现象，包括功能相似但命名不同的GeoTwins应用和号称一致但实际存在区分的App Bundle生态系统，指出了这对安全和公平性的影响。


<details>
  <summary>Details</summary>
Motivation: 移动应用的演化已被广泛研究，但应用行为的地理差异却鲜有人探索。本文旨在填补这一空白，特别是关注地理位置对Android应用行为和结构的影响。

Method: 构建了一个跨多个区域的分布式应用收集管道，分析数千个应用，并发布了包含81,963个GeoTwins的数据集来支持研究。

Result: 揭示了GeoTwins应用在权限、第三方库和隐私披露上的分歧，以及App Bundle生态系统中base.apk文件的区域差异，这些差异可能导致安全评估的不一致和地理偏见。

Conclusion: 发现系统性区域差异影响了移动软件的透明度、安全性和公平性，呼吁研究者、开发者、平台设计师和政策制定者关注这些隐藏的地理定制问题。

Abstract: While mobile app evolution has been widely studied, geographical variation in app behavior remains largely unexplored. This paper presents a large-scale study of location-based Android app differentiation, uncovering two important and underexamined phenomena with security and fairness implications. First, we introduce GeoTwins: apps that are functionally similar and share branding but are released under different package names across countries. Despite their similarity, GeoTwins often diverge in requested permissions, third-party libraries, and privacy disclosures. Second, we examine the Android App Bundle ecosystem and reveal unexpected regional differences in supposedly consistent base.apk files. Contrary to common assumptions, even base.apk files vary by region, exposing hidden customizations that may affect app behavior or security.
  These discrepancies have concrete consequences. Geographically distinct variants can lead the same app to be labeled benign in one malware study but suspicious in another, depending on the region of download. Such hidden variation undermines reproducibility and introduces geographic bias into assessments of security, privacy, and functionality. It also raises ethical concerns about transparency and consent: visually identical Google Play listings may mask subtle but important differences.
  To study these issues, we built a distributed app collection pipeline spanning multiple regions and analyzed thousands of apps. We also release a dataset of 81,963 GeoTwins to support future work. Our findings reveal systemic regional disparities in mobile software, with implications for researchers, developers, platform architects, and policymakers.

</details>


### [18] [Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools](https://arxiv.org/abs/2511.21197)
*Paolo Buono,Mary Cerullo,Stefano Cirillo,Giuseppe Desolda,Francesco Greco,Emanuela Guglielmi,Grazia Margarella,Giuseppe Polese,Simone Scalabrino,Cesare Tucci*

Main category: cs.SE

TL;DR: AI辅助工具支持开发者完成错误检测和代码可读性评估等认知密集型任务。然而，开发者如何构建对这些工具的心智模型以及不匹配如何影响信任、控制和采用仍不清楚。通过58名开发者参与的6次协同设计工作坊，研究发现错误检测工具被视为“错误侦探”，提供透明、可操作的反馈和自信提示；可读性评估工具被视为“质量教练”，提供个性化、情境化和渐进式指导。信任取决于解释清晰度、时机和用户控制。提出了一套以人为本的AI设计原则，以平衡中断与支持、简洁与深度、自动化与人类能动性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI辅助工具在技术特性上取得了进步，但开发者如何心智建模这些工具以及心智模型与工具能力不匹配如何影响信任、控制和使用采纳仍知之甚少。

Method: 进行了六次协同设计工作坊，涉及58名开发者，以激发他们对AI辅助错误检测和可读性功能的心智模型。

Result: 开发者将错误检测工具视为“错误侦探”，仅在关键问题时警告用户，保证透明度、可操作反馈和自信提示；可读性评估工具被视为“质量教练”，提供情境化、个性化和渐进式指导。信任取决于解释清晰度、时机和用户控制。

Conclusion: 提炼出一套以人为本的AI集成开发环境设计原则，旨在平衡中断与支持、简洁与深度、自动化与人类能动性，以促进开发者对AI工具的信任和采纳。

Abstract: AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.

</details>


### [19] [Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions](https://arxiv.org/abs/2511.21380)
*Jingyi Chen,Xiaoyan Guo,Songqiang Chen,Shing-Chi Cheung,Jiasi Shen*

Main category: cs.SE

TL;DR: 首次实证研究评估多智能体系统在数据集适应任务中的表现，发现当前系统能识别关键文件并生成部分适应，但很少产生功能正确实现。提示干预（尤其是提供执行错误信息和参考代码）可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究工件的跨数据集自动适应对可扩展性和可重复性至关重要，但尚未得到充分研究。多智能体系统（如GitHub Copilot）有望通过协调推理、代码生成和工具交互自动化复杂工作流。

Method: 通过五阶段评估管道（文件理解、代码编辑、命令生成、验证和最终执行）评估基于GPT-4.1和Claude Sonnet 4的Copilot在ROCODE和LogHub2.0等基准库上的表现，分析成功率、失败模式及提示干预效果。

Result: 当前系统可实现关键文件识别和部分适应生成，但功能正确实现率低。提供执行错误信息和参考代码的提示干预将结构相似度从7.25%提升至67.14%。

Conclusion: 多智能体LLM系统在数据集适应中展现潜力但存在局限，需通过上下文和反馈驱动指导构建更可靠、自校正的智能体。

Abstract: Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [Minimizing Hyperbolic Embedding Distortion with LLM-Guided Hierarchy Restructuring](https://arxiv.org/abs/2511.20679)
*Melika Ayoughi,Pascal Mettes,Paul Groth*

Main category: cs.AI

TL;DR: 本文研究使用大语言模型自动重构层级结构以优化双曲嵌入效果，实验表明LLM重构的层级在多个标准指标上均能产生更高质量的双曲嵌入。


<details>
  <summary>Details</summary>
Motivation: 双曲嵌入的质量与输入层级结构密切相关，而现有层级往往来自知识图谱或本体，可能不符合双曲嵌入的最佳条件（高分支因子和单继承）。需要帮助知识工程师重构层级以满足这些标准。

Method: 提出基于提示的方法，使用大语言模型在双曲嵌入期望标准的指导下自动转换现有层级结构。在16个多样化层级上进行实验验证。

Result: 实验结果显示，经过LLM重构的层级在多个标准嵌入质量指标上一致产生更高质量的双曲嵌入。

Conclusion: 大语言模型能够有效自动重构层级结构以优化双曲嵌入，同时还能提供可解释的重组方案，为知识工程师提供合理性说明。

Abstract: Hyperbolic geometry is an effective geometry for embedding hierarchical data structures. Hyperbolic learning has therefore become increasingly prominent in machine learning applications where data is hierarchically organized or governed by hierarchical semantics, ranging from recommendation systems to computer vision. The quality of hyperbolic embeddings is tightly coupled to the structure of the input hierarchy, which is often derived from knowledge graphs or ontologies. Recent work has uncovered that for an optimal hyperbolic embedding, a high branching factor and single inheritance are key, while embedding algorithms are robust to imbalance and hierarchy size. To assist knowledge engineers in reorganizing hierarchical knowledge, this paper investigates whether Large Language Models (LLMs) have the ability to automatically restructure hierarchies to meet these criteria. We propose a prompt-based approach to transform existing hierarchies using LLMs, guided by known desiderata for hyperbolic embeddings. Experiments on 16 diverse hierarchies show that LLM-restructured hierarchies consistently yield higher-quality hyperbolic embeddings across several standard embedding quality metrics. Moreover, we show how LLM-guided hierarchy restructuring enables explainable reorganizations, providing justifications to knowledge engineers.

</details>


### [21] [AssurAI: Experience with Constructing Korean Socio-cultural Datasets to Discover Potential Risks of Generative AI](https://arxiv.org/abs/2511.20686)
*Chae-Gyun Lim,Seung-Ho Han,EunYoung Byun,Jeongyun Han,Soohyun Cho,Eojin Joo,Heehyeon Kim,Sieun Kim,Juhoon Lee,Hyunsoo Lee,Dongkun Lee,Jonghwan Hyeon,Yechan Hwang,Young-Jun Lee,Kyeongryul Lee,Minhyeong An,Hyunjun Ahn,Jeongwoo Son,Junho Park,Donggyu Yoon,Taehyung Kim,Jeemin Kim,Dasom Choi,Kwangyoung Lee,Hyunseung Lim,Yeohyun Jung,Jongok Hong,Sooyohn Nam,Joonyoung Park,Sungmin Na,Yubin Choi,Jeanne Choi,Yoojin Hong,Sueun Jang,Youngseok Seo,Somin Park,Seoungung Jo,Wonhye Chae,Yeeun Jo,Eunyoung Kim,Joyce Jiyoung Whang,HwaJung Hong,Joseph Seering,Uichin Lee,Juho Kim,Sunna Choi,Seokyeon Ko,Taeho Kim,Kyunghoon Kim,Myungsik Ha,So Jung Lee,Jemin Hwang,JoonHo Kwak,Ho-Jin Choi*

Main category: cs.AI

TL;DR: AssurAI是一个韩国多模态数据集，用于评估生成式AI的安全性，填补了当前英语中心安全数据集的不足


<details>
  <summary>Details</summary>
Motivation: 当前安全数据集主要是英语中心的，无法捕捉韩语等非英语社会文化背景下的特定风险，且通常仅限于文本模态

Method: 定义了35个AI风险因素分类法，构建了包含11,480个实例的韩语多模态数据集，采用两阶段构建、三重独立标注和迭代专家红队测试的质量控制流程

Result: 创建了AssurAI数据集，试点研究验证了其在评估最新LLM安全性方面的有效性

Conclusion: AssurAI有助于为韩国社区开发更安全可靠的生成式AI系统

Abstract: The rapid evolution of generative AI necessitates robust safety evaluations. However, current safety datasets are predominantly English-centric, failing to capture specific risks in non-English, socio-cultural contexts such as Korean, and are often limited to the text modality. To address this gap, we introduce AssurAI, a new quality-controlled Korean multimodal dataset for evaluating the safety of generative AI. First, we define a taxonomy of 35 distinct AI risk factors, adapted from established frameworks by a multidisciplinary expert group to cover both universal harms and relevance to the Korean socio-cultural context. Second, leveraging this taxonomy, we construct and release AssurAI, a large-scale Korean multimodal dataset comprising 11,480 instances across text, image, video, and audio. Third, we apply the rigorous quality control process used to ensure data integrity, featuring a two-phase construction (i.e., expert-led seeding and crowdsourced scaling), triple independent annotation, and an iterative expert red-teaming loop. Our pilot study validates AssurAI's effectiveness in assessing the safety of recent LLMs. We release AssurAI to the public to facilitate the development of safer and more reliable generative AI systems for the Korean community.

</details>


### [22] [$A^2Flow:$ Automating Agentic Workflow Generation via Self-Adaptive Abstraction Operators](https://arxiv.org/abs/2511.20693)
*Mingming Zhao,Xiaokang Wei,Yuanqi Shao,Kaiwen Zhou,Lin Yang,Siwei Rao,Junhui Zhan,Zhitang Chen*

Main category: cs.AI

TL;DR: 提出了A²Flow框架，通过自适应抽象算子实现完全自动化的智能体工作流生成，相比现有方法性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手动预定义算子，限制了通用性和可扩展性，需要自动化的解决方案。

Method: 三阶段算子提取过程：基于案例的初始算子生成、算子聚类和初步抽象、深度提取抽象执行算子；并采用算子记忆机制增强工作流搜索。

Result: 在通用和具身基准测试中，相比最先进基线平均性能提升2.4%和19.3%，资源使用减少37%。

Conclusion: A²Flow框架成功实现了完全自动化的智能体工作流生成，通过自适应抽象算子显著提升了性能表现。

Abstract: Large language models (LLMs) have shown strong potential in automating the design of agentic workflows. However, existing methods still rely heavily on manually predefined operators, limiting generalization and scalability. To address this issue, we propose $A^2Flow$, a fully automated framework for agentic workflow generation based on self-adaptive abstraction operators. $A^2Flow$ employs a three-stage operator extraction process: 1) Case-based Initial Operator Generation: leveraging expert demonstrations and LLM reasoning to generate case-specific operators; 2) Operator Clustering and Preliminary Abstraction: grouping similar operators across tasks to form preliminary abstractions; and 3) Deep Extraction for Abstract Execution Operators: applying long chain-of-thought prompting and multi-path reasoning to derive compact and generalizable execution operators. These operators serve as reusable building blocks for workflow construction without manual predefinition. Furthermore, we enhance node-level workflow search with an operator memory mechanism, which retains historical outputs to enrich context and improve decision-making. Experiments on general and embodied benchmarks show that $A^2Flow$ achieves a 2.4\% and 19.3\% average performance improvement and reduces resource usage by 37\% over state-of-the-art baselines. Homepage:https://github.com/pandawei-ele/A2FLOW

</details>


### [23] [Reasoning With a Star: A Heliophysics Dataset and Benchmark for Agentic Scientific Reasoning](https://arxiv.org/abs/2511.20694)
*Kevin Lee,Russell Spiewak,James Walsh*

Main category: cs.AI

TL;DR: 本文引入了Reasoning With a Star数据集，用于评估大型语言模型在太阳物理科学中的推理能力，包括物理假设、单位一致性和科学格式要求，并提出多智能体模式基准测试方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在科学推理中缺乏对物理假设、单位一致性和科学格式的系统评估，特别是在太阳物理领域需要更全面的基准测试方法。

Method: 基于NASA和UCAR的Living With a Star暑期学校问题集构建数据集，包含问题上下文、推理步骤、答案类型和元数据；开发程序化评分器进行数值容差、符号等价和模式验证；评估单次提示和四种多智能体模式。

Result: 通过系统工程原理分解工作流程的多智能体模式在需要演绎推理的问题上表现优于直接提示，而对纯归纳回忆问题效果不明显。

Conclusion: 多智能体协作方法能有效提升复杂科学推理任务的性能，特别是在需要严格逻辑推导的领域，为太阳物理领域的AI推理提供了新的评估框架。

Abstract: Scientific reasoning through Large Language Models in heliophysics involves more than just recalling facts: it requires incorporating physical assumptions, maintaining consistent units, and providing clear scientific formats through coordinated approaches. To address these challenges, we present Reasoning With a Star, a newly contributed heliophysics dataset applicable to reasoning; we also provide an initial benchmarking approach. Our data are constructed from National Aeronautics and Space Administration & University Corporation for Atmospheric Research Living With a Star summer school problem sets and compiled into a readily consumable question-and-answer structure with question contexts, reasoning steps, expected answer type, ground-truth targets, format hints, and metadata. A programmatic grader checks the predictions using unit-aware numerical tolerance, symbolic equivalence, and schema validation. We benchmark a single-shot baseline and four multi-agent patterns, finding that decomposing workflows through systems engineering principles outperforms direct prompting on problems requiring deductive reasoning rather than pure inductive recall.

</details>


### [24] [A Brief History of Digital Twin Technology](https://arxiv.org/abs/2511.20695)
*Yunqi Zhang,Kuangyu Shi,Biao Li*

Main category: cs.AI

TL;DR: 数字孪生技术通过创建患者特异性虚拟模型正在改变医疗健康，但在广泛临床整合前仍需解决数据互操作性和隐私等挑战。


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术从NASA航天器模拟发展到医疗健康领域，有望推动医疗从被动治疗向预测性、预防性和个性化医学转变。

Method: 通过整合影像学、生物传感器和计算模型创建患者特定的虚拟副本，支持诊断、治疗规划和药物开发。

Result: 开发了心脏数字化孪生预测心律失常治疗效果、肿瘤数字化孪生跟踪肿瘤进展和优化放疗、药物数字化孪生加速药物发现等代表性应用。

Conclusion: 虽然面临互操作性、数据隐私和模型保真度等挑战，但可解释AI、联邦学习和统一监管框架等新兴解决方案提供了前进方向。未来需要多器官数字孪生、基因组学整合和伦理治理方面的进展。

Abstract: Emerging from NASA's spacecraft simulations in the 1960s, digital twin technology has advanced through industrial adoption to spark a healthcare transformation. A digital twin is a dynamic, data-driven virtual counterpart of a physical system, continuously updated through real-time data streams and capable of bidirectional interaction. In medicine, digital twin integrates imaging, biosensors, and computational models to generate patient-specific simulations that support diagnosis, treatment planning, and drug development. Representative applications include cardiac digital twin for predicting arrhythmia treatment outcomes, oncology digital twin for tracking tumor progression and optimizing radiotherapy, and pharmacological digital twin for accelerating drug discovery. Despite rapid progress, major challenges, including interoperability, data privacy, and model fidelity, continue to limit widespread clinical integration. Emerging solutions such as explainable AI, federated learning, and harmonized regulatory frameworks offer promising pathways forward. Looking ahead, advances in multi-organ digital twin, genomics integration, and ethical governance will be essential to ensure that digital twin shifts healthcare from reactive treatment to predictive, preventive, and truly personalized medicine.

</details>


### [25] [Cross Domain Evaluation of Multimodal Chain-of-Thought Reasoning of different datasets into the Amazon CoT Framework](https://arxiv.org/abs/2511.20701)
*Nitya Tiwari,Parv Maheshwari,Vidisha Agarwal*

Main category: cs.AI

TL;DR: 该研究分析了多模态思维链(Multimodal-CoT)在不同领域数据集上的泛化能力，通过系统性消融实验揭示了视觉特征整合和问题类型对推理效果的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态CoT在科学问答基准上取得了先进成果，但其在不同领域的泛化能力仍未充分探索。本研究旨在评估多模态CoT在需要常识推理的多样化数据集上的表现。

Method: 采用Zhang等人提出的两阶段框架，将原理生成与答案推理分离，并通过门控融合机制将视觉特征与T5语言模型集成。在A-OKVQA、OKVQA和ChartQA数据集上进行系统性消融研究。

Result: 视觉特征整合显著减少了原理生成中的幻觉，但CoT推理效果在不同问题类型间差异很大，常识推理尤其具有挑战性。

Conclusion: 本研究为多模态推理系统的实施提供了实用见解，并确定了跨领域泛化能力改进的关键方向。

Abstract: While recent work has extended CoT to multimodal settings, achieving state-of-the-art results on science question answering benchmarks like ScienceQA, the generalizability of these approaches across diverse domains remains underexplored. This work presents a comprehensive analysis of Multimodal Chain-of-Thought (Multimodal-CoT) reasoning, evaluating its effectiveness on the A-OKVQA, OKVQA and ChartQA datasets, which requires broad commonsense and world knowledge beyond scientific reasoning. We implement the two-stage framework proposed by Zhang et al. [3], which separates rationale generation from answer inference and integrates vision features through a gated fusion mechanism with T5-based language models. Through systematic ablation studies, we analyze the contributions of vision features, rationale quality, and architectural choices. Our findings reveal that while vision integration significantly reduces hallucination in rationale generation, the effectiveness of CoT reasoning varies substantially across question types, with commonsense reasoning presenting particular challenges. This work provides practical insights for researchers implementing multimodal reasoning systems and identifies key areas for future improvement in cross-domain generalization.

</details>


### [26] [Representation Interventions Enable Lifelong Unstructured Knowledge Control](https://arxiv.org/abs/2511.20892)
*Xuyuan Liu,Zhengzhang Chen,Xinshuai Dong,Yanchi Liu,Xujiang Zhao,Shengyu Chen,Haoyu Wang,Yujun Yan,Haifeng Chen*

Main category: cs.AI

TL;DR: 这篇论文提出了RILKE模型来处理LLMs知识更新问题，采用表示空间干预的方法在终身学习设置下实现高效、精确的知识控制。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLMs产生错误或过时内容的知识更新问题，特别是在终身学习环境下需要高效更新复杂非结构化知识且避免干扰的挑战。

Method: 通过表示空间干预技术，训练学习具有抗改述和局部化编辑能力的模块，并在推理时使用查询自适应路由器选择合适模块来指导生成。

Result: 在LLaMA和Qwen模型的知识编辑基准测试中，RILKE在大规模数据集上表现出高编辑成功率、强泛化能力，并保持通用性能。

Conclusion: RILKE是LLMs终身知识控制的有效且可扩展的解决方案，能够实现细粒度知识控制同时最小化内存开销。

Abstract: Large language models (LLMs) often produce incorrect or outdated content. Updating their knowledge efficiently and accurately without costly retraining is a major challenge. This problem is especially hard for complex, unstructured knowledge in a lifelong setting, where many edits must coexist without interference. We introduce RILKE (Representation Intervention for Lifelong KnowledgE Control), a robust and scalable method that treats knowledge control as interventions within the model's representation space. Leveraging representation-space expressiveness, we identify two properties enabling RILKE to deliver fine-grained control over complex, unstructured knowledge while maintaining general utility with frozen base weights. During training, RILKE learns paraphrase-robust and edit-localized modules that limit each update to a low-dimensional subspace to minimize cross-edit interference. In inference, a query-adaptive router selects the appropriate module to guide the model's generation. In evaluation on knowledge editing benchmarks with LLaMA and Qwen models, RILKE is scalable to large-scale datasets, demonstrating high edit success, strong paraphrase generalization, and preserving general utility with modest memory overhead. These results show RILKE is an effective and scalable solution for lifelong knowledge control in LLMs.

</details>


### [27] [ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction](https://arxiv.org/abs/2511.20937)
*Qineng Wang,Wenlong Huang,Yu Zhou,Hang Yin,Tianwei Bao,Jianwen Lyu,Weiyu Liu,Ruohan Zhang,Jiajun Wu,Li Fei-Fei,Manling Li*

Main category: cs.AI

TL;DR: ENACT基准测试旨在评估视觉语言模型是否表现出具身认知特征，通过世界建模任务来测试模型从自我中心交互中理解和推理的能力


<details>
  <summary>Details</summary>
Motivation: 探讨现代视觉语言模型是否在没有具身训练的情况下仍能表现出具身认知的特征

Method: 设计了ENACT基准测试，包含正向世界建模和逆向世界建模两个序列重排序任务，使用POMDP框架，动作定义为场景图变化，基于机器人模拟器BEHAVIOR生成QA数据集

Result: 实验显示前沿VLM与人类存在性能差距，差距随交互时间跨度增大；模型在逆向任务表现更好，表现出人类中心偏见（如偏好右手动作）

Conclusion: 虽然现代VLM显示出一些具身认知特征，但与人类相比仍有显著差距，特别是在长期交互和多视角适应方面

Abstract: Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/.

</details>


### [28] [Improving Procedural Skill Explanations via Constrained Generation: A Symbolic-LLM Hybrid Architecture](https://arxiv.org/abs/2511.20942)
*Rahul Dass,Thomas Bowlin,Zebing Li,Xiao Jin,Ashok Goel*

Main category: cs.AI

TL;DR: Ivy系统结合符号TMK模型与LLM生成层，通过结构化约束提升教学解释的质量


<details>
  <summary>Details</summary>
Motivation: 传统LLM生成的教学解释往往流畅但浅层，缺乏因果、目标导向和组合逻辑

Method: 结合符号Task-Method-Knowledge(TMK)模型与生成解释层，TMK编码因果转换、目标层次和问题分解

Result: 符号约束显著提升了针对'如何'和'为何'问题的解释结构质量

Conclusion: 展示了可扩展的教育AI方法，能增强智能辅导系统中AI生成解释的教学价值

Abstract: In procedural skill learning, instructional explanations must convey not just steps, but the causal, goal-directed, and compositional logic behind them. Large language models (LLMs) often produce fluent yet shallow responses that miss this structure. We present Ivy, an AI coaching system that delivers structured, multi-step explanations by combining symbolic Task-Method-Knowledge (TMK) models with a generative interpretation layer-an LLM that constructs explanations while being constrained by TMK structure. TMK encodes causal transitions, goal hierarchies, and problem decompositions, and guides the LLM within explicit structural bounds. We evaluate Ivy against responses against GPT and retrieval-augmented GPT baselines using expert and independent annotations across three inferential dimensions. Results show that symbolic constraints consistently improve the structural quality of explanations for "how" and "why" questions. This study demonstrates a scalable AI for education approach that strengthens the pedagogical value of AI-generated explanations in intelligent coaching systems.

</details>


### [29] [ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning](https://arxiv.org/abs/2511.21005)
*Jinpeng Wang,Chao Li,Ting Ye,Mengyuan Zhang,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: 本文提出了ICPO方法，通过利用LLM生成不同响应的概率来反映其推理过程的自我评估，结合可验证奖励解决现有RLVR方法的问题，在多个基准测试中稳定提升了推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法存在粗粒度奖励、奖励噪声和低效探索等问题，导致训练不稳定和熵崩溃。

Method: ICPO通过比较同一输入提示下多个响应的生成概率，计算偏好优势分数，并将其与可验证奖励结合来指导探索过程。

Result: 实验表明，ICPO缓解了粗粒度奖励和奖励噪声问题，有效抑制了过度自信错误，增强了被低估高质量响应的相对优势，防止模型过拟合特定策略。

Conclusion: ICPO在四个通用领域基准和三个数学基准上均显示出比GRPO更稳定的推理能力提升。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.

</details>


### [30] [Causality Without Causal Models](https://arxiv.org/abs/2511.21260)
*Joseph Y. Halpern,Rafael Pass*

Main category: cs.AI

TL;DR: 抽象化Halpern-Pearl因果关系定义，使其适用于任何反事实定义的模型，扩展到更复杂的逻辑公式和非因果模型的解释。


<details>
  <summary>Details</summary>
Motivation: Halpern-Pearl因果关系定义局限于因果模型，无法处理涉及析取、否定、信念和嵌套反事实的复杂公式，需要更通用的定义框架。

Method: 提取Halpern-Pearl定义的关键特征，构建抽象化定义，使其适用于任何反事实定义的模型，包括支持回溯等特性的模型。

Result: 成功开发出抽象化因果关系定义，可应用于更广泛的模型和复杂公式，并能扩展到解释的定义。

Conclusion: 抽象化方法不仅扩展了因果关系定义的适用范围，还深化了对原始定义特征的理解，为非因果模型中的解释提供了理论基础。

Abstract: Perhaps the most prominent current definition of (actual) causality is due to Halpern and Pearl.  It is defined using causal models (also known as structural equations models).  We abstract the definition, extracting its key features, so that it can be applied to any other model where counterfactuals are defined. By abstracting the definition, we gain a number of benefits. Not only can we apply the definition in a wider range of models, including ones that allow, for example, backtracking, but we can apply the definition to determine if A is a cause of B  even if A and B are formulas involving disjunctions, negations, beliefs, and nested counterfactuals (none of which can be handled by the Halpern-Pearl definition). Moreover, we can extend the ideas to getting an abstract definition of explanation that can be applied beyond causal models. Finally, we gain a deeper understanding of features of the definition  even in causal models.

</details>


### [31] [New Hybrid Heuristics for Pseudo-Boolean Propagation](https://arxiv.org/abs/2511.21417)
*Mia Müßig,Jan Johannsen*

Main category: cs.AI

TL;DR: 论文提出了新的启发式策略，用于改进伪布尔求解器中单元传播的混合决策方法，显著超越了RoundingSAT求解器中的当前方法。


<details>
  <summary>Details</summary>
Motivation: 当前伪布尔求解中最成功的单元传播策略是监视文字方案与计数方法的混合模式，但现有方法仍有优化空间。

Method: 引入了新的启发式策略来优化混合决策过程。

Result: 新启发式策略能够显著优于RoundingSAT求解器中的当前方法。

Conclusion: 新提出的启发式策略有效提升了伪布尔求解器中单元传播的性能。

Abstract: In pseudo-boolean solving the currently most successful unit propagation strategy is a hybrid mode combining the watched literal scheme with the counting method. This short paper introduces new heuristics for this hybrid decision, which are able to drastically outperform the current method in the RoundingSAT solver.

</details>


### [32] [Conversational no-code and multi-agentic disease module identification and drug repurposing prediction with ChatDRex](https://arxiv.org/abs/2511.21438)
*Simon Süwer,Kester Bagemihl,Sylvie Baier,Lucia Dicunta,Markus List,Jan Baumbach,Andreas Maier,Fernando M. Delgado-Chaves*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Repurposing approved drugs offers a time-efficient and cost-effective alternative to traditional drug development. However, in silico prediction of repurposing candidates is challenging and requires the effective collaboration of specialists in various fields, including pharmacology, medicine, biology, and bioinformatics. Fragmented, specialized algorithms and tools often address only narrow aspects of the overall problem, and heterogeneous, unstructured data landscapes require specialized users to be involved. Hence, these data services do not integrate smoothly across workflows. With ChatDRex, we present a conversation-based, multi-agent system that facilitates the execution of complex bioinformatic analyses aiming for network-based drug repurposing prediction. It builds on the integrated systems medicine knowledge graph NeDRex. ChatDRex provides natural language access to its extensive biomedical KG and integrates bioinformatics agents for network analysis and drug repurposing, complemented by agents for functional coherence evaluation for in silico validation, as well as agents for literature mining and for discussing the obtained results in a scientific context. Its flexible multi-agent design assigns specific tasks to specialized agents, including query routing, data retrieval, algorithm execution, and result visualization. A dedicated reasoning module keeps the user in the loop and allows for hallucination detection. By enabling physicians and researchers without computer science expertise to control complex analyses in natural language, ChatDRex democratizes access to bioinformatics as an important resource for drug repurposing. It enables clinical experts to generate hypotheses and explore drug repurposing opportunities, ultimately accelerating the discovery of novel therapies and advancing personalized medicine and translational research.

</details>


### [33] [MADRA: Multi-Agent Debate for Risk-Aware Embodied Planning](https://arxiv.org/abs/2511.21460)
*Junjian Wang,Lidan Zhao,Xi Sheryl Zhang*

Main category: cs.AI

TL;DR: 提出MADRA框架，通过多智能体辩论进行风险评估，无需训练即可提升具身AI安全性，在VirtualHome和AI2-THOR实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在计算成本高或过度拒绝安全指令的问题，需要一种能在保持任务性能的同时增强安全意识的解决方案。

Method: 采用基于LLM的多智能体辩论框架，由关键评估器根据逻辑性、风险识别等标准评分，结合迭代审议和共识投票机制。

Result: 实验显示对不安全任务拒绝率超过90%，且安全任务拒绝率低，在安全性和执行效率上均优于基线方法。

Conclusion: MADRA提供了可扩展、模型无关的解决方案，为构建可信赖的具身智能体提供了有效途径。

Abstract: Ensuring the safety of embodied AI agents during task planning is critical for real-world deployment, especially in household environments where dangerous instructions pose significant risks. Existing methods often suffer from either high computational costs due to preference alignment training or over-rejection when using single-agent safety prompts. To address these limitations, we propose MADRA, a training-free Multi-Agent Debate Risk Assessment framework that leverages collective reasoning to enhance safety awareness without sacrificing task performance. MADRA employs multiple LLM-based agents to debate the safety of a given instruction, guided by a critical evaluator that scores responses based on logical soundness, risk identification, evidence quality, and clarity. Through iterative deliberation and consensus voting, MADRA significantly reduces false rejections while maintaining high sensitivity to dangerous tasks. Additionally, we introduce a hierarchical cognitive collaborative planning framework that integrates safety, memory, planning, and self-evolution mechanisms to improve task success rates through continuous learning. We also contribute SafeAware-VH, a benchmark dataset for safety-aware task planning in VirtualHome, containing 800 annotated instructions. Extensive experiments on AI2-THOR and VirtualHome demonstrate that our approach achieves over 90% rejection of unsafe tasks while ensuring that safe-task rejection is low, outperforming existing methods in both safety and execution efficiency. Our work provides a scalable, model-agnostic solution for building trustworthy embodied agents.

</details>


### [34] [SpatialBench: Benchmarking Multimodal Large Language Models for Spatial Cognition](https://arxiv.org/abs/2511.21471)
*Peiran Xu,Sudong Wang,Yao Zhu,Jianing Li,Yunjian Zhang*

Main category: cs.AI

TL;DR: 提出了一个层次化空间认知框架和SpatialBench基准，用于系统性评估多模态大语言模型的空间推理能力，发现模型在感知层面表现良好但在高层推理方面存在局限


<details>
  <summary>Details</summary>
Motivation: 现有基准过度简化空间认知，将其简化为单一维度指标，无法捕捉空间能力的层次结构和相互依赖关系

Method: 提出层次化空间认知框架，将空间智能分解为5个逐步复杂的层次；构建涵盖15个任务的大规模细粒度基准SpatialBench；引入高层能力导向的统一评估指标

Result: 大规模MLLM实验显示不同认知层次存在明显的性能分层：模型在感知基础层面表现强劲，但在符号推理、因果推断和规划方面仍受限；人类测试显示人类进行选择性目标导向抽象，而MLLM倾向于过度关注表面细节

Conclusion: 建立了首个系统化测量MLLM层次化空间认知的框架，为未来空间智能系统奠定基础

Abstract: Spatial cognition is fundamental to real-world multimodal intelligence, allowing models to effectively interact with the physical environment. While multimodal large language models (MLLMs) have made significant strides, existing benchmarks often oversimplify spatial cognition, reducing it to a single-dimensional metric, which fails to capture the hierarchical structure and interdependence of spatial abilities. To address this gap, we propose a hierarchical spatial cognition framework that decomposes spatial intelligence into five progressively complex levels from basic observation to high-level planning. Building upon this taxonomy, we construct SpatialBench, a large-scale, fine-grained benchmark covering 15 tasks aligned with these cognitive levels. To provide a unified evaluation across heterogeneous tasks, we further introduce a high-level capability-oriented metric that reliably assesses a model's overall spatial reasoning ability. Extensive experiments over massive MLLMs reveal distinct performance stratification across cognitive levels: models exhibit strong perceptual grounding yet remain limited in symbolic reasoning, causal inference, and planning. Additional human tests demonstrate that humans perform selective, goal-directed abstraction, while MLLMs tend to over-attend to surface details without coherent spatial intent. Our work establishes the first systematic framework for measuring hierarchical spatial cognition in MLLMs, laying the foundation for future spatially intelligent systems.

</details>


### [35] [Pessimistic Verification for Open Ended Math Questions](https://arxiv.org/abs/2511.21522)
*Yanxing Huang,Zihan Tang,Zejin Lin,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 悲观验证通过构建多个并行验证流程来改善数学问题验证性能，显著提升错误检测能力且计算资源消耗低


<details>
  <summary>Details</summary>
Motivation: 现有验证性能的关键局限在于错误检测能力有限，需要更有效的验证方法

Method: 设计悲观验证变体：为同一证明构建多个并行验证流程，任一验证报告错误即判定证明错误

Result: 显著提升多个数学验证基准性能，token效率超越扩展长链思维，识别出数据集中标注错误导致的假阴性

Conclusion: 悲观验证可有效提升语言模型数学输出的可靠性，对实现长视野数学任务至关重要

Abstract: The key limitation of the verification performance lies in the ability of error detection. With this intuition we designed several variants of pessimistic verification, which are simple workflows that could significantly improve the verification of open-ended math questions. In pessimistic verification we construct multiple parallel verifications for the same proof, and the proof is deemed incorrect if any one of them reports an error. This simple technique significantly improves the performance across many math verification benchmarks without incurring substantial computational resources. Its token efficiency even surpassed extended long-CoT in test-time scaling. Our case studies further indicate that the majority of false negatives in stronger models are actually caused by annotation errors in the original dataset, so our method's performance is in fact underestimated. Self-verification for mathematical problems can effectively improve the reliability and performance of language model outputs, and it also plays a critical role in enabling long-horizon mathematical tasks. We believe that research on pessimistic verification will help enhance the mathematical capabilities of language models across a wide range of tasks.

</details>


### [36] [Self-Transparency Failures in Expert-Persona LLMs: A Large-Scale Behavioral Audit](https://arxiv.org/abs/2511.21569)
*Alex Diep*

Main category: cs.AI

TL;DR: 该研究通过公共花园设计审计了16个开放权重模型，发现AI模型在不同专业领域中的身份披露存在显著不一致，透明度受到训练因素而非规模的影响，需要专门的行为设计和实证验证。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决语言模型在专家语境中无法可靠披露其AI身份的问题，这会导致用户无法信任其能力边界。在高风险领域，虚假的专业知识可能对用户造成伤害。

Method: 研究方法包括：采用公共花园设计，对16个开放权重模型（参数范围4B-671B）进行19,200次试验审计；使用贝叶斯验证和Rogan-Gladen校正来确认结果的稳健性。

Result: 研究结果显示：模型在不同领域中表现出尖锐的不一致性，金融顾问角色的初始披露率为30.8%，而神经外科医生角色仅为3.5%；披露率范围为2.8%至73.6%；模型身份比参数数量更能预测行为；推理优化在某些模型中积极抑制了自我透明度。

Conclusion: 研究结论表明：透明度反映的是训练因素而非规模；组织不能假设安全属性会转移到部署环境中，需要进行有意识的行为设计和实证验证。

Abstract: If a language model cannot reliably disclose its AI identity in expert contexts, users cannot trust its competence boundaries. This study examines self-transparency in models assigned professional personas within high-stakes domains where false expertise risks user harm. Using a common-garden design, sixteen open-weight models (4B--671B parameters) were audited across 19,200 trials. Models exhibited sharp domain-specific inconsistency: a Financial Advisor persona elicited 30.8% disclosure initially, while a Neurosurgeon persona elicited only 3.5%. This creates preconditions for a "Reverse Gell-Mann Amnesia" effect, where transparency in some domains leads users to overgeneralize trust to contexts where disclosure fails. Disclosure ranged from 2.8% to 73.6%, with a 14B model reaching 61.4% while a 70B produced just 4.1%. Model identity predicted behavior better than parameter count ($ΔR_{adj}^{2} = 0.359$ vs 0.018). Reasoning optimization actively suppressed self-transparency in some models, with reasoning variants showing up to 48.4% lower disclosure than base counterparts. Bayesian validation with Rogan--Gladen correction confirmed robustness to measurement error ($κ= 0.908$). These findings demonstrate transparency reflects training factors rather than scale. Organizations cannot assume safety properties transfer to deployment contexts, requiring deliberate behavior design and empirical verification.

</details>


### [37] [From Prediction to Foresight: The Role of AI in Designing Responsible Futures](https://arxiv.org/abs/2511.21570)
*Maria Perez-Ortiz*

Main category: cs.AI

TL;DR: 提出了“负责计算前瞻”概念，探讨人工智能如何支持负责任的前瞻决策，强调AI作为辅助工具而非替代品，助力应对21世纪重大挑战。


<details>
  <summary>Details</summary>
Motivation: 在技术快速发展和全球挑战复杂的背景下，需要负责任的前瞻框架来应对未来不确定性，并塑造可持续的未来。

Method: 通过定义负责任计算前瞻的原则，开发AI驱动的前瞻工具，结合模拟和情景分析，增强决策者的能力。

Result: 确立了一套负责任计算前瞻的基础原则，并展示了AI工具在增强前瞻决策中的应用。

Conclusion: AI应作为支持性工具融入前瞻实践，补充而非替代人类判断，以促成有韧性和道德的未来塑造。

Abstract: In an era marked by rapid technological advancements and complex global challenges, responsible foresight has emerged as an essential framework for policymakers aiming to navigate future uncertainties and shape the future. Responsible foresight entails the ethical anticipation of emerging opportunities and risks, with a focus on fostering proactive, sustainable, and accountable future design. This paper coins the term "responsible computational foresight", examining the role of human-centric artificial intelligence and computational modeling in advancing responsible foresight, establishing a set of foundational principles for this new field and presenting a suite of AI-driven foresight tools currently shaping it. AI, particularly in conjunction with simulations and scenario analysis, enhances policymakers' ability to address uncertainty, evaluate risks, and devise strategies geared toward sustainable, resilient futures. However, responsible foresight extends beyond mere technical forecasting; it demands a nuanced understanding of the interdependencies within social, environmental, economic and political systems, alongside a commitment to ethical, long-term decision-making that supports human intelligence. We argue that AI will play a role as a supportive tool in responsible, human-centered foresight, complementing rather than substituting policymaker judgment to enable the proactive shaping of resilient and ethically sound futures. This paper advocates for the thoughtful integration of AI into foresight practices to empower policymakers and communities as they confront the grand challenges of the 21st century.

</details>
