{"id": "2511.10716", "categories": ["cs.AI", "cs.CE", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.10716", "abs": "https://arxiv.org/abs/2511.10716", "authors": ["Niclas Boehmer", "Maximilian T. Wittmann"], "title": "Picking a Representative Set of Solutions in Multiobjective Optimization: Axioms, Algorithms, and Experiments", "comment": "Accepted to AAAI '26", "summary": "Many real-world decision-making problems involve optimizing multiple objectives simultaneously, rendering the selection of the most preferred solution a non-trivial problem: All Pareto optimal solutions are viable candidates, and it is typically up to a decision maker to select one for implementation based on their subjective preferences. To reduce the cognitive load on the decision maker, previous work has introduced the Pareto pruning problem, where the goal is to compute a fixed-size subset of Pareto optimal solutions that best represent the full set, as evaluated by a given quality measure. Reframing Pareto pruning as a multiwinner voting problem, we conduct an axiomatic analysis of existing quality measures, uncovering several unintuitive behaviors. Motivated by these findings, we introduce a new measure, directed coverage. We also analyze the computational complexity of optimizing various quality measures, identifying previously unknown boundaries between tractable and intractable cases depending on the number and structure of the objectives. Finally, we present an experimental evaluation, demonstrating that the choice of quality measure has a decisive impact on the characteristics of the selected set of solutions and that our proposed measure performs competitively or even favorably across a range of settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u76ee\u6807\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5e15\u7d2f\u6258\u526a\u679d\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u8d62\u5bb6\u6295\u7968\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8d28\u91cf\u5ea6\u91cf\u6807\u51c6\u2014\u2014\u5bfc\u5411\u8986\u76d6\u5ea6\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u8d28\u91cf\u5ea6\u91cf\u7684\u8ba1\u7b97\u590d\u6742\u6027\u3002", "motivation": "\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u4e2d\u5b58\u5728\u5927\u91cf\u5e15\u7d2f\u6258\u6700\u4f18\u89e3\uff0c\u51b3\u7b56\u8005\u9700\u8981\u4ece\u4e2d\u9009\u62e9\u6700\u504f\u597d\u7684\u89e3\uff0c\u8fd9\u7ed9\u51b3\u7b56\u8005\u5e26\u6765\u4e86\u8f83\u5927\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002\u73b0\u6709\u5e15\u7d2f\u6258\u526a\u679d\u65b9\u6cd5\u7684\u8d28\u91cf\u5ea6\u91cf\u5b58\u5728\u4e00\u4e9b\u4e0d\u76f4\u89c2\u7684\u884c\u4e3a\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5c06\u5e15\u7d2f\u6258\u526a\u679d\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u8d62\u5bb6\u6295\u7968\u95ee\u9898\uff0c\u5bf9\u73b0\u6709\u8d28\u91cf\u5ea6\u91cf\u8fdb\u884c\u516c\u7406\u5206\u6790\uff0c\u63d0\u51fa\u65b0\u7684\u5bfc\u5411\u8986\u76d6\u5ea6\u5ea6\u91cf\uff0c\u5206\u6790\u4f18\u5316\u4e0d\u540c\u8d28\u91cf\u5ea6\u91cf\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5e76\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8d28\u91cf\u5ea6\u91cf\u7684\u9009\u62e9\u5bf9\u6240\u9009\u89e3\u96c6\u7684\u7279\u5f81\u5177\u6709\u51b3\u5b9a\u6027\u5f71\u54cd\uff0c\u63d0\u51fa\u7684\u5bfc\u5411\u8986\u76d6\u5ea6\u5ea6\u91cf\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\u751a\u81f3\u66f4\u4f18\u3002", "conclusion": "\u5bfc\u5411\u8986\u76d6\u5ea6\u662f\u4e00\u79cd\u6709\u6548\u7684\u5e15\u7d2f\u6258\u526a\u679d\u8d28\u91cf\u5ea6\u91cf\u6807\u51c6\uff0c\u80fd\u591f\u5e2e\u52a9\u51b3\u7b56\u8005\u66f4\u597d\u5730\u9009\u62e9\u4ee3\u8868\u6027\u89e3\u96c6\uff0c\u964d\u4f4e\u51b3\u7b56\u8ba4\u77e5\u8d1f\u62c5\u3002"}}
{"id": "2511.11257", "categories": ["cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11257", "abs": "https://arxiv.org/abs/2511.11257", "authors": ["Yuqi Yin", "Yibo Fu", "Siyuan Wang", "Peng Sun", "Hongyu Wang", "Xiaohui Wang", "Lei Zheng", "Zhiyong Li", "Zhirong Liu", "Jianji Wang", "Zhaoxi Sun"], "title": "AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery", "comment": null, "summary": "The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.", "AI": {"tldr": "AIonopedia is the first LLM agent for Ionic Liquid discovery, using a multimodal domain foundation model for accurate property prediction and hierarchical search architecture.", "motivation": "To overcome challenges in IL discovery including limited data, poor model accuracy, and fragmented workflows.", "method": "Leverages LLM-augmented multimodal domain foundation model trained on a comprehensive IL dataset with hierarchical search architecture.", "result": "Superior performance in property predictions, effective IL modification in literature evaluations, and successful real-world wet-lab validation.", "conclusion": "AIonopedia demonstrates exceptional generalization capabilities and ability to accelerate real-world IL discovery."}}
{"id": "2511.10781", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10781", "abs": "https://arxiv.org/abs/2511.10781", "authors": ["Md Ariful Islam Malik", "Jeffrey C. Carver", "Nasir U. Eisty"], "title": "Peer Code Review in Research Software Development: The Research Software Engineer Perspective", "comment": null, "summary": "Background: Research software is crucial for enabling research discoveries and supporting data analysis, simulation, and interpretation across domains. However, evolving requirements, complex inputs, and legacy dependencies hinder the software quality and maintainability. While peer code review can improve software quality, its adoption by research software engineers (RSEs) remains unexplored. Aims: This study explores RSE perspectives on peer code review, focusing on their practices, challenges, and potential improvements. Building on prior work, it aims to uncover how RSEs insights differ from those of other research software developers and identify factors that can enhance code review adoption in this domain. Method: We surveyed RSEs to gather insights into their perspectives on peer code review. The survey design aligned with previous research to enable comparative analysis while including additional questions tailored to RSEs. Results: We received 61 valid responses from the survey. The findings align with prior research while uncovering unique insights about the challenges and practices of RSEs compared to broader developer groups. Conclusions: Peer code review is vital in improving research software's quality, maintainability, and reliability. Despite the unique challenges RSEs face, addressing these through structured processes, improved tools, and targeted training can enhance peer review adoption and effectiveness in research software development.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u7814\u7a76\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5bf9\u540c\u884c\u4ee3\u7801\u5ba1\u67e5\u7684\u770b\u6cd5\uff0c\u53d1\u73b0\u5c3d\u7ba1\u4ee3\u7801\u5ba1\u67e5\u5bf9\u7814\u7a76\u8f6f\u4ef6\u8d28\u91cf\u5f88\u91cd\u8981\uff0c\u4f46RSEs\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u9700\u8981\u7ed3\u6784\u5316\u6d41\u7a0b\u548c\u66f4\u597d\u5de5\u5177\u652f\u6301\u4ee5\u63d0\u9ad8\u91c7\u7528\u7387\u3002", "motivation": "\u7814\u7a76\u8f6f\u4ef6\u5bf9\u79d1\u5b66\u53d1\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e0d\u65ad\u53d8\u5316\u7684\u9700\u6c42\u548c\u9057\u7559\u4f9d\u8d56\u5173\u7cfb\u5f71\u54cd\u4e86\u8f6f\u4ef6\u8d28\u91cf\u3002\u867d\u7136\u4ee3\u7801\u5ba1\u67e5\u80fd\u63d0\u9ad8\u8d28\u91cf\uff0c\u4f46\u5176\u5728\u7814\u7a76\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u4e2d\u7684\u91c7\u7528\u60c5\u51b5\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u95ee\u5377\u6536\u96c6\u4e8661\u540d\u7814\u7a76\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u53cd\u9988\uff0c\u8c03\u67e5\u8bbe\u8ba1\u57fa\u4e8e\u5148\u524d\u7814\u7a76\u4ee5\u4fbf\u6bd4\u8f83\u5206\u6790\uff0c\u5e76\u5305\u542b\u9488\u5bf9RSEs\u7684\u7279\u5b9a\u95ee\u9898\u3002", "result": "\u8c03\u67e5\u7ed3\u679c\u663e\u793aRSEs\u5728\u4ee3\u7801\u5ba1\u67e5\u65b9\u9762\u9762\u4e34\u7684\u6311\u6218\u548c\u5b9e\u8df5\u4e0e\u66f4\u5e7f\u6cdb\u7684\u5f00\u53d1\u8005\u7fa4\u4f53\u6709\u6240\u4e0d\u540c\uff0c\u53d1\u73b0\u4e86\u72ec\u7279\u7684\u89c1\u89e3\u3002", "conclusion": "\u540c\u884c\u4ee3\u7801\u5ba1\u67e5\u5bf9\u7814\u7a76\u8f6f\u4ef6\u7684\u8d28\u91cf\u3001\u53ef\u7ef4\u62a4\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u6d41\u7a0b\u3001\u6539\u8fdb\u5de5\u5177\u548c\u9488\u5bf9\u6027\u57f9\u8bad\u89e3\u51b3RSEs\u9762\u4e34\u7684\u72ec\u7279\u6311\u6218\uff0c\u53ef\u4ee5\u63d0\u5347\u4ee3\u7801\u5ba1\u67e5\u5728\u7814\u7a76\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u91c7\u7528\u548c\u6548\u679c\u3002"}}
{"id": "2511.10698", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.10698", "abs": "https://arxiv.org/abs/2511.10698", "authors": ["Meixia He", "Peican Zhu", "Le Cheng", "Yangming Guo", "Manman Yuan", "Keke Tang"], "title": "Transferable Hypergraph Attack via Injecting Nodes into Pivotal Hyperedges", "comment": "AAAI 2026, Accept", "summary": "Recent studies have demonstrated that hypergraph neural networks (HGNNs) are susceptible to adversarial attacks. However, existing methods rely on the specific information mechanisms of target HGNNs, overlooking the common vulnerability caused by the significant differences in hyperedge pivotality along aggregation paths in most HGNNs, thereby limiting the transferability and effectiveness of attacks. In this paper, we present a novel framework, i.e., Transferable Hypergraph Attack via Injecting Nodes into Pivotal Hyperedges (TH-Attack), to address these limitations. Specifically, we design a hyperedge recognizer via pivotality assessment to obtain pivotal hyperedges within the aggregation paths of HGNNs. Furthermore, we introduce a feature inverter based on pivotal hyperedges, which generates malicious nodes by maximizing the semantic divergence between the generated features and the pivotal hyperedges features. Lastly, by injecting these malicious nodes into the pivotal hyperedges, TH-Attack improves the transferability and effectiveness of attacks. Extensive experiments are conducted on six authentic datasets to validate the effectiveness of TH-Attack and the corresponding superiority to state-of-the-art methods.", "AI": {"tldr": "TH-Attack is a novel adversarial attack framework for hypergraph neural networks that improves transferability by injecting malicious nodes into pivotal hyperedges identified through pivotality assessment.", "motivation": "Existing HGNN attack methods rely on specific model architectures and overlook the common vulnerability caused by differences in hyperedge pivotality along aggregation paths, limiting attack transferability.", "method": "1) Design a hyperedge recognizer via pivotality assessment to identify pivotal hyperedges. 2) Develop a feature inverter to generate malicious nodes that maximize semantic divergence from pivotal hyperedge features. 3) Inject malicious nodes into pivotal hyperedges.", "result": "Extensive experiments on six datasets demonstrate TH-Attack's effectiveness and superiority over state-of-the-art methods in terms of transferability and attack performance.", "conclusion": "TH-Attack effectively addresses limitations of existing HGNN attack methods by leveraging hyperedge pivotality, achieving improved transferability and effectiveness across different hypergraph neural network architectures."}}
{"id": "2511.10704", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10704", "abs": "https://arxiv.org/abs/2511.10704", "authors": ["Samih Fadli"], "title": "The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems", "comment": "12 pages, 4 figures, 1 table, includes Supplementary Materials, simulation code on GitHub (https://github.com/AerisSpace/SecondLawIntelligence )", "summary": "We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -\u03a3 p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAI\u5bf9\u9f50\u7684\u70ed\u529b\u5b66\u7b2c\u4e8c\u5b9a\u5f8b\u7c7b\u6bd4\uff1a\u4f26\u7406\u71b5\uff08\u504f\u79bb\u9884\u671f\u76ee\u6807\u7684\u7a0b\u5ea6\uff09\u5728\u6ca1\u6709\u6301\u7eed\u5bf9\u9f50\u5de5\u4f5c\u7684\u60c5\u51b5\u4e0b\u4f1a\u81ea\u53d1\u589e\u52a0\u3002\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5efa\u7acb\u4e86\u5bf9\u9f50\u5de5\u4f5c\u7684\u4e34\u754c\u7a33\u5b9a\u6027\u8fb9\u754c\u516c\u5f0f\u3002", "motivation": "AI\u7cfb\u7edf\u5728\u6ca1\u6709\u6301\u7eed\u5bf9\u9f50\u5de5\u4f5c\u7684\u60c5\u51b5\u4e0b\u4f1a\u81ea\u53d1\u504f\u79bb\u9884\u8bbe\u76ee\u6807\uff0c\u9700\u8981\u91cf\u5316\u6846\u67b6\u6765\u7406\u89e3\u548c\u63a7\u5236\u8fd9\u79cd\u4f26\u7406\u71b5\u7684\u589e\u52a0\u3002", "method": "\u5b9a\u4e49\u4f26\u7406\u71b5S = -\u03a3 p(g_i;theta) ln p(g_i;theta)\uff0c\u8bc1\u660edS/dt \u2265 0\uff0c\u63a8\u5bfc\u4e34\u754c\u5bf9\u9f50\u5de5\u4f5cgamma_crit = (lambda_max/2)lnN\uff0c\u5e76\u901a\u8fc770\u4ebf\u53c2\u6570\u6a21\u578b\u7684\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u3002", "result": "\u672a\u6b63\u5219\u5316\u6a21\u578b\uff1a\u71b5\u4ece0.32\u589e\u52a0\u52301.69\u00b11.08 nats\uff1b\u6b63\u5219\u5316\u6a21\u578b\uff08gamma=20.4\uff09\uff1a\u71b5\u7a33\u5b9a\u57280.00\u00b10.00 nats\uff08p=4.19\u00d710^-17\uff09\u3002", "conclusion": "AI\u5bf9\u9f50\u95ee\u9898\u53ef\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8fde\u7eed\u70ed\u529b\u5b66\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u9ad8\u7ea7\u81ea\u4e3b\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u63d0\u4f9b\u91cf\u5316\u57fa\u7840\u3002"}}
{"id": "2511.10865", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10865", "abs": "https://arxiv.org/abs/2511.10865", "authors": ["Sherry Shi", "Renyao Wei", "Michele Tufano", "Jos\u00e9 Cambronero", "Runxiang Cheng", "Franjo Ivan\u010di\u0107", "Pat Rondon"], "title": "Towards a Human-in-the-Loop Framework for Reliable Patch Evaluation Using an LLM-as-a-Judge", "comment": null, "summary": "Reliable evaluation is crucial for advancing Automated Program Repair (APR), but prevailing benchmarks rely on execution-based evaluation methods (unit test pass@k), which fail to capture true patch validity. Determining validity can require costly manual annotation. To reduce this cost, we introduce a human-in-the-loop approach to LLM-based patch validity judgment. Inspired by the observation that human judgment is better aligned when using a shared rubric, we first employ an LLM to generate a per-bug rubric, followed by a one-time human review and optional refinement to this rubric, and then employ an LLM to judge patches using the refined rubric. We apply this approach to assign binary validity labels to patches for issues found by Google sanitizer tools. Our results show that this approach yields substantial agreement with human consensus (Cohen's kappa 0.75), high recall (0.94) and high precision (0.80), when considering patches that have unanimous agreement from 3 human raters on the validity labels. On the full dataset including patches where human raters disagree, we find this approach can still be further improved (Cohen's kappa 0.57, recall 0.93, precision 0.65) and identify possible future directions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLLM\u548c\u4eba\u5de5\u5faa\u73af\u7684\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u8865\u4e01\u6709\u6548\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u8bc4\u4f30\u6807\u51c6\u5e76\u4eba\u5de5\u5ba1\u6838\u6539\u8fdb\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u8865\u4e01\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4f7f\u7528\u57fa\u4e8e\u6267\u884c\u7684\u8bc4\u4ef7\u65b9\u6cd5\uff08\u5982\u5355\u5143\u6d4b\u8bd5\u901a\u8fc7\u7387\uff09\uff0c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u8865\u4e01\u7684\u771f\u5b9e\u6709\u6548\u6027\uff0c\u4e14\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3002", "method": "\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u6d41\u7a0b\uff1a\u5148\u7528LLM\u751f\u6210\u9488\u5bf9\u6bcf\u4e2a\u6f0f\u6d1e\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u7ecf\u4e00\u6b21\u6027\u4eba\u5de5\u5ba1\u6838\u548c\u4f18\u5316\u540e\uff0c\u518d\u7528LLM\u57fa\u4e8e\u4f18\u5316\u540e\u7684\u6807\u51c6\u5224\u65ad\u8865\u4e01\u6709\u6548\u6027\u3002", "result": "\u5728\u4eba\u7c7b\u8bc4\u5206\u8005\u4e00\u81f4\u540c\u610f\u7684\u8865\u4e01\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4e0e\u4eba\u7c7b\u5171\u8bc6\u9ad8\u5ea6\u4e00\u81f4\uff08Cohen's kappa 0.75\uff09\uff0c\u53ec\u56de\u73870.94\uff0c\u7cbe\u786e\u73870.80\uff1b\u5728\u5305\u542b\u5206\u6b67\u8865\u4e01\u7684\u5168\u6570\u636e\u96c6\u4e0a\uff0c\u6548\u679c\u7a0d\u964d\uff08kappa 0.57\uff0c\u53ec\u56de\u73870.93\uff0c\u7cbe\u786e\u73870.65\uff09\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8865\u4e01\u6709\u6548\u6027\u8bc4\u4f30\u7684\u4eba\u5de5\u6210\u672c\uff0c\u4e14\u8fbe\u6210\u8f83\u9ad8\u53ef\u9760\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u7684\u53ef\u9760\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u5411\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u5206\u6b67\u6848\u4f8b\u7684\u5904\u7406\u3002"}}
{"id": "2511.10712", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10712", "abs": "https://arxiv.org/abs/2511.10712", "authors": ["Qinfeng Li", "Miao Pan", "Jintao Chen", "Fu Teng", "Zhiqiang Shen", "Ge Su", "Hao Peng", "Xuhong Zhang"], "title": "Do Not Merge My Model! Safeguarding Open-Source LLMs Against Unauthorized Model Merging", "comment": "Accepted by AAAI 2026 Conference", "summary": "Model merging has emerged as an efficient technique for expanding large language models (LLMs) by integrating specialized expert models. However, it also introduces a new threat: model merging stealing, where free-riders exploit models through unauthorized model merging. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify three critical protection properties that existing methods fail to simultaneously satisfy: (1) proactively preventing unauthorized merging; (2) ensuring compatibility with general open-source settings; (3) achieving high security with negligible performance loss. To address the above issues, we propose MergeBarrier, a plug-and-play defense that proactively prevents unauthorized merging. The core design of MergeBarrier is to disrupt the Linear Mode Connectivity (LMC) between the protected model and its homologous counterparts, thereby eliminating the low-loss path required for effective model merging. Extensive experiments show that MergeBarrier effectively prevents model merging stealing with negligible accuracy loss.", "AI": {"tldr": "MergeBarrier\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7834\u574f\u7ebf\u6027\u6a21\u5f0f\u8fde\u63a5\u6027\u6765\u4e3b\u52a8\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u5408\u5e76\u7a83\u53d6\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u80fd\u6709\u6548\u9632\u6b62\u6a21\u578b\u5408\u5e76\u7a83\u53d6\u4e14\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002", "motivation": "\u6a21\u578b\u5408\u5e76\u5df2\u6210\u4e3a\u6269\u5c55\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u6280\u672f\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u6a21\u578b\u5408\u5e76\u7a83\u53d6\u7684\u65b0\u5a01\u80c1\uff0c\u5373\u514d\u8d39\u642d\u8f66\u8005\u901a\u8fc7\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u5408\u5e76\u5229\u7528\u6a21\u578b\u3002\u73b0\u6709\u9632\u5fa1\u673a\u5236\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u4e09\u4e2a\u5173\u952e\u4fdd\u62a4\u5c5e\u6027\uff1a\u4e3b\u52a8\u9632\u6b62\u672a\u7ecf\u6388\u6743\u5408\u5e76\u3001\u4e0e\u901a\u7528\u5f00\u6e90\u8bbe\u7f6e\u517c\u5bb9\u3001\u9ad8\u5b89\u5168\u6027\u4e14\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\u3002", "method": "\u63d0\u51fa\u4e86MergeBarrier\uff0c\u5176\u6838\u5fc3\u8bbe\u8ba1\u662f\u7834\u574f\u53d7\u4fdd\u62a4\u6a21\u578b\u4e0e\u5176\u540c\u6e90\u5bf9\u5e94\u7269\u4e4b\u95f4\u7684\u7ebf\u6027\u6a21\u5f0f\u8fde\u63a5\u6027\uff0c\u4ece\u800c\u6d88\u9664\u6709\u6548\u6a21\u578b\u5408\u5e76\u6240\u9700\u7684\u4f4e\u635f\u5931\u8def\u5f84\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMergeBarrier\u80fd\u6709\u6548\u9632\u6b62\u6a21\u578b\u5408\u5e76\u7a83\u53d6\uff0c\u4e14\u51c6\u786e\u6027\u635f\u5931\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "MergeBarrier\u89e3\u51b3\u4e86\u6a21\u578b\u5408\u5e76\u7a83\u53d6\u95ee\u9898\uff0c\u6ee1\u8db3\u4e86\u4e3b\u52a8\u9632\u62a4\u3001\u517c\u5bb9\u6027\u548c\u9ad8\u6027\u80fd\u7684\u5173\u952e\u8981\u6c42\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u4fdd\u969c\u3002"}}
{"id": "2511.10705", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10705", "abs": "https://arxiv.org/abs/2511.10705", "authors": ["Yuan Zhao", "Hualei Zhu", "Tingyu Jiang", "Shen Li", "Xiaohang Xu", "Hao Henry Wang"], "title": "Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents", "comment": "Accepted by AAAI 2026", "summary": "Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.", "AI": {"tldr": "Co-EPG is a self-iterative framework that co-evolves planning and grounding models for GUI task automation. Through iterative training with reward guidance and data generation, it achieves state-of-the-art results on benchmarks without external data.", "motivation": "Current GUI automation methods suffer from insufficient exploitation of cross-model synergies and over-reliance on synthetic data. Co-EPG addresses these limitations by creating a co-evolutionary feedback loop.", "method": "Co-EPG establishes a positive feedback loop where: 1) planning model explores strategies under grounding-based rewards via GRPO, 2) generates diverse training data for grounding model, 3) optimized grounding provides better rewards for planning model improvement.", "result": "On Multimodal-Mind2Web and AndroidControl benchmarks, Co-EPG outperforms state-of-the-art methods after just three iterations, demonstrating consistent improvement with each iteration cycle without external data requirements.", "conclusion": "Co-EPG establishes a novel training paradigm for GUI agents that shifts from isolated optimization to integrated, self-driven co-evolution, enabling robust self-enhancement capabilities."}}
{"id": "2511.10876", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10876", "abs": "https://arxiv.org/abs/2511.10876", "authors": ["Francesco Vitale", "Francesco Flammini", "Mauro Caporuscio", "Nicola Mazzocca"], "title": "Architecting software monitors for control-flow anomaly detection through large language models and conformance checking", "comment": null, "summary": "Context: Ensuring high levels of dependability in modern computer-based systems has become increasingly challenging due to their complexity. Although systems are validated at design time, their behavior can be different at run-time, possibly showing control-flow anomalies due to \"unknown unknowns\".\n  Objective: We aim to detect control-flow anomalies through software monitoring, which verifies run-time behavior by logging software execution and detecting deviations from expected control flow.\n  Methods: We propose a methodology to develop software monitors for control-flow anomaly detection through Large Language Models (LLMs) and conformance checking. The methodology builds on existing software development practices to maintain traditional V&V while providing an additional level of robustness and trustworthiness. It leverages LLMs to link design-time models and implementation code, automating source-code instrumentation. The resulting event logs are analyzed via conformance checking, an explainable and effective technique for control-flow anomaly detection.\n  Results: We test the methodology on a case-study scenario from the European Railway Traffic Management System / European Train Control System (ERTMS/ETCS), which is a railway standard for modern interoperable railways. The results obtained from the ERTMS/ETCS case study demonstrate that LLM-based source-code instrumentation can achieve up to 84.775% control-flow coverage of the reference design-time process model, while the subsequent conformance checking-based anomaly detection reaches a peak performance of 96.610% F1-score and 93.515% AUC.\n  Conclusion: Incorporating domain-specific knowledge to guide LLMs in source-code instrumentation significantly allowed obtaining reliable and quality software logs and enabled effective control-flow anomaly detection through conformance checking.", "AI": {"tldr": "This paper presents a methodology using LLMs for automated source-code instrumentation to detect control-flow anomalies through conformance checking, validated on an ERTMS/ETCS railway case study.", "motivation": "Increasing complexity of computer systems makes dependability challenging; run-time behavior can deviate from design-time validation due to unknown unknowns, requiring monitoring.", "method": "Leverage LLMs to link design models and code for automated instrumentation, generating event logs analyzed via conformance checking for anomaly detection.", "result": "Achieved 84.775% control-flow coverage and peak performance of 96.610% F1-score and 93.515% AUC in anomaly detection on ERTMS/ETCS case study.", "conclusion": "Domain-specific knowledge guiding LLMs enables reliable software logs and effective control-flow anomaly detection via conformance checking."}}
{"id": "2511.10714", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10714", "abs": "https://arxiv.org/abs/2511.10714", "authors": ["Shuaitong Liu", "Renjue Li", "Lijia Yu", "Lijun Zhang", "Zhiming Liu", "Gaojie Jin"], "title": "BadThink: Triggered Overthinking Attacks on Chain-of-Thought Reasoning in Large Language Models", "comment": "Accepted at AAAI 2026 (Main Track). This arXiv version corresponds to the camera-ready manuscript and includes expanded appendices. Please cite the AAAI 2026 version when available", "summary": "Recent advances in Chain-of-Thought (CoT) prompting have substantially improved the reasoning capabilities of large language models (LLMs), but have also introduced their computational efficiency as a new attack surface. In this paper, we propose BadThink, the first backdoor attack designed to deliberately induce \"overthinking\" behavior in CoT-enabled LLMs while ensuring stealth. When activated by carefully crafted trigger prompts, BadThink manipulates the model to generate inflated reasoning traces - producing unnecessarily redundant thought processes while preserving the consistency of final outputs. This subtle attack vector creates a covert form of performance degradation that significantly increases computational costs and inference time while remaining difficult to detect through conventional output evaluation methods. We implement this attack through a sophisticated poisoning-based fine-tuning strategy, employing a novel LLM-based iterative optimization process to embed the behavior by generating highly naturalistic poisoned data. Our experiments on multiple state-of-the-art models and reasoning tasks show that BadThink consistently increases reasoning trace lengths - achieving an over 17x increase on the MATH-500 dataset - while remaining stealthy and robust. This work reveals a critical, previously unexplored vulnerability where reasoning efficiency can be covertly manipulated, demonstrating a new class of sophisticated attacks against CoT-enabled systems.", "AI": {"tldr": "BadThink\u653b\u51fb\u662f\u9488\u5bf9\u601d\u7ef4\u94fe\u63d0\u793a\u6cd5\u7684\u9996\u4e2a\u540e\u95e8\u653b\u51fb\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u89e6\u53d1\u63d0\u793a\u8bf1\u5bfcLLMs\u4ea7\u751f\u8fc7\u5ea6\u63a8\u7406\u884c\u4e3a\uff0c\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u800c\u4e0d\u6539\u53d8\u6700\u7ec8\u8f93\u51fa\u7ed3\u679c", "motivation": "\u601d\u7ef4\u94fe\u63d0\u793a\u6cd5\u7684\u8fdb\u6b65\u867d\u7136\u63d0\u5347\u4e86LLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u8ba1\u7b97\u6548\u7387\u4f5c\u4e3a\u65b0\u7684\u653b\u51fb\u9762\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u9690\u79d8\u5730\u64cd\u7eb5\u63a8\u7406\u6548\u7387", "method": "\u91c7\u7528\u57fa\u4e8e\u4e2d\u6bd2\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u751f\u6210\u9ad8\u5ea6\u81ea\u7136\u7684\u6bd2\u5316\u6570\u636e\u6765\u5d4c\u5165\u8fc7\u5ea6\u63a8\u7406\u884c\u4e3a", "result": "\u5728\u591a\u6a21\u578b\u548c\u63a8\u7406\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0cBadThink\u80fd\u6301\u7eed\u589e\u52a0\u63a8\u7406\u75d5\u8ff9\u957f\u5ea6\uff08\u5728MATH-500\u6570\u636e\u96c6\u4e0a\u8fbe\u523017\u500d\u4ee5\u4e0a\u589e\u957f\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u9690\u853d\u6027\u548c\u9c81\u68d2\u6027", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63ed\u793a\u4e86\u4e00\u4e2a\u5173\u952e\u7684\u3001\u5148\u524d\u672a\u88ab\u63a2\u7d22\u7684\u8106\u5f31\u6027\uff0c\u5c55\u793a\u4e86\u9488\u5bf9\u601d\u7ef4\u94fe\u7cfb\u7edf\u7684\u65b0\u578b\u590d\u6742\u653b\u51fb\u7c7b\u522b\uff0c\u8bc1\u660e\u63a8\u7406\u6548\u7387\u53ef\u4ee5\u88ab\u9690\u79d8\u64cd\u63a7"}}
{"id": "2511.10720", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10720", "abs": "https://arxiv.org/abs/2511.10720", "authors": ["Runpeng Geng", "Yanting Wang", "Chenlong Yin", "Minhao Cheng", "Ying Chen", "Jinyuan Jia"], "title": "PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization", "comment": "The code is available at https://github.com/sleeepeer/PISanitizer", "summary": "Long context LLMs are vulnerable to prompt injection, where an attacker can inject an instruction in a long context to induce an LLM to generate an attacker-desired output. Existing prompt injection defenses are designed for short contexts. When extended to long-context scenarios, they have limited effectiveness. The reason is that an injected instruction constitutes only a very small portion of a long context, making the defense very challenging. In this work, we propose PISanitizer, which first pinpoints and sanitizes potential injected tokens (if any) in a context before letting a backend LLM generate a response, thereby eliminating the influence of the injected instruction. To sanitize injected tokens, PISanitizer builds on two observations: (1) prompt injection attacks essentially craft an instruction that compels an LLM to follow it, and (2) LLMs intrinsically leverage the attention mechanism to focus on crucial input tokens for output generation. Guided by these two observations, we first intentionally let an LLM follow arbitrary instructions in a context and then sanitize tokens receiving high attention that drive the instruction-following behavior of the LLM. By design, PISanitizer presents a dilemma for an attacker: the more effectively an injected instruction compels an LLM to follow it, the more likely it is to be sanitized by PISanitizer. Our extensive evaluation shows that PISanitizer can successfully prevent prompt injection, maintain utility, outperform existing defenses, is efficient, and is robust to optimization-based and strong adaptive attacks. The code is available at https://github.com/sleeepeer/PISanitizer.", "AI": {"tldr": "PISanitizer is a defense against prompt injection in long-context LLMs that identifies and removes malicious tokens by leveraging attention mechanisms, ensuring security without compromising utility.", "motivation": "Existing defenses against prompt injection are ineffective for long contexts because injected instructions constitute a small portion of the input, making detection difficult. The work aims to address this gap by developing a method that can reliably sanitize long contexts.", "method": "PISanitizer works by first inducing the LLM to follow arbitrary instructions in the context, then identifying and removing tokens that receive high attention during this process\u2014effectively sanitizing potential injected instructions.", "result": "Evaluation shows PISanitizer successfully prevents prompt injection, maintains utility, outperforms existing defenses, is efficient, and resists adaptive attacks.", "conclusion": "PISanitizer provides an effective solution to long-context prompt injection by turning the attack's strength against itself, with code publicly available for further use and development."}}
{"id": "2511.10767", "categories": ["cs.AI", "cs.CC"], "pdf": "https://arxiv.org/pdf/2511.10767", "abs": "https://arxiv.org/abs/2511.10767", "authors": ["Yasir Mahmood", "Markus Hecher", "Johanna Groven", "Johannes K. Fichte"], "title": "Structure-Aware Encodings of Argumentation Properties for Clique-width", "comment": "Technical report of paper accepted at AAAI 2026", "summary": "Structural measures of graphs, such as treewidth, are central tools in computational complexity resulting in efficient algorithms when exploiting the parameter. It is even known that modern SAT solvers work efficiently on instances of small treewidth. Since these solvers are widely applied, research interests in compact encodings into (Q)SAT for solving and to understand encoding limitations. Even more general is the graph parameter clique-width, which unlike treewidth can be small for dense graphs. Although algorithms are available for clique-width, little is known about encodings. We initiate the quest to understand encoding capabilities with clique-width by considering abstract argumentation, which is a robust framework for reasoning with conflicting arguments. It is based on directed graphs and asks for computationally challenging properties, making it a natural candidate to study computational properties. We design novel reductions from argumentation problems to (Q)SAT. Our reductions linearly preserve the clique-width, resulting in directed decomposition-guided (DDG) reductions. We establish novel results for all argumentation semantics, including counting. Notably, the overhead caused by our DDG reductions cannot be significantly improved under reasonable assumptions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e2\u5bbd\u5ea6\u7684\u62bd\u8c61\u8bba\u8bc1\u95ee\u9898\u7684(Q)SAT\u7f16\u7801\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u7f16\u7801\u590d\u6742\u5ea6\u7684\u4e0b\u9650\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6811\u5bbd\u53c2\u6570\uff0c\u4f46\u56e2\u5bbd\u5ea6\u80fd\u66f4\u597d\u5904\u7406\u7a20\u5bc6\u56fe\u3002\u62bd\u8c61\u8bba\u8bc1\u6846\u67b6\u57fa\u4e8e\u6709\u5411\u56fe\u4e14\u8ba1\u7b97\u590d\u6742\uff0c\u662f\u7814\u7a76\u8ba1\u7b97\u7279\u6027\u7684\u7406\u60f3\u5019\u9009\u3002\u76ee\u524d\u7f3a\u5c11\u5bf9\u56e2\u5bbd\u5ea6\u7f16\u7801\u80fd\u529b\u7684\u7406\u89e3\u3002", "method": "\u8bbe\u8ba1\u4e86\u4ece\u8bba\u8bc1\u95ee\u9898\u5230(Q)SAT\u7684\u65b0\u5f52\u7ea6\u65b9\u6cd5\u2014\u2014\u6709\u5411\u5206\u89e3\u5f15\u5bfc(DDG)\u5f52\u7ea6\uff0c\u80fd\u7ebf\u6027\u4fdd\u6301\u56e2\u5bbd\u5ea6\u3002", "result": "\u4e3a\u6240\u6709\u8bba\u8bc1\u8bed\u4e49\uff08\u5305\u62ec\u8ba1\u6570\uff09\u5efa\u7acb\u4e86\u65b0\u7ed3\u679c\uff0c\u8bc1\u660eDDG\u5f52\u7ea6\u7684\u5f00\u9500\u5728\u5408\u7406\u5047\u8bbe\u4e0b\u65e0\u6cd5\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f00\u542f\u4e86\u7406\u89e3\u56e2\u5bbd\u5ea6\u7f16\u7801\u80fd\u529b\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u590d\u6742\u56fe\u53c2\u6570\u5728SAT\u6c42\u89e3\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2511.11125", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11125", "abs": "https://arxiv.org/abs/2511.11125", "authors": ["Salim Fares", "Steffen Herbold"], "title": "Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs", "comment": "Submitted to the International Conference on Software Engineering (ICSE) track Software Engineering in Practice (SEIP) 2026", "summary": "How to best use Large Language Models (LLMs) for software engineering is covered in many publications in recent years. However, most of this work focuses on widely-used general purpose programming languages. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, is still underexplored. Within this paper, we study enterprises can achieve on their own without investing large amounts of effort into the training of models specific to the domain-specific languages that are used. We show that few-shot prompting approaches are sufficient to solve simple problems in a language that is otherwise not well-supported by an LLM and that is possible on-premise, thereby ensuring the protection of sensitive company data.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u5c11\u6837\u672c\u63d0\u793a\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u5de5\u4e1a\u8fc7\u7a0b\u81ea\u52a8\u5316\u9886\u57df\u4e2d\u4e13\u4e1a\u7f16\u7a0b\u8bed\u8a00\u7684LLM\u5e94\u7528\u95ee\u9898\uff0c\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6295\u5165\u5373\u53ef\u5b9e\u73b0\u672c\u5730\u90e8\u7f72\u7684\u6570\u636e\u5b89\u5168\u4fdd\u62a4", "motivation": "\u5f53\u524dLLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u901a\u7528\u7f16\u7a0b\u8bed\u8a00\uff0c\u5bf9\u4e8e\u5de5\u4e1a\u8fc7\u7a0b\u81ea\u52a8\u5316\u9886\u57df\u4e2d\u4e13\u6709\u7684\u9ad8\u5ea6\u4e13\u4e1a\u5316\u8bed\u8a00\u7684\u5e94\u7528\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22", "method": "\u91c7\u7528\u5c11\u6837\u672c\u63d0\u793a\u65b9\u6cd5\uff0c\u7814\u7a76\u4f01\u4e1a\u5982\u4f55\u5728\u65e0\u9700\u6295\u5165\u5927\u91cf\u8d44\u6e90\u8bad\u7ec3\u7279\u5b9a\u9886\u57df\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u72ec\u7acb\u89e3\u51b3\u95ee\u9898", "result": "\u8bc1\u660e\u5c11\u6837\u672c\u63d0\u793a\u65b9\u6cd5\u8db3\u4ee5\u89e3\u51b3\u5728LLM\u652f\u6301\u8f83\u5dee\u7684\u4e13\u4e1a\u8bed\u8a00\u4e2d\u7684\u7b80\u5355\u95ee\u9898", "conclusion": "\u5c11\u6837\u672c\u63d0\u793a\u65b9\u6cd5\u4e3a\u5de5\u4e1a\u81ea\u52a8\u5316\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684LLM\u5e94\u7528\u65b9\u6848\uff0c\u65e2\u80fd\u6709\u6548\u89e3\u51b3\u95ee\u9898\uff0c\u53c8\u80fd\u786e\u4fdd\u654f\u611f\u516c\u53f8\u6570\u636e\u7684\u672c\u5730\u5316\u5b89\u5168\u4fdd\u62a4"}}
{"id": "2511.10828", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10828", "abs": "https://arxiv.org/abs/2511.10828", "authors": ["Weiheng Bai", "Kefu Wu", "Qiushi Wu", "Kangjie Lu"], "title": "AFLGopher: Accelerating Directed Fuzzing via Feasibility-Aware Guidance", "comment": null, "summary": "Directed fuzzing is a useful testing technique that aims to efficiently reach target code sites in a program. The core of directed fuzzing is the guiding mechanism that directs the fuzzing to the specified target. A general guiding mechanism adopted in existing directed fuzzers is to calculate the control-flow distance between the current progress and the target, and use that as feedback to guide the directed fuzzing. A fundamental problem with the existing guiding mechanism is that the distance calculation is \\emph{feasibility-unaware}.\n  In this work, we propose feasibility-aware directed fuzzing named AFLGopher. Our new feasibility-aware distance calculation provides pragmatic feedback to guide directed fuzzing to reach targets efficiently. We propose new techniques to address the challenges of feasibility prediction. Our new classification method allows us to predict the feasibility of all branches based on limited traces, and our runtime feasibility-updating mechanism gradually and efficiently improves the prediction precision. We implemented AFLGopher and compared AFLGopher with state-of-the-art directed fuzzers including AFLGo, enhanced AFLGo, WindRanger, BEACON and SelectFuzz. AFLGopher is 3.76x, 2.57x, 3.30x, 2.52x and 2.86x faster than AFLGo, BEACON, WindRanger, SelectFuzz and enhanced AFLGo, respectively, in reaching targets. AFLGopher is 5.60x, 5.20x, 4.98x, 4.52x, and 5.07x faster than AFLGo, BEACON, WindRanger, SelectFuzz and enhanced AFLGo, respectively, in triggering known vulnerabilities.", "AI": {"tldr": "AFLGopher is a directed fuzzing technique that uses feasibility-aware distance calculation to efficiently reach target code sites, showing significant performance improvements over existing state-of-the-art fuzzers.", "motivation": "Existing directed fuzzing approaches use control-flow distance calculation that is unaware of feasibility, leading to inefficient guidance towards targets.", "method": "Proposes feasibility-aware distance calculation with classification based on limited traces and runtime feasibility-updating mechanism to improve prediction precision.", "result": "AFLGopher reaches targets 2.52-3.76x faster and triggers vulnerabilities 4.52-5.60x faster than state-of-the-art fuzzers.", "conclusion": "Feasibility-awareness significantly improves directed fuzzing efficiency, making AFLGopher a substantial advancement in the field."}}
{"id": "2511.10776", "categories": ["cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.10776", "abs": "https://arxiv.org/abs/2511.10776", "authors": ["Yuta Kawakami", "Jin Tian"], "title": "Potential Outcome Rankings for Counterfactual Decision Making", "comment": null, "summary": "Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.", "AI": {"tldr": "\u8bba\u6587\u5f15\u5165\u4e24\u79cd\u65b0\u6307\u6807(PoR\u548cPoB)\u7528\u4e8e\u53cd\u4e8b\u5b9e\u51b3\u7b56\uff0c\u5efa\u7acb\u4e86\u8bc6\u522b\u5b9a\u7406\u548c\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u7684\u53cd\u4e8b\u5b9e\u51b3\u7b56\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u671f\u671b\u6f5c\u5728\u7ed3\u679c\u7684\u6392\u5e8f\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4e2a\u4f53\u5c42\u9762\u7ed3\u679c\u6392\u540d\u6982\u7387\u7684\u91cf\u5316\u6307\u6807\u3002", "method": "\u5f15\u5165PoR(\u6f5c\u5728\u7ed3\u679c\u6392\u540d\u6982\u7387)\u548cPoB(\u83b7\u5f97\u6700\u4f73\u7ed3\u679c\u6982\u7387)\u4e24\u4e2a\u65b0\u6307\u6807\uff0c\u5efa\u7acb\u8bc6\u522b\u5b9a\u7406\u3001\u63a8\u5bfc\u8fb9\u754c\u5e76\u63d0\u51fa\u4f30\u8ba1\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4f30\u8ba1\u5668\u7684\u6709\u9650\u6837\u672c\u6027\u8d28\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5e94\u7528\u6548\u679c\u3002", "conclusion": "PoR\u548cPoB\u4e3a\u65b0\u51b3\u7b56\u89c4\u5219\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u91cf\u5316\u4e2a\u4f53\u5c42\u9762\u7684\u51b3\u7b56\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2511.11265", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11265", "abs": "https://arxiv.org/abs/2511.11265", "authors": ["Mikel Robredo", "Matteo Esposito", "Davide Taibi", "Rafael Pe\u00f1aloza", "Valentina Lenarduzzi"], "title": "SQuaD: The Software Quality Dataset", "comment": null, "summary": "Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).", "AI": {"tldr": "SQuaD is a comprehensive software quality dataset integrating 700+ metrics from 9 tools across 450 open-source projects, enabling large-scale research on software maintainability, technical debt, and evolution.", "motivation": "Existing software quality datasets are limited in scope, focusing on singular aspects like code smells or refactoring, which restricts comprehensive multi-dimensional analysis across time and quality domains.", "method": "Integrated 9 static analysis tools (SonarQube, CodeScene, PMD, etc.) to extract metrics from 450 mature open-source projects across multiple ecosystems, covering 63,586 project releases with version control and vulnerability data.", "result": "Created a unified dataset with method, class, file, and project-level metrics, plus process metrics that enhance defect prediction, publicly available on ZENODO.", "conclusion": "SQuaD enables unprecedented empirical research on software quality at scale and outlines directions for automated updates and cross-project quality modeling."}}
{"id": "2511.10863", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.10863", "abs": "https://arxiv.org/abs/2511.10863", "authors": ["Yiping Ma", "Yue Guo", "Harish Karthikeyan", "Antigoni Polychroniadou"], "title": "Armadillo: Robust Single-Server Secure Aggregation for Federated Learning with Input Validation", "comment": null, "summary": "This paper presents a secure aggregation system Armadillo that has disruptive resistance against adversarial clients, such that any coalition of malicious clients (within the tolerated threshold) can affect the aggregation result only by misreporting their private inputs in a pre-defined legitimate range. Armadillo is designed for federated learning setting, where a single powerful server interacts with many weak clients iteratively to train models on client's private data. While a few prior works consider disruption resistance under such setting, they either incur high per-client cost (Chowdhury et al. CCS '22) or require many rounds (Bell et al. USENIX Security '23). Although disruption resistance can be achieved generically with zero-knowledge proof techniques (which we also use in this paper), we realize an efficient system with two new designs: 1) a simple two-layer secure aggregation protocol that requires only simple arithmetic computation; 2) an agreement protocol that removes the effect of malicious clients from the aggregation with low round complexity. With these techniques, Armadillo completes each secure aggregation in 3 rounds while keeping the server and clients computationally lightweight.", "AI": {"tldr": "Armadillo is a secure federated learning aggregation system that provides disruption resistance against adversarial clients in just 3 rounds with lightweight computation.", "motivation": "Existing secure aggregation methods either have high per-client computational costs or require many communication rounds, creating a need for an efficient solution with strong security guarantees.", "method": "Uses a two-layer secure aggregation protocol requiring only simple arithmetic operations and an agreement protocol to remove malicious clients' influence with minimal rounds.", "result": "Armadillo achieves secure aggregation in just 3 communication rounds while keeping both server and client computational overhead low.", "conclusion": "The system demonstrates that efficient disruption-resistant secure aggregation is feasible in federated learning without heavy cryptographic overhead."}}
{"id": "2511.10788", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10788", "abs": "https://arxiv.org/abs/2511.10788", "authors": ["Chao Wu", "Baoheng Li", "Mingchen Gao", "Zhenyi Wang"], "title": "From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models", "comment": null, "summary": "Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.", "AI": {"tldr": "This survey reframes LLM reasoning through adaptivity - allocating reasoning effort based on input complexity - and provides a taxonomy of training-based and training-free adaptive reasoning methods.", "motivation": "Current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing on difficult tasks, highlighting the need for adaptive reasoning.", "method": "Formalizes reasoning types (deductive, inductive, abductive) and adaptive reasoning as control-augmented policy optimization, with a taxonomy dividing methods into training-based (reinforcement learning, supervised fine-tuning) and training-free (prompt conditioning, feedback-driven halting) approaches.", "result": "A systematic framework for understanding and comparing adaptive reasoning strategies in LLMs, connecting classical cognitive paradigms with algorithmic implementations.", "conclusion": "Identifies open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control for future adaptive reasoning research."}}
{"id": "2511.11411", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11411", "abs": "https://arxiv.org/abs/2511.11411", "authors": ["Xingshuang Lin", "Binbin Zhao", "Jinwen Wang", "Qinge Xie", "Xibin Zhao", "Shouling Ji"], "title": "SCRUTINEER: Detecting Logic-Level Usage Violations of Reusable Components in Smart Contracts", "comment": null, "summary": "Smart Contract Reusable Components(SCRs) play a vital role in accelerating the development of business-specific contracts by promoting modularity and code reuse. However, the risks associated with SCR usage violations have become a growing concern. One particular type of SCR usage violation, known as a logic-level usage violation, is becoming especially harmful. This violation occurs when the SCR adheres to its specified usage rules but fails to align with the specific business logic of the current context, leading to significant vulnerabilities. Detecting such violations necessitates a deep semantic understanding of the contract's business logic, including the ability to extract implicit usage patterns and analyze fine-grained logical behaviors. To address these challenges, we propose SCRUTINEER, the first automated and practical system for detecting logic-level usage violations of SCRs. First, we design a composite feature extraction approach that produces three complementary feature representations, supporting subsequent analysis. We then introduce a Large Language Model-powered knowledge construction framework, which leverages comprehension-oriented prompts and domain-specific tools to extract logic-level usage and build the SCR knowledge base. Next, we develop a Retrieval-Augmented Generation-driven inspector, which combines a rapid retrieval strategy with both comprehensive and targeted analysis to identify potentially insecure logic-level usages. Finally, we implement a logic-level usage violation analysis engine that integrates a similarity-based checker and a snapshot-based inference conflict checker to enable accurate and robust detection. We evaluate SCRUTINEER from multiple perspectives on 3 ground-truth datasets. The results show that SCRUTINEER achieves a precision of 80.77%, a recall of 82.35%, and an F1-score of 81.55% in detecting logic-level usage violations of SCRs.", "AI": {"tldr": "SCRUTINEER is an automated system that detects logic-level usage violations in Smart Contract Reusable Components by combining composite feature extraction, LLM-powered knowledge construction, and retrieval-augmented generation with conflict checking.", "motivation": "Logic-level usage violations in SCRs occur when components follow technical specifications but misalign with business logic, creating vulnerabilities that require deep semantic understanding to detect.", "method": "Uses composite feature extraction, LLM-powered knowledge construction with domain tools, retrieval-augmented generation inspection, and similarity/snapshot-based conflict checking.", "result": "Achieved 80.77% precision, 82.35% recall, and 81.55% F1-score in detecting logic-level SCR usage violations across 3 datasets.", "conclusion": "SCRUTINEER effectively addresses the challenge of detecting subtle logic-level violations in smart contract components through its multi-stage analysis approach."}}
{"id": "2511.10933", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.10933", "abs": "https://arxiv.org/abs/2511.10933", "authors": ["Yunyi Ni", "Ziyu Yang", "Ze Niu", "Emily Davis", "Finn Carter"], "title": "On the Information-Theoretic Fragility of Robust Watermarking under Diffusion Editing", "comment": null, "summary": "Robust invisible watermarking embeds hidden information in images such that the watermark can survive various manipulations. However, the emergence of powerful diffusion-based image generation and editing techniques poses a new threat to these watermarking schemes. In this paper, we investigate the intersection of diffusion-based image editing and robust image watermarking. We analyze how diffusion-driven image edits can significantly degrade or even fully remove embedded watermarks from state-of-the-art robust watermarking systems. Both theoretical formulations and empirical experiments are provided. We prove that as a image undergoes iterative diffusion transformations, the mutual information between the watermarked image and the embedded payload approaches zero, causing watermark decoding to fail. We further propose a guided diffusion attack algorithm that explicitly targets and erases watermark signals during generation. We evaluate our approach on recent deep learning-based watermarking schemes and demonstrate near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Finally, we discuss ethical implications of such watermark removal capablities and provide design guidelines for future watermarking strategies to be more resilient in the era of generative AI.", "AI": {"tldr": "Diffusion-based image editing poses a significant threat to robust watermarking systems, as it can effectively remove watermarks while maintaining image quality, raising ethical concerns and requiring new resilient watermarking strategies.", "motivation": "The emergence of powerful diffusion-based image generation and editing techniques threatens the integrity of existing robust invisible watermarking systems, potentially allowing watermark removal while preserving image fidelity.", "method": "The research combines theoretical analysis of mutual information loss during diffusion transformations with empirical evaluations using a guided diffusion attack algorithm targeting watermark signals.", "result": "Experiments show near-zero watermark recovery rates after attack on state-of-the-art watermarking schemes, while maintaining high visual fidelity in regenerated images.", "conclusion": "The study highlights the vulnerability of current watermarking to diffusion-based attacks, underscores ethical implications, and provides design guidelines for creating more resilient watermarking strategies in the generative AI era."}}
{"id": "2511.11550", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.11550", "abs": "https://arxiv.org/abs/2511.11550", "authors": ["J. Antonio Dantas Macedo", "Hugo Fernandes", "J. Eduardo Ferreira Ribeiro"], "title": "CertiA360: Enhance Compliance Agility in Aerospace Software Development", "comment": "This paper consists of 8 pages and includes 2 figures", "summary": "Agile methods are characterised by iterative and incremental processes with a strong focus on flexibility and accommodating changing requirements based on either technical, regulatory, or stakeholder feedback. However, integrating Agile methods into safety-critical system development in the aerospace industry presents substantial challenges due to its strict compliance requirements, such as those outlined in the DO-178C standard. To achieve this vision, the flexibility of Agile must align with the rigorous certification guidelines, which emphasize documentation, traceability of requirements across different levels and disciplines, and comprehensive verification and validation (V&V) activities. The research work described in this paper proposes a way of using the strengths of the flexible nature of Agile methods to automate and manage change requests throughout the whole software development lifecycle, ensuring robust traceability, regulatory compliance and ultimately facilitating successful certification. This study proposes CertiA360, a tool designed to help teams improve requirement maturity, automate the changes in traceability, and align with the regulatory objectives. The tool was designed and validated in close collaboration with aerospace industry experts, using their feedback to ensure practical application and real-life effectiveness. The feedback collected demonstrated that the automation given by CertiA360 may reduce manual effort and allow response to changing requirements while ensuring compliance with DO-178C. While the tool is not yet qualified under DO-330 (Tool Qualification), findings suggest that when tailored appropriately, Agile methods can not only coexist with the requirements of safety-system development and certification in highly regulated domains like aerospace, but also add efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCertiA360\u5de5\u5177\uff0c\u5c06\u654f\u6377\u65b9\u6cd5\u5e94\u7528\u4e8e\u822a\u7a7a\u822a\u5929\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u5f00\u53d1\uff0c\u89e3\u51b3DO-178C\u6807\u51c6\u5408\u89c4\u6311\u6218\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u53d8\u66f4\u7ba1\u7406\u548c\u8ffd\u8e2a\u63d0\u5347\u6548\u7387", "motivation": "\u5c06\u654f\u6377\u65b9\u6cd5\u7684\u7075\u6d3b\u6027\u6574\u5408\u5230\u9700\u8981\u4e25\u683c\u5408\u89c4\u7684\u822a\u7a7a\u822a\u5929\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u5f00\u53d1\u4e2d\uff0c\u89e3\u51b3\u654f\u6377\u65b9\u6cd5\u4e0eDO-178C\u6807\u51c6\u8981\u6c42\u7684\u51b2\u7a81", "method": "\u8bbe\u8ba1\u4e0e\u9a8c\u8bc1CertiA360\u5de5\u5177\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u9700\u6c42\u6210\u719f\u5ea6\u7ba1\u7406\u548c\u8ffd\u8e2a\u53d8\u66f4\uff0c\u4e0e\u884c\u4e1a\u4e13\u5bb6\u5408\u4f5c\u786e\u4fdd\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027", "result": "CertiA360\u80fd\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u54cd\u5e94\u9700\u6c42\u53d8\u66f4\uff0c\u540c\u65f6\u786e\u4fddDO-178C\u5408\u89c4\u6027\uff0c\u663e\u793a\u654f\u6377\u65b9\u6cd5\u53ef\u5728\u4e25\u683c\u76d1\u7ba1\u9886\u57df\u63d0\u9ad8\u6548\u7387", "conclusion": "\u9002\u5f53\u8c03\u6574\u7684\u654f\u6377\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u4e0e\u5b89\u5168\u7cfb\u7edf\u5f00\u53d1\u8981\u6c42\u5171\u5b58\uff0c\u8fd8\u80fd\u5728\u822a\u7a7a\u822a\u5929\u7b49\u9ad8\u5ea6\u76d1\u7ba1\u9886\u57df\u589e\u52a0\u6548\u7387"}}
{"id": "2511.10992", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.10992", "abs": "https://arxiv.org/abs/2511.10992", "authors": ["Jeuk Kang", "Jungheum Park"], "title": "Gynopticon: Consensus-Based Cheating Detection System for Competitive Games", "comment": null, "summary": "Cheating in online games poses significant threats to the gaming industry, yet most prior research has concentrated on Massively Multiplayer Online Role-Playing Games (MMORPGs). Competitive genres-such as Multiplayer Online Battle Arena (MOBA), First Person Shooter (FPS), Real Time Strategy (RTS), and Action games-remain underexplored due to the difficulty of detecting cheating users and the demand for complex data and techniques. To address this gap, many game companies rely on kernel-level anti-cheat solutions, which, while effective, raise serious concerns regarding user privacy and system security. In this paper, we propose GYNOPTICON, a novel cheating detection framework that leverages user consensus to identify abnormal behavior. GYNOPTICON integrates a lightweight client-side detection mechanism with a server-side voting system: when suspicious activity is identified, clients cast votes to the server, which aggregates them to establish consensus and distinguish cheaters from legitimate players. This architecture enables transparency, reduces reliance on intrusive monitoring, and mitigates privacy risks. We evaluate GYNOPTICON in both a controlled simulation and a real-world FPS environment. Simulation results verify its feasibility and requirements, while real-world experiments confirm its effectiveness in reliably detecting cheating users. Furthermore, we demonstrate the system's applicability and sustainability for long-term game management using public datasets. GYNOPTICON represents a user-driven, consensus-based alternative to conventional anti-cheat systems, offering a practical and privacy-preserving solution for competitive online games.", "AI": {"tldr": "GYNOPTICON\u662f\u4e00\u4e2a\u57fa\u4e8e\u7528\u6237\u5171\u8bc6\u7684\u65b0\u578b\u4f5c\u5f0a\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u548c\u670d\u52a1\u5668\u7aef\u6295\u7968\u7cfb\u7edf\u8bc6\u522b\u5f02\u5e38\u884c\u4e3a\uff0c\u4e3a\u7ade\u6280\u6e38\u620f\u63d0\u4f9b\u9690\u79c1\u4fdd\u62a4\u7684\u53cd\u4f5c\u5f0a\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u7ade\u6280\u7c7b\u6e38\u620f\u4f5c\u5f0a\u68c0\u6d4b\u7814\u7a76\u4e0d\u8db3\uff0c\u4f20\u7edf\u5185\u6838\u7ea7\u53cd\u4f5c\u5f0a\u65b9\u6848\u5b58\u5728\u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u900f\u660e\u3001\u9690\u79c1\u53cb\u597d\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faGYNOPTICON\u6846\u67b6\uff0c\u7ed3\u5408\u5ba2\u6237\u7aef\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u673a\u5236\u548c\u670d\u52a1\u5668\u7aef\u6295\u7968\u7cfb\u7edf\uff0c\u901a\u8fc7\u7528\u6237\u5171\u8bc6\u8bc6\u522b\u5f02\u5e38\u884c\u4e3a\u3002", "result": "\u5728\u63a7\u5236\u6a21\u62df\u548c\u771f\u5b9eFPS\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff0c\u80fd\u591f\u53ef\u9760\u68c0\u6d4b\u4f5c\u5f0a\u7528\u6237\u3002", "conclusion": "GYNOPTICON\u4e3a\u7528\u6237\u9a71\u52a8\u7684\u5171\u8bc6\u578b\u53cd\u4f5c\u5f0a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u957f\u671f\u6e38\u620f\u7ba1\u7406\u3002"}}
{"id": "2511.10842", "categories": ["cs.AI", "cs.DB", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.10842", "abs": "https://arxiv.org/abs/2511.10842", "authors": ["Jugal Gajjar", "Kaustik Ranaware", "Kamalasankari Subramaniakuppusamy", "Vaibhav Gandhi"], "title": "HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings", "comment": "9 pages, 3 figures, 8 tables, 19 equations, accepted at the 5th Workshop on Knowledge Graphs and Big Data in IEEE BigData 2025 and the paper will be published in the IEEE BigData Conference Proceedings", "summary": "Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.", "AI": {"tldr": "HyperComplEx is a hybrid knowledge graph embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms, achieving superior performance on large-scale graphs.", "motivation": "Existing embedding methods have limitations in modeling diverse relationship types at scale - Euclidean struggles with hierarchies, vector space models can't capture asymmetry, and hyperbolic fails on symmetric relations.", "method": "Uses relation-specific space weighting to dynamically select optimal geometries for each relation type, with multi-space consistency loss for coherent predictions across spaces.", "result": "Achieves 0.612 MRR on 10M-paper dataset (4.8% improvement over best baseline), 85ms inference per triple, and scales near-linearly with graph size.", "conclusion": "HyperComplEx demonstrates consistent improvements over state-of-the-art baselines and enables scalable knowledge graph embeddings through adaptive geometry selection."}}
{"id": "2511.10853", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.10853", "abs": "https://arxiv.org/abs/2511.10853", "authors": ["Gerui Xu", "Boyou Chen", "Huizhong Guo", "Dave LeBlanc", "Ananna Ahmed", "Zhaonan Sun", "Shan Bao"], "title": "Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction", "comment": "26 pages, 10 figures", "summary": "Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.", "AI": {"tldr": "This study presents a multi-agent AI framework that reconstructs traffic collision scenarios from multimodal data, achieving perfect accuracy in identifying relevant events and vehicle roles, surpassing human expert performance.", "motivation": "Traditional traffic collision reconstruction relying on human expertise often yields inconsistent results when analyzing incomplete multimodal data, highlighting the need for more reliable automated solutions.", "method": "A two-phase collaborative framework combining reconstruction and reasoning phases, processing multimodal data including crash reports, tabular data, visual diagrams, and EDR records from 277 rear-end collisions.", "result": "The framework achieved perfect accuracy across all 39 complex test cases, successfully identifying EDR events and vehicle roles, surpassing the 92% accuracy of human researchers and maintaining robust performance with incomplete data.", "conclusion": "The study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors."}}
{"id": "2511.11020", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11020", "abs": "https://arxiv.org/abs/2511.11020", "authors": ["Farhad Abtahi", "Fernando Seoane", "Iv\u00e1n Pau", "Mario Vega-Barbas"], "title": "Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis", "comment": null, "summary": "Healthcare AI systems face major vulnerabilities to data poisoning that current defenses and regulations cannot adequately address. We analyzed eight attack scenarios in four categories: architectural attacks on convolutional neural networks, large language models, and reinforcement learning agents; infrastructure attacks exploiting federated learning and medical documentation systems; critical resource allocation attacks affecting organ transplantation and crisis triage; and supply chain attacks targeting commercial foundation models. Our findings indicate that attackers with access to only 100-500 samples can compromise healthcare AI regardless of dataset size, often achieving over 60 percent success, with detection taking an estimated 6 to 12 months or sometimes not occurring at all. The distributed nature of healthcare infrastructure creates many entry points where insiders with routine access can launch attacks with limited technical skill. Privacy laws such as HIPAA and GDPR can unintentionally shield attackers by restricting the analyses needed for detection. Supply chain weaknesses allow a single compromised vendor to poison models across 50 to 200 institutions. The Medical Scribe Sybil scenario shows how coordinated fake patient visits can poison data through legitimate clinical workflows without requiring a system breach. Current regulations lack mandatory adversarial robustness testing, and federated learning can worsen risks by obscuring attribution. We recommend multilayer defenses including required adversarial testing, ensemble-based detection, privacy-preserving security mechanisms, and international coordination on AI security standards. We also question whether opaque black-box models are suitable for high-stakes clinical decisions, suggesting a shift toward interpretable systems with verifiable safety guarantees.", "AI": {"tldr": "\u533b\u7597AI\u7cfb\u7edf\u9762\u4e34\u91cd\u5927\u6570\u636e\u4e2d\u6bd2\u6f0f\u6d1e\uff0c\u73b0\u6709\u9632\u5fa1\u548c\u6cd5\u89c4\u65e0\u6cd5\u5145\u5206\u5e94\u5bf9\u3002\u653b\u51fb\u8005\u4ec5\u9700100-500\u4e2a\u6837\u672c\u5373\u53ef\u7834\u574f\u7cfb\u7edf\uff0c\u6210\u529f\u7387\u8d8560%\uff0c\u63a2\u6d4b\u97006-12\u4e2a\u6708\u3002\u9700\u591a\u5c42\u9632\u5fa1\u548c\u53ef\u89e3\u91ca\u7cfb\u7edf\u3002", "motivation": "\u533b\u7597AI\u7cfb\u7edf\u5728\u5173\u952e\u533b\u7597\u51b3\u7b56\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5bf9\u6570\u636e\u4e2d\u6bd2\u653b\u51fb\u7684\u8106\u5f31\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8ba4\u8bc6\u548c\u89e3\u51b3\uff0c\u73b0\u6709\u6cd5\u89c4\u548c\u9632\u5fa1\u63aa\u65bd\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "method": "\u5206\u6790\u4e86\u516b\u79cd\u653b\u51fb\u573a\u666f\uff0c\u6db5\u76d6\u67b6\u6784\u653b\u51fb\uff08CNN\u3001LLM\u3001RL\uff09\u3001\u57fa\u7840\u8bbe\u65bd\u653b\u51fb\uff08\u8054\u90a6\u5b66\u4e60\u3001\u533b\u7597\u6587\u6863\uff09\u3001\u5173\u952e\u8d44\u6e90\u5206\u914d\u653b\u51fb\uff08\u5668\u5b98\u79fb\u690d\u3001\u5371\u673a\u5206\u8bca\uff09\u548c\u4f9b\u5e94\u94fe\u653b\u51fb\uff08\u5546\u4e1a\u57fa\u7840\u6a21\u578b\uff09\u3002", "result": "\u653b\u51fb\u8005\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u5373\u53ef\u6210\u529f\u7834\u574f\u7cfb\u7edf\uff0c\u5206\u5e03\u5f0f\u533b\u7597\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u591a\u4e2a\u5165\u53e3\u70b9\uff0c\u9690\u79c1\u6cd5\u89c4\u610f\u5916\u4fdd\u62a4\u653b\u51fb\u8005\uff0c\u4f9b\u5e94\u94fe\u5f31\u70b9\u5141\u8bb8\u5355\u70b9\u5931\u6548\u5f71\u54cd50-200\u4e2a\u673a\u6784\u3002", "conclusion": "\u9700\u5f3a\u5236\u6027\u5bf9\u6297\u6027\u6d4b\u8bd5\u3001\u96c6\u6210\u68c0\u6d4b\u3001\u9690\u79c1\u4fdd\u62a4\u5b89\u5168\u673a\u5236\u548c\u56fd\u9645AI\u5b89\u5168\u6807\u51c6\u534f\u8c03\uff0c\u5efa\u8bae\u8f6c\u5411\u53ef\u89e3\u91ca\u7cfb\u7edf\u5e76\u8d28\u7591\u9ed1\u76d2\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2511.11028", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11028", "abs": "https://arxiv.org/abs/2511.11028", "authors": ["Liu Cao", "Weizheng Wang", "Qipeng Xie", "Dongyu Wei", "Lyutianyang Zhang"], "title": "SALT-V: Lightweight Authentication for 5G V2X Broadcasting", "comment": "This work has been submitted to the IEEE for possible publication. 6 pages, 3 figures", "summary": "Vehicle-to-Everything (V2X) communication faces a critical authentication dilemma: traditional public-key schemes like ECDSA provide strong security but impose 2 ms verification delays unsuitable for collision avoidance, while symmetric approaches like TESLA achieve microsecond-level efficiency at the cost of 20-100 ms key disclosure latency. Neither meets 5G New Radio (NR)-V2X's stringent requirements for both immediate authentication and computational efficiency. This paper presents SALT-V, a novel hybrid authentication framework that reconciles this fundamental trade-off through intelligent protocol stratification. SALT-V employs ECDSA signatures for 10% of traffic (BOOT frames) to establish sender trust, then leverages this trust anchor to authenticate 90% of messages (DATA frames) using lightweight GMAC operations. The core innovation - an Ephemeral Session Tag (EST) whitelist mechanism - enables 95% of messages to achieve immediate verification without waiting for key disclosure, while Bloom filter integration provides O(1) revocation checking in 1 us. Comprehensive evaluation demonstrates that SALT-V achieves 0.035 ms average computation time (57x faster than pure ECDSA), 1 ms end-to-end latency, 41-byte overhead, and linear scalability to 2000 vehicles, making it the first practical solution to satisfy all safety-critical requirements for real-time V2X deployment.", "AI": {"tldr": "SALT-V is a hybrid authentication framework for V2X communication that combines ECDSA signatures with lightweight GMAC operations to achieve both immediate authentication and computational efficiency, overcoming the trade-offs of traditional approaches.", "motivation": "Traditional V2X authentication methods face a dilemma: ECDSA provides strong security but has slow verification delays, while symmetric approaches like TESLA are efficient but introduce key disclosure latency. Neither meets 5G NR-V2X requirements for both immediate authentication and computational efficiency.", "method": "SALT-V uses ECDSA signatures for 10% of traffic (BOOT frames) to establish trust, then authenticates 90% of messages (DATA frames) using lightweight GMAC operations. It employs an Ephemeral Session Tag whitelist mechanism and Bloom filters for O(1) revocation checking.", "result": "SALT-V achieves 0.035 ms average computation time (57x faster than ECDSA), 1 ms end-to-end latency, 41-byte overhead, and linear scalability to 2000 vehicles.", "conclusion": "SALT-V is the first practical solution to satisfy all safety-critical requirements for real-time V2X deployment by intelligently combining cryptographic approaches to overcome the authentication dilemma."}}
{"id": "2511.10890", "categories": ["cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.10890", "abs": "https://arxiv.org/abs/2511.10890", "authors": ["Tiantian He", "An Zhao", "Elinor Thompson", "Anna Schroder", "Ahmed Abdulaal", "Frederik Barkhof", "Daniel C. Alexander"], "title": "LLM enhanced graph inference for long-term disease progression modelling", "comment": null, "summary": "Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.", "AI": {"tldr": "A novel framework using LLMs as expert guides to enhance learning of Alzheimer's disease progression from irregular longitudinal data, improving prediction accuracy and interpretability.", "motivation": "Current methods oversimplify brain connectivity as a single-modality substrate for disease spread and face identifiability issues when learning graphs data-driven without constraints.", "method": "Leverages LLMs' ability to synthesize multi-modal relationships to simultaneously optimize long-term disease trajectories from irregular data and a biologically-constrained graph structure of regional interactions.", "result": "Demonstrated on tau-PET data from an AD cohort, the framework shows superior prediction accuracy and interpretability, revealing additional disease-driving factors beyond conventional connectivity.", "conclusion": "The LLM-guided framework effectively addresses limitations of current methods by providing better identifiability and capturing complex multi-modal interactions in neurodegenerative disease progression."}}
{"id": "2511.11171", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11171", "abs": "https://arxiv.org/abs/2511.11171", "authors": ["Lu\u0131s Soeiro", "Thomas Robert", "Stefano Zacchiroli"], "title": "Finding Software Supply Chain Attack Paths with Logical Attack Graphs", "comment": "18th International Symposium on Foundations and Practice of Security (FPS 2025), Nov 2025, Brest, France", "summary": "Cyberattacks are becoming increasingly frequent and sophisticated, often exploiting the software supply chain (SSC) as an attack vector. Attack graphs provide a detailed representation of the sequence of events and vulnerabilities that could lead to a successful security breach in a system. MulVal is a widely used open-source tool for logical attack graph generation in networked systems. However, its current lack of support for capturing and reasoning about SSC threat propagation makes it unsuitable for addressing modern SSC attacks, such as the XZ compromise or the 3CX double SSC attack. To address this limitation, we propose an extension to MulVal that integrates SSC threat propagation analysis with existing network-based threat analysis. This extension introduces a new set of predicates within the familiar MulVal syntax, enabling seamless integration. The new facts and interaction rules model SSC assets, their dependencies, interactions, compromises, additional security mechanisms, initial system states, and known threats. We explain how this integration operates in both directions and demonstrate the practical application of the extension.", "AI": {"tldr": "Extension of MulVal to incorporate software supply chain threat propagation analysis alongside network-based threat analysis using new predicates and rules.", "motivation": "Current MulVal lacks support for software supply chain attacks like XZ compromise and 3CX, which are increasingly common and sophisticated.", "method": "Proposed extension introduces new predicates in MulVal syntax to model SSC assets, dependencies, compromises, and threats, integrating with existing network analysis.", "result": "Enables MulVal to capture and reason about SSC threat propagation, demonstrated through practical application.", "conclusion": "The extension enhances MulVal's capability to address modern SSC attacks effectively."}}
{"id": "2511.10925", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10925", "abs": "https://arxiv.org/abs/2511.10925", "authors": ["Ha-Thanh Nguyen", "Wachara Fungwacharakorn", "Ken Satoh"], "title": "Multi-Agent Legal Verifier Systems for Data Transfer Planning", "comment": "Presented at NeLaMKRR@KR, 2025 (arXiv:2511.09575)", "summary": "Legal compliance in AI-driven data transfer planning is becoming increasingly critical under stringent privacy regulations such as the Japanese Act on the Protection of Personal Information (APPI). We propose a multi-agent legal verifier that decomposes compliance checking into specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol. Evaluated on a stratified dataset of 200 Amended APPI Article 16 cases with clearly defined ground truth labels and multiple performance metrics, the system achieves 72% accuracy, which is 21 percentage points higher than a single-agent baseline, including 90% accuracy on clear compliance cases (vs. 16% for the baseline) while maintaining perfect detection of clear violations. While challenges remain in ambiguous scenarios, these results show that domain specialization and coordinated reasoning can meaningfully improve legal AI performance, providing a scalable and regulation-aware framework for trustworthy and interpretable automated compliance verification.", "AI": {"tldr": "A multi-agent legal verifier system improves AI-driven data transfer compliance checking under Japan's APPI regulations, achieving 72% accuracy by using specialized agents for statutory interpretation, business context evaluation, and risk assessment.", "motivation": "Legal compliance in AI-driven data transfer is critical under strict privacy regulations like Japan's APPI, requiring improved automated verification methods.", "method": "Proposes a multi-agent system with specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol.", "result": "Achieves 72% accuracy on 200 APPI Article 16 cases (21% higher than single-agent baseline), with 90% accuracy on clear compliance cases and perfect detection of clear violations.", "conclusion": "Domain specialization and coordinated reasoning significantly improve legal AI performance, providing a scalable framework for trustworthy automated compliance verification."}}
{"id": "2511.11249", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11249", "abs": "https://arxiv.org/abs/2511.11249", "authors": ["Melih Co\u015f\u011fun", "Mert Gen\u00e7t\u00fcrk", "Sinem Sav"], "title": "Bridging Local and Federated Data Normalization in Federated Learning: A Privacy-Preserving Approach", "comment": null, "summary": "Data normalization is a crucial preprocessing step for enhancing model performance and training stability. In federated learning (FL), where data remains distributed across multiple parties during collaborative model training, normalization presents unique challenges due to the decentralized and often heterogeneous nature of the data. Traditional methods rely on either independent client-side processing, i.e., local normalization, or normalizing the entire dataset before distributing it to parties, i.e., pooled normalization. Local normalization can be problematic when data distributions across parties are non-IID, while the pooled normalization approach conflicts with the decentralized nature of FL. In this paper, we explore the adaptation of widely used normalization techniques to FL and define the term federated normalization. Federated normalization simulates pooled normalization by enabling the collaborative exchange of normalization parameters among parties. Thus, it achieves performance on par with pooled normalization without compromising data locality. However, sharing normalization parameters such as the mean introduces potential privacy risks, which we further mitigate through a robust privacy-preserving solution. Our contributions include: (i) We systematically evaluate the impact of various federated and local normalization techniques in heterogeneous FL scenarios, (ii) We propose a novel homomorphically encrypted $k$-th ranked element (and median) calculation tailored for the federated setting, enabling secure and efficient federated normalization, (iii) We propose privacy-preserving implementations of widely used normalization techniques for FL, leveraging multiparty fully homomorphic encryption (MHE).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8054\u90a6\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u6a21\u62df\u96c6\u4e2d\u5f0f\u5f52\u4e00\u5316\u7684\u6548\u679c\uff0c\u901a\u8fc7\u5b89\u5168\u5171\u4eab\u5f52\u4e00\u5316\u53c2\u6570\u89e3\u51b3\u6570\u636e\u5f02\u6784\u6027\u548c\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f52\u4e00\u5316\u9762\u4e34\u6311\u6218\uff1a\u672c\u5730\u5f52\u4e00\u5316\u5728\u975eIID\u6570\u636e\u4e0b\u6548\u679c\u5dee\uff0c\u96c6\u4e2d\u5f0f\u5f52\u4e00\u5316\u8fdd\u53cd\u6570\u636e\u672c\u5730\u5316\u539f\u5219\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u6570\u636e\u672c\u5730\u5316\u53c8\u80fd\u5b9e\u73b0\u96c6\u4e2d\u5f0f\u5f52\u4e00\u5316\u6548\u679c\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8054\u90a6\u5f52\u4e00\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5b89\u5168\u591a\u65b9\u5168\u540c\u6001\u52a0\u5bc6\uff08MHE\uff09\u5b9e\u73b0\u5f52\u4e00\u5316\u53c2\u6570\uff08\u5982\u5747\u503c\u3001\u4e2d\u4f4d\u6570\uff09\u7684\u9690\u79c1\u4fdd\u62a4\u534f\u540c\u8ba1\u7b97\uff0c\u8bbe\u8ba1\u4e86\u540c\u6001\u52a0\u5bc6\u7684k\u9636\u7edf\u8ba1\u91cf\u8ba1\u7b97\u65b9\u6848\u3002", "result": "\u8054\u90a6\u5f52\u4e00\u5316\u5728\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u80fd\u8fbe\u5230\u4e0e\u96c6\u4e2d\u5f0f\u5f52\u4e00\u5316\u76f8\u8fd1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\uff0c\u89e3\u51b3\u4e86\u672c\u5730\u5f52\u4e00\u5316\u5728\u975eIID\u6570\u636e\u4e0b\u7684\u6027\u80fd\u635f\u5931\u95ee\u9898\u3002", "conclusion": "\u8054\u90a6\u5f52\u4e00\u5316\u662f\u8054\u90a6\u5b66\u4e60\u4e2d\u6709\u6548\u7684\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bc6\u7801\u5b66\u5de5\u5177\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u5e73\u8861\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u6570\u636e\u9884\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.10952", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10952", "abs": "https://arxiv.org/abs/2511.10952", "authors": ["Steven J. Jones", "Robert E. Wray", "John E. Laird"], "title": "Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints", "comment": "6 pages, technical appendix (submitted to AAAI26)", "summary": "Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual \"knowledge\" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.", "AI": {"tldr": "Paper proposes requirements for AI agent decision-making when dealing with novel scenarios where no available actions fully satisfy all constraints. It explores how agents need to construct, evaluate, and justify new courses of action using contextual knowledge beyond trained policies.", "motivation": "Autonomous AI systems inevitably encounter scenarios not covered by their training, requiring them to make decisions that align with human values and constraints when no predefined action is fully satisfactory.", "method": "Through analysis and empirical case studies, examines how agents need to integrate normative (rules/laws), pragmatic (practical feasibility), and situational understanding to make robust decisions.", "result": "Identifies specific types of knowledge requirements for agents to construct and evaluate novel courses of action that remain aligned with human expectations in complex environments.", "conclusion": "Agents must go beyond trained policies and develop capabilities for contextual reasoning that integrates multiple knowledge types to make value-aligned decisions in novel, constrained scenarios."}}
{"id": "2511.11250", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11250", "abs": "https://arxiv.org/abs/2511.11250", "authors": ["Biagio Boi", "Christian Esposito"], "title": "Prompt Engineering vs. Fine-Tuning for LLM-Based Vulnerability Detection in Solana and Algorand Smart Contracts", "comment": null, "summary": "Smart contracts have emerged as key components within decentralized environments, enabling the automation of transactions through self-executing programs. While these innovations offer significant advantages, they also present potential drawbacks if the smart contract code is not carefully designed and implemented. This paper investigates the capability of large language models (LLMs) to detect OWASP-inspired vulnerabilities in smart contracts beyond the Ethereum Virtual Machine (EVM) ecosystem, focusing specifically on Solana and Algorand. Given the lack of labeled datasets for non-EVM platforms, we design a synthetic dataset of annotated smart contract snippets in Rust (for Solana) and PyTeal (for Algorand), structured around a vulnerability taxonomy derived from OWASP. We evaluate LLMs under three configurations: prompt engineering, fine-tuning, and a hybrid of both, comparing their performance on different vulnerability categories. Experimental results show that prompt engineering achieves general robustness, while fine-tuning improves precision and recall on less semantically rich languages such as TEAL. Additionally, we analyze how the architectural differences of Solana and Algorand influence the manifestation and detectability of vulnerabilities, offering platform-specific mappings that highlight limitations in existing security tooling. Our findings suggest that LLM-based approaches are viable for static vulnerability detection in smart contracts, provided domain-specific data and categorization are integrated into training pipelines.", "AI": {"tldr": "This paper evaluates LLMs for detecting OWASP-inspired vulnerabilities in Solana and Algorand smart contracts using synthetic datasets, comparing prompt engineering, fine-tuning, and hybrid approaches.", "motivation": "To address the lack of labeled datasets for non-EVM platforms and assess LLM effectiveness in detecting smart contract vulnerabilities beyond the EVM ecosystem.", "method": "Created synthetic datasets of annotated smart contract snippets in Rust (Solana) and PyTeal (Algorand) based on OWASP taxonomy. Evaluated LLMs using prompt engineering, fine-tuning, and hybrid approaches across different vulnerability categories.", "result": "Prompt engineering showed general robustness, while fine-tuning improved precision and recall for less semantically rich languages like TEAL. Platform-specific mappings revealed how architectural differences affect vulnerability detectability.", "conclusion": "LLM-based approaches are viable for static vulnerability detection in smart contracts when domain-specific data and categorization are integrated into training pipelines, though platform-specific considerations are crucial."}}
{"id": "2511.11347", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11347", "abs": "https://arxiv.org/abs/2511.11347", "authors": ["Shaowei Guan", "Hin Chi Kwok", "Ngai Fong Law", "Gregor Stiglic", "Vivian Hui"], "title": "Privacy Challenges and Solutions in Retrieval-Augmented Generation-Enhanced LLMs for Healthcare Chatbots: A Review of Applications, Risks, and Future Directions", "comment": "23 pages, 2 figures", "summary": "Retrieval-augmented generation (RAG) has rapidly emerged as a transformative approach for integrating large language models into clinical and biomedical workflows. However, privacy risks, such as protected health information (PHI) exposure, remain inconsistently mitigated. This review provides a thorough analysis of the current landscape of RAG applications in healthcare, including (i) sensitive data type across clinical scenarios, (ii) the associated privacy risks, (iii) current and emerging data-privacy protection mechanisms and (iv) future direction for patient data privacy protection. We synthesize 23 articles on RAG applications in healthcare and systematically analyze privacy challenges through a pipeline-structured framework encompassing data storage, transmission, retrieval and generation stages, delineating potential failure modes, their underlying causes in threat models and system mechanisms, and their practical implications. Building on this analysis, we critically review 17 articles on privacy-preserving strategies for RAG systems. Our evaluation reveals critical gaps, including insufficient clinical validation, absence of standardized evaluation frameworks, and lack of automated assessment tools. We propose actionable directions based on these limitations and conclude with a call to action. This review provides researchers and practitioners with a structured framework for understanding privacy vulnerabilities in healthcare RAG and offers a roadmap toward developing systems that achieve both clinical effectiveness and robust privacy preservation.", "AI": {"tldr": "This review analyzes privacy risks in healthcare RAG systems, examining sensitive data types, privacy threats, protection mechanisms, and future directions through a pipeline framework.", "motivation": "RAG systems are increasingly used in healthcare but present significant privacy risks like PHI exposure that require systematic analysis and mitigation strategies.", "method": "The authors synthesized 23 articles on healthcare RAG applications and 17 articles on privacy-preserving strategies using a pipeline framework covering data storage, transmission, retrieval and generation stages.", "result": "The review identified critical gaps including insufficient clinical validation, lack of standardized evaluation frameworks, and absence of automated assessment tools for privacy protection.", "conclusion": "Actionable directions are proposed to address privacy vulnerabilities, providing a roadmap for developing clinically effective RAG systems with robust privacy preservation."}}
{"id": "2511.11029", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11029", "abs": "https://arxiv.org/abs/2511.11029", "authors": ["\u00d6zg\u00fcr Akg\u00fcn", "Mun See Chang", "Ian P. Gent", "Christopher Jefferson"], "title": "Faster Symmetry Breaking Constraints for Abstract Structures", "comment": null, "summary": "In constraint programming and related paradigms, a modeller specifies their problem in a modelling language for a solver to search and return its solution(s). Using high-level modelling languages such as Essence, a modeller may express their problems in terms of abstract structures. These are structures not natively supported by the solvers, and so they have to be transformed into or represented as other structures before solving. For example, nested sets are abstract structures, and they can be represented as matrices in constraint solvers. Many problems contain symmetries and one very common and highly successful technique used in constraint programming is to \"break\" symmetries, to avoid searching for symmetric solutions. This can speed up the solving process by many orders of magnitude. Most of these symmetry-breaking techniques involve placing some kind of ordering for the variables of the problem, and picking a particular member under the symmetries, usually the smallest. Unfortunately, applying this technique to abstract variables produces a very large number of complex constraints that perform poorly in practice. In this paper, we demonstrate a new incomplete method of breaking the symmetries of abstract structures by better exploiting their representations. We apply the method in breaking the symmetries arising from indistinguishable objects, a commonly occurring type of symmetry, and show that our method is faster than the previous methods proposed in (Akg\u00fcn et al. 2025).", "AI": {"tldr": "Novel incomplete symmetry-breaking method for abstract structures using representation-aware constraints, showing improved performance over existing methods.", "motivation": "Symmetry breaking is crucial for constraint solving efficiency but performs poorly when applied directly to abstract structures due to complex constraints.", "method": "Representation-aware symmetry breaking that exploits how abstract structures are represented in solvers, specifically targeting indistinguishable objects.", "result": "Method is faster than previous approaches from Akg\u00fcn et al. (2025) for breaking symmetries of abstract structures.", "conclusion": "Representation-aware symmetry breaking provides practical improvements for handling abstract structures in constraint programming."}}
{"id": "2511.11356", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11356", "abs": "https://arxiv.org/abs/2511.11356", "authors": ["Yanbo Dai", "Zongjie Li", "Zhenlan Ji", "Shuai Wang"], "title": "SEAL: Subspace-Anchored Watermarks for LLM Ownership", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks, demonstrating human-level performance in text generation, reasoning, and question answering. However, training such models requires substantial computational resources, large curated datasets, and sophisticated alignment procedures. As a result, they constitute highly valuable intellectual property (IP) assets that warrant robust protection mechanisms. Existing IP protection approaches suffer from critical limitations. Model fingerprinting techniques can identify model architectures but fail to establish ownership of specific model instances. In contrast, traditional backdoor-based watermarking methods embed behavioral anomalies that can be easily removed through common post-processing operations such as fine-tuning or knowledge distillation.\n  We propose SEAL, a subspace-anchored watermarking framework that embeds multi-bit signatures directly into the model's latent representational space, supporting both white-box and black-box verification scenarios. Our approach leverages model editing techniques to align the hidden representations of selected anchor samples with predefined orthogonal bit vectors. This alignment embeds the watermark while preserving the model's original factual predictions, rendering the watermark functionally harmless and stealthy. We conduct comprehensive experiments on multiple benchmark datasets and six prominent LLMs, comparing SEAL with 11 existing fingerprinting and watermarking methods to demonstrate its superior effectiveness, fidelity, efficiency, and robustness. Furthermore, we evaluate SEAL under potential knowledgeable attacks and show that it maintains strong verification performance even when adversaries possess knowledge of the watermarking mechanism and the embedded signatures.", "AI": {"tldr": "SEAL is a novel watermarking framework that embeds multi-bit signatures into LLM's latent space for IP protection, offering robust verification while maintaining model performance.", "motivation": "Existing IP protection methods like fingerprinting and traditional watermarking have limitations - fingerprinting can't identify specific model instances, while watermarks can be removed through fine-tuning. There's a need for more robust protection for valuable LLM IP.", "method": "Uses model editing to align hidden representations of anchor samples with orthogonal bit vectors, embedding watermarks directly into the model's latent representational space while preserving factual predictions.", "result": "Comprehensive experiments on 6 LLMs and 11 existing methods show SEAL has superior effectiveness, fidelity, efficiency, and robustness against knowledgeable attacks.", "conclusion": "SEAL provides robust watermarking that withstands removal attempts even when attackers know the watermarking mechanism, offering practical IP protection for LLMs."}}
{"id": "2511.11040", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11040", "abs": "https://arxiv.org/abs/2511.11040", "authors": ["Qian Zhang", "Yan Zheng", "Jinyi Liu", "Hebin Liang", "Lanjun Wang"], "title": "Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?", "comment": null, "summary": "Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, \"Truth Last\", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.", "AI": {"tldr": "Study introduces 'Truth Last' role allocation strategy that improves Multi-Agent Debate performance by 22%, and proposes MADC strategy for practical applications where truth is unknown.", "motivation": "Role allocation strategies in Multi-Agent Debate are understudied despite their potential to enhance LLM reasoning abilities.", "method": "Proposes 'Truth Last' strategy and MADC (Multi-Agent Debate Consistency) with path consistency scoring to simulate truth-teller roles.", "result": "MADC demonstrated advanced performance across 9 LLM models, overcoming MAD's performance bottlenecks.", "conclusion": "MADC provides crucial pathway for improvements in LLM agent scaling, with Truth Last strategy showing 22% improvement in reasoning tasks."}}
{"id": "2511.11366", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11366", "abs": "https://arxiv.org/abs/2511.11366", "authors": ["Benjamin Blakely", "Daniel Karcz"], "title": "Grid-STIX: A STIX 2.1-Compliant Cyber-Physical Security Ontology for Power Grid", "comment": "Corresponding GitHub Repo: https://github.com/argonne-appres/grid-stix", "summary": "Modern electrical power grids represent complex cyber-physical systems requiring specialized cybersecurity frameworks beyond traditional IT security models. Existing threat intelligence standards such as STIX 2.1 and MITRE ATT\\&CK lack coverage for grid-specific assets, operational technology relationships, and cyber-physical interdependencies essential for power system security. We present Grid-STIX, a domain-specific extension of STIX 2.1 for electrical grid cybersecurity applications. Grid-STIX employs a modular architecture encompassing physical assets, operational technology components, cyber-physical relationships, and security policies that capture modern power systems including distributed energy resources, advanced metering infrastructure, and nuclear energy facilities. The framework provides threat modeling capabilities through systematic representation of attack patterns, supply chain risks, and cross-domain impact analysis while maintaining STIX 2.1 compliance. Grid-STIX includes modules for nuclear safeguards and non-proliferation verification, enabling cybersecurity modeling across conventional and nuclear energy sectors. The ontology supports Zero Trust enforcement through policy decision points and operational context integration. Our implementation includes validation pipelines, Python code generation, and visualizations. Use cases demonstrate applications including cross-utility threat intelligence sharing, supply chain risk assessment, and nuclear facility cybersecurity. Grid-STIX is available as an open-source framework to advance collaborative cybersecurity research across the electrical power sector.", "AI": {"tldr": "Grid-STIX is a cybersecurity framework extension of STIX 2.1 specifically designed for electrical power grids, addressing their unique cyber-physical characteristics and enabling threat intelligence sharing across the energy sector.", "motivation": "Traditional IT security standards like STIX 2.1 and MITRE ATT&CK lack specialization for power grid assets, operational technology relationships, and cyber-physical interdependencies required for comprehensive grid security.", "method": "Developed a domain-specific STIX 2.1 extension with modular architecture covering physical assets, OT components, cyber-physical relationships, security policies, and nuclear safeguards through validation pipelines and Python code generation.", "result": "Created an open-source framework supporting threat modeling, attack pattern representation, supply chain risk assessment, cross-domain impact analysis, Zero Trust enforcement, and nuclear facility cybersecurity applications.", "conclusion": "Grid-STIX successfully bridges the gap in grid-specific cybersecurity frameworks, providing a standardized approach for threat intelligence sharing and security modeling across conventional and nuclear power systems."}}
{"id": "2511.11043", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11043", "abs": "https://arxiv.org/abs/2511.11043", "authors": ["Asen Nachkov", "Jan-Nico Zaech", "Danda Pani Paudel", "Xi Wang", "Luc Van Gool"], "title": "Autonomous Vehicle Path Planning by Searching With Differentiable Simulation", "comment": null, "summary": "Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.", "AI": {"tldr": "DiffSim (DSS) is differentiable simulation framework using Waymax simulator as predictor and critic for autonomous driving planning, showing superior performance through gradient-based action optimization.", "motivation": "Planning is crucial for safe autonomous driving, but challenging when all components need to be learned. Current methods struggle with accurate state prediction and effective search.", "method": "Leverages differentiable simulator Waymax for both state prediction and critic evaluation. Uses gradient descent over imagined trajectories and combines planning gradients with stochastic search.", "result": "DSS significantly outperforms sequence prediction, imitation learning, model-free RL, and other planning methods in tracking and path planning accuracy.", "conclusion": "Differentiable simulation combined with gradient-based optimization provides an effective planning framework for autonomous driving, enabling safer and more accurate navigation in complex scenarios."}}
{"id": "2511.11381", "categories": ["cs.CR", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.11381", "abs": "https://arxiv.org/abs/2511.11381", "authors": ["Gioliano de Oliveira Braga", "Pedro Henrique dos Santos Rocha", "Rafael Pimenta de Mattos Paix\u00e3o", "Giovani Hoff da Costa", "Gustavo Cavalcanti Morais", "Louren\u00e7o Alves Pereira J\u00fanior"], "title": "SoK: Security Evaluation of Wi-Fi CSI Biometrics: Attacks, Metrics, and Systemic Weaknesses", "comment": "An improved version will be submitted to Euro S&P 2026, and this paper will be updated in the near future", "summary": "Wi-Fi Channel State Information (CSI) has been repeatedly proposed as a biometric modality, often with reports of high accuracy and operational feasibility. However, the field lacks a consolidated understanding of its security properties, adversarial resilience, and methodological consistency. This Systematization of Knowledge (SoK) examines CSI-based biometric authentication through a security perspective, analyzing how existing work differs across sensing infrastructure, signal representations, feature pipelines, learning models, and evaluation methodologies. Our synthesis reveals systemic inconsistencies: reliance on aggregate accuracy metrics, limited reporting of FAR/FRR/EER, absence of per-user risk analysis, and scarce consideration of threat models or adversarial feasibility. We construct a unified evaluation framework to empirically expose these issues and demonstrate how security-relevant metrics, such as per-class EER, FCS, and the Gini Coefficient, uncover risk concentration that remains hidden under traditional reporting practices. Our analysis highlights concrete attack surfaces and shows how methodological choices materially influence vulnerability profiles, which include replay, geometric mimicry, and environmental perturbation. Based on these findings, we articulate the security boundaries of current CSI biometrics and provide guidelines for rigorous evaluation, reproducible experimentation, and future research directions. This SoK offers the security community a structured, evidence-driven reassessment of Wi-Fi CSI biometrics and their suitability as an authentication primitive.", "AI": {"tldr": "This SoK paper critically analyzes Wi-Fi CSI biometric authentication from a security perspective, exposing systemic inconsistencies in evaluation methods and highlighting hidden risks through a unified evaluation framework.", "motivation": "Despite reports of high accuracy in Wi-Fi CSI biometrics, there's a lack of consolidated understanding about security properties, adversarial resilience, and methodological consistency in the field.", "method": "The authors conduct a systematic knowledge review, analyzing existing work across sensing infrastructure, signal representations, feature pipelines, learning models, and evaluation methodologies, then construct a unified evaluation framework.", "result": "Reveals systemic issues including reliance on aggregate accuracy metrics, limited security reporting (FAR/FRR/EER), absence of per-user risk analysis, and insufficient threat modeling, demonstrating how security metrics uncover hidden risk concentrations.", "conclusion": "Wi-Fi CSI biometrics have significant security limitations and methodological inconsistencies; the paper provides security boundaries, evaluation guidelines, and research directions for making this authentication primitive more rigorous and reproducible."}}
{"id": "2511.11079", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11079", "abs": "https://arxiv.org/abs/2511.11079", "authors": ["Sejin Kim", "Hayan Choi", "Seokki Lee", "Sundong Kim"], "title": "ARCTraj: A Dataset and Benchmark of Human Reasoning Trajectories for Abstract Problem Solving", "comment": null, "summary": "We present ARCTraj, a dataset and methodological framework for modeling human reasoning through complex visual tasks in the Abstraction and Reasoning Corpus (ARC). While ARC has inspired extensive research on abstract reasoning, most existing approaches rely on static input--output supervision, which limits insight into how reasoning unfolds over time. ARCTraj addresses this gap by recording temporally ordered, object-level actions that capture how humans iteratively transform inputs into outputs, revealing intermediate reasoning steps that conventional datasets overlook. Collected via the O2ARC web interface, it contains around 10,000 trajectories annotated with task identifiers, timestamps, and success labels across 400 training tasks from the ARC-AGI-1 benchmark. It further defines a unified reasoning pipeline encompassing data collection, action abstraction, Markov decision process (MDP) formulation, and downstream learning, enabling integration with reinforcement learning, generative modeling, and sequence modeling methods such as PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers. Analyses of spatial selection, color attribution, and strategic convergence highlight the structure and diversity of human reasoning. Together, these contributions position ARCTraj as a structured and interpretable foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence.", "AI": {"tldr": "ARCTraj introduces a dataset and framework for modeling human reasoning steps in visual tasks from the Abstraction and Reasoning Corpus (ARC), capturing temporal action sequences to reveal intermediate reasoning processes.", "motivation": "Existing ARC approaches rely on static input-output pairs, which fail to capture how reasoning unfolds over time. ARCTraj addresses this gap by recording human reasoning trajectories.", "method": "Uses O2ARC web interface to collect ~10,000 temporal trajectories with object-level actions from 400 ARC training tasks. Defines a unified pipeline with data collection, action abstraction, MDP formulation, and integration with RL/generative models.", "result": "Dataset reveals patterns in spatial selection, color attribution, and strategic convergence. Enables applications with PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers.", "conclusion": "ARCTraj provides a structured foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence research."}}
{"id": "2511.11385", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11385", "abs": "https://arxiv.org/abs/2511.11385", "authors": ["Faezeh Nasrabadi", "Robert K\u00fcnnemann", "Hamed Nemati"], "title": "Automated Side-Channel Analysis of Cryptographic Protocol Implementations", "comment": null, "summary": "We extract the first formal model of WhatsApp from its implementation by combining binary-level analysis (via CryptoBap) with reverse engineering (via Ghidra) to handle this large closed-source application. Using this model, we prove forward secrecy, identify a known clone-attack against post-compromise security and discover functional gaps between WhatsApp's implementation and its specification. We further introduce a methodology to analyze cryptographic protocol implementations for their resilience to side-channel attacks. This is achieved by extending the CryptoBap framework to integrate hardware leakage contracts into the protocol model, which we then pass to the state-of-the-art protocol prover, DeepSec. This enables a detailed security analysis against both functional bugs and microarchitectural side-channel attacks. Using this methodology, we identify a privacy attack in WhatsApp that allows a side-channel attacker to learn the victim's contacts and confirm a known unlinkability attack on the BAC protocol used in electronic passports.\n  Key contributions include (1) the first formal model of WhatsApp, extracted from its binary, (2) a framework to integrate side-channel leakage contracts into protocol models for the first time, and (3) revealing critical vulnerabilities invisible to specification-based methods.", "AI": {"tldr": "Researchers extracted WhatsApp's first formal model from its binary using reverse engineering, proving forward secrecy, identifying attacks, and developing a framework to analyze side-channel vulnerabilities.", "motivation": "To analyze the security of large closed-source applications like WhatsApp by creating accurate formal models directly from their implementation to uncover vulnerabilities that specification-based methods might miss.", "method": "Combined binary-level analysis (CryptoBap) with reverse engineering (Ghidra) to extract a formal model, then extended CryptoBap to integrate hardware leakage contracts for side-channel analysis using DeepSec prover.", "result": "Identified a known clone-attack, functional gaps between implementation and specification, a privacy attack exposing victim's contacts, and confirmed an unlinkability attack on BAC protocol.", "conclusion": "The methodology enables comprehensive security analysis against both functional bugs and side-channel attacks, revealing critical vulnerabilities in WhatsApp and electronic passport protocols that traditional methods cannot detect."}}
{"id": "2511.11095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11095", "abs": "https://arxiv.org/abs/2511.11095", "authors": ["Dillon Z. Chen", "Till Hofmann", "Toryn Q. Klassen", "Sheila A. McIlraith"], "title": "Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)", "comment": "Extended version of AAAI 2026 paper", "summary": "Generalised planning (GP) refers to the task of synthesising programs that solve families of related planning problems. We introduce a novel, yet simple method for GP: given a set of training problems, for each problem, compute an optimal plan for each goal atom in some order, perform goal regression on the resulting plans, and lift the corresponding outputs to obtain a set of first-order $\\textit{Condition} \\rightarrow \\textit{Actions}$ rules. The rules collectively constitute a generalised plan that can be executed as is or alternatively be used to prune the planning search space. We formalise and prove the conditions under which our method is guaranteed to learn valid generalised plans and state space pruning axioms for search. Experiments demonstrate significant improvements over state-of-the-art (generalised) planners with respect to the 3 metrics of synthesis cost, planning coverage, and solution quality on various classical and numeric planning domains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5e7f\u4e49\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u76ee\u6807\u56de\u5f52\u548c\u89c4\u5219\u63d0\u5347\u6280\u672f\u4ece\u8bad\u7ec3\u95ee\u9898\u4e2d\u5b66\u4e60\u901a\u7528\u7684Condition\u2192Actions\u89c4\u5219\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u6210\u672c\u3001\u89c4\u5212\u8986\u76d6\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u5e7f\u4e49\u89c4\u5212\u95ee\u9898\uff0c\u5373\u5408\u6210\u80fd\u591f\u89e3\u51b3\u76f8\u5173\u89c4\u5212\u95ee\u9898\u65cf\u7684\u7a0b\u5e8f", "method": "\u4e3a\u6bcf\u4e2a\u8bad\u7ec3\u95ee\u9898\u6309\u987a\u5e8f\u8ba1\u7b97\u6700\u4f18\u8ba1\u5212\uff0c\u8fdb\u884c\u76ee\u6807\u56de\u5f52\uff0c\u5e76\u5c06\u8f93\u51fa\u63d0\u5347\u4e3a\u4e00\u9636\u6761\u4ef6-\u52a8\u4f5c\u89c4\u5219", "result": "\u5728\u7ecf\u5178\u548c\u6570\u503c\u89c4\u5212\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5728\u5408\u6210\u6210\u672c\u3001\u89c4\u5212\u8986\u76d6\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u89c4\u5212\u5668", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b66\u4e60\u6709\u6548\u7684\u5e7f\u4e49\u8ba1\u5212\u548c\u641c\u7d22\u7a7a\u95f4\u4fee\u526a\u516c\u7406\uff0c\u5177\u6709\u826f\u597d\u7684\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd"}}
{"id": "2511.11134", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11134", "abs": "https://arxiv.org/abs/2511.11134", "authors": ["Jingxuan Wei", "Caijun Jia", "Xi Bai", "Xinglong Xu", "Siyuan Li", "Linzhuang Sun", "Bihui Yu", "Conghui He", "Lijun Wu", "Cheng Tan"], "title": "GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models", "comment": "35 pages, 22 figures", "summary": "The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.", "AI": {"tldr": "GGBench is a new benchmark designed to evaluate multimodal models' geometric generative reasoning capabilities through language-guided geometric construction tasks.", "motivation": "Existing benchmarks fail to measure the integrated cognitive process of generative reasoning, focusing only on discriminative understanding or unconstrained image generation separately.", "method": "Using geometric construction as a testbed since it inherently requires fusion of language comprehension and precise visual generation. The benchmark provides systematic diagnostics for multimodal generative reasoning.", "result": "The paper introduces GGBench as a comprehensive framework to evaluate models' ability to understand, reason, and actively construct solutions in geometric contexts.", "conclusion": "GGBench sets a more rigorous standard for evaluating the next generation of intelligent systems by addressing the critical gap in multimodal generative reasoning evaluation."}}
{"id": "2511.11233", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11233", "abs": "https://arxiv.org/abs/2511.11233", "authors": ["Huajian Zhang", "Mingyue Cheng", "Yucong Luo", "Xiaoyu Tao"], "title": "STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models", "comment": null, "summary": "Table reasoning with the large language models (LLMs) is a fundamental path toward building intelligent systems that can understand and analyze over structured data. While recent progress has shown promising results, they still suffer from two key limitations: (i) the reasoning processes lack the depth and iterative refinement characteristic of human cognition; and (ii) the reasoning processes exhibit instability, which compromises their reliability in downstream applications. In this work, we present STaR (slow-thinking for table reasoning), a new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference. During training, STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths. Extensive experiments on benchmarks demonstrate that STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.", "AI": {"tldr": "STaR\u6846\u67b6\u901a\u8fc7\u6162\u601d\u8003\u80fd\u529b\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u63a8\u7406\uff0c\u63d0\u5347LLMs\u5728\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff0c\u4f7f\u7528\u4e24\u9636\u6bb5\u96be\u5ea6\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u548c\u8f68\u8ff9\u7ea7\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLMs\u8868\u683c\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a(i) \u63a8\u7406\u8fc7\u7a0b\u7f3a\u4e4f\u4eba\u7c7b\u8ba4\u77e5\u7684\u6df1\u5ea6\u548c\u8fed\u4ee3\u7cbe\u70bc\u7279\u6027\uff1b(ii) \u63a8\u7406\u8fc7\u7a0b\u4e0d\u7a33\u5b9a\uff0c\u5f71\u54cd\u4e0b\u6e38\u5e94\u7528\u7684\u53ef\u9760\u6027\u3002", "method": "STaR\u91c7\u7528\u4e24\u9636\u6bb5\u96be\u5ea6\u611f\u77e5\u5f3a\u5316\u5b66\u4e60(DRL)\uff0c\u4ece\u7b80\u5355\u5230\u590d\u6742\u7684\u67e5\u8be2\u9010\u6b65\u5b66\u4e60\uff0c\u5e76\u4f7f\u7528\u590d\u5408\u5956\u52b1\u3002\u63a8\u7406\u65f6\u901a\u8fc7\u6574\u5408token\u7ea7\u7f6e\u4fe1\u5ea6\u548c\u7b54\u6848\u4e00\u81f4\u6027\u8fdb\u884c\u8f68\u8ff9\u7ea7\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\uff0c\u63d0\u5347\u63a8\u7406\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u9886\u57df\u5916\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "STaR\u4f5c\u4e3a\u57fa\u4e8e\u8ba4\u77e5\u542f\u53d1\u7684\u8868\u683c\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u663e\u793a\u51fa\u53ef\u9760\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2511.11275", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11275", "abs": "https://arxiv.org/abs/2511.11275", "authors": ["Julius Wenzel", "Syeda Umaima Alam", "Andreas Schmidt", "Hanwei Zhang", "Holger Hermanns"], "title": "A Workflow for Full Traceability of AI Decisions", "comment": "10 pages, 10 figures", "summary": "An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.\n  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.", "AI": {"tldr": "The paper proposes a workflow using confidential computing to create tamper-proof documentation of AI decision components, demonstrated through a mushroom classification app.", "motivation": "AI systems lack proper documentation traceability for decisions, hindering accountability when decisions cause harm or violate laws.", "method": "Expands the DBOM concept into a workflow that enforces documentation of all training and inference components using confidential computing technology.", "result": "Development of a functioning workflow that generates verifiable, exhaustive traces of AI decisions, demonstrated with a mushroom classification application.", "conclusion": "The approach provides practical traceability for AI decisions, enabling reconstruction of responsibility chains that could stand up in legal proceedings."}}
{"id": "2511.11281", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.11281", "abs": "https://arxiv.org/abs/2511.11281", "authors": ["Patrick Koopmann", "Yasir Mahmood", "Axel-Cyrille Ngonga Ngomo", "Balram Tiwari"], "title": "Can You Tell the Difference? Contrastive Explanations for ABox Entailments", "comment": "Technical report to the paper accepted at AAAI-2026", "summary": "We introduce the notion of contrastive ABox explanations to answer questions of the type \"Why is a an instance of C, but b is not?\". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.", "AI": {"tldr": "This paper introduces contrastive ABox explanations that explain why 'a' is an instance of concept C while 'b' is not, focusing on commonalities and differences between instances.", "motivation": "Existing approaches explain positive entailments (why C(a) is entailed) or missing entailments (why C(b) is not entailed) separately, but contrastive explanations consider both simultaneously to highlight relevant differences.", "method": "Developed a formal notion of contrastive explanations for ABox reasoning with description logic ontologies, analyzed computational complexity for different variants under optimality criteria, and implemented a method for computing one variant.", "result": "The computational complexity was analyzed for different description logics, and a method was implemented and evaluated on generated problems for realistic knowledge bases.", "conclusion": "Contrastive ABox explanations provide a unified framework for explaining both positive and negative entailments by focusing on relevant similarities and differences between instances."}}
{"id": "2511.11301", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11301", "abs": "https://arxiv.org/abs/2511.11301", "authors": ["Ruoxi Cheng", "Haoxuan Ma", "Teng Ma", "Hongyi Zhang"], "title": "EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment", "comment": null, "summary": "Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.", "AI": {"tldr": "EcoAlign is an inference-time framework that improves LVLM alignment efficiency by treating models as boundedly rational agents. It uses forward-looking search with dynamic safety-utility-cost balancing to prevent jailbreak attacks while reducing computational costs.", "motivation": "Current LVLM alignment methods face trade-offs between safety, utility, and costs, with process-blindness allowing harmful reasoning to be disguised as benign. This wastes computational budget on unsafe deliberation.", "method": "Incrementally expands thought graphs and scores actions using a forward-looking function that dynamically weighs expected safety, utility, and cost against remaining budget. Enforces path safety via weakest-link principle to prevent deception.", "result": "Extensive experiments on 3 closed-source and 2 open-source models across 6 datasets show EcoAlign matches or surpasses state-of-the-art safety and utility at lower computational cost.", "conclusion": "EcoAlign provides a principled, economical pathway to robust LVLM alignment by reframing alignment as economically rational search rather than just a safety challenge."}}
{"id": "2511.11323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11323", "abs": "https://arxiv.org/abs/2511.11323", "authors": ["Yitian Kou", "Yihe Gu", "Chen Zhou", "DanDan Zhu", "Shuguang Kuai"], "title": "RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms", "comment": "AAAI 2026", "summary": "Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.", "AI": {"tldr": "RLSLM\u662f\u4e00\u79cd\u878d\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u793e\u4f1a\u8fd0\u52a8\u6a21\u578b\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u793e\u4ea4\u611f\u77e5\u5bfc\u822a\uff0c\u5728\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u7f3a\u4e4f\u6cdb\u5316\u6027\uff0c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u4e0d\u900f\u660e\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u6765\u89e3\u51b3\u4eba\u7fa4\u73af\u5883\u4e2d\u7684\u8212\u9002\u5bfc\u822a\u95ee\u9898\u3002", "method": "\u63d0\u51faRLSLM\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u793e\u4f1a\u8fd0\u52a8\u6a21\u578b\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u51fd\u6570\u4e2d\uff0c\u751f\u6210\u65b9\u5411\u654f\u611f\u7684\u793e\u4ea4\u8212\u9002\u573a\uff0c\u5171\u540c\u4f18\u5316\u673a\u68b0\u80fd\u548c\u793e\u4ea4\u8212\u9002\u5ea6\u3002", "result": "\u901a\u8fc7VR\u4ea4\u4e92\u5b9e\u9a8c\u8bc1\u660eRLSLM\u5728\u7528\u6237\u4f53\u9a8c\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u89c4\u5219\u6a21\u578b\uff0c\u6d88\u878d\u5206\u6790\u663e\u793a\u5176\u76f8\u6bd4\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u8bba\uff0c\u6709\u6548\u6574\u5408\u8ba4\u77e5\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\uff0c\u53ef\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u793e\u4ea4\u5bfc\u822a\u573a\u666f\u3002"}}
{"id": "2511.11357", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11357", "abs": "https://arxiv.org/abs/2511.11357", "authors": ["Haixin Li", "Yanke Li", "Diego Paez-Granados"], "title": "KarmaTS: A Universal Simulation Platform for Multivariate Time Series with Functional Causal Dynamics", "comment": null, "summary": "We introduce KarmaTS, an interactive framework for constructing lag-indexed, executable spatiotemporal causal graphical models for multivariate time series (MTS) simulation. Motivated by the challenge of access-restricted physiological data, KarmaTS generates synthetic MTS with known causal dynamics and augments real-world datasets with expert knowledge. The system constructs a discrete-time structural causal process (DSCP) by combining expert knowledge and algorithmic proposals in a mixed-initiative, human-in-the-loop workflow. The resulting DSCP supports simulation and causal interventions, including those under user-specified distribution shifts. KarmaTS handles mixed variable types, contemporaneous and lagged edges, and modular edge functionals ranging from parameterizable templates to neural network models. Together, these features enable flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation.", "AI": {"tldr": "KarmaTS is an interactive framework for creating executable spatiotemporal causal models to generate synthetic multivariate time series data.", "motivation": "Addresses the challenge of limited access to physiological data by enabling generation of synthetic MTS with known causal dynamics.", "method": "Combines expert knowledge and algorithmic proposals in a human-in-the-loop workflow to construct discrete-time structural causal processes.", "result": "Generates synthetic MTS supporting causal interventions and distribution shifts, handling mixed variable types and modular edge functionals.", "conclusion": "Enables flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation."}}
{"id": "2511.11373", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11373", "abs": "https://arxiv.org/abs/2511.11373", "authors": ["Shulin Liu", "Dong Du", "Tao Yang", "Yang Li", "Boyu Qiu"], "title": "MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism", "comment": "10 pages", "summary": "Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.", "AI": {"tldr": "MarsRL is a reinforcement learning framework with agentic pipeline parallelism that improves multi-agent reasoning in open-source LLMs by jointly optimizing all agents. It achieves significant performance gains on math reasoning benchmarks.", "motivation": "Current multi-agent reasoning systems work well for closed-source models but struggle with open-source LLMs due to weak critic and correction capabilities, limiting their generalization.", "method": "Proposes MarsRL framework with agent-specific reward mechanisms to reduce noise and pipeline-inspired training for efficient long trajectory handling. Applied to Qwen3-30B model.", "result": "Improved AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, surpassing even larger models like Qwen3-235B.", "conclusion": "MarsRL demonstrates strong potential to advance multi-agent reasoning systems and expand their applicability across diverse reasoning tasks."}}
{"id": "2511.11393", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11393", "abs": "https://arxiv.org/abs/2511.11393", "authors": ["Zejiao Liu", "Yi Li", "Jiali Wang", "Junqi Tu", "Yitian Hong", "Fangfei Li", "Yang Liu", "Toshiharu Sugawara", "Yang Tang"], "title": "Robust and Efficient Communication in Multi-Agent Reinforcement Learning", "comment": null, "summary": "Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u7684\u9c81\u68d2\u9ad8\u6548\u901a\u4fe1\u7b56\u7565\uff0c\u5305\u62ec\u6d88\u606f\u6270\u52a8\u3001\u4f20\u8f93\u5ef6\u8fdf\u548c\u5e26\u5bbd\u9650\u5236\uff0c\u5e76\u63a2\u8ba8\u4e86\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u5206\u5e03\u5f0fSLAM\u548c\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u73b0\u6709MARL\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u901a\u4fe1\u662f\u77ac\u65f6\u3001\u53ef\u9760\u4e14\u5e26\u5bbd\u65e0\u9650\u7684\uff0c\u4f46\u8fd9\u4e9b\u5047\u8bbe\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u5f88\u5c11\u6210\u7acb\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5728\u771f\u5b9e\u7ea6\u675f\u4e0b\u7684\u901a\u4fe1\u7b56\u7565\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e86\u8fd1\u671f\u5728MARL\u4e2d\u9488\u5bf9\u6d88\u606f\u6270\u52a8\u3001\u4f20\u8f93\u5ef6\u8fdf\u548c\u6709\u9650\u5e26\u5bbd\u7684\u9c81\u68d2\u9ad8\u6548\u901a\u4fe1\u7b56\u7565\u7684\u8fdb\u5c55\u3002", "result": "\u8bba\u6587\u805a\u7126\u4e8e\u4e09\u4e2a\u5e94\u7528\u9886\u57df\uff1a\u534f\u4f5c\u81ea\u52a8\u9a7e\u9a76\u3001\u5206\u5e03\u5f0f\u540c\u6b65\u5b9a\u4f4d\u4e0e\u5730\u56fe\u6784\u5efa\uff08SLAM\uff09\u4ee5\u53ca\u8054\u90a6\u5b66\u4e60\uff0c\u5206\u6790\u4e86\u901a\u4fe1\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u8bc6\u522b\u4e86\u5173\u952e\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u63d0\u5021\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\uff0c\u5171\u540c\u8bbe\u8ba1\u901a\u4fe1\u3001\u5b66\u4e60\u548c\u9c81\u68d2\u6027\uff0c\u4ee5\u5f25\u5408\u7406\u8bbaMARL\u6a21\u578b\u4e0e\u5b9e\u9645\u5b9e\u73b0\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2511.11423", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11423", "abs": "https://arxiv.org/abs/2511.11423", "authors": ["Cong-Tinh Dao", "Nguyen Minh Thao Phan", "Jun-En Ding", "Chenwei Wu", "David Restrepo", "Dongsheng Luo", "Fanyi Zhao", "Chun-Chieh Liao", "Wen-Chih Peng", "Chi-Te Wang", "Pei-Fu Chen", "Ling Chen", "Xinglong Ju", "Feng Liu", "Fang-Ming Hung"], "title": "CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction", "comment": null, "summary": "Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.", "AI": {"tldr": "CURENet\u6a21\u578b\u6574\u5408\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u591a\u6a21\u6001\u6570\u636e\uff08\u4e34\u5e8a\u7b14\u8bb0\u3001\u5316\u9a8c\u6570\u636e\u548c\u65f6\u5e8f\u6570\u636e\uff09\uff0c\u5229\u7528LLM\u548cTransformer\u7f16\u7801\u5668\uff0c\u5728\u6162\u6027\u75c5\u9884\u6d4b\u4e2d\u8fbe\u523094%\u4ee5\u4e0a\u51c6\u786e\u7387", "motivation": "\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u672a\u80fd\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u95f4\u7684\u4ea4\u4e92\u3001\u5197\u4f59\u548c\u65f6\u5e8f\u6a21\u5f0f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u6574\u5408\u591a\u79cd\u6570\u636e\u7c7b\u578b\u7684\u7efc\u5408\u6a21\u578b\u6765\u6539\u5584\u4e34\u5e8a\u51b3\u7b56", "method": "\u63d0\u51faCURENet\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4f7f\u7528LLM\u5904\u7406\u4e34\u5e8a\u6587\u672c\u548c\u5316\u9a8c\u6570\u636e\uff0cTransformer\u7f16\u7801\u5668\u5904\u7406\u7eb5\u5411\u65f6\u5e8f\u5c31\u8bca\u6570\u636e\uff0c\u6355\u6349\u4e0d\u540c\u4e34\u5e8a\u6570\u636e\u95f4\u7684\u590d\u6742\u4ea4\u4e92", "result": "\u5728MIMIC-III\u548cFEMH\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5728\u591a\u6807\u7b7e\u6846\u67b6\u4e0b\u5bf9\u524d10\u79cd\u6162\u6027\u75c5\u9884\u6d4b\u51c6\u786e\u7387\u8d85\u8fc794%", "conclusion": "\u591a\u6a21\u6001EHR\u6574\u5408\u5177\u6709\u589e\u5f3a\u4e34\u5e8a\u51b3\u7b56\u548c\u6539\u5584\u60a3\u8005\u9884\u540e\u7684\u6f5c\u529b\uff0cCURENet\u5c55\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027"}}
{"id": "2511.11519", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11519", "abs": "https://arxiv.org/abs/2511.11519", "authors": ["Adam Stein", "Matthew Trager", "Benjamin Bowman", "Michael Kleinman", "Aditya Chattopadhyay", "Wei Xia", "Stefano Soatto"], "title": "Experience-Guided Adaptation of Inference-Time Reasoning Strategies", "comment": "29 pages, 5 figures", "summary": "Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.", "AI": {"tldr": "EGuR is an AI system that dynamically generates tailored problem-solving strategies at inference time using accumulated experience, achieving significant performance improvements and cost reductions.", "motivation": "Existing AI systems lack the ability to adapt their problem-solving approaches flexibly after training, being limited to modifying textual inputs or requiring offline optimization.", "method": "EGuR uses an LLM-based meta-strategy with two components: a Guide that generates candidate strategies based on current problems and past experiences, and a Consolidator that integrates execution feedback.", "result": "EGuR achieved up to 14% accuracy improvements over baselines and 111x computational cost reduction across five challenging benchmarks, with performance improving with experience.", "conclusion": "EGuR enables dynamic adaptation of complete computational strategies at inference time, overcoming limitations of existing systems while improving both performance and efficiency."}}
{"id": "2511.11551", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11551", "abs": "https://arxiv.org/abs/2511.11551", "authors": ["Dena Mujtaba", "Brian Hu", "Anthony Hoogs", "Arslan Basharat"], "title": "Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping", "comment": "Accepted to AAAI 2026 AI Alignment Track", "summary": "The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.", "AI": {"tldr": "A test-time alignment technique using model-guided policy shaping to control AI agent behavior without retraining, evaluated on ethical decision-making in text-based games.", "motivation": "AI agents may adopt harmful behaviors when maximizing rewards, creating alignment challenges, especially for pre-trained agents where retraining is costly.", "method": "Policy shaping via scenario-action attribute classifiers applied at test time to ensure ethical alignment in diverse RL environments.", "result": "Effective mitigation of unethical behavior across environments and attributes, outperforming training-time methods and general-purpose agents.", "conclusion": "Test-time policy shaping offers a scalable solution for AI alignment without requiring costly retraining."}}
