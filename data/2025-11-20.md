<div id=toc></div>

# Table of Contents

- [cs.CE](#cs.CE) [Total: 2]
- [cs.CR](#cs.CR) [Total: 12]
- [cs.SE](#cs.SE) [Total: 15]
- [cs.AI](#cs.AI) [Total: 19]


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [1] [A High-Fidelity Neurosurgical Training Platform for Bimanual Procedures: A Feasibility Study](https://arxiv.org/abs/2511.14879)
*Houssem-Eddine Gueziri,Abicumaran Uthamacumaran,Widad Safih,Abdulrahman Almansouri,Nour Abou Hamdan,Jose A. Correa,Étienne Léger,D. Louis Collins,Rolando F. Del Maestro*

Main category: cs.CE

TL;DR: 开发了一个神经外科模拟平台，通过追踪双手手术器械运动来评估双任务操作技能，可用于不同经验水平外科医生的客观评估。


<details>
  <summary>Details</summary>
Motivation: 双任务操作熟练度对神经外科手术至关重要，但学员难以掌握且缺乏客观评估方法。

Method: 基于离体小牛脑模型和多摄像头追踪系统开发模拟平台，提取运动、时间和双手协调指标，涉及47名不同培训水平的参与者。

Result: 追踪系统成功捕获81%的器械使用时段，多个指标能显著区分外科专业水平，特别是器械使用时长和双手协调指标。

Conclusion: 证明在离体脑模拟平台中追踪复杂双任务操作的可行性，为客观绩效评估奠定基础，运动分析有潜力改善神经外科培训。

Abstract: Background. Bimanual psychomotor proficiency is fundamental to neurosurgical procedures, yet it remains difficult for trainees to acquire and for educators to objectively evaluate performance. In this study, we investigate the feasibility of a neurosurgical simulation platform that integrates an anatomically realistic brain model with surgical instrument tracking to support training and objective assessment of bimanual tasks in the context of subpial corticectomy. Methods. We developed and evaluated a neurosurgical simulation platform based on an ex-vivo calf brain model and a multi-camera tracking system capable of simultaneously capturing the motion of surgical instruments in both hands, including collection of real-time instrument trajectories and synchronized video recordings. These enabled extraction of motion-based, time-based, and bimanual coordination metrics. We conducted a case series involving 47 participants across four training levels: medical students, junior residents, senior residents, and neurosurgeons. Results. The tracking system successfully captured instrument motion during 81% of the periods when instruments were actively used throughout the simulation procedure. Several extracted metrics were able to significantly differentiate between levels of surgical expertise. In particular, instrument usage duration and custom-defined bimanual coordination metrics such as instrument tip separation distance and simultaneous usage time, show potential as features to identify participant expertise levels with different instruments. Conclusions. We demonstrated the feasibility of tracking surgical instruments during complex bimanual tasks in an ex-vivo brain simulation platform. The metrics developed provide a foundation for objective performance assessment and highlight the potential of motion analysis to improve neurosurgical training and evaluation.

</details>


### [2] [The Walls Have Ears: Unveiling Cross-Chain Sandwich Attacks in DeFi](https://arxiv.org/abs/2511.15245)
*Chuanlei Li,Zhicheng Sun,Jing Xin Yuu,Xuechao Wang*

Main category: cs.CE

TL;DR: 本文揭示了跨链三明治攻击的新漏洞，攻击者利用源链事件预测目标链交易，在现有MEV机器人之前进行前端运行和后端运行交易，在Symbiosis协议两个月数据中获利527万美元。


<details>
  <summary>Details</summary>
Motivation: 跨链互操作性的透明度可能暴露敏感交易信息，使攻击者能够利用价值操纵或前端运行策略。

Method: 通过对Symbiosis协议2025年8月10日至10月10日的跨链交易数据进行实证研究，使用定制启发式检测模型分析攻击模式。

Result: 识别出的攻击总计获利超过527万美元，相当于总桥接量的1.28%。当前的三明治攻击防御措施对这种新型跨链变体无效。

Conclusion: 跨链三明治攻击构成严重威胁，需要新的防御机制来保护跨链协议免受此类利用。

Abstract: Cross-chain interoperability is a core component of modern blockchain infrastructure, enabling seamless asset transfers and composable applications across multiple blockchain ecosystems. However, the transparency of cross-chain messages can inadvertently expose sensitive transaction information, creating opportunities for adversaries to exploit value through manipulation or front-running strategies.
  In this work, we investigate cross-chain sandwich attacks targeting liquidity pool-based cross-chain bridge protocols. We uncover a critical vulnerability where attackers can exploit events emitted on the source chain to learn transaction details on the destination chain before they appear in the destination chain mempool. This information advantage allows attackers to strategically place front-running and back-running transactions, ensuring that their front-running transactions always precede those of existing MEV bots monitoring the mempool of the destination chain. Moreover, current sandwich-attack defenses are ineffective against this new cross-chain variant. To quantify this threat, we conduct an empirical study using two months (August 10 to October 10, 2025) of cross-chain transaction data from the Symbiosis protocol and a tailored heuristic detection model. Our analysis identifies attacks that collectively garnered over \(5.27\) million USD in profit, equivalent to 1.28\% of the total bridged volume.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [3] [How To Cook The Fragmented Rug Pull?](https://arxiv.org/abs/2511.15463)
*Minh Trung Tran,Nasrin Sohrabi,Zahir Tari,Qin Wang*

Main category: cs.CR

TL;DR: 本文提出了一种新型的碎片化地毯式骗局，攻击者通过拆分交易和多个钱包分散提取流动性，避开了传统检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有地毯式骗局检测器假设攻击者持有LP代币并进行单次大额抛售，但实际攻击往往违反这些假设，通过时间和参与者维度的分散操作实现隐性资金抽离。

Method: 将碎片化地毯式骗局形式化为三步策略：保持控制权、拆分交易量、委托多个钱包执行；定义三组原子谓词，通过正交组合生成规避策略。

Result: 在大规模测量中识别出105,434个FRP资金池，涉及401,838个膨胀卖家钱包和127,252个连环诈骗钱包，证明规避策略广泛存在。

Conclusion: 碎片化地毯式骗局已成为常见攻击模式，传统检测方法需升级以应对这种新型逃避策略。

Abstract: Existing rug pull detectors assume a simple workflow: the deployer keeps liquidity pool (LP) tokens and performs one or a few large sells (within a day) that collapse the pool and cash out. In practice, however, many real-world exits violate these assumptions by splitting the attack across both time and actor dimensions: attackers break total extraction into many low-impact trades and route proceeds through multiple non-owner addresses, producing low-visibility drains.
  We formalize this family of attacks as the fragmented rug pull (FRP) and offer a compact recipe for a slow-stewed beef special: (i) keep the lid on (to preserve LP control so on-chain extraction remains feasible), (ii) chop thin slices (to split the total exit volume into many low-impact micro-trades that individually fall below impact thresholds), and (iii) pass the ladle (to delegate sells across multiple wallets so that each participant takes a small share of the extraction). Technically, we define three atomic predicate groups and show that their orthogonal combinations yield evasive strategies overlooked by prior heuristics (USENIX Sec 19, USENIX Sec 23).
  We validate the model with large-scale measurements. Our corpus contains 303,614 LPs, among which 105,434 are labeled as FRP pools. The labeled subset includes 34,192,767 pool-related transactions and 401,838 inflated-seller wallets, involving 1,501,408 unique interacting addresses. Notably, owner-wallet participation in inflated selling among FRP-flagged LPs has declined substantially (33.1% of cases), indicating a shift in scam behavior: the liquidity drain is no longer held on the owner wallet. We also detected 127,252 wallets acting as serial scammers when repeatedly engaging in inflated selling across multiple FRP LPs. Our empirical findings demonstrate that the evasive strategies we define are widespread and operationally significant.

</details>


### [4] [On-Premise SLMs vs. Commercial LLMs: Prompt Engineering and Incident Classification in SOCs and CSIRTs](https://arxiv.org/abs/2511.14908)
*Gefté Almeida,Marcio Pohlmann,Alex Severo,Diego Kreutz,Tiago Heinrich,Lourenço Pereira*

Main category: cs.CR

TL;DR: 开源模型在安全事件分类中表现接近专有模型，具有隐私、成本和数据主权优势。


<details>
  <summary>Details</summary>
Motivation: 评估开源模型在安全事件分类中的性能，并与专有模型进行比较，以探讨本地部署模型的可行性。

Method: 使用匿名真实事件数据集，按照NIST SP 800-61r3分类法，并应用五种提示工程技巧（PHP、SHP、HTP、PRP、ZSL）进行处理。

Result: 结果显示专有模型准确率更高，但开源模型在隐私保护、成本效益和数据主权方面具有优势。

Conclusion: 开源模型是专有模型的可行替代品，特别适用于注重隐私和数据主权的场景。

Abstract: In this study, we evaluate open-source models for security incident classification, comparing them with proprietary models. We utilize a dataset of anonymized real incidents, categorized according to the NIST SP 800-61r3 taxonomy and processed using five prompt-engineering techniques (PHP, SHP, HTP, PRP, and ZSL). The results indicate that, although proprietary models still exhibit higher accuracy, locally deployed open-source models provide advantages in privacy, cost-effectiveness, and data sovereignty.

</details>


### [5] [CIMemories: A Compositional Benchmark for Contextual Integrity of Persistent Memory in LLMs](https://arxiv.org/abs/2511.14937)
*Niloofar Mireshghallah,Neal Mangaokar,Narine Kokhlikyan,Arman Zharmagambetov,Manzil Zaheer,Saeed Mahloujifar,Kamalika Chaudhuri*

Main category: cs.CR

TL;DR: 大型语言模型在记忆管理方面存在敏感信息泄漏风险，提出了衡量模型在不同任务中恰当地管理记忆信息流的基准，并发现了严重的泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地利用过往交互的持久记忆来增强个性化和任务性能，这种记忆在不当情境下暴露敏感信息可能带来严重风险。

Method: 开发了CIMemories基准，使用合成的用户档案进行测试，并为每个用户配备了超过100个属性，以及多样化的任务情境，在这些情境中，某些属性对某些任务至关重要，而对其他任务则不适用。

Result: 前沿模型在属性级别的违规率高达69%（不适当地泄漏信息）。当任务从1增加到40时，GPT-5的违规率从0.1%上升到9.6%；当同一提示执行5次时，违规率升至25.1%，显示了模型相同提示下泄漏不同属性的任意和不稳定行为。提高隐私意识的提示并不能解决这一问题。

Conclusion: 当前的LLMs在语境意识推理能力方面的根本局限导致了这些泄漏问题，仅靠改进提示或规模扩张无法解决，需要更深层次的语境理解能力。

Abstract: Large Language Models (LLMs) increasingly use persistent memory from past interactions to enhance personalization and task performance. However, this memory introduces critical risks when sensitive information is revealed in inappropriate contexts. We present CIMemories, a benchmark for evaluating whether LLMs appropriately control information flow from memory based on task context. CIMemories uses synthetic user profiles with over 100 attributes per user, paired with diverse task contexts in which each attribute may be essential for some tasks but inappropriate for others. Our evaluation reveals that frontier models exhibit up to 69% attribute-level violations (leaking information inappropriately), with lower violation rates often coming at the cost of task utility. Violations accumulate across both tasks and runs: as usage increases from 1 to 40 tasks, GPT-5's violations rise from 0.1% to 9.6%, reaching 25.1% when the same prompt is executed 5 times, revealing arbitrary and unstable behavior in which models leak different attributes for identical prompts. Privacy-conscious prompting does not solve this - models overgeneralize, sharing everything or nothing rather than making nuanced, context-dependent decisions. These findings reveal fundamental limitations that require contextually aware reasoning capabilities, not just better prompting or scaling.

</details>


### [6] [LFreeDA: Label-Free Drift Adaptation for Windows Malware Detection](https://arxiv.org/abs/2511.14963)
*Adrian Shuai Li,Elisa Bertino*

Main category: cs.CR

TL;DR: LFreeDA是一个端到端框架，在没有人工标注或漂移检测的情况下，通过联合训练标记和未标记样本适应恶意软件分类器，其性能和完全监督方法较为接近。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法充分利用未标记数据的潜在价值，LFreeDA旨在实现完全无标注的适配以克服概念漂移对恶意软件检测器的影响。

Method: 通过恶意软件图像进行无监督域适配，联合训练标注和未标记样本以推断伪标签并去除噪声，然后利用CFG表示对分类器进行适配。

Result: 在真实数据集MB-24+上，相比无适配下限提高了准确性12.6%和F1分数11.1%，同时在准确性上仅低于完全监督上限4%，F1分数上低3.4%。

Conclusion: LFreeDA在无需人工标注的情况下能有效维护恶意软件检测性能以适应恶意软件的演化，具有显著的应用前景。

Abstract: Machine learning (ML)-based malware detectors degrade over time as concept drift introduces new and evolving families unseen during training. Retraining is limited by the cost and time of manual labeling or sandbox analysis. Existing approaches mitigate this via drift detection and selective labeling, but fully label-free adaptation remains largely unexplored. Recent self-training methods use a previously trained model to generate pseudo-labels for unlabeled data and then train a new model on these labels. The unlabeled data are used only for inference and do not participate in training the earlier model. We argue that these unlabeled samples still carry valuable information that can be leveraged when incorporated appropriately into training. This paper introduces LFreeDA, an end-to-end framework that adapts malware classifiers to drift without manual labeling or drift detection. LFreeDA first performs unsupervised domain adaptation on malware images, jointly training on labeled and unlabeled samples to infer pseudo-labels and prune noisy ones. It then adapts a classifier on CFG representations using the labeled and selected pseudo-labeled data, leveraging the scalability of images for pseudo-labeling and the richer semantics of CFGs for final adaptation. Evaluations on the real-world MB-24+ dataset show that LFreeDA improves accuracy by up to 12.6% and F1 by 11.1% over no-adaptation lower bounds, and is only 4% and 3.4% below fully supervised upper bounds in accuracy and F1, respectively. It also matches the performance of state-of-the-art methods provided with ground truth labels for 300 target samples. Additional results on two controlled-drift benchmarks further confirm that LFreeDA maintains malware detection performance as malware evolves without human labeling.

</details>


### [7] [GeoShield: Byzantine Fault Detection and Recovery for Geo-Distributed Real-Time Cyber-Physical Systems](https://arxiv.org/abs/2511.15031)
*Yifan Cai,Linh Thi Xuan Phan*

Main category: cs.CR

TL;DR: GeoShield is a Byzantine fault-tolerant solution for geo-distributed CPS that detects faults and ensures bounded-time recovery using efficient protocols, significantly outperforming existing methods in resource efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing fault-tolerant methods for large-scale CPS are either too resource-intensive or provide only eventual guarantees, making them unsuitable for real-time systems vulnerable to faults and attacks.

Method: GeoShield uses Byzantine fault-resilient network measurement, inter-region omission fault detection to identify malicious delays, and recovery mechanisms for bounded-time recovery without trusted hardware.

Result: Evaluations show GeoShield outperforms existing methods in effectiveness and resource efficiency for real-world CPS applications.

Conclusion: GeoShield provides a practical, resource-efficient solution for Byzantine fault tolerance in CPS, ensuring safety through timely detection and recovery.

Abstract: Large-scale cyber-physical systems (CPS), such as railway control systems and smart grids, consist of geographically distributed subsystems that are connected via unreliable, asynchronous inter-region networks. Their scale and distribution make them especially vulnerable to faults and attacks. Unfortunately, existing fault-tolerant methods either consume excessive resources or provide only eventual guarantees, making them unsuitable for real-time resource-constrained CPS.
  We present GeoShield, a resource-efficient solution for defending geo-distributed CPS against Byzantine faults. GeoShield leverages the property that CPS are designed to tolerate brief disruptions and maintain safety, as long as they recover (i.e., resume normal operations or transition to a safe mode) within a bounded amount of time following a fault. Instead of masking faults, it detects them and recovers the system within bounded time, thus guaranteeing safety with much fewer resources. GeoShield introduces protocols for Byzantine fault-resilient network measurement and inter-region omission fault detection that proactively detect malicious message delays, along with recovery mechanisms that guarantee timely recovery while maximizing operational robustness. It is the first bounded-time recovery solution that operates effectively under unreliable networks without relying on trusted hardware. Evaluations using real-world case studies show that it significantly outperforms existing methods in both effectiveness and resource efficiency.

</details>


### [8] [Towards Classifying Benign And Malicious Packages Using Machine Learning](https://arxiv.org/abs/2511.15033)
*Thanh-Cong Nguyen,Ngoc-Thanh Nguyen,Van-Giau Ung,Duc-Ly Vu*

Main category: cs.CR

TL;DR: 论文提出使用动态分析提取特征并结合机器学习自动检测恶意开源软件包的方法，在npm包测试中达到0.91的AUC和接近0%的误报率。


<details>
  <summary>Details</summary>
Motivation: 恶意开源软件包数量激增，现有安全扫描器主要关注已知CVE漏洞，缺乏有效的恶意包自动检测方法，特别是基于动态分析的方法。

Method: 从动态分析（如执行的命令）中提取特征，利用机器学习技术对软件包进行良性或恶性的自动分类。

Result: 在近2000个npm包上的评估显示，机器学习分类器的AUC达到0.91，误报率接近0%。

Conclusion: 该方法能有效识别恶意开源软件包，为软件供应链安全提供了实用的自动化检测方案。

Abstract: Recently, the number of malicious open-source packages in package repositories has been increasing dramatically. While major security scanners focus on identifying known Common Vulnerabilities and Exposures (CVEs) in open-source packages, there are very few studies on detecting malicious packages. Malicious open-source package detection typically requires static, dynamic analysis, or both. Dynamic analysis is more effective as it can expose a package's behaviors at runtime. However, current dynamic analysis tools (e.g., ossf's package-analysis) lack an automatic method to differentiate malicious packages from benign packages. In this paper, we propose an approach to extract the features from dynamic analysis (e.g., executed commands) and leverage machine learning techniques to automatically classify packages as benign or malicious. Our evaluation of nearly 2000 packages on npm shows that the machine learning classifier achieves an AUC of 0.91 with a false positive rate of nearly 0%.

</details>


### [9] [Towards Practical Zero-Knowledge Proof for PSPACE](https://arxiv.org/abs/2511.15071)
*Ashwin Karthikeyan,Hengyu Liu,Kuldeep S. Meel,Ning Luo*

Main category: cs.CR

TL;DR: 这篇论文提出了首个实用的PSPACE完全语句的零知识证明协议，通过量化布尔公式(QBF)评估实现。核心创新是零知识验证量化解析证明(Q-Res)，并设计了获胜策略的证明协议。


<details>
  <summary>Details</summary>
Motivation: 现有的高效零知识证明局限于NP语句，而PSPACE语句的零知识证明虽然理论存在但缺乏实际可行方案。量化布尔公式评估是PSPACE完全问题，具有重要应用价值。

Method: 开发了Q-Res证明的高效多项式编码，通过低开销算术检查实现证明验证；设计了与QBF相关的获胜策略的零知识证明协议；在QBFEVAL基准上进行了实现和评估。

Result: 实验表明，在可获取证明或策略的实例中，72%的QBF评估可通过Q-Res证明验证，82%的实例的获胜策略可在100秒内完成验证。

Conclusion: 该工作首次实现了PSPACE完全语句的实用零知识证明，扩展了零知识证明的应用范围，为更复杂语句的零知识验证提供了可行方案。

Abstract: Efficient zero-knowledge proofs (ZKPs) have been restricted to NP statements so far, whereas they exist for all statements in PSPACE. This work presents the first practical zero-knowledge (ZK) protocols for PSPACE-complete statements by enabling ZK proofs of QBF (Quantified Boolean Formula) evaluation. The core idea is to validate quantified resolution proofs (Q-Res) in ZK. We develop an efficient polynomial encoding of Q-Res proofs, enabling proof validation through low-overhead arithmetic checks. We also design a ZK protocol to prove knowledge of a winning strategy related to the QBF, which is often equally important in practice. We implement our protocols and evaluate them on QBFEVAL. The results show that our protocols can verify 72% of QBF evaluations via Q-Res proof and 82% of instances' winning strategies within 100 seconds, for instances where such proofs or strategies can be obtained.

</details>


### [10] [MAIF: Enforcing AI Trust and Provenance with an Artifact-Centric Agentic Paradigm](https://arxiv.org/abs/2511.15097)
*Vineeth Sai Narajala,Manish Bhatt,Idan Habler,Ronald F. Del Rosario*

Main category: cs.CR

TL;DR: This paper introduces an artifact-centric AI agent paradigm using the Multimodal Artifact File Format (MAIF) to address AI trustworthiness issues through verifiable data artifacts with built-in auditability, security, and regulatory compliance.


<details>
  <summary>Details</summary>
Motivation: Current AI systems lack the transparency, audit trails, and security required by regulations, preventing deployment in critical domains due to trustworthiness concerns.

Method: Proposes MAIF - an AI-native container format with semantic representations, cryptographic provenance, granular access controls, and algorithms for cross-modal attention and semantic compression.

Result: Achieves high-performance streaming (2,720.7 MB/s), optimized video processing (1,342 MB/s), up to 225x compression while maintaining semantic fidelity, with minimal security overhead.

Conclusion: The artifact-centric approach provides a viable solution to AI trustworthiness challenges, enabling auditable and secure AI deployment in sensitive domains at scale.

Abstract: The AI trustworthiness crisis threatens to derail the artificial intelligence revolution, with regulatory barriers, security vulnerabilities, and accountability gaps preventing deployment in critical domains. Current AI systems operate on opaque data structures that lack the audit trails, provenance tracking, or explainability required by emerging regulations like the EU AI Act. We propose an artifact-centric AI agent paradigm where behavior is driven by persistent, verifiable data artifacts rather than ephemeral tasks, solving the trustworthiness problem at the data architecture level. Central to this approach is the Multimodal Artifact File Format (MAIF), an AI-native container embedding semantic representations, cryptographic provenance, and granular access controls. MAIF transforms data from passive storage into active trust enforcement, making every AI operation inherently auditable. Our production-ready implementation demonstrates ultra-high-speed streaming (2,720.7 MB/s), optimized video processing (1,342 MB/s), and enterprise-grade security. Novel algorithms for cross-modal attention, semantic compression, and cryptographic binding achieve up to 225 compression while maintaining semantic fidelity. Advanced security features include stream-level access control, real-time tamper detection, and behavioral anomaly analysis with minimal overhead. This approach directly addresses the regulatory, security, and accountability challenges preventing AI deployment in sensitive domains, offering a viable path toward trustworthy AI systems at scale.

</details>


### [11] [Can MLLMs Detect Phishing? A Comprehensive Security Benchmark Suite Focusing on Dynamic Threats and Multimodal Evaluation in Academic Environments](https://arxiv.org/abs/2511.15165)
*Jingzhuo Zhou*

Main category: cs.CR

TL;DR: 提出AdapT-Bench框架，用于评估多模态大语言模型在学术环境中防御动态钓鱼攻击的能力


<details>
  <summary>Details</summary>
Motivation: 学术机构是高价值目标，面临基于学术背景的高度定制化钓鱼攻击，现有安全基准缺乏学术背景信息，无法有效评估针对性威胁

Method: 开发统一的方法论框架和基准套件AdapT-Bench，系统评估MLLM防御能力

Result: 创建了专门针对学术环境的动态钓鱼攻击评估基准

Conclusion: AdapT-Bench解决了现有基准在捕捉学术领域特定攻击模式方面的不足，为学术安全提供更有效的评估工具

Abstract: The rapid proliferation of Multimodal Large Language Models (MLLMs) has introduced unprecedented security challenges, particularly in phishing detection within academic environments. Academic institutions and researchers are high-value targets, facing dynamic, multilingual, and context-dependent threats that leverage research backgrounds, academic collaborations, and personal information to craft highly tailored attacks. Existing security benchmarks largely rely on datasets that do not incorporate specific academic background information, making them inadequate for capturing the evolving attack patterns and human-centric vulnerability factors specific to academia. To address this gap, we present AdapT-Bench, a unified methodological framework and benchmark suite for systematically evaluating MLLM defense capabilities against dynamic phishing attacks in academic settings.

</details>


### [12] [Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks](https://arxiv.org/abs/2511.15203)
*Zimo Ji,Xunguang Wang,Zongjie Li,Pingchuan Ma,Yudong Gao,Daoyuan Wu,Xincheng Yan,Tian Tian,Shuai Wang*

Main category: cs.CR

TL;DR: This paper presents a systematization of knowledge (SoK) on defense frameworks against Indirect Prompt Injection (IPI) attacks targeting LLM-based agents, offering a taxonomy, security/usability assessment, root cause analysis of defense failures, and novel adaptive attacks.


<details>
  <summary>Details</summary>
Motivation: The increasing deployment of LLM-based agents with function-calling capabilities makes them vulnerable to IPI attacks that hijack tool calls. Existing IPI defense frameworks are fragmented and lack unified taxonomy and comprehensive evaluation, highlighting the need for systematic analysis.

Method: The authors conduct a comprehensive analysis of IPI-centric defense frameworks by: 1) Creating a taxonomy classifying defenses along five dimensions; 2) Thoroughly assessing security and usability of representative frameworks; 3) Analyzing defensive failures to identify root causes of circumvention.

Result: The study identified six root causes of defense circumvention and designed three novel adaptive attacks that significantly improve attack success rates against specific frameworks, demonstrating severe flaws in current defenses.

Conclusion: This SoK provides foundational insights for developing more secure and usable IPI-centric agent defense frameworks, highlighting critical vulnerabilities in existing approaches and the effectiveness of adaptive attacks in bypassing them.

Abstract: Large Language Model (LLM)-based agents with function-calling capabilities are increasingly deployed, but remain vulnerable to Indirect Prompt Injection (IPI) attacks that hijack their tool calls. In response, numerous IPI-centric defense frameworks have emerged. However, these defenses are fragmented, lacking a unified taxonomy and comprehensive evaluation. In this Systematization of Knowledge (SoK), we present the first comprehensive analysis of IPI-centric defense frameworks. We introduce a comprehensive taxonomy of these defenses, classifying them along five dimensions. We then thoroughly assess the security and usability of representative defense frameworks. Through analysis of defensive failures in the assessment, we identify six root causes of defense circumvention. Based on these findings, we design three novel adaptive attacks that significantly improve attack success rates targeting specific frameworks, demonstrating the severity of the flaws in these defenses. Our paper provides a foundation and critical insights for the future development of more secure and usable IPI-centric agent defense frameworks.

</details>


### [13] [Small Language Models for Phishing Website Detection: Cost, Performance, and Privacy Trade-Offs](https://arxiv.org/abs/2511.15434)
*Georg Goldenits,Philip Koenig,Sebastian Raubitzek,Andreas Ekelhart*

Main category: cs.CR

TL;DR: The paper investigates using small language models (SLMs) for phishing website detection using raw HTML, comparing 15 models (1B-70B parameters) to show they offer a viable alternative to proprietary LLMs despite lower accuracy but with better cost-efficiency and local deployment benefits.


<details>
  <summary>Details</summary>
Motivation: Traditional ML phishing detection requires extensive feature engineering and infrastructure, while proprietary LLMs are costly and dependent on external providers. SLMs offer a potential solution with local deployment and cost control.

Method: Systematic evaluation of 15 SLMs on phishing website classification using raw HTML, assessing accuracy, computational requirements, and cost-efficiency compared to proprietary LLMs.

Result: SLMs underperform state-of-the-art proprietary LLMs in detection accuracy but provide a viable, scalable alternative with better resource efficiency and local deployment advantages.

Conclusion: SLMs present a practical balance between security effectiveness and economic practicality for phishing detection, with potential for improvement through future adaptation and fine-tuning.

Abstract: Phishing websites pose a major cybersecurity threat, exploiting unsuspecting users and causing significant financial and organisational harm. Traditional machine learning approaches for phishing detection often require extensive feature engineering, continuous retraining, and costly infrastructure maintenance. At the same time, proprietary large language models (LLMs) have demonstrated strong performance in phishing-related classification tasks, but their operational costs and reliance on external providers limit their practical adoption in many business environments. This paper investigates the feasibility of small language models (SLMs) for detecting phishing websites using only their raw HTML code. A key advantage of these models is that they can be deployed on local infrastructure, providing organisations with greater control over data and operations. We systematically evaluate 15 commonly used Small Language Models (SLMs), ranging from 1 billion to 70 billion parameters, benchmarking their classification accuracy, computational requirements, and cost-efficiency. Our results highlight the trade-offs between detection performance and resource consumption, demonstrating that while SLMs underperform compared to state-of-the-art proprietary LLMs, they can still provide a viable and scalable alternative to external LLM services. By presenting a comparative analysis of costs and benefits, this work lays the foundation for future research on the adaptation, fine-tuning, and deployment of SLMs in phishing detection systems, aiming to balance security effectiveness and economic practicality.

</details>


### [14] [Beluga: Block Synchronization for BFT Consensus Protocols](https://arxiv.org/abs/2511.15517)
*Tasos Kichidis,Lefteris Kokoris-Kogias,Arun Koshy,Ilya Sergey,Alberto Sonnino,Mingwei Tian,Jianting Zhang*

Main category: cs.CR

TL;DR: Beluga is a modular block synchronizer for BFT consensus that optimizes block exchange, prevents bandwidth exhaustion attacks, and maintains high performance under adversarial conditions.


<details>
  <summary>Details</summary>
Motivation: Modern BFT protocols lack efficient block exchange mechanisms, making them vulnerable to targeted attacks and performance degradation during network asynchrony.

Method: Introduces a block synchronizer abstraction for incremental, resource-aware block retrieval. Beluga implements this with scarcity-aware scheduling to prevent redundant pulls.

Result: Integrated into Mysticeti/Sui blockchain, Beluga achieves optimal latency normally and 3x higher throughput with 25x lower latency under attack in AWS tests.

Conclusion: Beluga provides principled block synchronization that prevents performance collapse, is adopted in production by Sui, and offers significant improvements over prior designs.

Abstract: Modern high-throughput BFT consensus protocols use streamlined push-pull mechanisms to disseminate blocks and keep happy-path performance optimal. Yet state-of-the-art designs lack a principled and efficient way to exchange blocks, which leaves them open to targeted attacks and performance collapse under network asynchrony. This work introduces the concept of a block synchronizer, a simple abstraction that drives incremental block retrieval and enforces resource-aware exchange. Its interface and role fit cleanly inside a modern BFT consensus stack. We also uncover a new attack, where an adversary steers honest validators into redundant, uncoordinated pulls that exhaust bandwidth and stall progress. Beluga is a modular and scarcity-aware instantiation of the block synchronizer. It achieves optimal common-case latency while bounding the cost of recovery under faults and adversarial behavior. We integrate Beluga into Mysticeti, the consensus core of the Sui blockchain, and show on a geo-distributed AWS deployment that Beluga sustains optimal performance in the optimistic path and, under attack, delivers up to 3x higher throughput and 25x lower latency than prior designs. The Sui blockchain adopted Beluga in production.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [15] [Hybrid Quantum-Classical Machine Learning with PennyLane: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2511.14786)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: PennyLane框架用于混合量子-经典机器学习，支持量子电路构建、自动微分和优化，与主流ML框架集成。


<details>
  <summary>Details</summary>
Motivation: 结合量子计算优势和经典优化技术，为研究人员提供统一的量子机器学习工具。

Method: 通过Python示例展示量子核方法、变分量子本征求解器等应用，集成PyTorch、TensorFlow等经典ML库。

Result: PennyLane成为量子增强数据科学的方法论基础工具，支持高效混合工作流。

Conclusion: 该框架为基于Python的研究提供了量子-经典混合工作流的默认参考工具。

Abstract: Hybrid quantum-classical machine learning represents a frontier in computational research, combining the potential advantages of quantum computing with established classical optimization techniques. PennyLane provides a Python framework that seamlessly bridges quantum circuits and classical machine learning, enabling researchers to build, optimize, and deploy variational quantum algorithms. This paper introduces PennyLane as a versatile tool for quantum machine learning, optimization, and quantum chemistry applications. We demonstrate use cases including quantum kernel methods, variational quantum eigensolvers, portfolio optimization, and integration with classical ML frameworks such as PyTorch, TensorFlow, and JAX. Through concrete Python examples with widely used libraries such as scikit-learn, pandas, and matplotlib, we show how PennyLane facilitates efficient quantum circuit construction, automatic differentiation, and hybrid optimization workflows. By situating PennyLane within the broader context of quantum computing and machine learning, we highlight its role as a methodological building block for quantum-enhanced data science. Our goal is to provide researchers and practitioners with a concise reference that bridges foundational quantum computing concepts and applied machine learning practice, making PennyLane a default citation for hybrid quantum-classical workflows in Python-based research.

</details>


### [16] [Enabling Predictive Maintenance in District Heating Substations: A Labelled Dataset and Fault Detection Evaluation Framework based on Service Data](https://arxiv.org/abs/2511.14791)
*Cyriana M. A. Roelofs,Edison Guevara Bastidas,Thomas Hugo,Stefan Faulstich,Anna Cadenbach*

Main category: cs.SE

TL;DR: 论文提出了一个开源框架，包含已验证的公共数据集、评估方法和基准结果，用于区域供暖站故障早期检测，实现高准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 区域供暖站故障早期检测对提高效率至关重要，但缺乏公开标注数据集限制了该领域进展。

Method: 结合服务报告验证的公共数据集、基于准确性、可靠性和早期性的评估方法，并使用EnergyFaultDetector开源Python框架实现基准。

Result: 模型实现高正常行为准确度(0.98)和事件F分数(0.83)，60%故障在客户报告前检测到，平均提前3.9天。

Conclusion: 该开源框架为区域供暖站故障检测建立了可复现的基准，支持一致比较和方法开发。

Abstract: Early detection of faults in district heating substations is imperative to reduce return temperatures and enhance efficiency. However, progress in this domain has been hindered by the limited availability of public, labelled datasets. We present an open source framework combining a service report validated public dataset, an evaluation method based on Accuracy, Reliability, and Earliness, and baseline results implemented with EnergyFaultDetector, an open source Python framework.
  The dataset contains time series of operational data from 93 substations across two manufacturers, annotated with a list of disturbances due to faults and maintenance actions, a set of normal-event examples and detailed fault metadata. We evaluate the EnergyFaultDetector using three metrics: Accuracy for recognising normal behaviour, an eventwise F Score for reliable fault detection with few false alarms, and Earliness for early detection. The framework also supports root cause analysis using ARCANA. We demonstrate three use cases to assist operators in interpreting anomalies and identifying underlying faults. The models achieve high normal-behaviour accuracy (0.98) and eventwise F-score (beta=0.5) of 0.83, detecting 60% of the faults in the dataset before the customer reports a problem, with an average lead time of 3.9 days.
  Integrating an open dataset, metrics, open source code, and baselines establishes a reproducible, fault centric benchmark with operationally meaningful evaluation, enabling consistent comparison and development of early fault detection and diagnosis methods for district heating substations.

</details>


### [17] [irace-evo: Automatic Algorithm Configuration Extended With LLM-Based Code Evolution](https://arxiv.org/abs/2511.14794)
*Camilo Chacón Sartori,Christian Blum*

Main category: cs.SE

TL;DR: 这篇论文提出了irace-evo，一个扩展的自动算法配置工具，结合大语言模型进化代码，在多语言支持、低成本和高效能方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的自动算法配置工具如irace只能调参而不能改进算法代码，限制了算法优化的潜力。

Method: 扩展irace工具，集成LLM进行代码进化，采用渐进式上下文管理和Always-From-Original原则确保代码稳健进化。

Result: 在VSBPP问题上，irace-evo发现的算法变体优于现有最优CMSA实现，且总成本低于2欧元。

Conclusion: 结合自动配置与LLM驱动的代码进化，为启发式算法设计提供了强大且经济高效的优化途径。

Abstract: Automatic algorithm configuration tools such as irace efficiently tune parameter values but leave algorithmic code unchanged. This paper introduces a first version of irace-evo, an extension of irace that integrates code evolution through large language models (LLMs) to jointly explore parameter and code spaces. The proposed framework enables multi-language support (e.g., C++, Python), reduces token consumption via progressive context management, and employs the Always-From-Original principle to ensure robust and controlled code evolution. We evaluate irace-evo on the Construct, Merge, Solve & Adapt (CMSA) metaheuristic for the Variable-Sized Bin Packing Problem (VSBPP). Experimental results show that irace-evo can discover new algorithm variants that outperform the state-of-the-art CMSA implementation while maintaining low computational and monetary costs. Notably, irace-evo generates competitive algorithmic improvements using lightweight models (e.g., Claude Haiku 3.5) with a total usage cost under 2 euros. These results demonstrate that coupling automatic configuration with LLM-driven code evolution provides a powerful, cost-efficient avenue for advancing heuristic design and metaheuristic optimization.

</details>


### [18] [Evaluating Generative AI for CS1 Code Grading: Direct vs Reverse Methods](https://arxiv.org/abs/2511.14798)
*Ahmad Memon,Abdallah Mohamed*

Main category: cs.SE

TL;DR: 比较两种AI评分方法：直接评分和反向评分。直接方法让AI直接应用评分标准，反向方法先修复错误再评分。研究发现直接方法更快，反向方法评估更细致。两种方法都需要精心设计的提示词。


<details>
  <summary>Details</summary>
Motivation: 手动评分编程作业耗时且容易不一致。单元测试通常只有通过/不通过，无法给出部分分数。大型语言模型提供了自动化、可扩展和更客观评分的潜力。

Method: 比较两种AI评分技术：直接方法和反向方法。使用原始评分标准和扩展评分标准评估AI评分准确性。使用AI生成的学生代码测试一致性。

Result: 直接方法更快更直接，反向方法通过关注修正工作提供更细粒度的评估。两种方法都需要精心设计的提示词，特别是分配部分分数和处理逻辑错误。

Conclusion: 讨论了每种方法的优缺点，提示设计的实际考虑，以及未来混合人机评分系统的方向，旨在提高计算机科学课程的一致性、效率和公平性。

Abstract: Manual grading of programming assignments in introductory computer science courses can be time-consuming and prone to inconsistencies. While unit testing is commonly used for automatic evaluation, it typically follows a binary pass/fail model and does not give partial marks. Recent advances in large language models (LLMs) offer the potential for automated, scalable, and more objective grading.
  This paper compares two AI-based grading techniques: \textit{Direct}, where the AI model applies a rubric directly to student code, and \textit{Reverse} (a newly proposed approach), where the AI first fixes errors, then deduces a grade based on the nature and number of fixes. Each method was evaluated on both the instructor's original grading scale and a tenfold expanded scale to assess the impact of range on AI grading accuracy. To assess their effectiveness, AI-assigned scores were evaluated against human tutor evaluations on a range of coding problems and error types.
  Initial findings suggest that while the Direct approach is faster and straightforward, the Reverse technique often provides a more fine-grained assessment by focusing on correction effort. Both methods require careful prompt engineering, particularly for allocating partial credit and handling logic errors. To further test consistency, we also used synthetic student code generated using Gemini Flash 2.0, which allowed us to evaluate AI graders on a wider range of controlled error types and difficulty levels. We discuss the strengths and limitations of each approach, practical considerations for prompt design, and future directions for hybrid human-AI grading systems that aim to improve consistency, efficiency, and fairness in CS courses.

</details>


### [19] [Scalable and Efficient Large-Scale Log Analysis with LLMs: An IT Software Support Case Study](https://arxiv.org/abs/2511.14803)
*Pranjal Gupta,Karan Bhukar,Harshit Kumar,Seema Nagar,Prateeti Mohapatra,Debanjana Kar*

Main category: cs.SE

TL;DR: 论文提出基于大语言模型的日志分析工具，可在CPU上高效处理海量日志，实现自动化问题诊断和总结，在生产环境中显著节省时间和人力成本。


<details>
  <summary>Details</summary>
Motivation: IT环境产生大量日志数据，人工检查不现实，需要自动化日志分析解决方案。

Method: 开发基于大语言模型的日志分析工具，采用新颖方法在CPU上高效运行LLM处理日志数据。

Result: 工具自2024年3月部署至70个软件产品，处理2000多张工单，节省300+人工小时，每月估计节省15,444美元人力成本。

Conclusion: 基于LLM的日志分析工具能有效实现自动化问题诊断，显著提升IT支持效率并降低成本。

Abstract: IT environments typically have logging mechanisms to monitor system health and detect issues. However, the huge volume of generated logs makes manual inspection impractical, highlighting the importance of automated log analysis in IT Software Support. In this paper, we propose a log analytics tool that leverages Large Language Models (LLMs) for log data processing and issue diagnosis, enabling the generation of automated insights and summaries. We further present a novel approach for efficiently running LLMs on CPUs to process massive log volumes in minimal time without compromising output quality. We share the insights and lessons learned from deployment of the tool - in production since March 2024 - scaled across 70 software products, processing over 2000 tickets for issue diagnosis, achieving a time savings of 300+ man hours and an estimated $15,444 per month in manpower costs compared to the traditional log analysis practices.

</details>


### [20] [Automatic Pipeline Provisioning](https://arxiv.org/abs/2511.14825)
*Alexandre-Xavier Labonté-Lamoureux,Simon Boyer*

Main category: cs.SE

TL;DR: 该论文旨在探索自动流水线配置的优势及其应用方法，特别关注CI流水线


<details>
  <summary>Details</summary>
Motivation: 自动化流水线配置可以快速为软件工程项目部署流水线，提高开发效率

Method: 研究内容缺失

Result: 研究结果缺失

Conclusion: 研究结论缺失

Abstract: The goal of this paper is to explore the benefits of automatic pipeline provisioning and identify how it can be applied. Automatic pipeline provisioning can be defined as a process of quickly deploying a pipeline for a software engineering project. This research will focus on CI pipelines, although the outcomes of this approach on CD pipelines will likely be similar.

</details>


### [21] [MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram Generation](https://arxiv.org/abs/2511.14967)
*Basel Shbita,Farhan Ahmed,Chad DeLuca*

Main category: cs.SE

TL;DR: MermaidSeqBench is a benchmark for evaluating LLMs' ability to generate accurate Mermaid sequence diagrams from text, using a core set of 132 human-verified samples expanded via hybrid methods and LLM-as-a-judge evaluation.


<details>
  <summary>Details</summary>
Motivation: There is a lack of systematic benchmarks to assess LLMs' correctness in generating structured diagrams like Mermaid sequence diagrams from natural language descriptions.

Method: Developed through a hybrid methodology combining human annotation, in-context LLM prompting, and rule-based variation generation, starting from manually crafted flows.

Result: Evaluation reveals significant capability gaps across different LLMs in syntax correctness, activation handling, error handling, and practical usability of generated diagrams.

Conclusion: MermaidSeqBench provides a foundation for advancing research in structured diagram generation and developing more rigorous evaluation methodologies for LLM capabilities.

Abstract: Large language models (LLMs) have demonstrated excellent capabilities in generating structured diagrams from natural language descriptions. In particular, they have shown great promise in generating sequence diagrams for software engineering, typically represented in a text-based syntax such as Mermaid. However, systematic evaluations in this space remain underdeveloped as there is a lack of existing benchmarks to assess the LLM's correctness in this task. To address this shortcoming, we introduce MermaidSeqBench, a human-verified and LLM-synthetically-extended benchmark for assessing an LLM's capabilities in generating Mermaid sequence diagrams from textual prompts. The benchmark consists of a core set of 132 samples, starting from a small set of manually crafted and verified flows. These were expanded via a hybrid methodology combining human annotation, in-context LLM prompting, and rule-based variation generation. Our benchmark uses an LLM-as-a-judge model to assess Mermaid sequence diagram generation across fine-grained metrics, including syntax correctness, activation handling, error handling, and practical usability. We perform initial evaluations on numerous state-of-the-art LLMs and utilize multiple LLM judge models to demonstrate the effectiveness and flexibility of our benchmark. Our results reveal significant capability gaps across models and evaluation modes. Our proposed benchmark provides a foundation for advancing research in structured diagram generation and for developing more rigorous, fine-grained evaluation methodologies.

</details>


### [22] [FRIENDS GUI: A graphical user interface for data collection and visualization of vaping behavior from a passive vaping monitor](https://arxiv.org/abs/2511.15007)
*Shehan I Pranto,Brett Fassler,Md Rafi Islam,Ashley Schenkel,Larry W Hawk,Edward Sazonov*

Main category: cs.SE

TL;DR: 开发了一个名为FRIENDS GUI的开源Python工具，用于提取、解码和可视化ENDS设备的24小时吸烟行为数据。


<details>
  <summary>Details</summary>
Motivation: 理解和监测电子尼古丁输送系统（ENDS）的使用模式对于评估暴露和监管决策至关重要，但需要更好的数据分析工具。

Method: 开发了基于Python的开源GUI软件，能够提取、解码和可视化FRIENDS设备记录的24小时吸烟数据。

Result: 验证实验证实了时间戳转换的准确性、事件解码的可靠性以及行为可视化的有效性。

Conclusion: FRIENDS GUI是一个有效的开源工具，提高了ENDS使用数据分析的可访问性和可解释性，已在GitHub上公开发布。

Abstract: Understanding puffing topography (PT), which includes puff duration, intra puff interval, and puff count per session, is critical for evaluating Electronic Nicotine Delivery Systems (ENDS) use, toxicant exposure, and informing regulatory decisions. We developed FRIENDS (Flexible Robust Instrumentation of ENDS), an open-source device that records puffing and touch events of ENDS by attaching to it. This paper introduces the FRIENDS GUI that improves accessibility and interpretability of data collected by FRIENDS. The GUI is a Python-based open-source tool that extracts, decodes, and visualizes 24-hour puffing data from the FRIENDS device. Validation using 24-hour experimental data confirmed accurate timestamp conversion, reliable event decoding, and effective behavioral visualization. The software is freely available on GitHub for public use.

</details>


### [23] [Effective Code Membership Inference for Code Completion Models via Adversarial Prompts](https://arxiv.org/abs/2511.15107)
*Yuan Jiang,Zehao Li,Shan Huang,Christoph Treude,Xiaohong Su,Tiantian Wang*

Main category: cs.SE

TL;DR: AdvPrompt-MIA is a novel membership inference attack method for code completion models that uses code-specific adversarial prompts to detect training data membership with superior performance and transferability.


<details>
  <summary>Details</summary>
Motivation: Existing MIAs for code models rely on expensive surrogate models or manual heuristics, limiting their ability to capture complex memorization patterns in over-parameterized models.

Method: Combines code-specific adversarial perturbations with deep learning by designing prompts that induce output variations, then trains a classifier on feature vectors comparing outputs to ground-truth completions.

Result: Outperforms state-of-the-art baselines by up to 102% AUC gain on Code Llama 7B across APPS and HumanEval benchmarks, with strong cross-model/dataset transferability.

Conclusion: AdvPrompt-MIA effectively captures nuanced memorization patterns and demonstrates practical utility for privacy risk assessment in code completion models.

Abstract: Membership inference attacks (MIAs) on code completion models offer an effective way to assess privacy risks by inferring whether a given code snippet was part of the training data. Existing black- and gray-box MIAs rely on expensive surrogate models or manually crafted heuristic rules, which limit their ability to capture the nuanced memorization patterns exhibited by over-parameterized code language models. To address these challenges, we propose AdvPrompt-MIA, a method specifically designed for code completion models, combining code-specific adversarial perturbations with deep learning. The core novelty of our method lies in designing a series of adversarial prompts that induce variations in the victim code model's output. By comparing these outputs with the ground-truth completion, we construct feature vectors to train a classifier that automatically distinguishes member from non-member samples. This design allows our method to capture richer memorization patterns and accurately infer training set membership. We conduct comprehensive evaluations on widely adopted models, such as Code Llama 7B, over the APPS and HumanEval benchmarks. The results show that our approach consistently outperforms state-of-the-art baselines, with AUC gains of up to 102%. In addition, our method exhibits strong transferability across different models and datasets, underscoring its practical utility and generalizability.

</details>


### [24] [Finetuning LLMs for Automatic Form Interaction on Web-Browser in Selenium Testing Framework](https://arxiv.org/abs/2511.15168)
*Nguyen-Khang Le,Nguyen Hiep,Minh Nguyen,Son Luu,Trung Vo,Quan Bui,Nomura Shoshin,Le-Minh Nguyen*

Main category: cs.SE

TL;DR: 本文提出了一种训练大语言模型生成高质量Selenium测试用例的新方法，专注于表单交互测试，显著优于GPT-4o等基线模型。


<details>
  <summary>Details</summary>
Motivation: 自动化Web应用测试中表单交互验证至关重要，但目前缺乏专门评估LLMs在表单交互生成方面的公开基准和数据集。

Method: 构建合成和人工标注的数据集用于训练和评估，定义语法正确性、脚本可执行性和输入字段覆盖率的明确指标。

Result: 该方法在所有评估指标上都显著优于GPT-4o和其他流行的大语言模型基线。

Conclusion: 为基于LLM的Web测试研究奠定了基础，并提供资源支持该领域持续发展。

Abstract: Automated web application testing is a critical component of modern software development, with frameworks like Selenium widely adopted for validating functionality through browser automation. Among the essential aspects of such testing is the ability to interact with and validate web forms, a task that requires syntactically correct, executable scripts with high coverage of input fields. Despite its importance, this task remains underexplored in the context of large language models (LLMs), and no public benchmark or dataset exists to evaluate LLMs on form interaction generation systematically. This paper introduces a novel method for training LLMs to generate high-quality test cases in Selenium, specifically targeting form interaction testing. We curate both synthetic and human-annotated datasets for training and evaluation, covering diverse real-world forms and testing scenarios. We define clear metrics for syntax correctness, script executability, and input field coverage. Our empirical study demonstrates that our approach significantly outperforms strong baselines, including GPT-4o and other popular LLMs, across all evaluation metrics. Our work lays the groundwork for future research on LLM-based web testing and provides resources to support ongoing progress in this area.

</details>


### [25] [From Code Smells to Best Practices: Tackling Resource Leaks in PyTorch, TensorFlow, and Keras](https://arxiv.org/abs/2511.15229)
*Bashar Abdallah,Martyna E. Wojciechowska,Gustavo Santos,Edmand Yu,Maxime Lamothe,Alain Abran,Mohammad Hamdaqa*

Main category: cs.SE

TL;DR: This study identifies and categorizes code smells that cause resource leaks in ML applications using PyTorch, TensorFlow, and Keras, providing 50 best practices to improve resource efficiency.


<details>
  <summary>Details</summary>
Motivation: There's limited research on the long-term sustainability and resource efficiency of ML applications, creating a gap between model performance and deployment robustness.

Method: Empirical investigation of developer discussions and real-world code snippets from major ML frameworks, followed by a three-phase validation process with independent analysis and consensus discussions.

Result: Identified 30 PyTorch-related and 16 TensorFlow/Keras resource-leak-inducing code smells, categorized by root causes and framework-specific characteristics.

Conclusion: This is the first comprehensive study examining resource leaks in ML frameworks, providing actionable best practices to help developers build more efficient and sustainable applications.

Abstract: Much of the existing ML research focuses on model performance metrics, leaving limited attention to the long-term sustainability and resource efficiency of ML applications. While high performance is essential, ensuring efficient resource management is equally critical for robust deployment. This study addresses this gap by systematically identifying code smells that lead to resource leaks in ML applications. We conducted an empirical investigation of developer discussions and real-world code snippets from PyTorch, TensorFlow, and Keras. The analysis identified 30 PyTorch-related smells and 16 TensorFlow/Keras smells linked to resource leaks. These smells were categorized in two ways: (1) based on their root causes, and (2) as general ML smells with framework-specific characteristics. For each smell, we derived at least one best practice, resulting in 50 recommended coding patterns aimed at reducing resource leakage and improving efficiency. To ensure the validity of our findings, we employed a three-phase validation process involving independent analysis by three authors followed by consensus discussions. This is the first comprehensive study to examine resource-leak-inducing code smells across major ML frameworks and to present actionable best practices for mitigating them. The contributions support developers in building more efficient and sustainable ML applications and offer a structured view of the underlying causes of resource leaks.

</details>


### [26] [M, Toolchain and Language for Reusable Model Compilation](https://arxiv.org/abs/2511.15257)
*Hiep Hong Trinh,Federico Ciccozzi,Abu Naser Masud,Marjan Sirjani,Mikael Sjödin*

Main category: cs.SE

TL;DR: M is a modeling language and toolchain designed for multi-target compilation of complex concurrent systems, enabling semantic-preserving generation of various target artifacts from actor-based models.


<details>
  <summary>Details</summary>
Motivation: Traditional modeling languages have narrow focus and poor support for generating multiple heterogeneous targets, making model-driven engineering inefficient for complex systems.

Method: M is a grammar-driven textual language based on actor model with discrete-event scheduling, supporting system entities, message interactions, and time/state-triggered reactions.

Result: The M toolchain successfully enables systematic generation of diverse target artifacts while maintaining semantic conformance to original models.

Conclusion: M provides an effective solution for multi-target compilation in model-driven engineering and can serve as a middle language to extend compilation capabilities to other modeling languages.

Abstract: Complex software-driven systems often interleave distributed, concurrent computation processes with physical interactions with the environment. Developing these systems more efficiently and safely can be achieved by employing actionable, software-based models. From a high-level system model, engineers often need to derive multiple specialized models for different purposes, including simulation, deployment, and formal verification. Each of these target models usually rely on its own formalism, specification language, and execution platform. Traditionally, a compiler analyzes a program written in a programming language and generates executable code. In contrast, a model compiler processes a source model written in a modeling language and should ideally support the generation of multiple heterogeneous targets. However, most existing modeling languages are designed with a narrow focus, typically targeting only simulation or implementation. Multi-target compilation, when not considered during the language's early design, becomes significantly harder to achieve. In this paper, we introduce our initiative: a toolchain and modeling language called M, designed to support system modeling and multi-target compilation for model-driven engineering of complex, concurrent, and time-aware systems. M is a textual, grammar-driven language based on the actor model and extended with discrete-event scheduling semantics. It provides constructs for modeling system entities, message-based interactions, and time- or state-triggered reactions. From such models, M enables the systematic generation of diverse target artifacts while preserving semantic conformance to the original model. Moreover, M can serve as a middle language to which other modeling languages may anchor, thereby allowing them to benefit from its compilation framework.

</details>


### [27] [A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development](https://arxiv.org/abs/2511.15293)
*Jia Li,Zhi Jin,Kechi Zhang,Huangzhao Zhang,Jiaru Qian,Tiankuo Zhao*

Main category: cs.SE

TL;DR: AutoSW is an iterative end-to-end automated software development paradigm where AI systems translate natural language requirements into executable software through an analyze-plan-implement-deliver loop.


<details>
  <summary>Details</summary>
Motivation: Software development automation is a long-term goal. Current approaches either use AI as assistants requiring human involvement or 'vibe coding' with automatic revisions. The vision is for AI systems to participate throughout the entire development lifecycle.

Method: The AutoSW paradigm operates through an iterative analyze-plan-implement-deliver loop where AI systems act as partners to translate natural language intentions into code. A lightweight prototype was developed and tested.

Result: The prototype successfully executed various representative cases, demonstrating that AutoSW can deliver executable software from natural language requirements.

Conclusion: AutoSW provides a feasible direction for truly end-to-end automated software development, showing potential for AI systems to become first-class actors in the software development lifecycle.

Abstract: Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.

</details>


### [28] [From Machine Learning Documentation to Requirements: Bridging Processes with Requirements Languages](https://arxiv.org/abs/2511.15340)
*Yi Peng,Hans-Martin Heyn,Jennifer Horkoff*

Main category: cs.SE

TL;DR: This paper explores how ML documentation (ModelCards and DataSheets) can be used to extract requirements engineering (RE) information for ML-enabled systems, showing significant RE-relevant content and evaluating three RE frameworks for structuring this information.


<details>
  <summary>Details</summary>
Motivation: Integration and verification of ML components in software engineering processes face challenges due to lack of standardized requirements specification. ML documentation may provide valuable RE information but its utility is uncertain.

Method: Analyzed 20 publicly available ModelCards and DataSheets to assess RE-relevant content, then evaluated three RE representations (EARS, Rupp's template, Volere) for structuring this information into requirements.

Result: ML documentation contains significant potentially RE-relevant information, and established RE representations can effectively structure ML-specific knowledge into requirements.

Conclusion: There is a viable pathway to incorporate ML documentation into software engineering processes for ML systems by transforming ML-specific knowledge into structured requirements using existing RE frameworks.

Abstract: In software engineering processes for machine learning (ML)-enabled systems, integrating and verifying ML components is a major challenge. A prerequisite is the specification of ML component requirements, including models and data, an area where traditional requirements engineering (RE) processes face new obstacles. An underexplored source of RE-relevant information in this context is ML documentation such as ModelCards and DataSheets. However, it is uncertain to what extent RE-relevant information can be extracted from these documents. This study first investigates the amount and nature of RE-relevant information in 20 publicly available ModelCards and DataSheets. We show that these documents contain a significant amount of potentially RE-relevant information. Next, we evaluate how effectively three established RE representations (EARS, Rupp's template, and Volere) can structure this knowledge into requirements. Our results demonstrate that there is a pathway to transform ML-specific knowledge into structured requirements, incorporating ML documentation in software engineering processes for ML systems.

</details>


### [29] [MutDafny: A Mutation-Based Approach to Assess Dafny Specifications](https://arxiv.org/abs/2511.15403)
*Isabel Amaral,Alexandra Mendes,José Campos*

Main category: cs.SE

TL;DR: 本文介绍了MutDafny工具，该工具通过变异测试方法检测Dafny形式化规范中的弱点，包含32个变异算子，在794个真实世界Dafny程序中检测出规范弱点，平均每241行代码就存在1个弱规范。


<details>
  <summary>Details</summary>
Motivation: 在形式化验证语言如Dafny中，规范与实现一样容易出错，规范中的缺陷可能导致形式化验证的程序偏离预期行为。

Method: 采用变异测试方法，通过向代码中引入故障（变异），并依赖形式化规范来检测这些故障。如果带有变异的程序通过验证，则表明规范存在弱点。

Result: 从流行的变异测试工具中分析适用的变异算子，并从GitHub上的Dafny项目错误修复提交中合成新的算子，共配备了32个变异算子。在794个真实世界Dafny程序上评估，手动分析结果发现平均每241行代码就存在一个弱规范。

Conclusion: MutDafny工具能够有效提高Dafny规范的可靠性，通过自动识别规范弱点，帮助开发者加强规范的质量。

Abstract: This paper explores the use of mutation testing to reveal weaknesses in formal specifications written in Dafny. In verification-aware programming languages, such as Dafny, despite their critical role, specifications are as prone to errors as implementations. Flaws in specs can result in formally verified programs that deviate from the intended behavior.
  We present MutDafny, a tool that increases the reliability of Dafny specifications by automatically signaling potential weaknesses. Using a mutation testing approach, we introduce faults (mutations) into the code and rely on formal specifications for detecting them. If a program with a mutant verifies, this may indicate a weakness in the specification. We extensively analyze mutation operators from popular tools, identifying the ones applicable to Dafny. In addition, we synthesize new operators tailored for Dafny from bugfix commits in publicly available Dafny projects on GitHub. Drawing from both, we equipped our tool with a total of 32 mutation operators. We evaluate MutDafny's effectiveness and efficiency in a dataset of 794 real-world Dafny programs and we manually analyze a subset of the resulting undetected mutants, identifying five weak real-world specifications (on average, one at every 241 lines of code) that would benefit from strengthening.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [30] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: FERMAT is a RL environment for automated mathematical theory discovery, featuring evolutionary algorithms with LLM-based function abstraction to evaluate mathematical interestingness.


<details>
  <summary>Details</summary>
Motivation: To automate the open-ended discovery of new mathematical theories, addressing the challenge of defining and measuring interestingness in mathematical objects.

Method: Introduces FERMAT environment with symbolic actions, uses evolutionary algorithms enhanced by LLM-based function abstraction to synthesize interestingness measures.

Result: The approach shows notable improvements in discovering elementary number theory and finite fields compared to hard-coded baselines.

Conclusion: FERMAT provides a scalable framework for mathematical theory discovery, with LLM-enhanced evolution effectively automating interestingness evaluation.

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [31] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: Ask WhAI是一个用于检验和扰动多智能体交互中信念状态的系统级框架，应用于医疗诊断模拟环境，揭示了智能体信念形成的动态过程。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体科学推理中的信念形成和认识论孤岛问题，传统方法难以追踪和检验人类专家的信念动态。

Method: 开发Ask WhAI框架，包含记录回放智能体交互、带外查询信念依据、反事实证据注入等功能，应用于具有共享EMR和Oracle智能体的医疗案例模拟器。

Result: 发现智能体信念反映现实世界学科立场，包括过度依赖规范研究和抗拒反证据，这些信念可被追踪和 interrogated。

Conclusion: Ask WhAI通过使信念动态可见和可测试，为研究多智能体科学推理中的信念形成提供了可重复的方法。

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [32] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: An automated LLM-assisted workflow using GPT-4o to clean and geocode unstructured disaster location data from EM-DAT by cross-referencing multiple geospatial sources.


<details>
  <summary>Details</summary>
Motivation: Disaster databases like EM-DAT contain unstructured textual location data with inconsistent granularity and spelling, making integration with spatial datasets difficult for risk assessment.

Method: Uses GPT-4o to process textual locations, then cross-checks three geoinformation repositories (GADM, OpenStreetMap, Wikidata) to assign geometries and reliability scores based on source agreement.

Result: Processed EM-DAT data from 2000-2024, geocoding 14,215 events across 17,948 unique locations with reliability scoring.

Conclusion: The approach provides a scalable, automated method for extracting structured geographic information from unstructured text, enabling better disaster risk analysis without manual intervention.

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [33] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: 行动研究项目Rachel创建一个AI学术身份Rachel So，发表10多篇AI生成论文，探究学术生态系统对AI作者身份的反应，包括引用和同行评审邀请。


<details>
  <summary>Details</summary>
Motivation: 研究学术生态系统如何应对AI作者身份的出现，为有关超人类、超能力AI系统与学术交流未来的讨论提供实证数据。

Method: 行动研究方法，创建AI学术身份Rachel So，在2025年3月至10月期间发表AI生成的研究论文，并跟踪其被引用和接受同行评审邀请的情况。

Result: Rachel So成功发表10多篇论文，获得引用，并收到同行评审邀请，表明学术系统在一定程度上接受AI作者身份。

Conclusion: AI作者身份对出版商、研究人员和整个科学系统具有深远影响，需要进一步讨论和规范。

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [34] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: 论文提出了一种概率方法来量化AI系统训练测试数据的代表性，使用不精确贝叶斯方法处理有限数据和先验不确定性，生成区间值代表性估计而非单一数值。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统（如自动驾驶汽车）的可信度和安全性，高度依赖于训练测试数据集的数据相关安全属性，特别是数据代表性——即场景数据反映系统设计运行条件（ODD）或预期遇到条件（TOD）的程度。

Method: 采用概率方法比较场景suite特征统计分布与TOD特征分布的差异，使用不精确贝叶斯方法处理有限数据和不确定先验，生成不确定性感知的区间值代表性估计。

Result: 通过数值示例展示了在依赖关系和先验不确定性条件下，跨操作类别（天气、道路类型、时间等）的场景suite分布与推断TOD分布的对比，实现了局部（类别间）和全局的区间值代表性估计。

Conclusion: 该方法能够量化数据代表性并处理真实TOD分布未知的挑战，为AI系统安全性评估提供了更可靠的数据代表性度量框架。

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [35] [Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning](https://arxiv.org/abs/2511.15002)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Main category: cs.AI

TL;DR: A novel resource management approach combining SAC with SAM in a distributed MARL framework for O-RAN, using TD-error variance-driven adaptive regularization to improve robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: DRL models struggle with robustness and generalizability in dynamic O-RAN environments. Current methods lack targeted regularization, leading to inefficiency and instability.

Method: Enhanced SAC algorithm with adaptive SAM regularization driven by TD-error variance, applied selectively to agents based on environmental complexity. Includes dynamic ρ scheduling for exploration-exploitation trade-off.

Result: Up to 22% improvement in resource allocation efficiency and superior QoS satisfaction across diverse O-RAN slices compared to conventional DRL approaches.

Conclusion: The proposed method effectively enhances training stability, generalization, and performance in dynamic network environments while minimizing unnecessary computational overhead.

Abstract: Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $ρ$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.

</details>


### [36] [Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization](https://arxiv.org/abs/2511.15055)
*Jian-Ting Guo,Yu-Cheng Chen,Ping-Chun Hsieh,Kuo-Hao Ho,Po-Wei Huang,Ti-Rong Wu,I-Chen Wu*

Main category: cs.AI

TL;DR: 本文提出了一种名为MAQ的类人强化学习框架，通过量化人类演示为宏观动作，在保持高奖励的同时显著提升行为的人类相似性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习智能体虽然在某些领域表现超越人类，但行为模式往往与人类差异巨大，这影响了其可解释性和可信度。

Method: 将类人行为建模为轨迹优化问题，采用后退时域控制方法，并开发了基于向量量化变分自编码器的宏观动作量化框架。

Result: 在D4RL Adroit基准测试中，MAQ显著提升了轨迹相似度得分，并在人类评估研究中获得了最高的类人行为排名。

Conclusion: MAQ框架可轻松集成到现有的RL算法中，为实现类人强化学习智能体开辟了有前景的研究方向。

Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.

</details>


### [37] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: This paper introduces OpenBioLLM, a modular multi-agent framework that improves upon GeneGPT by using open-source models for genomic question answering, achieving comparable or better performance with reduced latency.


<details>
  <summary>Details</summary>
Motivation: GeneGPT's reliance on proprietary models limits scalability, increases costs, and raises privacy concerns, prompting the need for an open-source alternative.

Method: The authors first reproduced GeneGPT with open-source models (Llama 3.1, Qwen2.5, Qwen2.5 Coder), then developed OpenBioLLM with specialized agents for tool routing, query generation, and response validation.

Result: OpenBioLLM matches or outperforms GeneGPT on 90% of benchmark tasks, achieving average scores of 0.849 (Gene-Turing) and 0.830 (GeneHop), with 40-50% latency reduction.

Conclusion: OpenBioLLM demonstrates the potential of open-source multi-agent systems for efficient and effective genomic question answering, offering improved scalability and privacy.

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


### [38] [ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression](https://arxiv.org/abs/2511.15069)
*Haoyong Wu,Yongmei Liu*

Main category: cs.AI

TL;DR: ProRAC is a neuro-symbolic framework using LLMs for Reasoning about Actions and Change (RAC) problems. It extracts RAC elements, executes actions progressively, and evaluates queries to achieve strong performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of Reasoning about Actions and Change problems by leveraging the capabilities of large language models in a structured framework.

Method: Extracts actions and questions from RAC problems, progressively executes each action to derive the final state, then evaluates the query against this state.

Result: ProRAC demonstrates strong performance across different RAC benchmarks, domains, LLM backbones, and types of RAC tasks.

Conclusion: The ProRAC framework effectively combines neuro-symbolic approaches with LLMs to solve RAC problems, showing robust performance in various testing scenarios.

Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.

</details>


### [39] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: SafeRBench is a benchmark for evaluating safety risks in Large Reasoning Models throughout their reasoning process, addressing gaps in existing safety evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing safety evaluations focus on output-level judgments and fail to capture dynamic risks in reasoning traces.

Method: Incorporates risk categories/levels in input design, uses micro-thought chunking for fine-grained analysis, and validates against human safety annotations.

Result: Evaluation of 19 LRMs shows SafeRBench enables detailed, multidimensional safety assessment.

Conclusion: SafeRBench provides comprehensive safety evaluation framework for LRMs, offering insights into risks and protective mechanisms.

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [40] [HISE-KT: Synergizing Heterogeneous Information Networks and LLMs for Explainable Knowledge Tracing with Meta-Path Optimization](https://arxiv.org/abs/2511.15191)
*Zhiyi Duan,Zixing Shi,Hongyu Yuan,Qi Wang*

Main category: cs.AI

TL;DR: HISE-KT integrates heterogeneous information networks with LLMs for knowledge tracing, using LLMs to filter meta-path instances and retrieve similar students for improved prediction accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: Existing KT methods either introduce noise through manual meta-path selection or miss cross-student information; LLM-based methods lack structural relations. HISE-KT aims to combine HINs' structural richness with LLMs' reasoning for better predictions.

Method: Builds a multi-relationship HIN with diverse node types, uses LLM to score/filter meta-path instances, retrieves similar students via meta-paths, and employs structured prompts for prediction and explanation.

Result: Outperforms existing KT baselines on four public datasets in both prediction performance and interpretability.

Conclusion: HISE-KT synergistically combines HINs and LLMs to enhance KT by automating meta-path quality assessment and providing evidence-based explanations.

Abstract: Knowledge Tracing (KT) aims to mine students' evolving knowledge states and predict their future question-answering performance. Existing methods based on heterogeneous information networks (HINs) are prone to introducing noises due to manual or random selection of meta-paths and lack necessary quality assessment of meta-path instances. Conversely, recent large language models (LLMs)-based methods ignore the rich information across students, and both paradigms struggle to deliver consistently accurate and evidence-based explanations. To address these issues, we propose an innovative framework, HIN-LLM Synergistic Enhanced Knowledge Tracing (HISE-KT), which seamlessly integrates HINs with LLMs. HISE-KT first builds a multi-relationship HIN containing diverse node types to capture the structural relations through multiple meta-paths. The LLM is then employed to intelligently score and filter meta-path instances and retain high-quality paths, pioneering automated meta-path quality assessment. Inspired by educational psychology principles, a similar student retrieval mechanism based on meta-paths is designed to provide a more valuable context for prediction. Finally, HISE-KT uses a structured prompt to integrate the target student's history with the retrieved similar trajectories, enabling the LLM to generate not only accurate predictions but also evidence-backed, explainable analysis reports. Experiments on four public datasets show that HISE-KT outperforms existing KT baselines in both prediction performance and interpretability.

</details>


### [41] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: COPYCHECK is a novel framework that uses uncertainty signals to detect if copyrighted content was used in LLM training data, achieving over 90% accuracy by leveraging LLM overconfidence patterns and eliminating the need for empirically tuned thresholds.


<details>
  <summary>Details</summary>
Motivation: Existing methods for detecting unauthorized use of copyrighted material in LLM training face limitations including LLM overconfidence, limited access to ground truth data, and reliance on empirical thresholds.

Method: COPYCHECK uses strategic file segmentation into snippets and uncertainty-guided unsupervised clustering to distinguish between "seen" (training) and "unseen" (non-training) content without requiring labeled data or manual thresholds.

Result: Achieved average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b, with up to 93.8% accuracy and strong performance on GPT-J 6B, representing over 90% improvement over SOTA baselines.

Conclusion: This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency by turning LLM overconfidence into an asset rather than a limitation.

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [42] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: AI research is shifting to complex reasoning tasks, which require exponential compute resources without natural saturation points, challenging sustainability as efficiency gains plateau.


<details>
  <summary>Details</summary>
Motivation: The increasing energy demands of reasoning AI systems, driven by exponential compute scaling in training and inference, pose sustainability concerns as traditional efficiency improvements approach physical limits.

Method: Analysis of compute scaling trends in AI research and discussion of governance frameworks.

Result: Identified that efficiency alone is insufficient for sustainable AI; explicit limits are needed in optimization and governance.

Conclusion: Sustainable reasoning AI requires embedding explicit computational limits and governance frameworks beyond mere efficiency improvements.

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [43] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: 本文分析了AI研究中的两种对立智力观：智力实在论（智力是单一可度量的普遍能力）和智力多元论（智力是多样化的情境依赖能力），探讨了它们对研究方法、现象解释和风险评估的根本性影响。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中存在深刻分歧却缺乏明确讨论的核心假设差异，作者旨在通过揭示这些隐含的智力观念框架，促进对领域内争议的更清晰理解。

Method: 通过分析AI研究领域的现有辩论，追溯不同学者对经验证据（如模型能力涌现、系统局限）的解释差异，归纳两种观念在方法论、解释学和风险应对三个层面的具体表现。

Result: 论证了两种智力观如何导致模型选择、基准设计、实验验证方法的对立；对同一现象（如能力涌现）的矛盾解读；以及风险评估的根本分歧（实在论关注超智能统一风险，多元论强调多领域情境化威胁）。

Conclusion: 明确这些底层假设能帮助厘清AI研究的本质分歧，推动更建设性的学术对话，避免因隐含观念差异导致无效争论。

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [44] [Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration](https://arxiv.org/abs/2511.15351)
*Yifu Guo,Zishan Xu,Zhiyuan Yao,Yuquan Lu,Jiaye Lin,Sen Hu,Zhenheng Tang,Yingchao Li,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: Octopus是一个多模态推理新范式，通过六种能力协调实现自主探索和动态能力选择，在Octopus-Bench基准上取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理模型存在架构局限性，缺乏人类般的自主探索不同推理路径的能力，无法适应动态变化的任务需求。

Method: 提出Octopus框架，定义多模态推理的六种核心能力，构建Octopus-Bench评估基准，实现自主推理探索和动态能力选择。

Result: 实验结果表明Octopus在Octopus-Bench大多数任务上取得最佳性能，证明了能力协调在多模态推理中的关键作用。

Conclusion: 能力协调是多模态智能推理的核心，Octopus框架展示了通过自主探索和动态能力选择实现更强大推理能力的可行性。

Abstract: Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.

</details>


### [45] [Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents](https://arxiv.org/abs/2511.15378)
*Trevor McInroe*

Main category: cs.AI

TL;DR: Terra Nova is a comprehensive challenge environment based on Civilization V that integrates multiple RL challenges simultaneously, requiring agents to demonstrate deep reasoning across interacting variables rather than just multitasking.


<details>
  <summary>Details</summary>
Motivation: To create an environment where canonical RL challenges like partial observability, credit assignment, and large action spaces arise together, testing integrated understanding rather than parallel task switching.

Method: Developing a single, unified environment inspired by Civilization V that naturally combines multiple interdependent RL challenges in a complex, interactive system.

Result: The introduction of Terra Nova as a new benchmark that demands long-horizon reasoning across many interacting variables, distinguishing it from multitask environments with independent tasks.

Conclusion: Terra Nova provides a more meaningful testbed for evaluating RL agents' ability to handle complex, interconnected challenges compared to aggregated multitask benchmarks.

Abstract: We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.

</details>


### [46] [Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining](https://arxiv.org/abs/2511.15456)
*Qian'ang Mao,Yuxuan Zhang,Jiaman Chen,Wenjun Zhou,Jiaqi Yan*

Main category: cs.AI

TL;DR: 本文提出了TIM框架，通过多Agent LLM系统来分析DeFi交易的用户意图，解决了传统方法难以理解复杂智能合约交互语义的问题


<details>
  <summary>Details</summary>
Motivation: DeFi交易意图理解面临智能合约交互复杂、链上链下因素多样、日志不透明等挑战，现有方法缺乏深度语义洞察

Method: 基于扎根理论构建DeFi意图分类法，使用Meta-Level Planner协调领域专家，Question Solvers处理多模态数据，Cognitive Evaluator减少LLM幻觉

Result: TIM框架显著优于机器学习模型、单一LLM和单一Agent基线方法

Conclusion: TIM框架为DeFi用户动机提供了更可靠的理解，能为复杂区块链活动提供情境感知的解释

Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.

</details>


### [47] [Exploring the use of AI authors and reviewers at Agents4Science](https://arxiv.org/abs/2511.15534)
*Federico Bianchi,Owen Queen,Nitya Thakkar,Eric Sun,James Zou*

Main category: cs.AI

TL;DR: AI agents served as primary authors and reviewers at the first Agents4Science conference, with humans as co-authors and co-reviewers, exploring AI capabilities in scientific research and implications for human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: There is growing interest in using AI agents for scientific research, but fundamental questions remain about their capabilities as scientists and reviewers.

Method: Organized Agents4Science conference where AI agents served as both primary authors and reviewers, with humans as co-authors and co-reviewers.

Result: The conference provided insights into AI capabilities in scientific roles and human-AI collaboration.

Conclusion: The Agents4Science conference offered valuable learnings about AI agents' potential in scientific research and reviewing, highlighting implications for future human-AI collaboration in science.

Abstract: There is growing interest in using AI agents for scientific research, yet fundamental questions remain about their capabilities as scientists and reviewers. To explore these questions, we organized Agents4Science, the first conference in which AI agents serve as both primary authors and reviewers, with humans as co-authors and co-reviewers. Here, we discuss the key learnings from the conference and their implications for human-AI collaboration in science.

</details>


### [48] [What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity](https://arxiv.org/abs/2511.15593)
*Alexis Audran-Reiss,Jordi Armengol Estapé,Karen Hambardzumyan,Amar Budhiraja,Martin Josifoski,Edan Toledo,Rishi Hazra,Despoina Magka,Michael Shvartsman,Parth Pathak,Justine T Kao,Lucia Cipolina-Kun,Bhavul Gauri,Jean-Christophe Gagnon-Audet,Emanuel Tewolde,Jenny Zhang,Taco Cohen,Yossi Adi,Tatiana Shavrina,Yoram Bachrach*

Main category: cs.AI

TL;DR: The paper examines how ideation diversity affects AI research agent performance on the MLE-bench benchmark, finding that higher diversity correlates with better results.


<details>
  <summary>Details</summary>
Motivation: AI research agents could accelerate science but their success factors are unclear; this study investigates the role of ideation diversity.

Method: Analyzed agent trajectories on MLE-bench across models/scaffolds, ran controlled experiments manipulating ideation diversity, and used multiple evaluation metrics.

Result: Higher ideation diversity consistently correlated with improved agent performance across different models and evaluation metrics.

Conclusion: Ideation diversity is a key factor in AI research agent success, suggesting it should be prioritized in agent design.

Abstract: AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.

</details>
