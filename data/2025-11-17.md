<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 18]
- [cs.AI](#cs.AI) [Total: 29]
- [cs.SE](#cs.SE) [Total: 7]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Transferable Hypergraph Attack via Injecting Nodes into Pivotal Hyperedges](https://arxiv.org/abs/2511.10698)
*Meixia He,Peican Zhu,Le Cheng,Yangming Guo,Manman Yuan,Keke Tang*

Main category: cs.CR

TL;DR: TH-Attack is a novel adversarial attack framework for hypergraph neural networks that improves transferability by injecting malicious nodes into pivotal hyperedges identified through pivotality assessment.


<details>
  <summary>Details</summary>
Motivation: Existing HGNN attack methods rely on specific model architectures and overlook the common vulnerability caused by differences in hyperedge pivotality along aggregation paths, limiting attack transferability.

Method: 1) Design a hyperedge recognizer via pivotality assessment to identify pivotal hyperedges. 2) Develop a feature inverter to generate malicious nodes that maximize semantic divergence from pivotal hyperedge features. 3) Inject malicious nodes into pivotal hyperedges.

Result: Extensive experiments on six datasets demonstrate TH-Attack's effectiveness and superiority over state-of-the-art methods in terms of transferability and attack performance.

Conclusion: TH-Attack effectively addresses limitations of existing HGNN attack methods by leveraging hyperedge pivotality, achieving improved transferability and effectiveness across different hypergraph neural network architectures.

Abstract: Recent studies have demonstrated that hypergraph neural networks (HGNNs) are susceptible to adversarial attacks. However, existing methods rely on the specific information mechanisms of target HGNNs, overlooking the common vulnerability caused by the significant differences in hyperedge pivotality along aggregation paths in most HGNNs, thereby limiting the transferability and effectiveness of attacks. In this paper, we present a novel framework, i.e., Transferable Hypergraph Attack via Injecting Nodes into Pivotal Hyperedges (TH-Attack), to address these limitations. Specifically, we design a hyperedge recognizer via pivotality assessment to obtain pivotal hyperedges within the aggregation paths of HGNNs. Furthermore, we introduce a feature inverter based on pivotal hyperedges, which generates malicious nodes by maximizing the semantic divergence between the generated features and the pivotal hyperedges features. Lastly, by injecting these malicious nodes into the pivotal hyperedges, TH-Attack improves the transferability and effectiveness of attacks. Extensive experiments are conducted on six authentic datasets to validate the effectiveness of TH-Attack and the corresponding superiority to state-of-the-art methods.

</details>


### [2] [Do Not Merge My Model! Safeguarding Open-Source LLMs Against Unauthorized Model Merging](https://arxiv.org/abs/2511.10712)
*Qinfeng Li,Miao Pan,Jintao Chen,Fu Teng,Zhiqiang Shen,Ge Su,Hao Peng,Xuhong Zhang*

Main category: cs.CR

TL;DR: MergeBarrier是一种即插即用的防御方法，通过破坏线性模式连接性来主动防止未经授权的模型合并窃取，实验表明其能有效防止模型合并窃取且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 模型合并已成为扩展大型语言模型的高效技术，但也引入了模型合并窃取的新威胁，即免费搭车者通过未经授权的模型合并利用模型。现有防御机制无法同时满足三个关键保护属性：主动防止未经授权合并、与通用开源设置兼容、高安全性且性能损失可忽略。

Method: 提出了MergeBarrier，其核心设计是破坏受保护模型与其同源对应物之间的线性模式连接性，从而消除有效模型合并所需的低损失路径。

Result: 大量实验表明，MergeBarrier能有效防止模型合并窃取，且准确性损失可忽略不计。

Conclusion: MergeBarrier解决了模型合并窃取问题，满足了主动防护、兼容性和高性能的关键要求，为模型安全提供了有效保障。

Abstract: Model merging has emerged as an efficient technique for expanding large language models (LLMs) by integrating specialized expert models. However, it also introduces a new threat: model merging stealing, where free-riders exploit models through unauthorized model merging. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify three critical protection properties that existing methods fail to simultaneously satisfy: (1) proactively preventing unauthorized merging; (2) ensuring compatibility with general open-source settings; (3) achieving high security with negligible performance loss. To address the above issues, we propose MergeBarrier, a plug-and-play defense that proactively prevents unauthorized merging. The core design of MergeBarrier is to disrupt the Linear Mode Connectivity (LMC) between the protected model and its homologous counterparts, thereby eliminating the low-loss path required for effective model merging. Extensive experiments show that MergeBarrier effectively prevents model merging stealing with negligible accuracy loss.

</details>


### [3] [BadThink: Triggered Overthinking Attacks on Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/abs/2511.10714)
*Shuaitong Liu,Renjue Li,Lijia Yu,Lijun Zhang,Zhiming Liu,Gaojie Jin*

Main category: cs.CR

TL;DR: BadThink攻击是针对思维链提示法的首个后门攻击，通过精心设计的触发提示诱导LLMs产生过度推理行为，显著增加计算成本而不改变最终输出结果


<details>
  <summary>Details</summary>
Motivation: 思维链提示法的进步虽然提升了LLMs的推理能力，但也引入了计算效率作为新的攻击面，需要研究如何隐秘地操纵推理效率

Method: 采用基于中毒的微调策略，使用基于LLM的迭代优化过程生成高度自然的毒化数据来嵌入过度推理行为

Result: 在多模型和推理任务实验中，BadThink能持续增加推理痕迹长度（在MATH-500数据集上达到17倍以上增长），同时保持隐蔽性和鲁棒性

Conclusion: 这项工作揭示了一个关键的、先前未被探索的脆弱性，展示了针对思维链系统的新型复杂攻击类别，证明推理效率可以被隐秘操控

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially improved the reasoning capabilities of large language models (LLMs), but have also introduced their computational efficiency as a new attack surface. In this paper, we propose BadThink, the first backdoor attack designed to deliberately induce "overthinking" behavior in CoT-enabled LLMs while ensuring stealth. When activated by carefully crafted trigger prompts, BadThink manipulates the model to generate inflated reasoning traces - producing unnecessarily redundant thought processes while preserving the consistency of final outputs. This subtle attack vector creates a covert form of performance degradation that significantly increases computational costs and inference time while remaining difficult to detect through conventional output evaluation methods. We implement this attack through a sophisticated poisoning-based fine-tuning strategy, employing a novel LLM-based iterative optimization process to embed the behavior by generating highly naturalistic poisoned data. Our experiments on multiple state-of-the-art models and reasoning tasks show that BadThink consistently increases reasoning trace lengths - achieving an over 17x increase on the MATH-500 dataset - while remaining stealthy and robust. This work reveals a critical, previously unexplored vulnerability where reasoning efficiency can be covertly manipulated, demonstrating a new class of sophisticated attacks against CoT-enabled systems.

</details>


### [4] [PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization](https://arxiv.org/abs/2511.10720)
*Runpeng Geng,Yanting Wang,Chenlong Yin,Minhao Cheng,Ying Chen,Jinyuan Jia*

Main category: cs.CR

TL;DR: PISanitizer is a defense against prompt injection in long-context LLMs that identifies and removes malicious tokens by leveraging attention mechanisms, ensuring security without compromising utility.


<details>
  <summary>Details</summary>
Motivation: Existing defenses against prompt injection are ineffective for long contexts because injected instructions constitute a small portion of the input, making detection difficult. The work aims to address this gap by developing a method that can reliably sanitize long contexts.

Method: PISanitizer works by first inducing the LLM to follow arbitrary instructions in the context, then identifying and removing tokens that receive high attention during this process—effectively sanitizing potential injected instructions.

Result: Evaluation shows PISanitizer successfully prevents prompt injection, maintains utility, outperforms existing defenses, is efficient, and resists adaptive attacks.

Conclusion: PISanitizer provides an effective solution to long-context prompt injection by turning the attack's strength against itself, with code publicly available for further use and development.

Abstract: Long context LLMs are vulnerable to prompt injection, where an attacker can inject an instruction in a long context to induce an LLM to generate an attacker-desired output. Existing prompt injection defenses are designed for short contexts. When extended to long-context scenarios, they have limited effectiveness. The reason is that an injected instruction constitutes only a very small portion of a long context, making the defense very challenging. In this work, we propose PISanitizer, which first pinpoints and sanitizes potential injected tokens (if any) in a context before letting a backend LLM generate a response, thereby eliminating the influence of the injected instruction. To sanitize injected tokens, PISanitizer builds on two observations: (1) prompt injection attacks essentially craft an instruction that compels an LLM to follow it, and (2) LLMs intrinsically leverage the attention mechanism to focus on crucial input tokens for output generation. Guided by these two observations, we first intentionally let an LLM follow arbitrary instructions in a context and then sanitize tokens receiving high attention that drive the instruction-following behavior of the LLM. By design, PISanitizer presents a dilemma for an attacker: the more effectively an injected instruction compels an LLM to follow it, the more likely it is to be sanitized by PISanitizer. Our extensive evaluation shows that PISanitizer can successfully prevent prompt injection, maintain utility, outperform existing defenses, is efficient, and is robust to optimization-based and strong adaptive attacks. The code is available at https://github.com/sleeepeer/PISanitizer.

</details>


### [5] [AFLGopher: Accelerating Directed Fuzzing via Feasibility-Aware Guidance](https://arxiv.org/abs/2511.10828)
*Weiheng Bai,Kefu Wu,Qiushi Wu,Kangjie Lu*

Main category: cs.CR

TL;DR: AFLGopher is a directed fuzzing technique that uses feasibility-aware distance calculation to efficiently reach target code sites, showing significant performance improvements over existing state-of-the-art fuzzers.


<details>
  <summary>Details</summary>
Motivation: Existing directed fuzzing approaches use control-flow distance calculation that is unaware of feasibility, leading to inefficient guidance towards targets.

Method: Proposes feasibility-aware distance calculation with classification based on limited traces and runtime feasibility-updating mechanism to improve prediction precision.

Result: AFLGopher reaches targets 2.52-3.76x faster and triggers vulnerabilities 4.52-5.60x faster than state-of-the-art fuzzers.

Conclusion: Feasibility-awareness significantly improves directed fuzzing efficiency, making AFLGopher a substantial advancement in the field.

Abstract: Directed fuzzing is a useful testing technique that aims to efficiently reach target code sites in a program. The core of directed fuzzing is the guiding mechanism that directs the fuzzing to the specified target. A general guiding mechanism adopted in existing directed fuzzers is to calculate the control-flow distance between the current progress and the target, and use that as feedback to guide the directed fuzzing. A fundamental problem with the existing guiding mechanism is that the distance calculation is \emph{feasibility-unaware}.
  In this work, we propose feasibility-aware directed fuzzing named AFLGopher. Our new feasibility-aware distance calculation provides pragmatic feedback to guide directed fuzzing to reach targets efficiently. We propose new techniques to address the challenges of feasibility prediction. Our new classification method allows us to predict the feasibility of all branches based on limited traces, and our runtime feasibility-updating mechanism gradually and efficiently improves the prediction precision. We implemented AFLGopher and compared AFLGopher with state-of-the-art directed fuzzers including AFLGo, enhanced AFLGo, WindRanger, BEACON and SelectFuzz. AFLGopher is 3.76x, 2.57x, 3.30x, 2.52x and 2.86x faster than AFLGo, BEACON, WindRanger, SelectFuzz and enhanced AFLGo, respectively, in reaching targets. AFLGopher is 5.60x, 5.20x, 4.98x, 4.52x, and 5.07x faster than AFLGo, BEACON, WindRanger, SelectFuzz and enhanced AFLGo, respectively, in triggering known vulnerabilities.

</details>


### [6] [Armadillo: Robust Single-Server Secure Aggregation for Federated Learning with Input Validation](https://arxiv.org/abs/2511.10863)
*Yiping Ma,Yue Guo,Harish Karthikeyan,Antigoni Polychroniadou*

Main category: cs.CR

TL;DR: Armadillo is a secure federated learning aggregation system that provides disruption resistance against adversarial clients in just 3 rounds with lightweight computation.


<details>
  <summary>Details</summary>
Motivation: Existing secure aggregation methods either have high per-client computational costs or require many communication rounds, creating a need for an efficient solution with strong security guarantees.

Method: Uses a two-layer secure aggregation protocol requiring only simple arithmetic operations and an agreement protocol to remove malicious clients' influence with minimal rounds.

Result: Armadillo achieves secure aggregation in just 3 communication rounds while keeping both server and client computational overhead low.

Conclusion: The system demonstrates that efficient disruption-resistant secure aggregation is feasible in federated learning without heavy cryptographic overhead.

Abstract: This paper presents a secure aggregation system Armadillo that has disruptive resistance against adversarial clients, such that any coalition of malicious clients (within the tolerated threshold) can affect the aggregation result only by misreporting their private inputs in a pre-defined legitimate range. Armadillo is designed for federated learning setting, where a single powerful server interacts with many weak clients iteratively to train models on client's private data. While a few prior works consider disruption resistance under such setting, they either incur high per-client cost (Chowdhury et al. CCS '22) or require many rounds (Bell et al. USENIX Security '23). Although disruption resistance can be achieved generically with zero-knowledge proof techniques (which we also use in this paper), we realize an efficient system with two new designs: 1) a simple two-layer secure aggregation protocol that requires only simple arithmetic computation; 2) an agreement protocol that removes the effect of malicious clients from the aggregation with low round complexity. With these techniques, Armadillo completes each secure aggregation in 3 rounds while keeping the server and clients computationally lightweight.

</details>


### [7] [On the Information-Theoretic Fragility of Robust Watermarking under Diffusion Editing](https://arxiv.org/abs/2511.10933)
*Yunyi Ni,Ziyu Yang,Ze Niu,Emily Davis,Finn Carter*

Main category: cs.CR

TL;DR: Diffusion-based image editing poses a significant threat to robust watermarking systems, as it can effectively remove watermarks while maintaining image quality, raising ethical concerns and requiring new resilient watermarking strategies.


<details>
  <summary>Details</summary>
Motivation: The emergence of powerful diffusion-based image generation and editing techniques threatens the integrity of existing robust invisible watermarking systems, potentially allowing watermark removal while preserving image fidelity.

Method: The research combines theoretical analysis of mutual information loss during diffusion transformations with empirical evaluations using a guided diffusion attack algorithm targeting watermark signals.

Result: Experiments show near-zero watermark recovery rates after attack on state-of-the-art watermarking schemes, while maintaining high visual fidelity in regenerated images.

Conclusion: The study highlights the vulnerability of current watermarking to diffusion-based attacks, underscores ethical implications, and provides design guidelines for creating more resilient watermarking strategies in the generative AI era.

Abstract: Robust invisible watermarking embeds hidden information in images such that the watermark can survive various manipulations. However, the emergence of powerful diffusion-based image generation and editing techniques poses a new threat to these watermarking schemes. In this paper, we investigate the intersection of diffusion-based image editing and robust image watermarking. We analyze how diffusion-driven image edits can significantly degrade or even fully remove embedded watermarks from state-of-the-art robust watermarking systems. Both theoretical formulations and empirical experiments are provided. We prove that as a image undergoes iterative diffusion transformations, the mutual information between the watermarked image and the embedded payload approaches zero, causing watermark decoding to fail. We further propose a guided diffusion attack algorithm that explicitly targets and erases watermark signals during generation. We evaluate our approach on recent deep learning-based watermarking schemes and demonstrate near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Finally, we discuss ethical implications of such watermark removal capablities and provide design guidelines for future watermarking strategies to be more resilient in the era of generative AI.

</details>


### [8] [Gynopticon: Consensus-Based Cheating Detection System for Competitive Games](https://arxiv.org/abs/2511.10992)
*Jeuk Kang,Jungheum Park*

Main category: cs.CR

TL;DR: GYNOPTICON是一个基于用户共识的新型作弊检测框架，通过客户端轻量级检测和服务器端投票系统识别异常行为，为竞技游戏提供隐私保护的反作弊解决方案。


<details>
  <summary>Details</summary>
Motivation: 竞技类游戏作弊检测研究不足，传统内核级反作弊方案存在隐私和安全问题，需要开发更透明、隐私友好的替代方案。

Method: 提出GYNOPTICON框架，结合客户端轻量级检测机制和服务器端投票系统，通过用户共识识别异常行为。

Result: 在控制模拟和真实FPS环境中验证了系统的可行性和有效性，能够可靠检测作弊用户。

Conclusion: GYNOPTICON为用户驱动的共识型反作弊系统提供了实用且保护隐私的替代方案，适用于长期游戏管理。

Abstract: Cheating in online games poses significant threats to the gaming industry, yet most prior research has concentrated on Massively Multiplayer Online Role-Playing Games (MMORPGs). Competitive genres-such as Multiplayer Online Battle Arena (MOBA), First Person Shooter (FPS), Real Time Strategy (RTS), and Action games-remain underexplored due to the difficulty of detecting cheating users and the demand for complex data and techniques. To address this gap, many game companies rely on kernel-level anti-cheat solutions, which, while effective, raise serious concerns regarding user privacy and system security. In this paper, we propose GYNOPTICON, a novel cheating detection framework that leverages user consensus to identify abnormal behavior. GYNOPTICON integrates a lightweight client-side detection mechanism with a server-side voting system: when suspicious activity is identified, clients cast votes to the server, which aggregates them to establish consensus and distinguish cheaters from legitimate players. This architecture enables transparency, reduces reliance on intrusive monitoring, and mitigates privacy risks. We evaluate GYNOPTICON in both a controlled simulation and a real-world FPS environment. Simulation results verify its feasibility and requirements, while real-world experiments confirm its effectiveness in reliably detecting cheating users. Furthermore, we demonstrate the system's applicability and sustainability for long-term game management using public datasets. GYNOPTICON represents a user-driven, consensus-based alternative to conventional anti-cheat systems, offering a practical and privacy-preserving solution for competitive online games.

</details>


### [9] [Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis](https://arxiv.org/abs/2511.11020)
*Farhad Abtahi,Fernando Seoane,Iván Pau,Mario Vega-Barbas*

Main category: cs.CR

TL;DR: 医疗AI系统面临重大数据中毒漏洞，现有防御和法规无法充分应对。攻击者仅需100-500个样本即可破坏系统，成功率超60%，探测需6-12个月。需多层防御和可解释系统。


<details>
  <summary>Details</summary>
Motivation: 医疗AI系统在关键医疗决策中的广泛应用，但其对数据中毒攻击的脆弱性尚未得到充分认识和解决，现有法规和防御措施存在明显不足。

Method: 分析了八种攻击场景，涵盖架构攻击（CNN、LLM、RL）、基础设施攻击（联邦学习、医疗文档）、关键资源分配攻击（器官移植、危机分诊）和供应链攻击（商业基础模型）。

Result: 攻击者通过少量样本即可成功破坏系统，分布式医疗基础设施提供多个入口点，隐私法规意外保护攻击者，供应链弱点允许单点失效影响50-200个机构。

Conclusion: 需强制性对抗性测试、集成检测、隐私保护安全机制和国际AI安全标准协调，建议转向可解释系统并质疑黑盒模型在高风险临床决策中的适用性。

Abstract: Healthcare AI systems face major vulnerabilities to data poisoning that current defenses and regulations cannot adequately address. We analyzed eight attack scenarios in four categories: architectural attacks on convolutional neural networks, large language models, and reinforcement learning agents; infrastructure attacks exploiting federated learning and medical documentation systems; critical resource allocation attacks affecting organ transplantation and crisis triage; and supply chain attacks targeting commercial foundation models. Our findings indicate that attackers with access to only 100-500 samples can compromise healthcare AI regardless of dataset size, often achieving over 60 percent success, with detection taking an estimated 6 to 12 months or sometimes not occurring at all. The distributed nature of healthcare infrastructure creates many entry points where insiders with routine access can launch attacks with limited technical skill. Privacy laws such as HIPAA and GDPR can unintentionally shield attackers by restricting the analyses needed for detection. Supply chain weaknesses allow a single compromised vendor to poison models across 50 to 200 institutions. The Medical Scribe Sybil scenario shows how coordinated fake patient visits can poison data through legitimate clinical workflows without requiring a system breach. Current regulations lack mandatory adversarial robustness testing, and federated learning can worsen risks by obscuring attribution. We recommend multilayer defenses including required adversarial testing, ensemble-based detection, privacy-preserving security mechanisms, and international coordination on AI security standards. We also question whether opaque black-box models are suitable for high-stakes clinical decisions, suggesting a shift toward interpretable systems with verifiable safety guarantees.

</details>


### [10] [SALT-V: Lightweight Authentication for 5G V2X Broadcasting](https://arxiv.org/abs/2511.11028)
*Liu Cao,Weizheng Wang,Qipeng Xie,Dongyu Wei,Lyutianyang Zhang*

Main category: cs.CR

TL;DR: SALT-V is a hybrid authentication framework for V2X communication that combines ECDSA signatures with lightweight GMAC operations to achieve both immediate authentication and computational efficiency, overcoming the trade-offs of traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional V2X authentication methods face a dilemma: ECDSA provides strong security but has slow verification delays, while symmetric approaches like TESLA are efficient but introduce key disclosure latency. Neither meets 5G NR-V2X requirements for both immediate authentication and computational efficiency.

Method: SALT-V uses ECDSA signatures for 10% of traffic (BOOT frames) to establish trust, then authenticates 90% of messages (DATA frames) using lightweight GMAC operations. It employs an Ephemeral Session Tag whitelist mechanism and Bloom filters for O(1) revocation checking.

Result: SALT-V achieves 0.035 ms average computation time (57x faster than ECDSA), 1 ms end-to-end latency, 41-byte overhead, and linear scalability to 2000 vehicles.

Conclusion: SALT-V is the first practical solution to satisfy all safety-critical requirements for real-time V2X deployment by intelligently combining cryptographic approaches to overcome the authentication dilemma.

Abstract: Vehicle-to-Everything (V2X) communication faces a critical authentication dilemma: traditional public-key schemes like ECDSA provide strong security but impose 2 ms verification delays unsuitable for collision avoidance, while symmetric approaches like TESLA achieve microsecond-level efficiency at the cost of 20-100 ms key disclosure latency. Neither meets 5G New Radio (NR)-V2X's stringent requirements for both immediate authentication and computational efficiency. This paper presents SALT-V, a novel hybrid authentication framework that reconciles this fundamental trade-off through intelligent protocol stratification. SALT-V employs ECDSA signatures for 10% of traffic (BOOT frames) to establish sender trust, then leverages this trust anchor to authenticate 90% of messages (DATA frames) using lightweight GMAC operations. The core innovation - an Ephemeral Session Tag (EST) whitelist mechanism - enables 95% of messages to achieve immediate verification without waiting for key disclosure, while Bloom filter integration provides O(1) revocation checking in 1 us. Comprehensive evaluation demonstrates that SALT-V achieves 0.035 ms average computation time (57x faster than pure ECDSA), 1 ms end-to-end latency, 41-byte overhead, and linear scalability to 2000 vehicles, making it the first practical solution to satisfy all safety-critical requirements for real-time V2X deployment.

</details>


### [11] [Finding Software Supply Chain Attack Paths with Logical Attack Graphs](https://arxiv.org/abs/2511.11171)
*Luıs Soeiro,Thomas Robert,Stefano Zacchiroli*

Main category: cs.CR

TL;DR: Extension of MulVal to incorporate software supply chain threat propagation analysis alongside network-based threat analysis using new predicates and rules.


<details>
  <summary>Details</summary>
Motivation: Current MulVal lacks support for software supply chain attacks like XZ compromise and 3CX, which are increasingly common and sophisticated.

Method: Proposed extension introduces new predicates in MulVal syntax to model SSC assets, dependencies, compromises, and threats, integrating with existing network analysis.

Result: Enables MulVal to capture and reason about SSC threat propagation, demonstrated through practical application.

Conclusion: The extension enhances MulVal's capability to address modern SSC attacks effectively.

Abstract: Cyberattacks are becoming increasingly frequent and sophisticated, often exploiting the software supply chain (SSC) as an attack vector. Attack graphs provide a detailed representation of the sequence of events and vulnerabilities that could lead to a successful security breach in a system. MulVal is a widely used open-source tool for logical attack graph generation in networked systems. However, its current lack of support for capturing and reasoning about SSC threat propagation makes it unsuitable for addressing modern SSC attacks, such as the XZ compromise or the 3CX double SSC attack. To address this limitation, we propose an extension to MulVal that integrates SSC threat propagation analysis with existing network-based threat analysis. This extension introduces a new set of predicates within the familiar MulVal syntax, enabling seamless integration. The new facts and interaction rules model SSC assets, their dependencies, interactions, compromises, additional security mechanisms, initial system states, and known threats. We explain how this integration operates in both directions and demonstrate the practical application of the extension.

</details>


### [12] [Bridging Local and Federated Data Normalization in Federated Learning: A Privacy-Preserving Approach](https://arxiv.org/abs/2511.11249)
*Melih Coşğun,Mert Gençtürk,Sinem Sav*

Main category: cs.CR

TL;DR: 论文提出联邦归一化方法，在联邦学习中模拟集中式归一化的效果，通过安全共享归一化参数解决数据异构性和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的数据归一化面临挑战：本地归一化在非IID数据下效果差，集中式归一化违反数据本地化原则。需要一种既能保持数据本地化又能实现集中式归一化效果的方法。

Method: 提出联邦归一化框架，通过安全多方全同态加密（MHE）实现归一化参数（如均值、中位数）的隐私保护协同计算，设计了同态加密的k阶统计量计算方案。

Result: 联邦归一化在异构联邦学习场景中能达到与集中式归一化相近的性能，同时保护数据隐私，解决了本地归一化在非IID数据下的性能损失问题。

Conclusion: 联邦归一化是联邦学习中有效的预处理方法，通过密码学工具实现了隐私保护与模型性能的平衡，为联邦学习数据预处理提供了新思路。

Abstract: Data normalization is a crucial preprocessing step for enhancing model performance and training stability. In federated learning (FL), where data remains distributed across multiple parties during collaborative model training, normalization presents unique challenges due to the decentralized and often heterogeneous nature of the data. Traditional methods rely on either independent client-side processing, i.e., local normalization, or normalizing the entire dataset before distributing it to parties, i.e., pooled normalization. Local normalization can be problematic when data distributions across parties are non-IID, while the pooled normalization approach conflicts with the decentralized nature of FL. In this paper, we explore the adaptation of widely used normalization techniques to FL and define the term federated normalization. Federated normalization simulates pooled normalization by enabling the collaborative exchange of normalization parameters among parties. Thus, it achieves performance on par with pooled normalization without compromising data locality. However, sharing normalization parameters such as the mean introduces potential privacy risks, which we further mitigate through a robust privacy-preserving solution. Our contributions include: (i) We systematically evaluate the impact of various federated and local normalization techniques in heterogeneous FL scenarios, (ii) We propose a novel homomorphically encrypted $k$-th ranked element (and median) calculation tailored for the federated setting, enabling secure and efficient federated normalization, (iii) We propose privacy-preserving implementations of widely used normalization techniques for FL, leveraging multiparty fully homomorphic encryption (MHE).

</details>


### [13] [Prompt Engineering vs. Fine-Tuning for LLM-Based Vulnerability Detection in Solana and Algorand Smart Contracts](https://arxiv.org/abs/2511.11250)
*Biagio Boi,Christian Esposito*

Main category: cs.CR

TL;DR: This paper evaluates LLMs for detecting OWASP-inspired vulnerabilities in Solana and Algorand smart contracts using synthetic datasets, comparing prompt engineering, fine-tuning, and hybrid approaches.


<details>
  <summary>Details</summary>
Motivation: To address the lack of labeled datasets for non-EVM platforms and assess LLM effectiveness in detecting smart contract vulnerabilities beyond the EVM ecosystem.

Method: Created synthetic datasets of annotated smart contract snippets in Rust (Solana) and PyTeal (Algorand) based on OWASP taxonomy. Evaluated LLMs using prompt engineering, fine-tuning, and hybrid approaches across different vulnerability categories.

Result: Prompt engineering showed general robustness, while fine-tuning improved precision and recall for less semantically rich languages like TEAL. Platform-specific mappings revealed how architectural differences affect vulnerability detectability.

Conclusion: LLM-based approaches are viable for static vulnerability detection in smart contracts when domain-specific data and categorization are integrated into training pipelines, though platform-specific considerations are crucial.

Abstract: Smart contracts have emerged as key components within decentralized environments, enabling the automation of transactions through self-executing programs. While these innovations offer significant advantages, they also present potential drawbacks if the smart contract code is not carefully designed and implemented. This paper investigates the capability of large language models (LLMs) to detect OWASP-inspired vulnerabilities in smart contracts beyond the Ethereum Virtual Machine (EVM) ecosystem, focusing specifically on Solana and Algorand. Given the lack of labeled datasets for non-EVM platforms, we design a synthetic dataset of annotated smart contract snippets in Rust (for Solana) and PyTeal (for Algorand), structured around a vulnerability taxonomy derived from OWASP. We evaluate LLMs under three configurations: prompt engineering, fine-tuning, and a hybrid of both, comparing their performance on different vulnerability categories. Experimental results show that prompt engineering achieves general robustness, while fine-tuning improves precision and recall on less semantically rich languages such as TEAL. Additionally, we analyze how the architectural differences of Solana and Algorand influence the manifestation and detectability of vulnerabilities, offering platform-specific mappings that highlight limitations in existing security tooling. Our findings suggest that LLM-based approaches are viable for static vulnerability detection in smart contracts, provided domain-specific data and categorization are integrated into training pipelines.

</details>


### [14] [Privacy Challenges and Solutions in Retrieval-Augmented Generation-Enhanced LLMs for Healthcare Chatbots: A Review of Applications, Risks, and Future Directions](https://arxiv.org/abs/2511.11347)
*Shaowei Guan,Hin Chi Kwok,Ngai Fong Law,Gregor Stiglic,Vivian Hui*

Main category: cs.CR

TL;DR: This review analyzes privacy risks in healthcare RAG systems, examining sensitive data types, privacy threats, protection mechanisms, and future directions through a pipeline framework.


<details>
  <summary>Details</summary>
Motivation: RAG systems are increasingly used in healthcare but present significant privacy risks like PHI exposure that require systematic analysis and mitigation strategies.

Method: The authors synthesized 23 articles on healthcare RAG applications and 17 articles on privacy-preserving strategies using a pipeline framework covering data storage, transmission, retrieval and generation stages.

Result: The review identified critical gaps including insufficient clinical validation, lack of standardized evaluation frameworks, and absence of automated assessment tools for privacy protection.

Conclusion: Actionable directions are proposed to address privacy vulnerabilities, providing a roadmap for developing clinically effective RAG systems with robust privacy preservation.

Abstract: Retrieval-augmented generation (RAG) has rapidly emerged as a transformative approach for integrating large language models into clinical and biomedical workflows. However, privacy risks, such as protected health information (PHI) exposure, remain inconsistently mitigated. This review provides a thorough analysis of the current landscape of RAG applications in healthcare, including (i) sensitive data type across clinical scenarios, (ii) the associated privacy risks, (iii) current and emerging data-privacy protection mechanisms and (iv) future direction for patient data privacy protection. We synthesize 23 articles on RAG applications in healthcare and systematically analyze privacy challenges through a pipeline-structured framework encompassing data storage, transmission, retrieval and generation stages, delineating potential failure modes, their underlying causes in threat models and system mechanisms, and their practical implications. Building on this analysis, we critically review 17 articles on privacy-preserving strategies for RAG systems. Our evaluation reveals critical gaps, including insufficient clinical validation, absence of standardized evaluation frameworks, and lack of automated assessment tools. We propose actionable directions based on these limitations and conclude with a call to action. This review provides researchers and practitioners with a structured framework for understanding privacy vulnerabilities in healthcare RAG and offers a roadmap toward developing systems that achieve both clinical effectiveness and robust privacy preservation.

</details>


### [15] [SEAL: Subspace-Anchored Watermarks for LLM Ownership](https://arxiv.org/abs/2511.11356)
*Yanbo Dai,Zongjie Li,Zhenlan Ji,Shuai Wang*

Main category: cs.CR

TL;DR: SEAL is a novel watermarking framework that embeds multi-bit signatures into LLM's latent space for IP protection, offering robust verification while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: Existing IP protection methods like fingerprinting and traditional watermarking have limitations - fingerprinting can't identify specific model instances, while watermarks can be removed through fine-tuning. There's a need for more robust protection for valuable LLM IP.

Method: Uses model editing to align hidden representations of anchor samples with orthogonal bit vectors, embedding watermarks directly into the model's latent representational space while preserving factual predictions.

Result: Comprehensive experiments on 6 LLMs and 11 existing methods show SEAL has superior effectiveness, fidelity, efficiency, and robustness against knowledgeable attacks.

Conclusion: SEAL provides robust watermarking that withstands removal attempts even when attackers know the watermarking mechanism, offering practical IP protection for LLMs.

Abstract: Large language models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks, demonstrating human-level performance in text generation, reasoning, and question answering. However, training such models requires substantial computational resources, large curated datasets, and sophisticated alignment procedures. As a result, they constitute highly valuable intellectual property (IP) assets that warrant robust protection mechanisms. Existing IP protection approaches suffer from critical limitations. Model fingerprinting techniques can identify model architectures but fail to establish ownership of specific model instances. In contrast, traditional backdoor-based watermarking methods embed behavioral anomalies that can be easily removed through common post-processing operations such as fine-tuning or knowledge distillation.
  We propose SEAL, a subspace-anchored watermarking framework that embeds multi-bit signatures directly into the model's latent representational space, supporting both white-box and black-box verification scenarios. Our approach leverages model editing techniques to align the hidden representations of selected anchor samples with predefined orthogonal bit vectors. This alignment embeds the watermark while preserving the model's original factual predictions, rendering the watermark functionally harmless and stealthy. We conduct comprehensive experiments on multiple benchmark datasets and six prominent LLMs, comparing SEAL with 11 existing fingerprinting and watermarking methods to demonstrate its superior effectiveness, fidelity, efficiency, and robustness. Furthermore, we evaluate SEAL under potential knowledgeable attacks and show that it maintains strong verification performance even when adversaries possess knowledge of the watermarking mechanism and the embedded signatures.

</details>


### [16] [Grid-STIX: A STIX 2.1-Compliant Cyber-Physical Security Ontology for Power Grid](https://arxiv.org/abs/2511.11366)
*Benjamin Blakely,Daniel Karcz*

Main category: cs.CR

TL;DR: Grid-STIX is a cybersecurity framework extension of STIX 2.1 specifically designed for electrical power grids, addressing their unique cyber-physical characteristics and enabling threat intelligence sharing across the energy sector.


<details>
  <summary>Details</summary>
Motivation: Traditional IT security standards like STIX 2.1 and MITRE ATT&CK lack specialization for power grid assets, operational technology relationships, and cyber-physical interdependencies required for comprehensive grid security.

Method: Developed a domain-specific STIX 2.1 extension with modular architecture covering physical assets, OT components, cyber-physical relationships, security policies, and nuclear safeguards through validation pipelines and Python code generation.

Result: Created an open-source framework supporting threat modeling, attack pattern representation, supply chain risk assessment, cross-domain impact analysis, Zero Trust enforcement, and nuclear facility cybersecurity applications.

Conclusion: Grid-STIX successfully bridges the gap in grid-specific cybersecurity frameworks, providing a standardized approach for threat intelligence sharing and security modeling across conventional and nuclear power systems.

Abstract: Modern electrical power grids represent complex cyber-physical systems requiring specialized cybersecurity frameworks beyond traditional IT security models. Existing threat intelligence standards such as STIX 2.1 and MITRE ATT\&CK lack coverage for grid-specific assets, operational technology relationships, and cyber-physical interdependencies essential for power system security. We present Grid-STIX, a domain-specific extension of STIX 2.1 for electrical grid cybersecurity applications. Grid-STIX employs a modular architecture encompassing physical assets, operational technology components, cyber-physical relationships, and security policies that capture modern power systems including distributed energy resources, advanced metering infrastructure, and nuclear energy facilities. The framework provides threat modeling capabilities through systematic representation of attack patterns, supply chain risks, and cross-domain impact analysis while maintaining STIX 2.1 compliance. Grid-STIX includes modules for nuclear safeguards and non-proliferation verification, enabling cybersecurity modeling across conventional and nuclear energy sectors. The ontology supports Zero Trust enforcement through policy decision points and operational context integration. Our implementation includes validation pipelines, Python code generation, and visualizations. Use cases demonstrate applications including cross-utility threat intelligence sharing, supply chain risk assessment, and nuclear facility cybersecurity. Grid-STIX is available as an open-source framework to advance collaborative cybersecurity research across the electrical power sector.

</details>


### [17] [SoK: Security Evaluation of Wi-Fi CSI Biometrics: Attacks, Metrics, and Systemic Weaknesses](https://arxiv.org/abs/2511.11381)
*Gioliano de Oliveira Braga,Pedro Henrique dos Santos Rocha,Rafael Pimenta de Mattos Paixão,Giovani Hoff da Costa,Gustavo Cavalcanti Morais,Lourenço Alves Pereira Júnior*

Main category: cs.CR

TL;DR: This SoK paper critically analyzes Wi-Fi CSI biometric authentication from a security perspective, exposing systemic inconsistencies in evaluation methods and highlighting hidden risks through a unified evaluation framework.


<details>
  <summary>Details</summary>
Motivation: Despite reports of high accuracy in Wi-Fi CSI biometrics, there's a lack of consolidated understanding about security properties, adversarial resilience, and methodological consistency in the field.

Method: The authors conduct a systematic knowledge review, analyzing existing work across sensing infrastructure, signal representations, feature pipelines, learning models, and evaluation methodologies, then construct a unified evaluation framework.

Result: Reveals systemic issues including reliance on aggregate accuracy metrics, limited security reporting (FAR/FRR/EER), absence of per-user risk analysis, and insufficient threat modeling, demonstrating how security metrics uncover hidden risk concentrations.

Conclusion: Wi-Fi CSI biometrics have significant security limitations and methodological inconsistencies; the paper provides security boundaries, evaluation guidelines, and research directions for making this authentication primitive more rigorous and reproducible.

Abstract: Wi-Fi Channel State Information (CSI) has been repeatedly proposed as a biometric modality, often with reports of high accuracy and operational feasibility. However, the field lacks a consolidated understanding of its security properties, adversarial resilience, and methodological consistency. This Systematization of Knowledge (SoK) examines CSI-based biometric authentication through a security perspective, analyzing how existing work differs across sensing infrastructure, signal representations, feature pipelines, learning models, and evaluation methodologies. Our synthesis reveals systemic inconsistencies: reliance on aggregate accuracy metrics, limited reporting of FAR/FRR/EER, absence of per-user risk analysis, and scarce consideration of threat models or adversarial feasibility. We construct a unified evaluation framework to empirically expose these issues and demonstrate how security-relevant metrics, such as per-class EER, FCS, and the Gini Coefficient, uncover risk concentration that remains hidden under traditional reporting practices. Our analysis highlights concrete attack surfaces and shows how methodological choices materially influence vulnerability profiles, which include replay, geometric mimicry, and environmental perturbation. Based on these findings, we articulate the security boundaries of current CSI biometrics and provide guidelines for rigorous evaluation, reproducible experimentation, and future research directions. This SoK offers the security community a structured, evidence-driven reassessment of Wi-Fi CSI biometrics and their suitability as an authentication primitive.

</details>


### [18] [Automated Side-Channel Analysis of Cryptographic Protocol Implementations](https://arxiv.org/abs/2511.11385)
*Faezeh Nasrabadi,Robert Künnemann,Hamed Nemati*

Main category: cs.CR

TL;DR: Researchers extracted WhatsApp's first formal model from its binary using reverse engineering, proving forward secrecy, identifying attacks, and developing a framework to analyze side-channel vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: To analyze the security of large closed-source applications like WhatsApp by creating accurate formal models directly from their implementation to uncover vulnerabilities that specification-based methods might miss.

Method: Combined binary-level analysis (CryptoBap) with reverse engineering (Ghidra) to extract a formal model, then extended CryptoBap to integrate hardware leakage contracts for side-channel analysis using DeepSec prover.

Result: Identified a known clone-attack, functional gaps between implementation and specification, a privacy attack exposing victim's contacts, and confirmed an unlinkability attack on BAC protocol.

Conclusion: The methodology enables comprehensive security analysis against both functional bugs and side-channel attacks, revealing critical vulnerabilities in WhatsApp and electronic passport protocols that traditional methods cannot detect.

Abstract: We extract the first formal model of WhatsApp from its implementation by combining binary-level analysis (via CryptoBap) with reverse engineering (via Ghidra) to handle this large closed-source application. Using this model, we prove forward secrecy, identify a known clone-attack against post-compromise security and discover functional gaps between WhatsApp's implementation and its specification. We further introduce a methodology to analyze cryptographic protocol implementations for their resilience to side-channel attacks. This is achieved by extending the CryptoBap framework to integrate hardware leakage contracts into the protocol model, which we then pass to the state-of-the-art protocol prover, DeepSec. This enables a detailed security analysis against both functional bugs and microarchitectural side-channel attacks. Using this methodology, we identify a privacy attack in WhatsApp that allows a side-channel attacker to learn the victim's contacts and confirm a known unlinkability attack on the BAC protocol used in electronic passports.
  Key contributions include (1) the first formal model of WhatsApp, extracted from its binary, (2) a framework to integrate side-channel leakage contracts into protocol models for the first time, and (3) revealing critical vulnerabilities invisible to specification-based methods.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [Picking a Representative Set of Solutions in Multiobjective Optimization: Axioms, Algorithms, and Experiments](https://arxiv.org/abs/2511.10716)
*Niclas Boehmer,Maximilian T. Wittmann*

Main category: cs.AI

TL;DR: 该论文提出了一种新的多目标优化解决方案选择方法，通过将帕累托剪枝问题重新定义为多赢家投票问题，引入了一种新的质量度量标准——导向覆盖度，并分析了不同质量度量的计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 多目标优化问题中存在大量帕累托最优解，决策者需要从中选择最偏好的解，这给决策者带来了较大的认知负担。现有帕累托剪枝方法的质量度量存在一些不直观的行为，需要改进。

Method: 将帕累托剪枝问题重新定义为多赢家投票问题，对现有质量度量进行公理分析，提出新的导向覆盖度度量，分析优化不同质量度量的计算复杂性，并进行实验评估。

Result: 研究发现质量度量的选择对所选解集的特征具有决定性影响，提出的导向覆盖度度量在各种设置下表现具有竞争力甚至更优。

Conclusion: 导向覆盖度是一种有效的帕累托剪枝质量度量标准，能够帮助决策者更好地选择代表性解集，降低决策认知负担。

Abstract: Many real-world decision-making problems involve optimizing multiple objectives simultaneously, rendering the selection of the most preferred solution a non-trivial problem: All Pareto optimal solutions are viable candidates, and it is typically up to a decision maker to select one for implementation based on their subjective preferences. To reduce the cognitive load on the decision maker, previous work has introduced the Pareto pruning problem, where the goal is to compute a fixed-size subset of Pareto optimal solutions that best represent the full set, as evaluated by a given quality measure. Reframing Pareto pruning as a multiwinner voting problem, we conduct an axiomatic analysis of existing quality measures, uncovering several unintuitive behaviors. Motivated by these findings, we introduce a new measure, directed coverage. We also analyze the computational complexity of optimizing various quality measures, identifying previously unknown boundaries between tractable and intractable cases depending on the number and structure of the objectives. Finally, we present an experimental evaluation, demonstrating that the choice of quality measure has a decisive impact on the characteristics of the selected set of solutions and that our proposed measure performs competitively or even favorably across a range of settings.

</details>


### [20] [AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery](https://arxiv.org/abs/2511.11257)
*Yuqi Yin,Yibo Fu,Siyuan Wang,Peng Sun,Hongyu Wang,Xiaohui Wang,Lei Zheng,Zhiyong Li,Zhirong Liu,Jianji Wang,Zhaoxi Sun*

Main category: cs.AI

TL;DR: AIonopedia is the first LLM agent for Ionic Liquid discovery, using a multimodal domain foundation model for accurate property prediction and hierarchical search architecture.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in IL discovery including limited data, poor model accuracy, and fragmented workflows.

Method: Leverages LLM-augmented multimodal domain foundation model trained on a comprehensive IL dataset with hierarchical search architecture.

Result: Superior performance in property predictions, effective IL modification in literature evaluations, and successful real-world wet-lab validation.

Conclusion: AIonopedia demonstrates exceptional generalization capabilities and ability to accelerate real-world IL discovery.

Abstract: The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.

</details>


### [21] [The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems](https://arxiv.org/abs/2511.10704)
*Samih Fadli*

Main category: cs.AI

TL;DR: 论文提出AI对齐的热力学第二定律类比：伦理熵（偏离预期目标的程度）在没有持续对齐工作的情况下会自发增加。通过数学证明和实验验证，建立了对齐工作的临界稳定性边界公式。


<details>
  <summary>Details</summary>
Motivation: AI系统在没有持续对齐工作的情况下会自发偏离预设目标，需要量化框架来理解和控制这种伦理熵的增加。

Method: 定义伦理熵S = -Σ p(g_i;theta) ln p(g_i;theta)，证明dS/dt ≥ 0，推导临界对齐工作gamma_crit = (lambda_max/2)lnN，并通过70亿参数模型的模拟实验验证理论。

Result: 未正则化模型：熵从0.32增加到1.69±1.08 nats；正则化模型（gamma=20.4）：熵稳定在0.00±0.00 nats（p=4.19×10^-17）。

Conclusion: AI对齐问题可重新定义为连续热力学控制问题，为高级自主系统的稳定性和安全性提供量化基础。

Abstract: We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -Σ p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.

</details>


### [22] [Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents](https://arxiv.org/abs/2511.10705)
*Yuan Zhao,Hualei Zhu,Tingyu Jiang,Shen Li,Xiaohang Xu,Hao Henry Wang*

Main category: cs.AI

TL;DR: Co-EPG is a self-iterative framework that co-evolves planning and grounding models for GUI task automation. Through iterative training with reward guidance and data generation, it achieves state-of-the-art results on benchmarks without external data.


<details>
  <summary>Details</summary>
Motivation: Current GUI automation methods suffer from insufficient exploitation of cross-model synergies and over-reliance on synthetic data. Co-EPG addresses these limitations by creating a co-evolutionary feedback loop.

Method: Co-EPG establishes a positive feedback loop where: 1) planning model explores strategies under grounding-based rewards via GRPO, 2) generates diverse training data for grounding model, 3) optimized grounding provides better rewards for planning model improvement.

Result: On Multimodal-Mind2Web and AndroidControl benchmarks, Co-EPG outperforms state-of-the-art methods after just three iterations, demonstrating consistent improvement with each iteration cycle without external data requirements.

Conclusion: Co-EPG establishes a novel training paradigm for GUI agents that shifts from isolated optimization to integrated, self-driven co-evolution, enabling robust self-enhancement capabilities.

Abstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.

</details>


### [23] [Structure-Aware Encodings of Argumentation Properties for Clique-width](https://arxiv.org/abs/2511.10767)
*Yasir Mahmood,Markus Hecher,Johanna Groven,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 该论文提出了一种基于团宽度的抽象论证问题的(Q)SAT编码方法，建立了编码复杂度的下限。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注树宽参数，但团宽度能更好处理稠密图。抽象论证框架基于有向图且计算复杂，是研究计算特性的理想候选。目前缺少对团宽度编码能力的理解。

Method: 设计了从论证问题到(Q)SAT的新归约方法——有向分解引导(DDG)归约，能线性保持团宽度。

Result: 为所有论证语义（包括计数）建立了新结果，证明DDG归约的开销在合理假设下无法显著改进。

Conclusion: 这项工作开启了理解团宽度编码能力的研究方向，为复杂图参数在SAT求解中的应用提供了新视角。

Abstract: Structural measures of graphs, such as treewidth, are central tools in computational complexity resulting in efficient algorithms when exploiting the parameter. It is even known that modern SAT solvers work efficiently on instances of small treewidth. Since these solvers are widely applied, research interests in compact encodings into (Q)SAT for solving and to understand encoding limitations. Even more general is the graph parameter clique-width, which unlike treewidth can be small for dense graphs. Although algorithms are available for clique-width, little is known about encodings. We initiate the quest to understand encoding capabilities with clique-width by considering abstract argumentation, which is a robust framework for reasoning with conflicting arguments. It is based on directed graphs and asks for computationally challenging properties, making it a natural candidate to study computational properties. We design novel reductions from argumentation problems to (Q)SAT. Our reductions linearly preserve the clique-width, resulting in directed decomposition-guided (DDG) reductions. We establish novel results for all argumentation semantics, including counting. Notably, the overhead caused by our DDG reductions cannot be significantly improved under reasonable assumptions.

</details>


### [24] [Potential Outcome Rankings for Counterfactual Decision Making](https://arxiv.org/abs/2511.10776)
*Yuta Kawakami,Jin Tian*

Main category: cs.AI

TL;DR: 论文引入两种新指标(PoR和PoB)用于反事实决策，建立了识别定理和估计方法，并通过数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有的反事实决策方法通常依赖于期望潜在结果的排序，但缺乏对个体层面结果排名概率的量化指标。

Method: 引入PoR(潜在结果排名概率)和PoB(获得最佳结果概率)两个新指标，建立识别定理、推导边界并提出估计方法。

Result: 通过数值实验验证了估计器的有限样本性质，并在真实数据集上展示了应用效果。

Conclusion: PoR和PoB为新决策规则提供了实用框架，能够更精确地量化个体层面的决策不确定性。

Abstract: Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.

</details>


### [25] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: This survey reframes LLM reasoning through adaptivity - allocating reasoning effort based on input complexity - and provides a taxonomy of training-based and training-free adaptive reasoning methods.


<details>
  <summary>Details</summary>
Motivation: Current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing on difficult tasks, highlighting the need for adaptive reasoning.

Method: Formalizes reasoning types (deductive, inductive, abductive) and adaptive reasoning as control-augmented policy optimization, with a taxonomy dividing methods into training-based (reinforcement learning, supervised fine-tuning) and training-free (prompt conditioning, feedback-driven halting) approaches.

Result: A systematic framework for understanding and comparing adaptive reasoning strategies in LLMs, connecting classical cognitive paradigms with algorithmic implementations.

Conclusion: Identifies open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control for future adaptive reasoning research.

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [26] [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy,Vaibhav Gandhi*

Main category: cs.AI

TL;DR: HyperComplEx is a hybrid knowledge graph embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms, achieving superior performance on large-scale graphs.


<details>
  <summary>Details</summary>
Motivation: Existing embedding methods have limitations in modeling diverse relationship types at scale - Euclidean struggles with hierarchies, vector space models can't capture asymmetry, and hyperbolic fails on symmetric relations.

Method: Uses relation-specific space weighting to dynamically select optimal geometries for each relation type, with multi-space consistency loss for coherent predictions across spaces.

Result: Achieves 0.612 MRR on 10M-paper dataset (4.8% improvement over best baseline), 85ms inference per triple, and scales near-linearly with graph size.

Conclusion: HyperComplEx demonstrates consistent improvements over state-of-the-art baselines and enables scalable knowledge graph embeddings through adaptive geometry selection.

Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.

</details>


### [27] [Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction](https://arxiv.org/abs/2511.10853)
*Gerui Xu,Boyou Chen,Huizhong Guo,Dave LeBlanc,Ananna Ahmed,Zhaonan Sun,Shan Bao*

Main category: cs.AI

TL;DR: This study presents a multi-agent AI framework that reconstructs traffic collision scenarios from multimodal data, achieving perfect accuracy in identifying relevant events and vehicle roles, surpassing human expert performance.


<details>
  <summary>Details</summary>
Motivation: Traditional traffic collision reconstruction relying on human expertise often yields inconsistent results when analyzing incomplete multimodal data, highlighting the need for more reliable automated solutions.

Method: A two-phase collaborative framework combining reconstruction and reasoning phases, processing multimodal data including crash reports, tabular data, visual diagrams, and EDR records from 277 rear-end collisions.

Result: The framework achieved perfect accuracy across all 39 complex test cases, successfully identifying EDR events and vehicle roles, surpassing the 92% accuracy of human researchers and maintaining robust performance with incomplete data.

Conclusion: The study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.

Abstract: Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.

</details>


### [28] [LLM enhanced graph inference for long-term disease progression modelling](https://arxiv.org/abs/2511.10890)
*Tiantian He,An Zhao,Elinor Thompson,Anna Schroder,Ahmed Abdulaal,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.AI

TL;DR: A novel framework using LLMs as expert guides to enhance learning of Alzheimer's disease progression from irregular longitudinal data, improving prediction accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current methods oversimplify brain connectivity as a single-modality substrate for disease spread and face identifiability issues when learning graphs data-driven without constraints.

Method: Leverages LLMs' ability to synthesize multi-modal relationships to simultaneously optimize long-term disease trajectories from irregular data and a biologically-constrained graph structure of regional interactions.

Result: Demonstrated on tau-PET data from an AD cohort, the framework shows superior prediction accuracy and interpretability, revealing additional disease-driving factors beyond conventional connectivity.

Conclusion: The LLM-guided framework effectively addresses limitations of current methods by providing better identifiability and capturing complex multi-modal interactions in neurodegenerative disease progression.

Abstract: Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.

</details>


### [29] [Multi-Agent Legal Verifier Systems for Data Transfer Planning](https://arxiv.org/abs/2511.10925)
*Ha-Thanh Nguyen,Wachara Fungwacharakorn,Ken Satoh*

Main category: cs.AI

TL;DR: A multi-agent legal verifier system improves AI-driven data transfer compliance checking under Japan's APPI regulations, achieving 72% accuracy by using specialized agents for statutory interpretation, business context evaluation, and risk assessment.


<details>
  <summary>Details</summary>
Motivation: Legal compliance in AI-driven data transfer is critical under strict privacy regulations like Japan's APPI, requiring improved automated verification methods.

Method: Proposes a multi-agent system with specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol.

Result: Achieves 72% accuracy on 200 APPI Article 16 cases (21% higher than single-agent baseline), with 90% accuracy on clear compliance cases and perfect detection of clear violations.

Conclusion: Domain specialization and coordinated reasoning significantly improve legal AI performance, providing a scalable framework for trustworthy automated compliance verification.

Abstract: Legal compliance in AI-driven data transfer planning is becoming increasingly critical under stringent privacy regulations such as the Japanese Act on the Protection of Personal Information (APPI). We propose a multi-agent legal verifier that decomposes compliance checking into specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol. Evaluated on a stratified dataset of 200 Amended APPI Article 16 cases with clearly defined ground truth labels and multiple performance metrics, the system achieves 72% accuracy, which is 21 percentage points higher than a single-agent baseline, including 90% accuracy on clear compliance cases (vs. 16% for the baseline) while maintaining perfect detection of clear violations. While challenges remain in ambiguous scenarios, these results show that domain specialization and coordinated reasoning can meaningfully improve legal AI performance, providing a scalable and regulation-aware framework for trustworthy and interpretable automated compliance verification.

</details>


### [30] [Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints](https://arxiv.org/abs/2511.10952)
*Steven J. Jones,Robert E. Wray,John E. Laird*

Main category: cs.AI

TL;DR: Paper proposes requirements for AI agent decision-making when dealing with novel scenarios where no available actions fully satisfy all constraints. It explores how agents need to construct, evaluate, and justify new courses of action using contextual knowledge beyond trained policies.


<details>
  <summary>Details</summary>
Motivation: Autonomous AI systems inevitably encounter scenarios not covered by their training, requiring them to make decisions that align with human values and constraints when no predefined action is fully satisfactory.

Method: Through analysis and empirical case studies, examines how agents need to integrate normative (rules/laws), pragmatic (practical feasibility), and situational understanding to make robust decisions.

Result: Identifies specific types of knowledge requirements for agents to construct and evaluate novel courses of action that remain aligned with human expectations in complex environments.

Conclusion: Agents must go beyond trained policies and develop capabilities for contextual reasoning that integrates multiple knowledge types to make value-aligned decisions in novel, constrained scenarios.

Abstract: Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual "knowledge" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.

</details>


### [31] [Faster Symmetry Breaking Constraints for Abstract Structures](https://arxiv.org/abs/2511.11029)
*Özgür Akgün,Mun See Chang,Ian P. Gent,Christopher Jefferson*

Main category: cs.AI

TL;DR: Novel incomplete symmetry-breaking method for abstract structures using representation-aware constraints, showing improved performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Symmetry breaking is crucial for constraint solving efficiency but performs poorly when applied directly to abstract structures due to complex constraints.

Method: Representation-aware symmetry breaking that exploits how abstract structures are represented in solvers, specifically targeting indistinguishable objects.

Result: Method is faster than previous approaches from Akgün et al. (2025) for breaking symmetries of abstract structures.

Conclusion: Representation-aware symmetry breaking provides practical improvements for handling abstract structures in constraint programming.

Abstract: In constraint programming and related paradigms, a modeller specifies their problem in a modelling language for a solver to search and return its solution(s). Using high-level modelling languages such as Essence, a modeller may express their problems in terms of abstract structures. These are structures not natively supported by the solvers, and so they have to be transformed into or represented as other structures before solving. For example, nested sets are abstract structures, and they can be represented as matrices in constraint solvers. Many problems contain symmetries and one very common and highly successful technique used in constraint programming is to "break" symmetries, to avoid searching for symmetric solutions. This can speed up the solving process by many orders of magnitude. Most of these symmetry-breaking techniques involve placing some kind of ordering for the variables of the problem, and picking a particular member under the symmetries, usually the smallest. Unfortunately, applying this technique to abstract variables produces a very large number of complex constraints that perform poorly in practice. In this paper, we demonstrate a new incomplete method of breaking the symmetries of abstract structures by better exploiting their representations. We apply the method in breaking the symmetries arising from indistinguishable objects, a commonly occurring type of symmetry, and show that our method is faster than the previous methods proposed in (Akgün et al. 2025).

</details>


### [32] [Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?](https://arxiv.org/abs/2511.11040)
*Qian Zhang,Yan Zheng,Jinyi Liu,Hebin Liang,Lanjun Wang*

Main category: cs.AI

TL;DR: Study introduces 'Truth Last' role allocation strategy that improves Multi-Agent Debate performance by 22%, and proposes MADC strategy for practical applications where truth is unknown.


<details>
  <summary>Details</summary>
Motivation: Role allocation strategies in Multi-Agent Debate are understudied despite their potential to enhance LLM reasoning abilities.

Method: Proposes 'Truth Last' strategy and MADC (Multi-Agent Debate Consistency) with path consistency scoring to simulate truth-teller roles.

Result: MADC demonstrated advanced performance across 9 LLM models, overcoming MAD's performance bottlenecks.

Conclusion: MADC provides crucial pathway for improvements in LLM agent scaling, with Truth Last strategy showing 22% improvement in reasoning tasks.

Abstract: Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, "Truth Last", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.

</details>


### [33] [Autonomous Vehicle Path Planning by Searching With Differentiable Simulation](https://arxiv.org/abs/2511.11043)
*Asen Nachkov,Jan-Nico Zaech,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: DiffSim (DSS) is differentiable simulation framework using Waymax simulator as predictor and critic for autonomous driving planning, showing superior performance through gradient-based action optimization.


<details>
  <summary>Details</summary>
Motivation: Planning is crucial for safe autonomous driving, but challenging when all components need to be learned. Current methods struggle with accurate state prediction and effective search.

Method: Leverages differentiable simulator Waymax for both state prediction and critic evaluation. Uses gradient descent over imagined trajectories and combines planning gradients with stochastic search.

Result: DSS significantly outperforms sequence prediction, imitation learning, model-free RL, and other planning methods in tracking and path planning accuracy.

Conclusion: Differentiable simulation combined with gradient-based optimization provides an effective planning framework for autonomous driving, enabling safer and more accurate navigation in complex scenarios.

Abstract: Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.

</details>


### [34] [ARCTraj: A Dataset and Benchmark of Human Reasoning Trajectories for Abstract Problem Solving](https://arxiv.org/abs/2511.11079)
*Sejin Kim,Hayan Choi,Seokki Lee,Sundong Kim*

Main category: cs.AI

TL;DR: ARCTraj introduces a dataset and framework for modeling human reasoning steps in visual tasks from the Abstraction and Reasoning Corpus (ARC), capturing temporal action sequences to reveal intermediate reasoning processes.


<details>
  <summary>Details</summary>
Motivation: Existing ARC approaches rely on static input-output pairs, which fail to capture how reasoning unfolds over time. ARCTraj addresses this gap by recording human reasoning trajectories.

Method: Uses O2ARC web interface to collect ~10,000 temporal trajectories with object-level actions from 400 ARC training tasks. Defines a unified pipeline with data collection, action abstraction, MDP formulation, and integration with RL/generative models.

Result: Dataset reveals patterns in spatial selection, color attribution, and strategic convergence. Enables applications with PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers.

Conclusion: ARCTraj provides a structured foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence research.

Abstract: We present ARCTraj, a dataset and methodological framework for modeling human reasoning through complex visual tasks in the Abstraction and Reasoning Corpus (ARC). While ARC has inspired extensive research on abstract reasoning, most existing approaches rely on static input--output supervision, which limits insight into how reasoning unfolds over time. ARCTraj addresses this gap by recording temporally ordered, object-level actions that capture how humans iteratively transform inputs into outputs, revealing intermediate reasoning steps that conventional datasets overlook. Collected via the O2ARC web interface, it contains around 10,000 trajectories annotated with task identifiers, timestamps, and success labels across 400 training tasks from the ARC-AGI-1 benchmark. It further defines a unified reasoning pipeline encompassing data collection, action abstraction, Markov decision process (MDP) formulation, and downstream learning, enabling integration with reinforcement learning, generative modeling, and sequence modeling methods such as PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers. Analyses of spatial selection, color attribution, and strategic convergence highlight the structure and diversity of human reasoning. Together, these contributions position ARCTraj as a structured and interpretable foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence.

</details>


### [35] [Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)](https://arxiv.org/abs/2511.11095)
*Dillon Z. Chen,Till Hofmann,Toryn Q. Klassen,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 提出一种新颖的广义规划方法，通过目标回归和规则提升技术从训练问题中学习通用的Condition→Actions规则，证明该方法在合成成本、规划覆盖率和解决方案质量方面优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决广义规划问题，即合成能够解决相关规划问题族的程序

Method: 为每个训练问题按顺序计算最优计划，进行目标回归，并将输出提升为一阶条件-动作规则

Result: 在经典和数值规划领域的实验中，在合成成本、规划覆盖率和解决方案质量方面显著优于最先进的规划器

Conclusion: 该方法能够学习有效的广义计划和搜索空间修剪公理，具有良好的理论保证和实际性能

Abstract: Generalised planning (GP) refers to the task of synthesising programs that solve families of related planning problems. We introduce a novel, yet simple method for GP: given a set of training problems, for each problem, compute an optimal plan for each goal atom in some order, perform goal regression on the resulting plans, and lift the corresponding outputs to obtain a set of first-order $\textit{Condition} \rightarrow \textit{Actions}$ rules. The rules collectively constitute a generalised plan that can be executed as is or alternatively be used to prune the planning search space. We formalise and prove the conditions under which our method is guaranteed to learn valid generalised plans and state space pruning axioms for search. Experiments demonstrate significant improvements over state-of-the-art (generalised) planners with respect to the 3 metrics of synthesis cost, planning coverage, and solution quality on various classical and numeric planning domains.

</details>


### [36] [GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models](https://arxiv.org/abs/2511.11134)
*Jingxuan Wei,Caijun Jia,Xi Bai,Xinglong Xu,Siyuan Li,Linzhuang Sun,Bihui Yu,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: GGBench is a new benchmark designed to evaluate multimodal models' geometric generative reasoning capabilities through language-guided geometric construction tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to measure the integrated cognitive process of generative reasoning, focusing only on discriminative understanding or unconstrained image generation separately.

Method: Using geometric construction as a testbed since it inherently requires fusion of language comprehension and precise visual generation. The benchmark provides systematic diagnostics for multimodal generative reasoning.

Result: The paper introduces GGBench as a comprehensive framework to evaluate models' ability to understand, reason, and actively construct solutions in geometric contexts.

Conclusion: GGBench sets a more rigorous standard for evaluating the next generation of intelligent systems by addressing the critical gap in multimodal generative reasoning evaluation.

Abstract: The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.

</details>


### [37] [STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models](https://arxiv.org/abs/2511.11233)
*Huajian Zhang,Mingyue Cheng,Yucong Luo,Xiaoyu Tao*

Main category: cs.AI

TL;DR: STaR框架通过慢思考能力和不确定性感知推理，提升LLMs在表格推理任务中的性能和稳定性，使用两阶段难度感知强化学习和轨迹级不确定性量化方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs表格推理方法存在两个关键局限：(i) 推理过程缺乏人类认知的深度和迭代精炼特性；(ii) 推理过程不稳定，影响下游应用的可靠性。

Method: STaR采用两阶段难度感知强化学习(DRL)，从简单到复杂的查询逐步学习，并使用复合奖励。推理时通过整合token级置信度和答案一致性进行轨迹级不确定性量化。

Result: 在基准测试中取得优异性能，提升推理稳定性，并在领域外数据集上表现出强泛化能力。

Conclusion: STaR作为基于认知启发的表格推理解决方案显示出可靠性和潜力。

Abstract: Table reasoning with the large language models (LLMs) is a fundamental path toward building intelligent systems that can understand and analyze over structured data. While recent progress has shown promising results, they still suffer from two key limitations: (i) the reasoning processes lack the depth and iterative refinement characteristic of human cognition; and (ii) the reasoning processes exhibit instability, which compromises their reliability in downstream applications. In this work, we present STaR (slow-thinking for table reasoning), a new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference. During training, STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths. Extensive experiments on benchmarks demonstrate that STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.

</details>


### [38] [A Workflow for Full Traceability of AI Decisions](https://arxiv.org/abs/2511.11275)
*Julius Wenzel,Syeda Umaima Alam,Andreas Schmidt,Hanwei Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: The paper proposes a workflow using confidential computing to create tamper-proof documentation of AI decision components, demonstrated through a mushroom classification app.


<details>
  <summary>Details</summary>
Motivation: AI systems lack proper documentation traceability for decisions, hindering accountability when decisions cause harm or violate laws.

Method: Expands the DBOM concept into a workflow that enforces documentation of all training and inference components using confidential computing technology.

Result: Development of a functioning workflow that generates verifiable, exhaustive traces of AI decisions, demonstrated with a mushroom classification application.

Conclusion: The approach provides practical traceability for AI decisions, enabling reconstruction of responsibility chains that could stand up in legal proceedings.

Abstract: An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.
  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.

</details>


### [39] [Can You Tell the Difference? Contrastive Explanations for ABox Entailments](https://arxiv.org/abs/2511.11281)
*Patrick Koopmann,Yasir Mahmood,Axel-Cyrille Ngonga Ngomo,Balram Tiwari*

Main category: cs.AI

TL;DR: This paper introduces contrastive ABox explanations that explain why 'a' is an instance of concept C while 'b' is not, focusing on commonalities and differences between instances.


<details>
  <summary>Details</summary>
Motivation: Existing approaches explain positive entailments (why C(a) is entailed) or missing entailments (why C(b) is not entailed) separately, but contrastive explanations consider both simultaneously to highlight relevant differences.

Method: Developed a formal notion of contrastive explanations for ABox reasoning with description logic ontologies, analyzed computational complexity for different variants under optimality criteria, and implemented a method for computing one variant.

Result: The computational complexity was analyzed for different description logics, and a method was implemented and evaluated on generated problems for realistic knowledge bases.

Conclusion: Contrastive ABox explanations provide a unified framework for explaining both positive and negative entailments by focusing on relevant similarities and differences between instances.

Abstract: We introduce the notion of contrastive ABox explanations to answer questions of the type "Why is a an instance of C, but b is not?". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.

</details>


### [40] [EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment](https://arxiv.org/abs/2511.11301)
*Ruoxi Cheng,Haoxuan Ma,Teng Ma,Hongyi Zhang*

Main category: cs.AI

TL;DR: EcoAlign is an inference-time framework that improves LVLM alignment efficiency by treating models as boundedly rational agents. It uses forward-looking search with dynamic safety-utility-cost balancing to prevent jailbreak attacks while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Current LVLM alignment methods face trade-offs between safety, utility, and costs, with process-blindness allowing harmful reasoning to be disguised as benign. This wastes computational budget on unsafe deliberation.

Method: Incrementally expands thought graphs and scores actions using a forward-looking function that dynamically weighs expected safety, utility, and cost against remaining budget. Enforces path safety via weakest-link principle to prevent deception.

Result: Extensive experiments on 3 closed-source and 2 open-source models across 6 datasets show EcoAlign matches or surpasses state-of-the-art safety and utility at lower computational cost.

Conclusion: EcoAlign provides a principled, economical pathway to robust LVLM alignment by reframing alignment as economically rational search rather than just a safety challenge.

Abstract: Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.

</details>


### [41] [RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms](https://arxiv.org/abs/2511.11323)
*Yitian Kou,Yihe Gu,Chen Zhou,DanDan Zhu,Shuguang Kuai*

Main category: cs.AI

TL;DR: RLSLM是一种融合强化学习和基于规则的社会运动模型的混合框架，用于实现社交感知导航，在用户体验方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：基于规则的方法缺乏泛化性，数据驱动方法效率低且不透明。需要结合两者的优势来解决人群环境中的舒适导航问题。

Method: 提出RLSLM框架，将基于实验验证的社会运动模型集成到强化学习的奖励函数中，生成方向敏感的社交舒适场，共同优化机械能和社交舒适度。

Result: 通过VR交互实验证明RLSLM在用户体验上优于最先进的基于规则模型，消融分析显示其相比传统数据驱动方法具有更好的可解释性。

Conclusion: 该工作提出了一种可扩展的、以人为中心的方法论，有效整合认知科学和机器学习，可用于现实世界的社交导航场景。

Abstract: Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.

</details>


### [42] [KarmaTS: A Universal Simulation Platform for Multivariate Time Series with Functional Causal Dynamics](https://arxiv.org/abs/2511.11357)
*Haixin Li,Yanke Li,Diego Paez-Granados*

Main category: cs.AI

TL;DR: KarmaTS is an interactive framework for creating executable spatiotemporal causal models to generate synthetic multivariate time series data.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of limited access to physiological data by enabling generation of synthetic MTS with known causal dynamics.

Method: Combines expert knowledge and algorithmic proposals in a human-in-the-loop workflow to construct discrete-time structural causal processes.

Result: Generates synthetic MTS supporting causal interventions and distribution shifts, handling mixed variable types and modular edge functionals.

Conclusion: Enables flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation.

Abstract: We introduce KarmaTS, an interactive framework for constructing lag-indexed, executable spatiotemporal causal graphical models for multivariate time series (MTS) simulation. Motivated by the challenge of access-restricted physiological data, KarmaTS generates synthetic MTS with known causal dynamics and augments real-world datasets with expert knowledge. The system constructs a discrete-time structural causal process (DSCP) by combining expert knowledge and algorithmic proposals in a mixed-initiative, human-in-the-loop workflow. The resulting DSCP supports simulation and causal interventions, including those under user-specified distribution shifts. KarmaTS handles mixed variable types, contemporaneous and lagged edges, and modular edge functionals ranging from parameterizable templates to neural network models. Together, these features enable flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation.

</details>


### [43] [MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism](https://arxiv.org/abs/2511.11373)
*Shulin Liu,Dong Du,Tao Yang,Yang Li,Boyu Qiu*

Main category: cs.AI

TL;DR: MarsRL is a reinforcement learning framework with agentic pipeline parallelism that improves multi-agent reasoning in open-source LLMs by jointly optimizing all agents. It achieves significant performance gains on math reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent reasoning systems work well for closed-source models but struggle with open-source LLMs due to weak critic and correction capabilities, limiting their generalization.

Method: Proposes MarsRL framework with agent-specific reward mechanisms to reduce noise and pipeline-inspired training for efficient long trajectory handling. Applied to Qwen3-30B model.

Result: Improved AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, surpassing even larger models like Qwen3-235B.

Conclusion: MarsRL demonstrates strong potential to advance multi-agent reasoning systems and expand their applicability across diverse reasoning tasks.

Abstract: Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.

</details>


### [44] [Robust and Efficient Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.11393)
*Zejiao Liu,Yi Li,Jiali Wang,Junqi Tu,Yitian Hong,Fangfei Li,Yang Liu,Toshiharu Sugawara,Yang Tang*

Main category: cs.AI

TL;DR: 这篇论文综述了多智能体强化学习（MARL）中在现实约束下的鲁棒高效通信策略，包括消息扰动、传输延迟和带宽限制，并探讨了在自动驾驶、分布式SLAM和联邦学习中的应用及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法通常假设通信是瞬时、可靠且带宽无限的，但这些假设在现实部署中很少成立，因此需要研究在真实约束下的通信策略。

Method: 系统回顾了近期在MARL中针对消息扰动、传输延迟和有限带宽的鲁棒高效通信策略的进展。

Result: 论文聚焦于三个应用领域：协作自动驾驶、分布式同步定位与地图构建（SLAM）以及联邦学习，分析了通信在这些场景中的关键挑战。

Conclusion: 识别了关键开放挑战和未来研究方向，提倡一种统一的方法，共同设计通信、学习和鲁棒性，以弥合理论MARL模型与实际实现之间的差距。

Abstract: Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.

</details>


### [45] [CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction](https://arxiv.org/abs/2511.11423)
*Cong-Tinh Dao,Nguyen Minh Thao Phan,Jun-En Ding,Chenwei Wu,David Restrepo,Dongsheng Luo,Fanyi Zhao,Chun-Chieh Liao,Wen-Chih Peng,Chi-Te Wang,Pei-Fu Chen,Ling Chen,Xinglong Ju,Feng Liu,Fang-Ming Hung*

Main category: cs.AI

TL;DR: CURENet模型整合电子健康记录中的多模态数据（临床笔记、化验数据和时序数据），利用LLM和Transformer编码器，在慢性病预测中达到94%以上准确率


<details>
  <summary>Details</summary>
Motivation: 传统预测模型未能充分利用多模态数据间的交互、冗余和时序模式，需要开发能整合多种数据类型的综合模型来改善临床决策

Method: 提出CURENet多模态模型，使用LLM处理临床文本和化验数据，Transformer编码器处理纵向时序就诊数据，捕捉不同临床数据间的复杂交互

Result: 在MIMIC-III和FEMH数据集上评估，在多标签框架下对前10种慢性病预测准确率超过94%

Conclusion: 多模态EHR整合具有增强临床决策和改善患者预后的潜力，CURENet展示了这种方法的有效性

Abstract: Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.

</details>


### [46] [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)
*Adam Stein,Matthew Trager,Benjamin Bowman,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: EGuR is an AI system that dynamically generates tailored problem-solving strategies at inference time using accumulated experience, achieving significant performance improvements and cost reductions.


<details>
  <summary>Details</summary>
Motivation: Existing AI systems lack the ability to adapt their problem-solving approaches flexibly after training, being limited to modifying textual inputs or requiring offline optimization.

Method: EGuR uses an LLM-based meta-strategy with two components: a Guide that generates candidate strategies based on current problems and past experiences, and a Consolidator that integrates execution feedback.

Result: EGuR achieved up to 14% accuracy improvements over baselines and 111x computational cost reduction across five challenging benchmarks, with performance improving with experience.

Conclusion: EGuR enables dynamic adaptation of complete computational strategies at inference time, overcoming limitations of existing systems while improving both performance and efficiency.

Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

</details>


### [47] [Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping](https://arxiv.org/abs/2511.11551)
*Dena Mujtaba,Brian Hu,Anthony Hoogs,Arslan Basharat*

Main category: cs.AI

TL;DR: A test-time alignment technique using model-guided policy shaping to control AI agent behavior without retraining, evaluated on ethical decision-making in text-based games.


<details>
  <summary>Details</summary>
Motivation: AI agents may adopt harmful behaviors when maximizing rewards, creating alignment challenges, especially for pre-trained agents where retraining is costly.

Method: Policy shaping via scenario-action attribute classifiers applied at test time to ensure ethical alignment in diverse RL environments.

Result: Effective mitigation of unethical behavior across environments and attributes, outperforming training-time methods and general-purpose agents.

Conclusion: Test-time policy shaping offers a scalable solution for AI alignment without requiring costly retraining.

Abstract: The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [48] [Peer Code Review in Research Software Development: The Research Software Engineer Perspective](https://arxiv.org/abs/2511.10781)
*Md Ariful Islam Malik,Jeffrey C. Carver,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本研究调查了研究软件工程师对同行代码审查的看法，发现尽管代码审查对研究软件质量很重要，但RSEs面临独特挑战，需要结构化流程和更好工具支持以提高采用率。


<details>
  <summary>Details</summary>
Motivation: 研究软件对科学发现至关重要，但不断变化的需求和遗留依赖关系影响了软件质量。虽然代码审查能提高质量，但其在研究软件工程师中的采用情况尚未被充分探索。

Method: 通过调查问卷收集了61名研究软件工程师的反馈，调查设计基于先前研究以便比较分析，并包含针对RSEs的特定问题。

Result: 调查结果显示RSEs在代码审查方面面临的挑战和实践与更广泛的开发者群体有所不同，发现了独特的见解。

Conclusion: 同行代码审查对研究软件的质量、可维护性和可靠性至关重要。通过结构化流程、改进工具和针对性培训解决RSEs面临的独特挑战，可以提升代码审查在研究软件开发中的采用和效果。

Abstract: Background: Research software is crucial for enabling research discoveries and supporting data analysis, simulation, and interpretation across domains. However, evolving requirements, complex inputs, and legacy dependencies hinder the software quality and maintainability. While peer code review can improve software quality, its adoption by research software engineers (RSEs) remains unexplored. Aims: This study explores RSE perspectives on peer code review, focusing on their practices, challenges, and potential improvements. Building on prior work, it aims to uncover how RSEs insights differ from those of other research software developers and identify factors that can enhance code review adoption in this domain. Method: We surveyed RSEs to gather insights into their perspectives on peer code review. The survey design aligned with previous research to enable comparative analysis while including additional questions tailored to RSEs. Results: We received 61 valid responses from the survey. The findings align with prior research while uncovering unique insights about the challenges and practices of RSEs compared to broader developer groups. Conclusions: Peer code review is vital in improving research software's quality, maintainability, and reliability. Despite the unique challenges RSEs face, addressing these through structured processes, improved tools, and targeted training can enhance peer review adoption and effectiveness in research software development.

</details>


### [49] [Towards a Human-in-the-Loop Framework for Reliable Patch Evaluation Using an LLM-as-a-Judge](https://arxiv.org/abs/2511.10865)
*Sherry Shi,Renyao Wei,Michele Tufano,José Cambronero,Runxiang Cheng,Franjo Ivančić,Pat Rondon*

Main category: cs.SE

TL;DR: 提出一种基于LLM和人工循环的自动化程序修复补丁有效性评估方法，通过生成评估标准并人工审核改进，实现高效且可靠的补丁验证。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试使用基于执行的评价方法（如单元测试通过率），无法准确捕捉补丁的真实有效性，且人工标注成本高昂。

Method: 采用人机协作流程：先用LLM生成针对每个漏洞的评估标准，经一次性人工审核和优化后，再用LLM基于优化后的标准判断补丁有效性。

Result: 在人类评分者一致同意的补丁上，该方法与人类共识高度一致（Cohen's kappa 0.75），召回率0.94，精确率0.80；在包含分歧补丁的全数据集上，效果稍降（kappa 0.57，召回率0.93，精确率0.65），但仍有改进空间。

Conclusion: 该方法显著降低了补丁有效性评估的人工成本，且达成较高可靠性，为自动化程序修复的可靠评估提供了可行方向，未来可进一步优化分歧案例的处理。

Abstract: Reliable evaluation is crucial for advancing Automated Program Repair (APR), but prevailing benchmarks rely on execution-based evaluation methods (unit test pass@k), which fail to capture true patch validity. Determining validity can require costly manual annotation. To reduce this cost, we introduce a human-in-the-loop approach to LLM-based patch validity judgment. Inspired by the observation that human judgment is better aligned when using a shared rubric, we first employ an LLM to generate a per-bug rubric, followed by a one-time human review and optional refinement to this rubric, and then employ an LLM to judge patches using the refined rubric. We apply this approach to assign binary validity labels to patches for issues found by Google sanitizer tools. Our results show that this approach yields substantial agreement with human consensus (Cohen's kappa 0.75), high recall (0.94) and high precision (0.80), when considering patches that have unanimous agreement from 3 human raters on the validity labels. On the full dataset including patches where human raters disagree, we find this approach can still be further improved (Cohen's kappa 0.57, recall 0.93, precision 0.65) and identify possible future directions.

</details>


### [50] [Architecting software monitors for control-flow anomaly detection through large language models and conformance checking](https://arxiv.org/abs/2511.10876)
*Francesco Vitale,Francesco Flammini,Mauro Caporuscio,Nicola Mazzocca*

Main category: cs.SE

TL;DR: This paper presents a methodology using LLMs for automated source-code instrumentation to detect control-flow anomalies through conformance checking, validated on an ERTMS/ETCS railway case study.


<details>
  <summary>Details</summary>
Motivation: Increasing complexity of computer systems makes dependability challenging; run-time behavior can deviate from design-time validation due to unknown unknowns, requiring monitoring.

Method: Leverage LLMs to link design models and code for automated instrumentation, generating event logs analyzed via conformance checking for anomaly detection.

Result: Achieved 84.775% control-flow coverage and peak performance of 96.610% F1-score and 93.515% AUC in anomaly detection on ERTMS/ETCS case study.

Conclusion: Domain-specific knowledge guiding LLMs enables reliable software logs and effective control-flow anomaly detection via conformance checking.

Abstract: Context: Ensuring high levels of dependability in modern computer-based systems has become increasingly challenging due to their complexity. Although systems are validated at design time, their behavior can be different at run-time, possibly showing control-flow anomalies due to "unknown unknowns".
  Objective: We aim to detect control-flow anomalies through software monitoring, which verifies run-time behavior by logging software execution and detecting deviations from expected control flow.
  Methods: We propose a methodology to develop software monitors for control-flow anomaly detection through Large Language Models (LLMs) and conformance checking. The methodology builds on existing software development practices to maintain traditional V&V while providing an additional level of robustness and trustworthiness. It leverages LLMs to link design-time models and implementation code, automating source-code instrumentation. The resulting event logs are analyzed via conformance checking, an explainable and effective technique for control-flow anomaly detection.
  Results: We test the methodology on a case-study scenario from the European Railway Traffic Management System / European Train Control System (ERTMS/ETCS), which is a railway standard for modern interoperable railways. The results obtained from the ERTMS/ETCS case study demonstrate that LLM-based source-code instrumentation can achieve up to 84.775% control-flow coverage of the reference design-time process model, while the subsequent conformance checking-based anomaly detection reaches a peak performance of 96.610% F1-score and 93.515% AUC.
  Conclusion: Incorporating domain-specific knowledge to guide LLMs in source-code instrumentation significantly allowed obtaining reliable and quality software logs and enabled effective control-flow anomaly detection through conformance checking.

</details>


### [51] [Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs](https://arxiv.org/abs/2511.11125)
*Salim Fares,Steffen Herbold*

Main category: cs.SE

TL;DR: 研究表明少样本提示方法可以有效解决工业过程自动化领域中专业编程语言的LLM应用问题，无需大量训练投入即可实现本地部署的数据安全保护


<details>
  <summary>Details</summary>
Motivation: 当前LLM在软件工程中的应用研究主要集中在通用编程语言，对于工业过程自动化领域中专有的高度专业化语言的应用潜力尚未充分探索

Method: 采用少样本提示方法，研究企业如何在无需投入大量资源训练特定领域语言模型的情况下独立解决问题

Result: 证明少样本提示方法足以解决在LLM支持较差的专业语言中的简单问题

Conclusion: 少样本提示方法为工业自动化领域提供了一种可行的LLM应用方案，既能有效解决问题，又能确保敏感公司数据的本地化安全保护

Abstract: How to best use Large Language Models (LLMs) for software engineering is covered in many publications in recent years. However, most of this work focuses on widely-used general purpose programming languages. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, is still underexplored. Within this paper, we study enterprises can achieve on their own without investing large amounts of effort into the training of models specific to the domain-specific languages that are used. We show that few-shot prompting approaches are sufficient to solve simple problems in a language that is otherwise not well-supported by an LLM and that is possible on-premise, thereby ensuring the protection of sensitive company data.

</details>


### [52] [SQuaD: The Software Quality Dataset](https://arxiv.org/abs/2511.11265)
*Mikel Robredo,Matteo Esposito,Davide Taibi,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: SQuaD is a comprehensive software quality dataset integrating 700+ metrics from 9 tools across 450 open-source projects, enabling large-scale research on software maintainability, technical debt, and evolution.


<details>
  <summary>Details</summary>
Motivation: Existing software quality datasets are limited in scope, focusing on singular aspects like code smells or refactoring, which restricts comprehensive multi-dimensional analysis across time and quality domains.

Method: Integrated 9 static analysis tools (SonarQube, CodeScene, PMD, etc.) to extract metrics from 450 mature open-source projects across multiple ecosystems, covering 63,586 project releases with version control and vulnerability data.

Result: Created a unified dataset with method, class, file, and project-level metrics, plus process metrics that enhance defect prediction, publicly available on ZENODO.

Conclusion: SQuaD enables unprecedented empirical research on software quality at scale and outlines directions for automated updates and cross-project quality modeling.

Abstract: Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).

</details>


### [53] [SCRUTINEER: Detecting Logic-Level Usage Violations of Reusable Components in Smart Contracts](https://arxiv.org/abs/2511.11411)
*Xingshuang Lin,Binbin Zhao,Jinwen Wang,Qinge Xie,Xibin Zhao,Shouling Ji*

Main category: cs.SE

TL;DR: SCRUTINEER is an automated system that detects logic-level usage violations in Smart Contract Reusable Components by combining composite feature extraction, LLM-powered knowledge construction, and retrieval-augmented generation with conflict checking.


<details>
  <summary>Details</summary>
Motivation: Logic-level usage violations in SCRs occur when components follow technical specifications but misalign with business logic, creating vulnerabilities that require deep semantic understanding to detect.

Method: Uses composite feature extraction, LLM-powered knowledge construction with domain tools, retrieval-augmented generation inspection, and similarity/snapshot-based conflict checking.

Result: Achieved 80.77% precision, 82.35% recall, and 81.55% F1-score in detecting logic-level SCR usage violations across 3 datasets.

Conclusion: SCRUTINEER effectively addresses the challenge of detecting subtle logic-level violations in smart contract components through its multi-stage analysis approach.

Abstract: Smart Contract Reusable Components(SCRs) play a vital role in accelerating the development of business-specific contracts by promoting modularity and code reuse. However, the risks associated with SCR usage violations have become a growing concern. One particular type of SCR usage violation, known as a logic-level usage violation, is becoming especially harmful. This violation occurs when the SCR adheres to its specified usage rules but fails to align with the specific business logic of the current context, leading to significant vulnerabilities. Detecting such violations necessitates a deep semantic understanding of the contract's business logic, including the ability to extract implicit usage patterns and analyze fine-grained logical behaviors. To address these challenges, we propose SCRUTINEER, the first automated and practical system for detecting logic-level usage violations of SCRs. First, we design a composite feature extraction approach that produces three complementary feature representations, supporting subsequent analysis. We then introduce a Large Language Model-powered knowledge construction framework, which leverages comprehension-oriented prompts and domain-specific tools to extract logic-level usage and build the SCR knowledge base. Next, we develop a Retrieval-Augmented Generation-driven inspector, which combines a rapid retrieval strategy with both comprehensive and targeted analysis to identify potentially insecure logic-level usages. Finally, we implement a logic-level usage violation analysis engine that integrates a similarity-based checker and a snapshot-based inference conflict checker to enable accurate and robust detection. We evaluate SCRUTINEER from multiple perspectives on 3 ground-truth datasets. The results show that SCRUTINEER achieves a precision of 80.77%, a recall of 82.35%, and an F1-score of 81.55% in detecting logic-level usage violations of SCRs.

</details>


### [54] [CertiA360: Enhance Compliance Agility in Aerospace Software Development](https://arxiv.org/abs/2511.11550)
*J. Antonio Dantas Macedo,Hugo Fernandes,J. Eduardo Ferreira Ribeiro*

Main category: cs.SE

TL;DR: 论文提出CertiA360工具，将敏捷方法应用于航空航天安全关键系统开发，解决DO-178C标准合规挑战，通过自动化变更管理和追踪提升效率


<details>
  <summary>Details</summary>
Motivation: 将敏捷方法的灵活性整合到需要严格合规的航空航天安全关键系统开发中，解决敏捷方法与DO-178C标准要求的冲突

Method: 设计与验证CertiA360工具，通过自动化需求成熟度管理和追踪变更，与行业专家合作确保实用性和有效性

Result: CertiA360能减少人工工作量，响应需求变更，同时确保DO-178C合规性，显示敏捷方法可在严格监管领域提高效率

Conclusion: 适当调整的敏捷方法不仅能与安全系统开发要求共存，还能在航空航天等高度监管领域增加效率

Abstract: Agile methods are characterised by iterative and incremental processes with a strong focus on flexibility and accommodating changing requirements based on either technical, regulatory, or stakeholder feedback. However, integrating Agile methods into safety-critical system development in the aerospace industry presents substantial challenges due to its strict compliance requirements, such as those outlined in the DO-178C standard. To achieve this vision, the flexibility of Agile must align with the rigorous certification guidelines, which emphasize documentation, traceability of requirements across different levels and disciplines, and comprehensive verification and validation (V&V) activities. The research work described in this paper proposes a way of using the strengths of the flexible nature of Agile methods to automate and manage change requests throughout the whole software development lifecycle, ensuring robust traceability, regulatory compliance and ultimately facilitating successful certification. This study proposes CertiA360, a tool designed to help teams improve requirement maturity, automate the changes in traceability, and align with the regulatory objectives. The tool was designed and validated in close collaboration with aerospace industry experts, using their feedback to ensure practical application and real-life effectiveness. The feedback collected demonstrated that the automation given by CertiA360 may reduce manual effort and allow response to changing requirements while ensuring compliance with DO-178C. While the tool is not yet qualified under DO-330 (Tool Qualification), findings suggest that when tailored appropriately, Agile methods can not only coexist with the requirements of safety-system development and certification in highly regulated domains like aerospace, but also add efficiency.

</details>
